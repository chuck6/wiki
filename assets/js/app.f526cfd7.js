(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var i,o,l=e[0],s=e[1],c=e[2],u=0,p=[];u<l.length;u++)o=l[u],Object.prototype.hasOwnProperty.call(r,o)&&r[o]&&p.push(r[o][0]),r[o]=0;for(i in s)Object.prototype.hasOwnProperty.call(s,i)&&(n[i]=s[i]);for(d&&d(e);p.length;)p.shift()();return a.push.apply(a,c||[]),t()}function t(){for(var n,e=0;e<a.length;e++){for(var t=a[e],i=!0,l=1;l<t.length;l++){var s=t[l];0!==r[s]&&(i=!1)}i&&(a.splice(e--,1),n=o(o.s=t[0]))}return n}var i={},r={1:0},a=[];function o(e){if(i[e])return i[e].exports;var t=i[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,o),t.l=!0,t.exports}o.e=function(n){var e=[],t=r[n];if(0!==t)if(t)e.push(t[2]);else{var i=new Promise((function(e,i){t=r[n]=[e,i]}));e.push(t[2]=i);var a,l=document.createElement("script");l.charset="utf-8",l.timeout=120,o.nc&&l.setAttribute("nonce",o.nc),l.src=function(n){return o.p+"assets/js/"+({}[n]||n)+"."+{2:"bde2a65f",3:"4bf9fd71",4:"4d1d832b",5:"4d291935",6:"9da572d6",7:"7c03cbe3",8:"29dd9da1",9:"ac5a0e3d",10:"a699be74",11:"55f56fc0",12:"a3cf84ef",13:"e86a976a",14:"fba01f52",15:"2c64e9a7",16:"c8c119e9",17:"b2bbeede",18:"f48c1dd3",19:"000746cf",20:"2c04fc4e",21:"2e263540",22:"47c72dfd",23:"c2d08a97",24:"aa4e6498",25:"ea7a6379",26:"c4d513a8",27:"12f62697",28:"043935a1",29:"993cbbde",30:"5eb66430",31:"71fe134e",32:"46b8693c",33:"4076c125",34:"5ec78ca2",35:"75bf0519",36:"0f276ce1",37:"fbf5575f",38:"349da7de",39:"dd92cadd"}[n]+".js"}(n);var s=new Error;a=function(e){l.onerror=l.onload=null,clearTimeout(c);var t=r[n];if(0!==t){if(t){var i=e&&("load"===e.type?"missing":e.type),a=e&&e.target&&e.target.src;s.message="Loading chunk "+n+" failed.\n("+i+": "+a+")",s.name="ChunkLoadError",s.type=i,s.request=a,t[1](s)}r[n]=void 0}};var c=setTimeout((function(){a({type:"timeout",target:l})}),12e4);l.onerror=l.onload=a,document.head.appendChild(l)}return Promise.all(e)},o.m=n,o.c=i,o.d=function(n,e,t){o.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},o.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},o.t=function(n,e){if(1&e&&(n=o(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(o.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var i in n)o.d(t,i,function(e){return n[e]}.bind(null,i));return t},o.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return o.d(e,"a",e),e},o.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},o.p="/wiki/",o.oe=function(n){throw console.error(n),n};var l=window.webpackJsonp=window.webpackJsonp||[],s=l.push.bind(l);l.push=e,l=l.slice();for(var c=0;c<l.length;c++)e(l[c]);var d=s;a.push([236,0]),t()}([function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(n,e,t){var i=t(0),r=t(38).f,a=t(27),o=t(14),l=t(112),s=t(119),c=t(105);n.exports=function(n,e){var t,d,u,p,m,h=n.target,f=n.global,g=n.stat;if(t=f?i:g?i[h]||l(h,{}):(i[h]||{}).prototype)for(d in e){if(p=e[d],u=n.noTargetGet?(m=r(t,d))&&m.value:t[d],!c(f?d:h+(g?".":"#")+d,n.forced)&&void 0!==u){if(typeof p==typeof u)continue;s(p,u)}(n.sham||u&&u.sham)&&a(p,"sham",!0),o(t,d,p,n)}}},function(n,e,t){var i=t(63),r=Function.prototype,a=r.bind,o=r.call,l=i&&a.bind(o,o);n.exports=i?function(n){return n&&l(n)}:function(n){return n&&function(){return o.apply(n,arguments)}}},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){var i=t(123),r=t(14),a=t(252);i||r(Object.prototype,"toString",a,{unsafe:!0})},function(n,e){n.exports=function(n){return"function"==typeof n}},function(n,e,t){var i=t(0),r=t(79),a=t(10),o=t(80),l=t(113),s=t(155),c=r("wks"),d=i.Symbol,u=d&&d.for,p=s?d:d&&d.withoutSetter||o;n.exports=function(n){if(!a(c,n)||!l&&"string"!=typeof c[n]){var e="Symbol."+n;l&&a(d,n)?c[n]=d[n]:c[n]=s&&u?u(e):p(e)}return c[n]}},function(n,e,t){var i=t(3);n.exports=!i((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e,t){var i=t(0),r=t(9),a=i.String,o=i.TypeError;n.exports=function(n){if(r(n))return n;throw o(a(n)+" is not an object")}},function(n,e,t){var i=t(5);n.exports=function(n){return"object"==typeof n?null!==n:i(n)}},function(n,e,t){var i=t(2),r=t(15),a=i({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return a(r(n),e)}},function(n,e,t){var i=t(63),r=Function.prototype.call;n.exports=i?r.bind(r):function(){return r.apply(r,arguments)}},function(n,e,t){var i=t(0),r=t(76),a=i.String;n.exports=function(n){if("Symbol"===r(n))throw TypeError("Cannot convert a Symbol value to a string");return a(n)}},function(n,e,t){var i=t(0),r=t(7),a=t(157),o=t(156),l=t(8),s=t(82),c=i.TypeError,d=Object.defineProperty,u=Object.getOwnPropertyDescriptor;e.f=r?o?function(n,e,t){if(l(n),e=s(e),l(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var i=u(n,e);i&&i.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:i.configurable,enumerable:"enumerable"in t?t.enumerable:i.enumerable,writable:!1})}return d(n,e,t)}:d:function(n,e,t){if(l(n),e=s(e),l(t),a)try{return d(n,e,t)}catch(n){}if("get"in t||"set"in t)throw c("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var i=t(0),r=t(5),a=t(10),o=t(27),l=t(112),s=t(87),c=t(39),d=t(66).CONFIGURABLE,u=c.get,p=c.enforce,m=String(String).split("String");(n.exports=function(n,e,t,s){var c,u=!!s&&!!s.unsafe,h=!!s&&!!s.enumerable,f=!!s&&!!s.noTargetGet,g=s&&void 0!==s.name?s.name:e;r(t)&&("Symbol("===String(g).slice(0,7)&&(g="["+String(g).replace(/^Symbol\(([^)]*)\)/,"$1")+"]"),(!a(t,"name")||d&&t.name!==g)&&o(t,"name",g),(c=p(t)).source||(c.source=m.join("string"==typeof g?g:""))),n!==i?(u?!f&&n[e]&&(h=!0):delete n[e],h?n[e]=t:o(n,e,t)):h?n[e]=t:l(e,t)})(Function.prototype,"toString",(function(){return r(this)&&u(this).source||s(this)}))},function(n,e,t){var i=t(0),r=t(18),a=i.Object;n.exports=function(n){return a(r(n))}},function(n,e,t){var i=t(0),r=t(5),a=function(n){return r(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?a(i[n]):i[n]&&i[n][e]}},function(n,e,t){"use strict";var i=t(1),r=t(93);i({target:"RegExp",proto:!0,forced:/./.exec!==r},{exec:r})},function(n,e,t){var i=t(0).TypeError;n.exports=function(n){if(null==n)throw i("Can't call method on "+n);return n}},function(n,e,t){var i=t(62),r=t(18);n.exports=function(n){return i(r(n))}},function(n,e,t){"use strict";function i(n,e,t,i,r,a,o,l){var s,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),i&&(c.functional=!0),a&&(c._scopeId="data-v-"+a),o?(s=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),r&&r.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(o)},c._ssrRegister=s):r&&(s=l?function(){r.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:r),s)if(c.functional){c._injectStyles=s;var d=c.render;c.render=function(n,e){return s.call(e),d(n,e)}}else{var u=c.beforeCreate;c.beforeCreate=u?[].concat(u,s):[s]}return{exports:n,options:c}}t.d(e,"a",(function(){return i}))},function(n,e,t){"use strict";var i=t(173).charAt,r=t(12),a=t(39),o=t(161),l=a.set,s=a.getterFor("String Iterator");o(String,"String",(function(n){l(this,{type:"String Iterator",string:r(n),index:0})}),(function(){var n,e=s(this),t=e.string,r=e.index;return r>=t.length?{value:void 0,done:!0}:(n=i(t,r),e.index+=n.length,{value:n,done:!1})}))},function(n,e,t){var i=t(52);n.exports=function(n){return i(n.length)}},function(n,e,t){"use strict";var i=t(1),r=t(57).filter;i({target:"Array",proto:!0,forced:!t(91)("filter")},{filter:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var i=t(0),r=t(174),a=t(175),o=t(144),l=t(27),s=t(6),c=s("iterator"),d=s("toStringTag"),u=o.values,p=function(n,e){if(n){if(n[c]!==u)try{l(n,c,u)}catch(e){n[c]=u}if(n[d]||l(n,d,e),r[e])for(var t in o)if(n[t]!==o[t])try{l(n,t,o[t])}catch(e){n[t]=o[t]}}};for(var m in r)p(i[m]&&i[m].prototype,m);p(a,"DOMTokenList")},function(n,e,t){var i=t(2),r=i({}.toString),a=i("".slice);n.exports=function(n){return a(r(n),8,-1)}},function(n,e){n.exports=!1},function(n,e,t){var i=t(7),r=t(13),a=t(48);n.exports=i?function(n,e,t){return r.f(n,e,a(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){"use strict";var i=t(1),r=t(181);i({target:"Array",proto:!0,forced:[].forEach!=r},{forEach:r})},function(n,e,t){var i=t(0),r=t(174),a=t(175),o=t(181),l=t(27),s=function(n){if(n&&n.forEach!==o)try{l(n,"forEach",o)}catch(e){n.forEach=o}};for(var c in r)r[c]&&s(i[c]&&i[c].prototype);s(a)},function(n,e,t){var i=t(14),r=t(267),a=Error.prototype;a.toString!==r&&i(a,"toString",r)},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var i=t(16);n.exports=i("navigator","userAgent")||""},function(n,e,t){var i=t(191),r="object"==typeof self&&self&&self.Object===Object&&self,a=i||r||Function("return this")();n.exports=a},function(n,e,t){var i,r=t(8),a=t(114),o=t(117),l=t(64),s=t(160),c=t(81),d=t(86),u=d("IE_PROTO"),p=function(){},m=function(n){return"<script>"+n+"<\/script>"},h=function(n){n.write(m("")),n.close();var e=n.parentWindow.Object;return n=null,e},f=function(){try{i=new ActiveXObject("htmlfile")}catch(n){}var n,e;f="undefined"!=typeof document?document.domain&&i?h(i):((e=c("iframe")).style.display="none",s.appendChild(e),e.src=String("javascript:"),(n=e.contentWindow.document).open(),n.write(m("document.F=Object")),n.close(),n.F):h(i);for(var t=o.length;t--;)delete f.prototype[o[t]];return f()};l[u]=!0,n.exports=Object.create||function(n,e){var t;return null!==n?(p.prototype=r(n),t=new p,p.prototype=null,t[u]=n):t=f(),void 0===e?t:a.f(t,e)}},function(n,e,t){var i=t(2);n.exports=i({}.isPrototypeOf)},function(n,e,t){var i=t(63),r=Function.prototype,a=r.apply,o=r.call;n.exports="object"==typeof Reflect&&Reflect.apply||(i?o.bind(a):function(){return o.apply(a,arguments)})},function(n,e,t){var i=t(0),r=t(5),a=t(84),o=i.TypeError;n.exports=function(n){if(r(n))return n;throw o(a(n)+" is not a function")}},function(n,e,t){var i=t(7),r=t(11),a=t(118),o=t(48),l=t(19),s=t(82),c=t(10),d=t(157),u=Object.getOwnPropertyDescriptor;e.f=i?u:function(n,e){if(n=l(n),e=s(e),d)try{return u(n,e)}catch(n){}if(c(n,e))return o(!r(a.f,n,e),n[e])}},function(n,e,t){var i,r,a,o=t(238),l=t(0),s=t(2),c=t(9),d=t(27),u=t(10),p=t(111),m=t(86),h=t(64),f=l.TypeError,g=l.WeakMap;if(o||p.state){var v=p.state||(p.state=new g),y=s(v.get),b=s(v.has),x=s(v.set);i=function(n,e){if(b(v,n))throw new f("Object already initialized");return e.facade=n,x(v,n,e),e},r=function(n){return y(v,n)||{}},a=function(n){return b(v,n)}}else{var _=m("state");h[_]=!0,i=function(n,e){if(u(n,_))throw new f("Object already initialized");return e.facade=n,d(n,_,e),e},r=function(n){return u(n,_)?n[_]:{}},a=function(n){return u(n,_)}}n.exports={set:i,get:r,has:a,enforce:function(n){return a(n)?r(n):i(n,{})},getterFor:function(n){return function(e){var t;if(!c(e)||(t=r(e)).type!==n)throw f("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){var i=t(1),r=t(0),a=t(36),o=t(263),l=r.WebAssembly,s=7!==Error("e",{cause:7}).cause,c=function(n,e){var t={};t[n]=o(n,e,s),i({global:!0,forced:s},t)},d=function(n,e){if(l&&l[n]){var t={};t[n]=o("WebAssembly."+n,e,s),i({target:"WebAssembly",stat:!0,forced:s},t)}};c("Error",(function(n){return function(e){return a(n,this,arguments)}})),c("EvalError",(function(n){return function(e){return a(n,this,arguments)}})),c("RangeError",(function(n){return function(e){return a(n,this,arguments)}})),c("ReferenceError",(function(n){return function(e){return a(n,this,arguments)}})),c("SyntaxError",(function(n){return function(e){return a(n,this,arguments)}})),c("TypeError",(function(n){return function(e){return a(n,this,arguments)}})),c("URIError",(function(n){return function(e){return a(n,this,arguments)}})),d("CompileError",(function(n){return function(e){return a(n,this,arguments)}})),d("LinkError",(function(n){return function(e){return a(n,this,arguments)}})),d("RuntimeError",(function(n){return function(e){return a(n,this,arguments)}}))},function(n,e,t){var i=t(287),r=t(290);n.exports=function(n,e){var t=r(n,e);return i(t)?t:void 0}},function(n,e,t){"use strict";var i=t(1),r=t(0),a=t(60),o=t(88),l=t(9),s=t(116),c=t(22),d=t(19),u=t(69),p=t(6),m=t(91),h=t(68),f=m("slice"),g=p("species"),v=r.Array,y=Math.max;i({target:"Array",proto:!0,forced:!f},{slice:function(n,e){var t,i,r,p=d(this),m=c(p),f=s(n,m),b=s(void 0===e?m:e,m);if(a(p)&&(t=p.constructor,(o(t)&&(t===v||a(t.prototype))||l(t)&&null===(t=t[g]))&&(t=void 0),t===v||void 0===t))return h(p,f,b);for(i=new(void 0===t?v:t)(y(b-f,0)),r=0;f<b;f++,r++)f in p&&u(i,r,p[f]);return i.length=r,i}})},function(n,e,t){"use strict";t.d(e,"e",(function(){return i})),t.d(e,"b",(function(){return a})),t.d(e,"j",(function(){return o})),t.d(e,"g",(function(){return s})),t.d(e,"h",(function(){return c})),t.d(e,"i",(function(){return d})),t.d(e,"c",(function(){return u})),t.d(e,"f",(function(){return p})),t.d(e,"l",(function(){return m})),t.d(e,"m",(function(){return h})),t.d(e,"d",(function(){return g})),t.d(e,"k",(function(){return v})),t.d(e,"n",(function(){return y})),t.d(e,"a",(function(){return x}));t(17),t(45),t(140),t(74),t(103),t(110),t(44),t(28),t(4),t(29),t(23),t(77),t(104),t(154),t(53),t(211),t(30),t(143);var i=/#.*$/,r=/\.(md|html)$/,a=/\/$/,o=/^[a-z]+:/i;function l(n){return decodeURI(n).replace(i,"").replace(r,"")}function s(n){return o.test(n)}function c(n){return/^mailto:/.test(n)}function d(n){return/^tel:/.test(n)}function u(n){if(s(n))return n;var e=n.match(i),t=e?e[0]:"",r=l(n);return a.test(r)?n:r+".html"+t}function p(n,e){var t=n.hash,r=function(n){var e=n.match(i);if(e)return e[0]}(e);return(!r||t===r)&&l(n.path)===l(e)}function m(n,e,t){if(s(e))return{type:"external",path:e};t&&(e=function(n,e,t){var i=n.charAt(0);if("/"===i)return n;if("?"===i||"#"===i)return e+n;var r=e.split("/");t&&r[r.length-1]||r.pop();for(var a=n.replace(/^\//,"").split("/"),o=0;o<a.length;o++){var l=a[o];".."===l?r.pop():"."!==l&&r.push(l)}""!==r[0]&&r.unshift("");return r.join("/")}(e,t));for(var i=l(e),r=0;r<n.length;r++)if(l(n[r].regularPath)===i)return Object.assign({},n[r],{type:"page",path:u(n[r].path)});return console.error('[vuepress] No matching page found for sidebar item "'.concat(e,'"')),{}}function h(n,e,t,i){var r=t.pages,a=t.themeConfig,o=i&&a.locales&&a.locales[i]||a;if("auto"===(n.frontmatter.sidebar||o.sidebar||a.sidebar))return f(n);var l=o.sidebar||a.sidebar;if(l){var s=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(var t in e)if(0===(i=n,/(\.html|\/)$/.test(i)?i:i+"/").indexOf(encodeURI(t)))return{base:t,config:e[t]};var i;return{}}(e,l),c=s.base,d=s.config;return"auto"===d?f(n):d?d.map((function(n){return function n(e,t,i){var r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1;if("string"==typeof e)return m(t,e,i);if(Array.isArray(e))return Object.assign(m(t,e[0],i),{title:e[1]});r>3&&console.error("[vuepress] detected a too deep nested sidebar group.");var a=e.children||[];return 0===a.length&&e.path?Object.assign(m(t,e.path,i),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:a.map((function(e){return n(e,t,i,r+1)})),collapsable:!1!==e.collapsable}}(n,r,c)})):[]}return[]}function f(n){var e=g(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map((function(e){return{type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}}))}]}function g(n){var e;return(n=n.map((function(n){return Object.assign({},n)}))).forEach((function(n){2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)})),n.filter((function(n){return 2===n.level}))}function v(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function y(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function b(n){var e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function x(n,e){return b(e)-b(n)}},function(n,e,t){"use strict";var i=t(1),r=t(57).map;i({target:"Array",proto:!0,forced:!t(91)("map")},{map:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var i=t(36),r=t(11),a=t(2),o=t(108),l=t(3),s=t(8),c=t(5),d=t(55),u=t(52),p=t(12),m=t(18),h=t(127),f=t(47),g=t(268),v=t(109),y=t(6)("replace"),b=Math.max,x=Math.min,_=a([].concat),k=a([].push),w=a("".indexOf),T=a("".slice),I="$0"==="a".replace(/./,"$0"),z=!!/./[y]&&""===/./[y]("a","$0");o("replace",(function(n,e,t){var a=z?"$":"$0";return[function(n,t){var i=m(this),a=null==n?void 0:f(n,y);return a?r(a,n,i,t):r(e,p(i),n,t)},function(n,r){var o=s(this),l=p(n);if("string"==typeof r&&-1===w(r,a)&&-1===w(r,"$<")){var m=t(e,o,l,r);if(m.done)return m.value}var f=c(r);f||(r=p(r));var y=o.global;if(y){var I=o.unicode;o.lastIndex=0}for(var z=[];;){var S=v(o,l);if(null===S)break;if(k(z,S),!y)break;""===p(S[0])&&(o.lastIndex=h(l,u(o.lastIndex),I))}for(var j,P="",E=0,q=0;q<z.length;q++){for(var O=p((S=z[q])[0]),A=b(x(d(S.index),l.length),0),$=[],C=1;C<S.length;C++)k($,void 0===(j=S[C])?j:String(j));var L=S.groups;if(f){var M=_([O],$,A,l);void 0!==L&&k(M,L);var D=p(i(r,void 0,M))}else D=g(O,l,A,$,L,r);A>=E&&(P+=T(l,E,A)+D,E=A+O.length)}return P+T(l,E)}]}),!!l((function(){var n=/./;return n.exec=function(){var n=[];return n.groups={a:"7"},n},"7"!=="".replace(n,"$<a>")}))||!I||z)},function(n,e,t){"use strict";var i=t(3);n.exports=function(n,e){var t=[][n];return!!t&&i((function(){t.call(null,e||function(){throw 1},1)}))}},function(n,e,t){var i=t(37);n.exports=function(n,e){var t=n[e];return null==t?void 0:i(t)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){"use strict";t.d(e,"a",(function(){return a}));t(75),t(70),t(23),t(4),t(379),t(28),t(29),t(176),t(380),t(99);function i(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function r(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(n);e&&(i=i.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,i)}return t}function a(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?r(Object(t),!0).forEach((function(e){i(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}},function(n,e,t){var i,r,a=t(0),o=t(32),l=a.process,s=a.Deno,c=l&&l.versions||s&&s.version,d=c&&c.v8;d&&(r=(i=d.split("."))[0]>0&&i[0]<4?1:+(i[0]+i[1])),!r&&o&&(!(i=o.match(/Edge\/(\d+)/))||i[1]>=74)&&(i=o.match(/Chrome\/(\d+)/))&&(r=+i[1]),n.exports=r},function(n,e,t){var i=t(55),r=Math.min;n.exports=function(n){return n>0?r(i(n),9007199254740991):0}},function(n,e,t){"use strict";var i=t(1),r=t(0),a=t(3),o=t(60),l=t(9),s=t(15),c=t(22),d=t(69),u=t(146),p=t(91),m=t(6),h=t(51),f=m("isConcatSpreadable"),g=r.TypeError,v=h>=51||!a((function(){var n=[];return n[f]=!1,n.concat()[0]!==n})),y=p("concat"),b=function(n){if(!l(n))return!1;var e=n[f];return void 0!==e?!!e:o(n)};i({target:"Array",proto:!0,forced:!v||!y},{concat:function(n){var e,t,i,r,a,o=s(this),l=u(o,0),p=0;for(e=-1,i=arguments.length;e<i;e++)if(b(a=-1===e?o:arguments[e])){if(p+(r=c(a))>9007199254740991)throw g("Maximum allowed index exceeded");for(t=0;t<r;t++,p++)t in a&&d(l,p,a[t])}else{if(p>=9007199254740991)throw g("Maximum allowed index exceeded");d(l,p++,a)}return l.length=p,l}})},function(n,e,t){var i=t(1),r=t(0),a=t(36),o=t(5),l=t(32),s=t(68),c=/MSIE .\./.test(l),d=r.Function,u=function(n){return function(e,t){var i=arguments.length>2,r=i?s(arguments,2):void 0;return n(i?function(){a(o(e)?e:d(e),this,r)}:e,t)}};i({global:!0,bind:!0,forced:c},{setTimeout:u(r.setTimeout),setInterval:u(r.setInterval)})},function(n,e){var t=Math.ceil,i=Math.floor;n.exports=function(n){var e=+n;return e!=e||0===e?0:(e>0?i:t)(e)}},function(n,e,t){var i=t(2),r=t(37),a=t(63),o=i(i.bind);n.exports=function(n,e){return r(n),void 0===e?n:a?o(n,e):function(){return n.apply(e,arguments)}}},function(n,e,t){var i=t(56),r=t(2),a=t(62),o=t(15),l=t(22),s=t(146),c=r([].push),d=function(n){var e=1==n,t=2==n,r=3==n,d=4==n,u=6==n,p=7==n,m=5==n||u;return function(h,f,g,v){for(var y,b,x=o(h),_=a(x),k=i(f,g),w=l(_),T=0,I=v||s,z=e?I(h,w):t||p?I(h,0):void 0;w>T;T++)if((m||T in _)&&(b=k(y=_[T],T,x),n))if(e)z[T]=b;else if(b)switch(n){case 3:return!0;case 5:return y;case 6:return T;case 2:c(z,y)}else switch(n){case 4:return!1;case 7:c(z,y)}return u?-1:r||d?d:z}};n.exports={forEach:d(0),map:d(1),filter:d(2),some:d(3),every:d(4),find:d(5),findIndex:d(6),filterReject:d(7)}},function(n,e,t){var i=t(159),r=t(117).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return i(n,r)}},function(n,e,t){var i=t(13).f,r=t(10),a=t(6)("toStringTag");n.exports=function(n,e,t){n&&!t&&(n=n.prototype),n&&!r(n,a)&&i(n,a,{configurable:!0,value:e})}},function(n,e,t){var i=t(25);n.exports=Array.isArray||function(n){return"Array"==i(n)}},function(n,e,t){var i=t(71),r=t(272),a=t(273),o=i?i.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":o&&o in Object(n)?r(n):a(n)}},function(n,e,t){var i=t(0),r=t(2),a=t(3),o=t(25),l=i.Object,s=r("".split);n.exports=a((function(){return!l("z").propertyIsEnumerable(0)}))?function(n){return"String"==o(n)?s(n,""):l(n)}:l},function(n,e,t){var i=t(3);n.exports=!i((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e){n.exports={}},function(n,e){n.exports={}},function(n,e,t){var i=t(7),r=t(10),a=Function.prototype,o=i&&Object.getOwnPropertyDescriptor,l=r(a,"name"),s=l&&"something"===function(){}.name,c=l&&(!i||i&&o(a,"name").configurable);n.exports={EXISTS:l,PROPER:s,CONFIGURABLE:c}},function(n,e,t){var i=t(2),r=t(8),a=t(239);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=i(Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set))(t,[]),e=t instanceof Array}catch(n){}return function(t,i){return r(t),a(i),e?n(t,i):t.__proto__=i,t}}():void 0)},function(n,e,t){var i=t(2);n.exports=i([].slice)},function(n,e,t){"use strict";var i=t(82),r=t(13),a=t(48);n.exports=function(n,e,t){var o=i(e);o in n?r.f(n,o,a(0,t)):n[o]=t}},function(n,e,t){"use strict";var i=t(1),r=t(0),a=t(16),o=t(36),l=t(11),s=t(2),c=t(26),d=t(7),u=t(113),p=t(3),m=t(10),h=t(60),f=t(5),g=t(9),v=t(35),y=t(83),b=t(8),x=t(15),_=t(19),k=t(82),w=t(12),T=t(48),I=t(34),z=t(85),S=t(58),j=t(183),P=t(121),E=t(38),q=t(13),O=t(114),A=t(118),$=t(68),C=t(14),L=t(79),M=t(86),D=t(64),R=t(80),B=t(6),N=t(184),U=t(185),F=t(59),H=t(39),G=t(57).forEach,K=M("hidden"),W=B("toPrimitive"),Q=H.set,V=H.getterFor("Symbol"),J=Object.prototype,X=r.Symbol,Y=X&&X.prototype,Z=r.TypeError,nn=r.QObject,en=a("JSON","stringify"),tn=E.f,rn=q.f,an=j.f,on=A.f,ln=s([].push),sn=L("symbols"),cn=L("op-symbols"),dn=L("string-to-symbol-registry"),un=L("symbol-to-string-registry"),pn=L("wks"),mn=!nn||!nn.prototype||!nn.prototype.findChild,hn=d&&p((function(){return 7!=I(rn({},"a",{get:function(){return rn(this,"a",{value:7}).a}})).a}))?function(n,e,t){var i=tn(J,e);i&&delete J[e],rn(n,e,t),i&&n!==J&&rn(J,e,i)}:rn,fn=function(n,e){var t=sn[n]=I(Y);return Q(t,{type:"Symbol",tag:n,description:e}),d||(t.description=e),t},gn=function(n,e,t){n===J&&gn(cn,e,t),b(n);var i=k(e);return b(t),m(sn,i)?(t.enumerable?(m(n,K)&&n[K][i]&&(n[K][i]=!1),t=I(t,{enumerable:T(0,!1)})):(m(n,K)||rn(n,K,T(1,{})),n[K][i]=!0),hn(n,i,t)):rn(n,i,t)},vn=function(n,e){b(n);var t=_(e),i=z(t).concat(_n(t));return G(i,(function(e){d&&!l(yn,t,e)||gn(n,e,t[e])})),n},yn=function(n){var e=k(n),t=l(on,this,e);return!(this===J&&m(sn,e)&&!m(cn,e))&&(!(t||!m(this,e)||!m(sn,e)||m(this,K)&&this[K][e])||t)},bn=function(n,e){var t=_(n),i=k(e);if(t!==J||!m(sn,i)||m(cn,i)){var r=tn(t,i);return!r||!m(sn,i)||m(t,K)&&t[K][i]||(r.enumerable=!0),r}},xn=function(n){var e=an(_(n)),t=[];return G(e,(function(n){m(sn,n)||m(D,n)||ln(t,n)})),t},_n=function(n){var e=n===J,t=an(e?cn:_(n)),i=[];return G(t,(function(n){!m(sn,n)||e&&!m(J,n)||ln(i,sn[n])})),i};(u||(C(Y=(X=function(){if(v(Y,this))throw Z("Symbol is not a constructor");var n=arguments.length&&void 0!==arguments[0]?w(arguments[0]):void 0,e=R(n),t=function(n){this===J&&l(t,cn,n),m(this,K)&&m(this[K],e)&&(this[K][e]=!1),hn(this,e,T(1,n))};return d&&mn&&hn(J,e,{configurable:!0,set:t}),fn(e,n)}).prototype,"toString",(function(){return V(this).tag})),C(X,"withoutSetter",(function(n){return fn(R(n),n)})),A.f=yn,q.f=gn,O.f=vn,E.f=bn,S.f=j.f=xn,P.f=_n,N.f=function(n){return fn(B(n),n)},d&&(rn(Y,"description",{configurable:!0,get:function(){return V(this).description}}),c||C(J,"propertyIsEnumerable",yn,{unsafe:!0}))),i({global:!0,wrap:!0,forced:!u,sham:!u},{Symbol:X}),G(z(pn),(function(n){U(n)})),i({target:"Symbol",stat:!0,forced:!u},{for:function(n){var e=w(n);if(m(dn,e))return dn[e];var t=X(e);return dn[e]=t,un[t]=e,t},keyFor:function(n){if(!y(n))throw Z(n+" is not a symbol");if(m(un,n))return un[n]},useSetter:function(){mn=!0},useSimple:function(){mn=!1}}),i({target:"Object",stat:!0,forced:!u,sham:!d},{create:function(n,e){return void 0===e?I(n):vn(I(n),e)},defineProperty:gn,defineProperties:vn,getOwnPropertyDescriptor:bn}),i({target:"Object",stat:!0,forced:!u},{getOwnPropertyNames:xn,getOwnPropertySymbols:_n}),i({target:"Object",stat:!0,forced:p((function(){P.f(1)}))},{getOwnPropertySymbols:function(n){return P.f(x(n))}}),en)&&i({target:"JSON",stat:!0,forced:!u||p((function(){var n=X();return"[null]"!=en([n])||"{}"!=en({a:n})||"{}"!=en(Object(n))}))},{stringify:function(n,e,t){var i=$(arguments),r=e;if((g(e)||void 0!==n)&&!y(n))return h(e)||(e=function(n,e){if(f(r)&&(e=l(r,this,n,e)),!y(e))return e}),i[1]=e,o(en,null,i)}});if(!Y[W]){var kn=Y.valueOf;C(Y,W,(function(n){return l(kn,this)}))}F(X,"Symbol"),D[K]=!0},function(n,e,t){var i=t(33).Symbol;n.exports=i},function(n,e,t){"use strict";t.d(e,"a",(function(){return a}));t(77);var i=t(73);t(70),t(92),t(4),t(126),t(21),t(24),t(186);var r=t(100);t(40),t(30);function a(n){return function(n){if(Array.isArray(n))return Object(i.a)(n)}(n)||function(n){if("undefined"!=typeof Symbol&&null!=n[Symbol.iterator]||null!=n["@@iterator"])return Array.from(n)}(n)||Object(r.a)(n)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";function i(n,e){(null==e||e>n.length)&&(e=n.length);for(var t=0,i=new Array(e);t<e;t++)i[t]=n[t];return i}t.d(e,"a",(function(){return i}))},function(n,e,t){"use strict";t(17);var i,r,a=t(1),o=t(0),l=t(11),s=t(2),c=t(5),d=t(9),u=(i=!1,(r=/[ac]/).exec=function(){return i=!0,/./.exec.apply(this,arguments)},!0===r.test("abc")&&i),p=o.Error,m=s(/./.test);a({target:"RegExp",proto:!0,forced:!u},{test:function(n){var e=this.exec;if(!c(e))return m(this,n);var t=l(e,this,n);if(null!==t&&!d(t))throw new p("RegExp exec method returned something other than an Object or null");return!!t}})},function(n,e,t){var i=t(1),r=t(15),a=t(85);i({target:"Object",stat:!0,forced:t(3)((function(){a(1)}))},{keys:function(n){return a(r(n))}})},function(n,e,t){var i=t(0),r=t(123),a=t(5),o=t(25),l=t(6)("toStringTag"),s=i.Object,c="Arguments"==o(function(){return arguments}());n.exports=r?o:function(n){var e,t,i;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=s(n),l))?t:c?o(e):"Object"==(i=o(e))&&a(e.callee)?"Arguments":i}},function(n,e,t){t(1)({target:"Array",stat:!0},{isArray:t(60)})},function(n,e,t){var i=t(7),r=t(66).EXISTS,a=t(2),o=t(13).f,l=Function.prototype,s=a(l.toString),c=/function\b(?:\s|\/\*[\S\s]*?\*\/|\/\/[^\n\r]*[\n\r]+)*([^\s(/]*)/,d=a(c.exec);i&&!r&&o(l,"name",{configurable:!0,get:function(){try{return d(c,s(this))[1]}catch(n){return""}}})},function(n,e,t){var i=t(26),r=t(111);(n.exports=function(n,e){return r[n]||(r[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.20.3",mode:i?"pure":"global",copyright:"© 2014-2022 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.20.3/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e,t){var i=t(2),r=0,a=Math.random(),o=i(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+o(++r+a,36)}},function(n,e,t){var i=t(0),r=t(9),a=i.document,o=r(a)&&r(a.createElement);n.exports=function(n){return o?a.createElement(n):{}}},function(n,e,t){var i=t(158),r=t(83);n.exports=function(n){var e=i(n,"string");return r(e)?e:e+""}},function(n,e,t){var i=t(0),r=t(16),a=t(5),o=t(35),l=t(155),s=i.Object;n.exports=l?function(n){return"symbol"==typeof n}:function(n){var e=r("Symbol");return a(e)&&o(e.prototype,s(n))}},function(n,e,t){var i=t(0).String;n.exports=function(n){try{return i(n)}catch(n){return"Object"}}},function(n,e,t){var i=t(159),r=t(117);n.exports=Object.keys||function(n){return i(n,r)}},function(n,e,t){var i=t(79),r=t(80),a=i("keys");n.exports=function(n){return a[n]||(a[n]=r(n))}},function(n,e,t){var i=t(2),r=t(5),a=t(111),o=i(Function.toString);r(a.inspectSource)||(a.inspectSource=function(n){return o(n)}),n.exports=a.inspectSource},function(n,e,t){var i=t(2),r=t(3),a=t(5),o=t(76),l=t(16),s=t(87),c=function(){},d=[],u=l("Reflect","construct"),p=/^\s*(?:class|function)\b/,m=i(p.exec),h=!p.exec(c),f=function(n){if(!a(n))return!1;try{return u(c,d,n),!0}catch(n){return!1}},g=function(n){if(!a(n))return!1;switch(o(n)){case"AsyncFunction":case"GeneratorFunction":case"AsyncGeneratorFunction":return!1}try{return h||!!m(p,s(n))}catch(n){return!0}};g.sham=!0,n.exports=!u||r((function(){var n;return f(f.call)||!f(Object)||!f((function(){n=!0}))||n}))?g:f},function(n,e,t){var i=t(25),r=t(0);n.exports="process"==i(r.process)},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(4);function i(n,e,t,i,r,a,o){try{var l=n[a](o),s=l.value}catch(n){return void t(n)}l.done?e(s):Promise.resolve(s).then(i,r)}function r(n){return function(){var e=this,t=arguments;return new Promise((function(r,a){var o=n.apply(e,t);function l(n){i(o,r,a,l,s,"next",n)}function s(n){i(o,r,a,l,s,"throw",n)}l(void 0)}))}}},function(n,e,t){var i=t(3),r=t(6),a=t(51),o=r("species");n.exports=function(n){return a>=51||!i((function(){var e=[];return(e.constructor={})[o]=function(){return{foo:1}},1!==e[n](Boolean).foo}))}},function(n,e,t){"use strict";var i=t(1),r=t(7),a=t(0),o=t(2),l=t(10),s=t(5),c=t(35),d=t(12),u=t(13).f,p=t(119),m=a.Symbol,h=m&&m.prototype;if(r&&s(m)&&(!("description"in h)||void 0!==m().description)){var f={},g=function(){var n=arguments.length<1||void 0===arguments[0]?void 0:d(arguments[0]),e=c(h,this)?new m(n):void 0===n?m():m(n);return""===n&&(f[e]=!0),e};p(g,m),g.prototype=h,h.constructor=g;var v="Symbol(test)"==String(m("test")),y=o(h.toString),b=o(h.valueOf),x=/^Symbol\((.*)\)[^)]+$/,_=o("".replace),k=o("".slice);u(h,"description",{configurable:!0,get:function(){var n=b(this),e=y(n);if(l(f,n))return"";var t=v?k(e,7,-1):_(e,x,"$1");return""===t?void 0:t}}),i({global:!0,forced:!0},{Symbol:g})}},function(n,e,t){"use strict";var i,r,a=t(11),o=t(2),l=t(12),s=t(148),c=t(107),d=t(79),u=t(34),p=t(39).get,m=t(224),h=t(229),f=d("native-string-replace",String.prototype.replace),g=RegExp.prototype.exec,v=g,y=o("".charAt),b=o("".indexOf),x=o("".replace),_=o("".slice),k=(r=/b*/g,a(g,i=/a/,"a"),a(g,r,"a"),0!==i.lastIndex||0!==r.lastIndex),w=c.BROKEN_CARET,T=void 0!==/()??/.exec("")[1];(k||T||w||m||h)&&(v=function(n){var e,t,i,r,o,c,d,m=this,h=p(m),I=l(n),z=h.raw;if(z)return z.lastIndex=m.lastIndex,e=a(v,z,I),m.lastIndex=z.lastIndex,e;var S=h.groups,j=w&&m.sticky,P=a(s,m),E=m.source,q=0,O=I;if(j&&(P=x(P,"y",""),-1===b(P,"g")&&(P+="g"),O=_(I,m.lastIndex),m.lastIndex>0&&(!m.multiline||m.multiline&&"\n"!==y(I,m.lastIndex-1))&&(E="(?: "+E+")",O=" "+O,q++),t=new RegExp("^(?:"+E+")",P)),T&&(t=new RegExp("^"+E+"$(?!\\s)",P)),k&&(i=m.lastIndex),r=a(g,j?t:m,O),j?r?(r.input=_(r.input,q),r[0]=_(r[0],q),r.index=m.lastIndex,m.lastIndex+=r[0].length):m.lastIndex=0:k&&r&&(m.lastIndex=m.global?r.index+r[0].length:i),T&&r&&r.length>1&&a(f,r[0],t,(function(){for(o=1;o<arguments.length-2;o++)void 0===arguments[o]&&(r[o]=void 0)})),r&&S)for(r.groups=c=u(null),o=0;o<S.length;o++)c[(d=S[o])[0]]=r[d[1]];return r}),n.exports=v},function(n,e,t){var i=t(277),r=t(278),a=t(279),o=t(280),l=t(281);function s(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var i=n[e];this.set(i[0],i[1])}}s.prototype.clear=i,s.prototype.delete=r,s.prototype.get=a,s.prototype.has=o,s.prototype.set=l,n.exports=s},function(n,e,t){var i=t(193);n.exports=function(n,e){for(var t=n.length;t--;)if(i(n[t][0],e))return t;return-1}},function(n,e,t){var i=t(41)(Object,"create");n.exports=i},function(n,e,t){var i=t(299);n.exports=function(n,e){var t=n.__data__;return i(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var i=t(135);n.exports=function(n){if("string"==typeof n||i(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var i=t(1),r=t(7),a=t(13).f;i({target:"Object",stat:!0,forced:Object.defineProperty!==a,sham:!r},{defineProperty:a})},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(42),t(4),t(78),t(186),t(21),t(17),t(74);var i=t(73);function r(n,e){if(n){if("string"==typeof n)return Object(i.a)(n,e);var t=Object.prototype.toString.call(n).slice(8,-1);return"Object"===t&&n.constructor&&(t=n.constructor.name),"Map"===t||"Set"===t?Array.from(n):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?Object(i.a)(n,e):void 0}}},function(n,e,t){var i,r;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(r="function"==typeof(i=function(){var n,e,t={version:"0.2.0"},i=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function r(n,e,t){return n<e?e:n>t?t:n}function a(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(i[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=r(n,i.minimum,1),t.status=1===n?null:n;var s=t.render(!e),c=s.querySelector(i.barSelector),d=i.speed,u=i.easing;return s.offsetWidth,o((function(e){""===i.positionUsing&&(i.positionUsing=t.getPositioningCSS()),l(c,function(n,e,t){var r;return(r="translate3d"===i.positionUsing?{transform:"translate3d("+a(n)+"%,0,0)"}:"translate"===i.positionUsing?{transform:"translate("+a(n)+"%,0)"}:{"margin-left":a(n)+"%"}).transition="all "+e+"ms "+t,r}(n,d,u)),1===n?(l(s,{transition:"none",opacity:1}),s.offsetWidth,setTimeout((function(){l(s,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),d)}),d)):setTimeout(e,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),i.trickleSpeed)};return i.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*r(Math.random()*e,.1,.95)),e=r(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*i.trickleRate)},n=0,e=0,t.promise=function(i){return i&&"resolved"!==i.state()?(0===e&&t.start(),n++,e++,i.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=i.template;var r,o=e.querySelector(i.barSelector),s=n?"-100":a(t.status||0),d=document.querySelector(i.parent);return l(o,{transition:"all 0 linear",transform:"translate3d("+s+"%,0,0)"}),i.showSpinner||(r=e.querySelector(i.spinnerSelector))&&p(r),d!=document.body&&c(d,"nprogress-custom-parent"),d.appendChild(e),e},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(i.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&p(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var o=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),l=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var i,r=n.length,a=e.charAt(0).toUpperCase()+e.slice(1);r--;)if((i=n[r]+a)in t)return i;return e}(t))}function i(n,e,i){e=t(e),n.style[e]=i}return function(n,e){var t,r,a=arguments;if(2==a.length)for(t in e)void 0!==(r=e[t])&&e.hasOwnProperty(t)&&i(n,t,r);else i(n,a[1],a[2])}}();function s(n,e){return("string"==typeof n?n:u(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=u(n),i=t+e;s(t,e)||(n.className=i.substring(1))}function d(n,e){var t,i=u(n);s(n,e)&&(t=i.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function u(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function p(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?i.call(e,t,e,n):i)||(n.exports=r)},function(n){n.exports=JSON.parse('{"name":"vuepress-plugin-comment","version":"0.7.3","description":"Comment plugin in vuepress, such as Gitalk, Valine...","main":"index.js","scripts":{"test":"echo \\"Error: no test specified\\" && exit 1"},"repository":{"type":"git","url":"git+ssh://git@github.com/dongyuanxin/vuepress-plugin-comment.git"},"keywords":["vuepress","comment","plugin","vue","gitalk","valine"],"author":"dongyuanxin","license":"MIT","bugs":{"url":"https://github.com/dongyuanxin/vuepress-plugin-comment/issues"},"homepage":"https://github.com/dongyuanxin/vuepress-plugin-comment#readme","dependencies":{"ejs":"^2.6.1","gitalk":"^1.5.0","gitalk-fix":"^1.5.2","i":"^0.3.6","npm":"^6.9.0","valine":"^1.3.9"}}')},function(n,e,t){"use strict";var i=t(36),r=t(11),a=t(2),o=t(108),l=t(147),s=t(8),c=t(18),d=t(124),u=t(127),p=t(52),m=t(12),h=t(47),f=t(125),g=t(109),v=t(93),y=t(107),b=t(3),x=y.UNSUPPORTED_Y,_=Math.min,k=[].push,w=a(/./.exec),T=a(k),I=a("".slice);o("split",(function(n,e,t){var a;return a="c"=="abbc".split(/(b)*/)[1]||4!="test".split(/(?:)/,-1).length||2!="ab".split(/(?:ab)*/).length||4!=".".split(/(.?)(.?)/).length||".".split(/()()/).length>1||"".split(/.?/).length?function(n,t){var a=m(c(this)),o=void 0===t?4294967295:t>>>0;if(0===o)return[];if(void 0===n)return[a];if(!l(n))return r(e,a,n,o);for(var s,d,u,p=[],h=(n.ignoreCase?"i":"")+(n.multiline?"m":"")+(n.unicode?"u":"")+(n.sticky?"y":""),g=0,y=new RegExp(n.source,h+"g");(s=r(v,y,a))&&!((d=y.lastIndex)>g&&(T(p,I(a,g,s.index)),s.length>1&&s.index<a.length&&i(k,p,f(s,1)),u=s[0].length,g=d,p.length>=o));)y.lastIndex===s.index&&y.lastIndex++;return g===a.length?!u&&w(y,"")||T(p,""):T(p,I(a,g)),p.length>o?f(p,0,o):p}:"0".split(void 0,0).length?function(n,t){return void 0===n&&0===t?[]:r(e,this,n,t)}:e,[function(e,t){var i=c(this),o=null==e?void 0:h(e,n);return o?r(o,e,i,t):r(a,m(i),e,t)},function(n,i){var r=s(this),o=m(n),l=t(a,r,o,i,a!==e);if(l.done)return l.value;var c=d(r,RegExp),h=r.unicode,f=(r.ignoreCase?"i":"")+(r.multiline?"m":"")+(r.unicode?"u":"")+(x?"g":"y"),v=new c(x?"^(?:"+r.source+")":r,f),y=void 0===i?4294967295:i>>>0;if(0===y)return[];if(0===o.length)return null===g(v,o)?[o]:[];for(var b=0,k=0,w=[];k<o.length;){v.lastIndex=x?0:k;var z,S=g(v,x?I(o,k):o);if(null===S||(z=_(p(v.lastIndex+(x?k:0)),o.length))===b)k=u(o,k,h);else{if(T(w,I(o,b,k)),w.length===y)return w;for(var j=1;j<=S.length-1;j++)if(T(w,S[j]),w.length===y)return w;k=b=z}}return T(w,I(o,b)),w}]}),!!b((function(){var n=/(?:)/,e=n.exec;n.exec=function(){return e.apply(this,arguments)};var t="ab".split(n);return 2!==t.length||"a"!==t[0]||"b"!==t[1]})),x)},function(n,e,t){"use strict";var i=t(1),r=t(2),a=t(115).indexOf,o=t(46),l=r([].indexOf),s=!!l&&1/l([1],1,-0)<0,c=o("indexOf");i({target:"Array",proto:!0,forced:s||!c},{indexOf:function(n){var e=arguments.length>1?arguments[1]:void 0;return s?l(this,n,e)||0:a(this,n,e)}})},function(n,e,t){var i=t(3),r=t(5),a=/#|\.prototype\./,o=function(n,e){var t=s[l(n)];return t==d||t!=c&&(r(e)?i(e):!!e)},l=o.normalize=function(n){return String(n).replace(a,".").toLowerCase()},s=o.data={},c=o.NATIVE="N",d=o.POLYFILL="P";n.exports=o},function(n,e,t){var i=t(76),r=t(47),a=t(65),o=t(6)("iterator");n.exports=function(n){if(null!=n)return r(n,o)||r(n,"@@iterator")||a[i(n)]}},function(n,e,t){var i=t(3),r=t(0).RegExp,a=i((function(){var n=r("a","y");return n.lastIndex=2,null!=n.exec("abcd")})),o=a||i((function(){return!r("a","y").sticky})),l=a||i((function(){var n=r("^r","gy");return n.lastIndex=2,null!=n.exec("str")}));n.exports={BROKEN_CARET:l,MISSED_STICKY:o,UNSUPPORTED_Y:a}},function(n,e,t){"use strict";t(17);var i=t(2),r=t(14),a=t(93),o=t(3),l=t(6),s=t(27),c=l("species"),d=RegExp.prototype;n.exports=function(n,e,t,u){var p=l(n),m=!o((function(){var e={};return e[p]=function(){return 7},7!=""[n](e)})),h=m&&!o((function(){var e=!1,t=/a/;return"split"===n&&((t={}).constructor={},t.constructor[c]=function(){return t},t.flags="",t[p]=/./[p]),t.exec=function(){return e=!0,null},t[p](""),!e}));if(!m||!h||t){var f=i(/./[p]),g=e(p,""[n],(function(n,e,t,r,o){var l=i(n),s=e.exec;return s===a||s===d.exec?m&&!o?{done:!0,value:f(e,t,r)}:{done:!0,value:l(t,e,r)}:{done:!1}}));r(String.prototype,n,g[0]),r(d,p,g[1])}u&&s(d[p],"sham",!0)}},function(n,e,t){var i=t(0),r=t(11),a=t(8),o=t(5),l=t(25),s=t(93),c=i.TypeError;n.exports=function(n,e){var t=n.exec;if(o(t)){var i=r(t,n,e);return null!==i&&a(i),i}if("RegExp"===l(n))return r(s,n,e);throw c("RegExp#exec called on incompatible receiver")}},function(n,e,t){"use strict";var i=t(1),r=t(2),a=t(62),o=t(19),l=t(46),s=r([].join),c=a!=Object,d=l("join",",");i({target:"Array",proto:!0,forced:c||!d},{join:function(n){return s(o(this),void 0===n?",":n)}})},function(n,e,t){var i=t(0),r=t(112),a=i["__core-js_shared__"]||r("__core-js_shared__",{});n.exports=a},function(n,e,t){var i=t(0),r=Object.defineProperty;n.exports=function(n,e){try{r(i,n,{value:e,configurable:!0,writable:!0})}catch(t){i[n]=e}return e}},function(n,e,t){var i=t(51),r=t(3);n.exports=!!Object.getOwnPropertySymbols&&!r((function(){var n=Symbol();return!String(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&i&&i<41}))},function(n,e,t){var i=t(7),r=t(156),a=t(13),o=t(8),l=t(19),s=t(85);e.f=i&&!r?Object.defineProperties:function(n,e){o(n);for(var t,i=l(e),r=s(e),c=r.length,d=0;c>d;)a.f(n,t=r[d++],i[t]);return n}},function(n,e,t){var i=t(19),r=t(116),a=t(22),o=function(n){return function(e,t,o){var l,s=i(e),c=a(s),d=r(o,c);if(n&&t!=t){for(;c>d;)if((l=s[d++])!=l)return!0}else for(;c>d;d++)if((n||d in s)&&s[d]===t)return n||d||0;return!n&&-1}};n.exports={includes:o(!0),indexOf:o(!1)}},function(n,e,t){var i=t(55),r=Math.max,a=Math.min;n.exports=function(n,e){var t=i(n);return t<0?r(t+e,0):a(t,e)}},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){"use strict";var i={}.propertyIsEnumerable,r=Object.getOwnPropertyDescriptor,a=r&&!i.call({1:2},1);e.f=a?function(n){var e=r(this,n);return!!e&&e.enumerable}:i},function(n,e,t){var i=t(10),r=t(120),a=t(38),o=t(13);n.exports=function(n,e,t){for(var l=r(e),s=o.f,c=a.f,d=0;d<l.length;d++){var u=l[d];i(n,u)||t&&i(t,u)||s(n,u,c(e,u))}}},function(n,e,t){var i=t(16),r=t(2),a=t(58),o=t(121),l=t(8),s=r([].concat);n.exports=i("Reflect","ownKeys")||function(n){var e=a.f(l(n)),t=o.f;return t?s(e,t(n)):e}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var i=t(0),r=t(10),a=t(5),o=t(15),l=t(86),s=t(163),c=l("IE_PROTO"),d=i.Object,u=d.prototype;n.exports=s?d.getPrototypeOf:function(n){var e=o(n);if(r(e,c))return e[c];var t=e.constructor;return a(t)&&e instanceof t?t.prototype:e instanceof d?u:null}},function(n,e,t){var i={};i[t(6)("toStringTag")]="z",n.exports="[object z]"===String(i)},function(n,e,t){var i=t(8),r=t(168),a=t(6)("species");n.exports=function(n,e){var t,o=i(n).constructor;return void 0===o||null==(t=i(o)[a])?e:r(t)}},function(n,e,t){var i=t(0),r=t(116),a=t(22),o=t(69),l=i.Array,s=Math.max;n.exports=function(n,e,t){for(var i=a(n),c=r(e,i),d=r(void 0===t?i:t,i),u=l(s(d-c,0)),p=0;c<d;c++,p++)o(u,p,n[c]);return u.length=p,u}},function(n,e,t){t(185)("iterator")},function(n,e,t){"use strict";var i=t(173).charAt;n.exports=function(n,e,t){return e+(t?i(n,e).length:1)}},function(n,e,t){var i=t(271),r=t(49),a=Object.prototype,o=a.hasOwnProperty,l=a.propertyIsEnumerable,s=i(function(){return arguments}())?i:function(n){return r(n)&&o.call(n,"callee")&&!l.call(n,"callee")};n.exports=s},function(n,e,t){var i=t(41)(t(33),"Map");n.exports=i},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var i=t(291),r=t(298),a=t(300),o=t(301),l=t(302);function s(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var i=n[e];this.set(i[0],i[1])}}s.prototype.clear=i,s.prototype.delete=r,s.prototype.get=a,s.prototype.has=o,s.prototype.set=l,n.exports=s},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var i=t(31),r=t(135),a=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,o=/^\w*$/;n.exports=function(n,e){if(i(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!r(n))||(o.test(n)||!a.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var i=t(61),r=t(49);n.exports=function(n){return"symbol"==typeof n||r(n)&&"[object Symbol]"==i(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){var i=t(1),r=t(0),a=t(59);i({global:!0},{Reflect:{}}),a(r.Reflect,"Reflect",!0)},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(77);t(70),t(92),t(4),t(126),t(21),t(24);var i=t(100);t(40),t(30);function r(n,e){return function(n){if(Array.isArray(n))return n}(n)||function(n,e){var t=null==n?null:"undefined"!=typeof Symbol&&n[Symbol.iterator]||n["@@iterator"];if(null!=t){var i,r,a=[],o=!0,l=!1;try{for(t=t.call(n);!(o=(i=t.next()).done)&&(a.push(i.value),!e||a.length!==e);o=!0);}catch(n){l=!0,r=n}finally{try{o||null==t.return||t.return()}finally{if(l)throw r}}return a}}(n,e)||Object(i.a)(n,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";var i=t(1),r=t(57).some;i({target:"Array",proto:!0,forced:!t(46)("some")},{some:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var i=t(11),r=t(108),a=t(8),o=t(52),l=t(12),s=t(18),c=t(47),d=t(127),u=t(109);r("match",(function(n,e,t){return[function(e){var t=s(this),r=null==e?void 0:c(e,n);return r?i(r,e,t):new RegExp(e)[n](l(t))},function(n){var i=a(this),r=l(n),s=t(e,i,r);if(s.done)return s.value;if(!i.global)return u(i,r);var c=i.unicode;i.lastIndex=0;for(var p,m=[],h=0;null!==(p=u(i,r));){var f=l(p[0]);m[h]=f,""===f&&(i.lastIndex=d(r,o(i.lastIndex),c)),h++}return 0===h?null:m}]}))},function(n,e,t){var i=t(6),r=t(34),a=t(13),o=i("unscopables"),l=Array.prototype;null==l[o]&&a.f(l,o,{configurable:!0,value:r(null)}),n.exports=function(n){l[o][n]=!0}},function(n,e,t){var i=function(n){"use strict";var e=Object.prototype,t=e.hasOwnProperty,i="function"==typeof Symbol?Symbol:{},r=i.iterator||"@@iterator",a=i.asyncIterator||"@@asyncIterator",o=i.toStringTag||"@@toStringTag";function l(n,e,t){return Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}),n[e]}try{l({},"")}catch(n){l=function(n,e,t){return n[e]=t}}function s(n,e,t,i){var r=e&&e.prototype instanceof u?e:u,a=Object.create(r.prototype),o=new w(i||[]);return a._invoke=function(n,e,t){var i="suspendedStart";return function(r,a){if("executing"===i)throw new Error("Generator is already running");if("completed"===i){if("throw"===r)throw a;return I()}for(t.method=r,t.arg=a;;){var o=t.delegate;if(o){var l=x(o,t);if(l){if(l===d)continue;return l}}if("next"===t.method)t.sent=t._sent=t.arg;else if("throw"===t.method){if("suspendedStart"===i)throw i="completed",t.arg;t.dispatchException(t.arg)}else"return"===t.method&&t.abrupt("return",t.arg);i="executing";var s=c(n,e,t);if("normal"===s.type){if(i=t.done?"completed":"suspendedYield",s.arg===d)continue;return{value:s.arg,done:t.done}}"throw"===s.type&&(i="completed",t.method="throw",t.arg=s.arg)}}}(n,t,o),a}function c(n,e,t){try{return{type:"normal",arg:n.call(e,t)}}catch(n){return{type:"throw",arg:n}}}n.wrap=s;var d={};function u(){}function p(){}function m(){}var h={};l(h,r,(function(){return this}));var f=Object.getPrototypeOf,g=f&&f(f(T([])));g&&g!==e&&t.call(g,r)&&(h=g);var v=m.prototype=u.prototype=Object.create(h);function y(n){["next","throw","return"].forEach((function(e){l(n,e,(function(n){return this._invoke(e,n)}))}))}function b(n,e){var i;this._invoke=function(r,a){function o(){return new e((function(i,o){!function i(r,a,o,l){var s=c(n[r],n,a);if("throw"!==s.type){var d=s.arg,u=d.value;return u&&"object"==typeof u&&t.call(u,"__await")?e.resolve(u.__await).then((function(n){i("next",n,o,l)}),(function(n){i("throw",n,o,l)})):e.resolve(u).then((function(n){d.value=n,o(d)}),(function(n){return i("throw",n,o,l)}))}l(s.arg)}(r,a,i,o)}))}return i=i?i.then(o,o):o()}}function x(n,e){var t=n.iterator[e.method];if(void 0===t){if(e.delegate=null,"throw"===e.method){if(n.iterator.return&&(e.method="return",e.arg=void 0,x(n,e),"throw"===e.method))return d;e.method="throw",e.arg=new TypeError("The iterator does not provide a 'throw' method")}return d}var i=c(t,n.iterator,e.arg);if("throw"===i.type)return e.method="throw",e.arg=i.arg,e.delegate=null,d;var r=i.arg;return r?r.done?(e[n.resultName]=r.value,e.next=n.nextLoc,"return"!==e.method&&(e.method="next",e.arg=void 0),e.delegate=null,d):r:(e.method="throw",e.arg=new TypeError("iterator result is not an object"),e.delegate=null,d)}function _(n){var e={tryLoc:n[0]};1 in n&&(e.catchLoc=n[1]),2 in n&&(e.finallyLoc=n[2],e.afterLoc=n[3]),this.tryEntries.push(e)}function k(n){var e=n.completion||{};e.type="normal",delete e.arg,n.completion=e}function w(n){this.tryEntries=[{tryLoc:"root"}],n.forEach(_,this),this.reset(!0)}function T(n){if(n){var e=n[r];if(e)return e.call(n);if("function"==typeof n.next)return n;if(!isNaN(n.length)){var i=-1,a=function e(){for(;++i<n.length;)if(t.call(n,i))return e.value=n[i],e.done=!1,e;return e.value=void 0,e.done=!0,e};return a.next=a}}return{next:I}}function I(){return{value:void 0,done:!0}}return p.prototype=m,l(v,"constructor",m),l(m,"constructor",p),p.displayName=l(m,o,"GeneratorFunction"),n.isGeneratorFunction=function(n){var e="function"==typeof n&&n.constructor;return!!e&&(e===p||"GeneratorFunction"===(e.displayName||e.name))},n.mark=function(n){return Object.setPrototypeOf?Object.setPrototypeOf(n,m):(n.__proto__=m,l(n,o,"GeneratorFunction")),n.prototype=Object.create(v),n},n.awrap=function(n){return{__await:n}},y(b.prototype),l(b.prototype,a,(function(){return this})),n.AsyncIterator=b,n.async=function(e,t,i,r,a){void 0===a&&(a=Promise);var o=new b(s(e,t,i,r),a);return n.isGeneratorFunction(t)?o:o.next().then((function(n){return n.done?n.value:o.next()}))},y(v),l(v,o,"Generator"),l(v,r,(function(){return this})),l(v,"toString",(function(){return"[object Generator]"})),n.keys=function(n){var e=[];for(var t in n)e.push(t);return e.reverse(),function t(){for(;e.length;){var i=e.pop();if(i in n)return t.value=i,t.done=!1,t}return t.done=!0,t}},n.values=T,w.prototype={constructor:w,reset:function(n){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(k),!n)for(var e in this)"t"===e.charAt(0)&&t.call(this,e)&&!isNaN(+e.slice(1))&&(this[e]=void 0)},stop:function(){this.done=!0;var n=this.tryEntries[0].completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(n){if(this.done)throw n;var e=this;function i(t,i){return o.type="throw",o.arg=n,e.next=t,i&&(e.method="next",e.arg=void 0),!!i}for(var r=this.tryEntries.length-1;r>=0;--r){var a=this.tryEntries[r],o=a.completion;if("root"===a.tryLoc)return i("end");if(a.tryLoc<=this.prev){var l=t.call(a,"catchLoc"),s=t.call(a,"finallyLoc");if(l&&s){if(this.prev<a.catchLoc)return i(a.catchLoc,!0);if(this.prev<a.finallyLoc)return i(a.finallyLoc)}else if(l){if(this.prev<a.catchLoc)return i(a.catchLoc,!0)}else{if(!s)throw new Error("try statement without catch or finally");if(this.prev<a.finallyLoc)return i(a.finallyLoc)}}}},abrupt:function(n,e){for(var i=this.tryEntries.length-1;i>=0;--i){var r=this.tryEntries[i];if(r.tryLoc<=this.prev&&t.call(r,"finallyLoc")&&this.prev<r.finallyLoc){var a=r;break}}a&&("break"===n||"continue"===n)&&a.tryLoc<=e&&e<=a.finallyLoc&&(a=null);var o=a?a.completion:{};return o.type=n,o.arg=e,a?(this.method="next",this.next=a.finallyLoc,d):this.complete(o)},complete:function(n,e){if("throw"===n.type)throw n.arg;return"break"===n.type||"continue"===n.type?this.next=n.arg:"return"===n.type?(this.rval=this.arg=n.arg,this.method="return",this.next="end"):"normal"===n.type&&e&&(this.next=e),d},finish:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.finallyLoc===n)return this.complete(t.completion,t.afterLoc),k(t),d}},catch:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.tryLoc===n){var i=t.completion;if("throw"===i.type){var r=i.arg;k(t)}return r}}throw new Error("illegal catch attempt")},delegateYield:function(n,e,t){return this.delegate={iterator:T(n),resultName:e,nextLoc:t},"next"===this.method&&(this.arg=void 0),d}},n}(n.exports);try{regeneratorRuntime=i}catch(n){"object"==typeof globalThis?globalThis.regeneratorRuntime=i:Function("r","regeneratorRuntime = r")(i)}},function(n,e,t){"use strict";var i=t(2),r=t(66).PROPER,a=t(14),o=t(8),l=t(35),s=t(12),c=t(3),d=t(148),u=RegExp.prototype,p=u.toString,m=i(d),h=c((function(){return"/a/b"!=p.call({source:"a",flags:"b"})})),f=r&&"toString"!=p.name;(h||f)&&a(RegExp.prototype,"toString",(function(){var n=o(this),e=s(n.source),t=n.flags;return"/"+e+"/"+s(void 0===t&&l(u,n)&&!("flags"in u)?m(n):t)}),{unsafe:!0})},function(n,e,t){"use strict";var i=t(19),r=t(141),a=t(65),o=t(39),l=t(13).f,s=t(161),c=t(26),d=t(7),u=o.set,p=o.getterFor("Array Iterator");n.exports=s(Array,"Array",(function(n,e){u(this,{type:"Array Iterator",target:i(n),index:0,kind:e})}),(function(){var n=p(this),e=n.target,t=n.kind,i=n.index++;return!e||i>=e.length?(n.target=void 0,{value:void 0,done:!0}):"keys"==t?{value:i,done:!1}:"values"==t?{value:e[i],done:!1}:{value:[i,e[i]],done:!1}}),"values");var m=a.Arguments=a.Array;if(r("keys"),r("values"),r("entries"),!c&&d&&"values"!==m.name)try{l(m,"name",{value:"values"})}catch(n){}},function(n,e,t){var i=t(0),r=t(11),a=t(37),o=t(8),l=t(84),s=t(106),c=i.TypeError;n.exports=function(n,e){var t=arguments.length<2?s(n):e;if(a(t))return o(r(t,n));throw c(l(n)+" is not iterable")}},function(n,e,t){var i=t(253);n.exports=function(n,e){return new(i(n))(0===e?0:e)}},function(n,e,t){var i=t(9),r=t(25),a=t(6)("match");n.exports=function(n){var e;return i(n)&&(void 0!==(e=n[a])?!!e:"RegExp"==r(n))}},function(n,e,t){"use strict";var i=t(8);n.exports=function(){var n=i(this),e="";return n.global&&(e+="g"),n.ignoreCase&&(e+="i"),n.multiline&&(e+="m"),n.dotAll&&(e+="s"),n.unicode&&(e+="u"),n.sticky&&(e+="y"),e}},function(n,e,t){var i=t(5),r=t(9),a=t(67);n.exports=function(n,e,t){var o,l;return a&&i(o=e.constructor)&&o!==t&&r(l=o.prototype)&&l!==t.prototype&&a(n,l),n}},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e){var t=/^\s+|\s+$/g,i=/^[-+]0x[0-9a-f]+$/i,r=/^0b[01]+$/i,a=/^0o[0-7]+$/i,o=parseInt,l="object"==typeof global&&global&&global.Object===Object&&global,s="object"==typeof self&&self&&self.Object===Object&&self,c=l||s||Function("return this")(),d=Object.prototype.toString,u=Math.max,p=Math.min,m=function(){return c.Date.now()};function h(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function f(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==d.call(n)}(n))return NaN;if(h(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=h(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var l=r.test(n);return l||a.test(n)?o(n.slice(2),l?2:8):i.test(n)?NaN:+n}n.exports=function(n,e,t){var i,r,a,o,l,s,c=0,d=!1,g=!1,v=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function y(e){var t=i,a=r;return i=r=void 0,c=e,o=n.apply(a,t)}function b(n){return c=n,l=setTimeout(_,e),d?y(n):o}function x(n){var t=n-s;return void 0===s||t>=e||t<0||g&&n-c>=a}function _(){var n=m();if(x(n))return k(n);l=setTimeout(_,function(n){var t=e-(n-s);return g?p(t,a-(n-c)):t}(n))}function k(n){return l=void 0,v&&i?y(n):(i=r=void 0,o)}function w(){var n=m(),t=x(n);if(i=arguments,r=this,s=n,t){if(void 0===l)return b(s);if(g)return l=setTimeout(_,e),y(s)}return void 0===l&&(l=setTimeout(_,e)),o}return e=f(e)||0,h(t)&&(d=!!t.leading,a=(g="maxWait"in t)?u(f(t.maxWait)||0,e):a,v="trailing"in t?!!t.trailing:v),w.cancel=function(){void 0!==l&&clearTimeout(l),c=0,i=s=r=l=void 0},w.flush=function(){return void 0===l?o:k(m())},w}},function(n,e,t){var i=t(2),r=t(18),a=t(12),o=t(153),l=i("".replace),s="["+o+"]",c=RegExp("^"+s+s+"*"),d=RegExp(s+s+"*$"),u=function(n){return function(e){var t=a(r(e));return 1&n&&(t=l(t,c,"")),2&n&&(t=l(t,d,"")),t}};n.exports={start:u(1),end:u(2),trim:u(3)}},function(n,e){n.exports="\t\n\v\f\r                　\u2028\u2029\ufeff"},function(n,e,t){var i=t(2),r=t(14),a=Date.prototype,o=i(a.toString),l=i(a.getTime);"Invalid Date"!=String(new Date(NaN))&&r(a,"toString",(function(){var n=l(this);return n==n?o(this):"Invalid Date"}))},function(n,e,t){var i=t(113);n.exports=i&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var i=t(7),r=t(3);n.exports=i&&r((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var i=t(7),r=t(3),a=t(81);n.exports=!i&&!r((function(){return 7!=Object.defineProperty(a("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var i=t(0),r=t(11),a=t(9),o=t(83),l=t(47),s=t(237),c=t(6),d=i.TypeError,u=c("toPrimitive");n.exports=function(n,e){if(!a(n)||o(n))return n;var t,i=l(n,u);if(i){if(void 0===e&&(e="default"),t=r(i,n,e),!a(t)||o(t))return t;throw d("Can't convert object to primitive value")}return void 0===e&&(e="number"),s(n,e)}},function(n,e,t){var i=t(2),r=t(10),a=t(19),o=t(115).indexOf,l=t(64),s=i([].push);n.exports=function(n,e){var t,i=a(n),c=0,d=[];for(t in i)!r(l,t)&&r(i,t)&&s(d,t);for(;e.length>c;)r(i,t=e[c++])&&(~o(d,t)||s(d,t));return d}},function(n,e,t){var i=t(16);n.exports=i("document","documentElement")},function(n,e,t){"use strict";var i=t(1),r=t(11),a=t(26),o=t(66),l=t(5),s=t(225),c=t(122),d=t(67),u=t(59),p=t(27),m=t(14),h=t(6),f=t(65),g=t(162),v=o.PROPER,y=o.CONFIGURABLE,b=g.IteratorPrototype,x=g.BUGGY_SAFARI_ITERATORS,_=h("iterator"),k=function(){return this};n.exports=function(n,e,t,o,h,g,w){s(t,e,o);var T,I,z,S=function(n){if(n===h&&O)return O;if(!x&&n in E)return E[n];switch(n){case"keys":case"values":case"entries":return function(){return new t(this,n)}}return function(){return new t(this)}},j=e+" Iterator",P=!1,E=n.prototype,q=E[_]||E["@@iterator"]||h&&E[h],O=!x&&q||S(h),A="Array"==e&&E.entries||q;if(A&&(T=c(A.call(new n)))!==Object.prototype&&T.next&&(a||c(T)===b||(d?d(T,b):l(T[_])||m(T,_,k)),u(T,j,!0,!0),a&&(f[j]=k)),v&&"values"==h&&q&&"values"!==q.name&&(!a&&y?p(E,"name","values"):(P=!0,O=function(){return r(q,this)})),h)if(I={values:S("values"),keys:g?O:S("keys"),entries:S("entries")},w)for(z in I)(x||P||!(z in E))&&m(E,z,I[z]);else i({target:e,proto:!0,forced:x||P},I);return a&&!w||E[_]===O||m(E,_,O,{name:h}),f[e]=O,I}},function(n,e,t){"use strict";var i,r,a,o=t(3),l=t(5),s=t(34),c=t(122),d=t(14),u=t(6),p=t(26),m=u("iterator"),h=!1;[].keys&&("next"in(a=[].keys())?(r=c(c(a)))!==Object.prototype&&(i=r):h=!0),null==i||o((function(){var n={};return i[m].call(n)!==n}))?i={}:p&&(i=s(i)),l(i[m])||d(i,m,(function(){return this})),n.exports={IteratorPrototype:i,BUGGY_SAFARI_ITERATORS:h}},function(n,e,t){var i=t(3);n.exports=!i((function(){function n(){}return n.prototype.constructor=null,Object.getPrototypeOf(new n)!==n.prototype}))},function(n,e,t){var i=t(0);n.exports=i.Promise},function(n,e,t){var i=t(6),r=t(65),a=i("iterator"),o=Array.prototype;n.exports=function(n){return void 0!==n&&(r.Array===n||o[a]===n)}},function(n,e,t){var i=t(11),r=t(8),a=t(47);n.exports=function(n,e,t){var o,l;r(n);try{if(!(o=a(n,"return"))){if("throw"===e)throw t;return t}o=i(o,n)}catch(n){l=!0,o=n}if("throw"===e)throw t;if(l)throw o;return r(o),t}},function(n,e,t){var i=t(6)("iterator"),r=!1;try{var a=0,o={next:function(){return{done:!!a++}},return:function(){r=!0}};o[i]=function(){return this},Array.from(o,(function(){throw 2}))}catch(n){}n.exports=function(n,e){if(!e&&!r)return!1;var t=!1;try{var a={};a[i]=function(){return{next:function(){return{done:t=!0}}}},n(a)}catch(n){}return t}},function(n,e,t){var i=t(0),r=t(88),a=t(84),o=i.TypeError;n.exports=function(n){if(r(n))return n;throw o(a(n)+" is not a constructor")}},function(n,e,t){var i,r,a,o,l=t(0),s=t(36),c=t(56),d=t(5),u=t(10),p=t(3),m=t(160),h=t(68),f=t(81),g=t(170),v=t(89),y=l.setImmediate,b=l.clearImmediate,x=l.process,_=l.Dispatch,k=l.Function,w=l.MessageChannel,T=l.String,I=0,z={};try{i=l.location}catch(n){}var S=function(n){if(u(z,n)){var e=z[n];delete z[n],e()}},j=function(n){return function(){S(n)}},P=function(n){S(n.data)},E=function(n){l.postMessage(T(n),i.protocol+"//"+i.host)};y&&b||(y=function(n){var e=h(arguments,1);return z[++I]=function(){s(d(n)?n:k(n),void 0,e)},r(I),I},b=function(n){delete z[n]},v?r=function(n){x.nextTick(j(n))}:_&&_.now?r=function(n){_.now(j(n))}:w&&!g?(o=(a=new w).port2,a.port1.onmessage=P,r=c(o.postMessage,o)):l.addEventListener&&d(l.postMessage)&&!l.importScripts&&i&&"file:"!==i.protocol&&!p(E)?(r=E,l.addEventListener("message",P,!1)):r="onreadystatechange"in f("script")?function(n){m.appendChild(f("script")).onreadystatechange=function(){m.removeChild(this),S(n)}}:function(n){setTimeout(j(n),0)}),n.exports={set:y,clear:b}},function(n,e,t){var i=t(32);n.exports=/(?:ipad|iphone|ipod).*applewebkit/i.test(i)},function(n,e,t){var i=t(8),r=t(9),a=t(172);n.exports=function(n,e){if(i(n),r(e)&&e.constructor===n)return e;var t=a.f(n);return(0,t.resolve)(e),t.promise}},function(n,e,t){"use strict";var i=t(37),r=function(n){var e,t;this.promise=new n((function(n,i){if(void 0!==e||void 0!==t)throw TypeError("Bad Promise constructor");e=n,t=i})),this.resolve=i(e),this.reject=i(t)};n.exports.f=function(n){return new r(n)}},function(n,e,t){var i=t(2),r=t(55),a=t(12),o=t(18),l=i("".charAt),s=i("".charCodeAt),c=i("".slice),d=function(n){return function(e,t){var i,d,u=a(o(e)),p=r(t),m=u.length;return p<0||p>=m?n?"":void 0:(i=s(u,p))<55296||i>56319||p+1===m||(d=s(u,p+1))<56320||d>57343?n?l(u,p):i:n?c(u,p,p+2):d-56320+(i-55296<<10)+65536}};n.exports={codeAt:d(!1),charAt:d(!0)}},function(n,e){n.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(n,e,t){var i=t(81)("span").classList,r=i&&i.constructor&&i.constructor.prototype;n.exports=r===Object.prototype?void 0:r},function(n,e,t){var i=t(1),r=t(7),a=t(120),o=t(19),l=t(38),s=t(69);i({target:"Object",stat:!0,sham:!r},{getOwnPropertyDescriptors:function(n){for(var e,t,i=o(n),r=l.f,c=a(i),d={},u=0;c.length>u;)void 0!==(t=r(i,e=c[u++]))&&s(d,e,t);return d}})},function(n,e,t){var i=t(1),r=t(3),a=t(15),o=t(122),l=t(163);i({target:"Object",stat:!0,forced:r((function(){o(1)})),sham:!l},{getPrototypeOf:function(n){return o(a(n))}})},function(n,e,t){"use strict";var i,r=t(1),a=t(2),o=t(38).f,l=t(52),s=t(12),c=t(179),d=t(18),u=t(180),p=t(26),m=a("".startsWith),h=a("".slice),f=Math.min,g=u("startsWith");r({target:"String",proto:!0,forced:!!(p||g||(i=o(String.prototype,"startsWith"),!i||i.writable))&&!g},{startsWith:function(n){var e=s(d(this));c(n);var t=l(f(arguments.length>1?arguments[1]:void 0,e.length)),i=s(n);return m?m(e,i,t):h(e,t,t+i.length)===i}})},function(n,e,t){var i=t(0),r=t(147),a=i.TypeError;n.exports=function(n){if(r(n))throw a("The method doesn't accept regular expressions");return n}},function(n,e,t){var i=t(6)("match");n.exports=function(n){var e=/./;try{"/./"[n](e)}catch(t){try{return e[i]=!1,"/./"[n](e)}catch(n){}}return!1}},function(n,e,t){"use strict";var i=t(57).forEach,r=t(46)("forEach");n.exports=r?[].forEach:function(n){return i(this,n,arguments.length>1?arguments[1]:void 0)}},function(n,e,t){var i=t(3);n.exports=!i((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(n,e,t){var i=t(25),r=t(19),a=t(58).f,o=t(125),l="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];n.exports.f=function(n){return l&&"Window"==i(n)?function(n){try{return a(n)}catch(n){return o(l)}}(n):a(r(n))}},function(n,e,t){var i=t(6);e.f=i},function(n,e,t){var i=t(260),r=t(10),a=t(184),o=t(13).f;n.exports=function(n){var e=i.Symbol||(i.Symbol={});r(e,n)||o(e,n,{value:a.f(n)})}},function(n,e,t){var i=t(1),r=t(261);i({target:"Array",stat:!0,forced:!t(167)((function(n){Array.from(n)}))},{from:r})},function(n,e,t){var i=t(12);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:i(n)}},function(n,e,t){t(1)({target:"Object",stat:!0,sham:!t(7)},{create:t(34)})},function(n,e,t){var i=t(1),r=t(0),a=t(16),o=t(36),l=t(2),s=t(3),c=r.Array,d=a("JSON","stringify"),u=l(/./.exec),p=l("".charAt),m=l("".charCodeAt),h=l("".replace),f=l(1..toString),g=/[\uD800-\uDFFF]/g,v=/^[\uD800-\uDBFF]$/,y=/^[\uDC00-\uDFFF]$/,b=function(n,e,t){var i=p(t,e-1),r=p(t,e+1);return u(v,n)&&!u(y,r)||u(y,n)&&!u(v,i)?"\\u"+f(m(n,0),16):n},x=s((function(){return'"\\udf06\\ud834"'!==d("\udf06\ud834")||'"\\udead"'!==d("\udead")}));d&&i({target:"JSON",stat:!0,forced:x},{stringify:function(n,e,t){for(var i=0,r=arguments.length,a=c(r);i<r;i++)a[i]=arguments[i];var l=o(d,null,a);return"string"==typeof l?h(l,g,b):l}})},function(n,e){n.exports=function(n,e){for(var t=-1,i=e.length,r=n.length;++t<i;)n[r+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var i=t(94),r=t(282),a=t(283),o=t(284),l=t(285),s=t(286);function c(n){var e=this.__data__=new i(n);this.size=e.size}c.prototype.clear=r,c.prototype.delete=a,c.prototype.get=o,c.prototype.has=l,c.prototype.set=s,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var i=t(61),r=t(130);n.exports=function(n){if(!r(n))return!1;var e=i(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var i=t(303),r=t(49);n.exports=function n(e,t,a,o,l){return e===t||(null==e||null==t||!r(e)&&!r(t)?e!=e&&t!=t:i(e,t,a,o,n,l))}},function(n,e,t){var i=t(198),r=t(306),a=t(199);n.exports=function(n,e,t,o,l,s){var c=1&t,d=n.length,u=e.length;if(d!=u&&!(c&&u>d))return!1;var p=s.get(n),m=s.get(e);if(p&&m)return p==e&&m==n;var h=-1,f=!0,g=2&t?new i:void 0;for(s.set(n,e),s.set(e,n);++h<d;){var v=n[h],y=e[h];if(o)var b=c?o(y,v,h,e,n,s):o(v,y,h,n,e,s);if(void 0!==b){if(b)continue;f=!1;break}if(g){if(!r(e,(function(n,e){if(!a(g,e)&&(v===n||l(v,n,t,o,s)))return g.push(e)}))){f=!1;break}}else if(v!==y&&!l(v,y,t,o,s)){f=!1;break}}return s.delete(n),s.delete(e),f}},function(n,e,t){var i=t(131),r=t(304),a=t(305);function o(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new i;++e<t;)this.add(n[e])}o.prototype.add=o.prototype.push=r,o.prototype.has=a,n.exports=o},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var i=t(316),r=t(322),a=t(204);n.exports=function(n){return a(n)?i(n):r(n)}},function(n,e,t){(function(n){var i=t(33),r=t(318),a=e&&!e.nodeType&&e,o=a&&"object"==typeof n&&n&&!n.nodeType&&n,l=o&&o.exports===a?i.Buffer:void 0,s=(l?l.isBuffer:void 0)||r;n.exports=s}).call(this,t(150)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var i=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==i||"symbol"!=i&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var i=t(319),r=t(320),a=t(321),o=a&&a.isTypedArray,l=o?r(o):i;n.exports=l},function(n,e,t){var i=t(194),r=t(133);n.exports=function(n){return null!=n&&r(n.length)&&!i(n)}},function(n,e,t){var i=t(41)(t(33),"Set");n.exports=i},function(n,e,t){var i=t(130);n.exports=function(n){return n==n&&!i(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var i=t(209),r=t(98);n.exports=function(n,e){for(var t=0,a=(e=i(e,n)).length;null!=n&&t<a;)n=n[r(e[t++])];return t&&t==a?n:void 0}},function(n,e,t){var i=t(31),r=t(134),a=t(333),o=t(336);n.exports=function(n,e){return i(n)?n:r(n,e)?[n]:a(o(n))}},function(n,e,t){"use strict";var i=t(0),r=t(2),a=t(37),o=t(9),l=t(10),s=t(68),c=t(63),d=i.Function,u=r([].concat),p=r([].join),m={},h=function(n,e,t){if(!l(m,e)){for(var i=[],r=0;r<e;r++)i[r]="a["+r+"]";m[e]=d("C,a","return new C("+p(i,",")+")")}return m[e](n,t)};n.exports=c?d.bind:function(n){var e=a(this),t=e.prototype,i=s(arguments,1),r=function(){var t=u(i,s(arguments));return this instanceof r?h(e,t.length,t):e.apply(n,t)};return o(t)&&(r.prototype=t),r}},function(n,e,t){"use strict";var i=t(1),r=t(368).start;i({target:"String",proto:!0,forced:t(370)},{padStart:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){},function(n,e,t){},function(n,e,t){t(1)({target:"Object",stat:!0},{setPrototypeOf:t(67)})},function(n,e,t){var i=t(1),r=t(16),a=t(36),o=t(210),l=t(168),s=t(8),c=t(9),d=t(34),u=t(3),p=r("Reflect","construct"),m=Object.prototype,h=[].push,f=u((function(){function n(){}return!(p((function(){}),[],n)instanceof n)})),g=!u((function(){p((function(){}))})),v=f||g;i({target:"Reflect",stat:!0,forced:v,sham:v},{construct:function(n,e){l(n),s(e);var t=arguments.length<3?n:l(arguments[2]);if(g&&!f)return p(n,e,t);if(n==t){switch(e.length){case 0:return new n;case 1:return new n(e[0]);case 2:return new n(e[0],e[1]);case 3:return new n(e[0],e[1],e[2]);case 4:return new n(e[0],e[1],e[2],e[3])}var i=[null];return a(h,i,e),new(a(o,n,i))}var r=t.prototype,u=d(c(r)?r:m),v=a(n,u,e);return c(v)?v:u}})},function(n,e,t){},function(n,e,t){},function(n,e,t){var i=t(269),r=t(274),a=t(345),o=t(353),l=t(362),s=t(232),c=a((function(n){var e=s(n);return l(e)&&(e=void 0),o(i(n,1,l,!0),r(e,2))}));n.exports=c},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var i=/["'&<>]/;n.exports=function(n){var e,t=""+n,r=i.exec(t);if(!r)return t;var a="",o=0,l=0;for(o=r.index;o<t.length;o++){switch(t.charCodeAt(o)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}l!==o&&(a+=t.substring(l,o)),l=o+1,a+=e}return l!==o?a+t.substring(l,o):a}},function(n,e,t){"use strict";
/**
 * @file Embedded JavaScript templating engine. {@link http://ejs.co}
 * @author Matthew Eernisse <mde@fleegix.org>
 * @author Tiancheng "Timothy" Gu <timothygu99@gmail.com>
 * @project EJS
 * @license {@link http://www.apache.org/licenses/LICENSE-2.0 Apache License, Version 2.0}
 */var i=t(382),r=t(383),a=t(384),o=!1,l=t(385).version,s=["delimiter","scope","context","debug","compileDebug","client","_with","rmWhitespace","strict","filename","async"],c=s.concat("cache"),d=/^\uFEFF/;function u(n,t){var r,a,o=t.views,l=/^[A-Za-z]+:\\|^\//.exec(n);if(l&&l.length)r=e.resolveInclude(n.replace(/^\/*/,""),t.root||"/",!0);else if(t.filename&&(a=e.resolveInclude(n,t.filename),i.existsSync(a)&&(r=a)),r||Array.isArray(o)&&o.some((function(t){return a=e.resolveInclude(n,t,!0),i.existsSync(a)}))&&(r=a),!r)throw new Error('Could not find the include file "'+t.escapeFunction(n)+'"');return r}function p(n,t){var i,r=n.filename,a=arguments.length>1;if(n.cache){if(!r)throw new Error("cache option requires a filename");if(i=e.cache.get(r))return i;a||(t=h(r).toString().replace(d,""))}else if(!a){if(!r)throw new Error("Internal EJS error: no file name or template provided");t=h(r).toString().replace(d,"")}return i=e.compile(t,n),n.cache&&e.cache.set(r,i),i}function m(n,t,i){var r;if(!i){if("function"==typeof e.promiseImpl)return new e.promiseImpl((function(e,i){try{e(r=p(n)(t))}catch(n){i(n)}}));throw new Error("Please provide a callback function")}try{r=p(n)(t)}catch(n){return i(n)}i(null,r)}function h(n){return e.fileLoader(n)}function f(n,e,t,i,r){var a=e.split("\n"),o=Math.max(i-3,0),l=Math.min(a.length,i+3),s=r(t),c=a.slice(o,l).map((function(n,e){var t=e+o+1;return(t==i?" >> ":"    ")+t+"| "+n})).join("\n");throw n.path=s,n.message=(s||"ejs")+":"+i+"\n"+c+"\n\n"+n.message,n}function g(n){return n.replace(/;(\s*$)/,"$1")}function v(n,t){t=t||{};var i={};this.templateText=n,this.mode=null,this.truncate=!1,this.currentLine=1,this.source="",this.dependencies=[],i.client=t.client||!1,i.escapeFunction=t.escape||t.escapeFunction||a.escapeXML,i.compileDebug=!1!==t.compileDebug,i.debug=!!t.debug,i.filename=t.filename,i.openDelimiter=t.openDelimiter||e.openDelimiter||"<",i.closeDelimiter=t.closeDelimiter||e.closeDelimiter||">",i.delimiter=t.delimiter||e.delimiter||"%",i.strict=t.strict||!1,i.context=t.context,i.cache=t.cache||!1,i.rmWhitespace=t.rmWhitespace,i.root=t.root,i.outputFunctionName=t.outputFunctionName,i.localsName=t.localsName||e.localsName||"locals",i.views=t.views,i.async=t.async,i.destructuredLocals=t.destructuredLocals,i.legacyInclude=void 0===t.legacyInclude||!!t.legacyInclude,i.strict?i._with=!1:i._with=void 0===t._with||t._with,this.opts=i,this.regex=this.createRegex()}e.cache=a.cache,e.fileLoader=i.readFileSync,e.localsName="locals",e.promiseImpl=new Function("return this;")().Promise,e.resolveInclude=function(n,e,t){var i=r.dirname,a=r.extname,o=(0,r.resolve)(t?e:i(e),n);return a(n)||(o+=".ejs"),o},e.compile=function(n,e){return e&&e.scope&&(o||(console.warn("`scope` option is deprecated and will be removed in EJS 3"),o=!0),e.context||(e.context=e.scope),delete e.scope),new v(n,e).compile()},e.render=function(n,e,t){var i=e||{},r=t||{};return 2==arguments.length&&a.shallowCopyFromList(r,i,s),p(r,n)(i)},e.renderFile=function(){var n,e,t,i=Array.prototype.slice.call(arguments),r=i.shift(),o={filename:r};return"function"==typeof arguments[arguments.length-1]&&(n=i.pop()),i.length?(e=i.shift(),i.length?a.shallowCopy(o,i.pop()):(e.settings&&(e.settings.views&&(o.views=e.settings.views),e.settings["view cache"]&&(o.cache=!0),(t=e.settings["view options"])&&a.shallowCopy(o,t)),a.shallowCopyFromList(o,e,c)),o.filename=r):e={},m(o,e,n)},e.Template=v,e.clearCache=function(){e.cache.reset()},v.modes={EVAL:"eval",ESCAPED:"escaped",RAW:"raw",COMMENT:"comment",LITERAL:"literal"},v.prototype={createRegex:function(){var n="(<%%|%%>|<%=|<%-|<%_|<%#|<%|%>|-%>|_%>)",e=a.escapeRegExpChars(this.opts.delimiter),t=a.escapeRegExpChars(this.opts.openDelimiter),i=a.escapeRegExpChars(this.opts.closeDelimiter);return n=n.replace(/%/g,e).replace(/</g,t).replace(/>/g,i),new RegExp(n)},compile:function(){var n,e,t,i=this.opts,o="",l="",s=i.escapeFunction;if(!this.source){if(this.generateSource(),o+='  var __output = "";\n  function __append(s) { if (s !== undefined && s !== null) __output += s }\n',i.outputFunctionName&&(o+="  var "+i.outputFunctionName+" = __append;\n"),i.destructuredLocals&&i.destructuredLocals.length){for(var c="  var __locals = ("+i.localsName+" || {}),\n",d=0;d<i.destructuredLocals.length;d++){var m=i.destructuredLocals[d];d>0&&(c+=",\n  "),c+=m+" = __locals."+m}o+=c+";\n"}!1!==i._with&&(o+="  with ("+i.localsName+" || {}) {\n",l+="  }\n"),l+="  return __output;\n",this.source=o+this.source+l}n=i.compileDebug?"var __line = 1\n  , __lines = "+JSON.stringify(this.templateText)+"\n  , __filename = "+(i.filename?JSON.stringify(i.filename):"undefined")+";\ntry {\n"+this.source+"} catch (e) {\n  rethrow(e, __lines, __filename, __line, escapeFn);\n}\n":this.source,i.client&&(n="escapeFn = escapeFn || "+s.toString()+";\n"+n,i.compileDebug&&(n="rethrow = rethrow || "+f.toString()+";\n"+n)),i.strict&&(n='"use strict";\n'+n),i.debug&&console.log(n),i.compileDebug&&i.filename&&(n=n+"\n//# sourceURL="+i.filename+"\n");try{if(i.async)try{t=new Function("return (async function(){}).constructor;")()}catch(n){throw n instanceof SyntaxError?new Error("This environment does not support async/await"):n}else t=Function;e=new t(i.localsName+", escapeFn, include, rethrow",n)}catch(n){throw n instanceof SyntaxError&&(i.filename&&(n.message+=" in "+i.filename),n.message+=" while compiling ejs\n\n",n.message+="If the above error is not helpful, you may want to try EJS-Lint:\n",n.message+="https://github.com/RyanZim/EJS-Lint",i.async||(n.message+="\n",n.message+="Or, if you meant to create an async function, pass `async: true` as an option.")),n}var h=i.client?e:function(n){return e.apply(i.context,[n||{},s,function(e,t){var r=a.shallowCopy({},n);return t&&(r=a.shallowCopy(r,t)),function(n,e){var t=a.shallowCopy({},e);return t.filename=u(n,t),p(t)}(e,i)(r)},f])};if(h.dependencies=this.dependencies,i.filename&&"function"==typeof Object.defineProperty){var g=i.filename,v=r.basename(g,r.extname(g));try{Object.defineProperty(h,"name",{value:v,writable:!1,enumerable:!1,configurable:!0})}catch(n){}}return h},generateSource:function(){var n=this.opts;n.rmWhitespace&&(this.templateText=this.templateText.replace(/[\r\n]+/g,"\n").replace(/^\s+|\s+$/gm,"")),this.templateText=this.templateText.replace(/[ \t]*<%_/gm,"<%_").replace(/_%>[ \t]*/gm,"_%>");var t=this,i=this.parseTemplateText(),r=this.opts.delimiter,o=this.opts.openDelimiter,l=this.opts.closeDelimiter;i&&i.length&&i.forEach((function(s,c){var p,m,f,g,y,b;if(0===s.indexOf(o+r)&&0!==s.indexOf(o+r+r)&&(m=i[c+2])!=r+l&&m!="-"+r+l&&m!="_"+r+l)throw new Error('Could not find matching close tag for "'+s+'".');if(n.legacyInclude&&(f=s.match(/^\s*include\s+(\S+)/))&&(p=i[c-1])&&(p==o+r||p==o+r+"-"||p==o+r+"_"))return g=a.shallowCopy({},t.opts),y=function(n,e){var t,i,r=a.shallowCopy({},e);i=h(t=u(n,r)).toString().replace(d,""),r.filename=t;var o=new v(i,r);return o.generateSource(),{source:o.source,filename:t,template:i}}(f[1],g),b=t.opts.compileDebug?"    ; (function(){\n      var __line = 1\n      , __lines = "+JSON.stringify(y.template)+"\n      , __filename = "+JSON.stringify(y.filename)+";\n      try {\n"+y.source+"      } catch (e) {\n        rethrow(e, __lines, __filename, __line, escapeFn);\n      }\n    ; }).call(this)\n":"    ; (function(){\n"+y.source+"    ; }).call(this)\n",t.source+=b,void t.dependencies.push(e.resolveInclude(f[1],g.filename));t.scanLine(s)}))},parseTemplateText:function(){for(var n,e=this.templateText,t=this.regex,i=t.exec(e),r=[];i;)0!==(n=i.index)&&(r.push(e.substring(0,n)),e=e.slice(n)),r.push(i[0]),e=e.slice(i[0].length),i=t.exec(e);return e&&r.push(e),r},_addOutput:function(n){if(this.truncate&&(n=n.replace(/^(?:\r\n|\r|\n)/,""),this.truncate=!1),!n)return n;n=(n=(n=(n=n.replace(/\\/g,"\\\\")).replace(/\n/g,"\\n")).replace(/\r/g,"\\r")).replace(/"/g,'\\"'),this.source+='    ; __append("'+n+'")\n'},scanLine:function(n){var e,t=this.opts.delimiter,i=this.opts.openDelimiter,r=this.opts.closeDelimiter;switch(e=n.split("\n").length-1,n){case i+t:case i+t+"_":this.mode=v.modes.EVAL;break;case i+t+"=":this.mode=v.modes.ESCAPED;break;case i+t+"-":this.mode=v.modes.RAW;break;case i+t+"#":this.mode=v.modes.COMMENT;break;case i+t+t:this.mode=v.modes.LITERAL,this.source+='    ; __append("'+n.replace(i+t+t,i+t)+'")\n';break;case t+t+r:this.mode=v.modes.LITERAL,this.source+='    ; __append("'+n.replace(t+t+r,t+r)+'")\n';break;case t+r:case"-"+t+r:case"_"+t+r:this.mode==v.modes.LITERAL&&this._addOutput(n),this.mode=null,this.truncate=0===n.indexOf("-")||0===n.indexOf("_");break;default:if(this.mode){switch(this.mode){case v.modes.EVAL:case v.modes.ESCAPED:case v.modes.RAW:n.lastIndexOf("//")>n.lastIndexOf("\n")&&(n+="\n")}switch(this.mode){case v.modes.EVAL:this.source+="    ; "+n+"\n";break;case v.modes.ESCAPED:this.source+="    ; __append(escapeFn("+g(n)+"))\n";break;case v.modes.RAW:this.source+="    ; __append("+g(n)+")\n";break;case v.modes.COMMENT:break;case v.modes.LITERAL:this._addOutput(n)}}else this._addOutput(n)}this.opts.compileDebug&&e&&(this.currentLine+=e,this.source+="    ; __line = "+this.currentLine+"\n")}},e.escapeXML=a.escapeXML,e.__express=e.renderFile,e.VERSION=l,e.name="ejs","undefined"!=typeof window&&(window.ejs=e)},function(n,e,t){"use strict";t.r(e);var i={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},r=(t(371),t(20)),a=Object(r.a)(i,(function(){var n=this.$createElement;return(this._self._c||n)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=a.exports},function(n,e,t){"use strict";t.r(e);t(28),t(4),t(29),t(44),t(23);var i={name:"CodeGroup",data:function(){return{codeTabs:[],activeCodeTabIndex:-1}},watch:{activeCodeTabIndex:function(n){this.codeTabs.forEach((function(n){n.elm.classList.remove("theme-code-block__active")})),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted:function(){var n=this;this.codeTabs=(this.$slots.default||[]).filter((function(n){return Boolean(n.componentOptions)})).map((function(e,t){return""===e.componentOptions.propsData.active&&(n.activeCodeTabIndex=t),{title:e.componentOptions.propsData.title,elm:e.elm}})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab:function(n){this.activeCodeTabIndex=n}}},r=(t(372),t(20)),a=Object(r.a)(i,(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"theme-code-group"},[t("div",{staticClass:"theme-code-group__nav"},[t("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(e,i){return t("li",{key:e.title,staticClass:"theme-code-group__li"},[t("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":i===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(i)}}},[n._v("\n            "+n._s(e.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?t("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=a.exports},function(n,e,t){"use strict";var i=t(7),r=t(0),a=t(2),o=t(105),l=t(14),s=t(10),c=t(149),d=t(35),u=t(83),p=t(158),m=t(3),h=t(58).f,f=t(38).f,g=t(13).f,v=t(367),y=t(152).trim,b=r.Number,x=b.prototype,_=r.TypeError,k=a("".slice),w=a("".charCodeAt),T=function(n){var e=p(n,"number");return"bigint"==typeof e?e:I(e)},I=function(n){var e,t,i,r,a,o,l,s,c=p(n,"number");if(u(c))throw _("Cannot convert a Symbol value to a number");if("string"==typeof c&&c.length>2)if(c=y(c),43===(e=w(c,0))||45===e){if(88===(t=w(c,2))||120===t)return NaN}else if(48===e){switch(w(c,1)){case 66:case 98:i=2,r=49;break;case 79:case 111:i=8,r=55;break;default:return+c}for(o=(a=k(c,2)).length,l=0;l<o;l++)if((s=w(a,l))<48||s>r)return NaN;return parseInt(a,i)}return+c};if(o("Number",!b(" 0o1")||!b("0b1")||b("+0x1"))){for(var z,S=function(n){var e=arguments.length<1?0:b(T(n)),t=this;return d(x,t)&&m((function(){v(t)}))?c(Object(e),t,S):e},j=i?h(b):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,isFinite,isInteger,isNaN,isSafeInteger,parseFloat,parseInt,fromString,range".split(","),P=0;j.length>P;P++)s(b,z=j[P])&&!s(S,z)&&g(S,z,f(b,z));S.prototype=x,x.constructor=S,l(r,"Number",S)}},function(n,e,t){var i=t(3),r=t(0).RegExp;n.exports=i((function(){var n=r(".","s");return!(n.dotAll&&n.exec("\n")&&"s"===n.flags)}))},function(n,e,t){"use strict";var i=t(162).IteratorPrototype,r=t(34),a=t(48),o=t(59),l=t(65),s=function(){return this};n.exports=function(n,e,t,c){var d=e+" Iterator";return n.prototype=r(i,{next:a(+!c,t)}),o(n,d,!1,!0),l[d]=s,n}},function(n,e,t){var i=t(14);n.exports=function(n,e,t){for(var r in e)i(n,r,e[r],t);return n}},function(n,e,t){"use strict";var i=t(16),r=t(13),a=t(6),o=t(7),l=a("species");n.exports=function(n){var e=i(n),t=r.f;o&&e&&!e[l]&&t(e,l,{configurable:!0,get:function(){return this}})}},function(n,e,t){var i=t(0),r=t(35),a=i.TypeError;n.exports=function(n,e){if(r(e,n))return n;throw a("Incorrect invocation")}},function(n,e,t){var i=t(3),r=t(0).RegExp;n.exports=i((function(){var n=r("(?<a>b)","g");return"b"!==n.exec("b").groups.a||"bc"!=="b".replace(n,"$<a>c")}))},function(n,e,t){"use strict";var i=t(1),r=t(115).includes,a=t(141);i({target:"Array",proto:!0},{includes:function(n){return r(this,n,arguments.length>1?arguments[1]:void 0)}}),a("includes")},function(n,e,t){"use strict";var i=t(1),r=t(2),a=t(179),o=t(18),l=t(12),s=t(180),c=r("".indexOf);i({target:"String",proto:!0,forced:!s("includes")},{includes:function(n){return!!~c(l(o(this)),l(a(n)),arguments.length>1?arguments[1]:void 0)}})},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){"use strict";var i=t(1),r=t(152).trim;i({target:"String",proto:!0,forced:t(365)("trim")},{trim:function(){return r(this)}})},function(n,e,t){var i=t(125),r=Math.floor,a=function(n,e){var t=n.length,s=r(t/2);return t<8?o(n,e):l(n,a(i(n,0,s),e),a(i(n,s),e),e)},o=function(n,e){for(var t,i,r=n.length,a=1;a<r;){for(i=a,t=n[a];i&&e(n[i-1],t)>0;)n[i]=n[--i];i!==a++&&(n[i]=t)}return n},l=function(n,e,t,i){for(var r=e.length,a=t.length,o=0,l=0;o<r||l<a;)n[o+l]=o<r&&l<a?i(e[o],t[l])<=0?e[o++]:t[l++]:o<r?e[o++]:t[l++];return n};n.exports=a},function(n,e,t){var i=t(0),r=t(7),a=t(107).MISSED_STICKY,o=t(25),l=t(13).f,s=t(39).get,c=RegExp.prototype,d=i.TypeError;r&&a&&l(c,"sticky",{configurable:!0,get:function(){if(this!==c){if("RegExp"===o(this))return!!s(this).sticky;throw d("Incompatible receiver, RegExp required")}}})},function(n,e,t){n.exports=t(388)},function(n,e,t){var i=t(0),r=t(11),a=t(5),o=t(9),l=i.TypeError;n.exports=function(n,e){var t,i;if("string"===e&&a(t=n.toString)&&!o(i=r(t,n)))return i;if(a(t=n.valueOf)&&!o(i=r(t,n)))return i;if("string"!==e&&a(t=n.toString)&&!o(i=r(t,n)))return i;throw l("Can't convert object to primitive value")}},function(n,e,t){var i=t(0),r=t(5),a=t(87),o=i.WeakMap;n.exports=r(o)&&/native code/.test(a(o))},function(n,e,t){var i=t(0),r=t(5),a=i.String,o=i.TypeError;n.exports=function(n){if("object"==typeof n||r(n))return n;throw o("Can't set "+a(n)+" as a prototype")}},function(n,e,t){"use strict";var i,r,a,o,l=t(1),s=t(26),c=t(0),d=t(16),u=t(11),p=t(164),m=t(14),h=t(226),f=t(67),g=t(59),v=t(227),y=t(37),b=t(5),x=t(9),_=t(228),k=t(87),w=t(241),T=t(167),I=t(124),z=t(169).set,S=t(242),j=t(171),P=t(245),E=t(172),q=t(246),O=t(247),A=t(39),$=t(105),C=t(6),L=t(248),M=t(89),D=t(51),R=C("species"),B="Promise",N=A.getterFor(B),U=A.set,F=A.getterFor(B),H=p&&p.prototype,G=p,K=H,W=c.TypeError,Q=c.document,V=c.process,J=E.f,X=J,Y=!!(Q&&Q.createEvent&&c.dispatchEvent),Z=b(c.PromiseRejectionEvent),nn=!1,en=$(B,(function(){var n=k(G),e=n!==String(G);if(!e&&66===D)return!0;if(s&&!K.finally)return!0;if(D>=51&&/native code/.test(n))return!1;var t=new G((function(n){n(1)})),i=function(n){n((function(){}),(function(){}))};return(t.constructor={})[R]=i,!(nn=t.then((function(){}))instanceof i)||!e&&L&&!Z})),tn=en||!T((function(n){G.all(n).catch((function(){}))})),rn=function(n){var e;return!(!x(n)||!b(e=n.then))&&e},an=function(n,e){var t,i,r,a=e.value,o=1==e.state,l=o?n.ok:n.fail,s=n.resolve,c=n.reject,d=n.domain;try{l?(o||(2===e.rejection&&dn(e),e.rejection=1),!0===l?t=a:(d&&d.enter(),t=l(a),d&&(d.exit(),r=!0)),t===n.promise?c(W("Promise-chain cycle")):(i=rn(t))?u(i,t,s,c):s(t)):c(a)}catch(n){d&&!r&&d.exit(),c(n)}},on=function(n,e){n.notified||(n.notified=!0,S((function(){for(var t,i=n.reactions;t=i.get();)an(t,n);n.notified=!1,e&&!n.rejection&&sn(n)})))},ln=function(n,e,t){var i,r;Y?((i=Q.createEvent("Event")).promise=e,i.reason=t,i.initEvent(n,!1,!0),c.dispatchEvent(i)):i={promise:e,reason:t},!Z&&(r=c["on"+n])?r(i):"unhandledrejection"===n&&P("Unhandled promise rejection",t)},sn=function(n){u(z,c,(function(){var e,t=n.facade,i=n.value;if(cn(n)&&(e=q((function(){M?V.emit("unhandledRejection",i,t):ln("unhandledrejection",t,i)})),n.rejection=M||cn(n)?2:1,e.error))throw e.value}))},cn=function(n){return 1!==n.rejection&&!n.parent},dn=function(n){u(z,c,(function(){var e=n.facade;M?V.emit("rejectionHandled",e):ln("rejectionhandled",e,n.value)}))},un=function(n,e,t){return function(i){n(e,i,t)}},pn=function(n,e,t){n.done||(n.done=!0,t&&(n=t),n.value=e,n.state=2,on(n,!0))},mn=function(n,e,t){if(!n.done){n.done=!0,t&&(n=t);try{if(n.facade===e)throw W("Promise can't be resolved itself");var i=rn(e);i?S((function(){var t={done:!1};try{u(i,e,un(mn,t,n),un(pn,t,n))}catch(e){pn(t,e,n)}})):(n.value=e,n.state=1,on(n,!1))}catch(e){pn({done:!1},e,n)}}};if(en&&(K=(G=function(n){_(this,K),y(n),u(i,this);var e=N(this);try{n(un(mn,e),un(pn,e))}catch(n){pn(e,n)}}).prototype,(i=function(n){U(this,{type:B,done:!1,notified:!1,parent:!1,reactions:new O,rejection:!1,state:0,value:void 0})}).prototype=h(K,{then:function(n,e){var t=F(this),i=J(I(this,G));return t.parent=!0,i.ok=!b(n)||n,i.fail=b(e)&&e,i.domain=M?V.domain:void 0,0==t.state?t.reactions.add(i):S((function(){an(i,t)})),i.promise},catch:function(n){return this.then(void 0,n)}}),r=function(){var n=new i,e=N(n);this.promise=n,this.resolve=un(mn,e),this.reject=un(pn,e)},E.f=J=function(n){return n===G||n===a?new r(n):X(n)},!s&&b(p)&&H!==Object.prototype)){o=H.then,nn||(m(H,"then",(function(n,e){var t=this;return new G((function(n,e){u(o,t,n,e)})).then(n,e)}),{unsafe:!0}),m(H,"catch",K.catch,{unsafe:!0}));try{delete H.constructor}catch(n){}f&&f(H,K)}l({global:!0,wrap:!0,forced:en},{Promise:G}),g(G,B,!1,!0),v(B),a=d(B),l({target:B,stat:!0,forced:en},{reject:function(n){var e=J(this);return u(e.reject,void 0,n),e.promise}}),l({target:B,stat:!0,forced:s||en},{resolve:function(n){return j(s&&this===a?G:this,n)}}),l({target:B,stat:!0,forced:tn},{all:function(n){var e=this,t=J(e),i=t.resolve,r=t.reject,a=q((function(){var t=y(e.resolve),a=[],o=0,l=1;w(n,(function(n){var s=o++,c=!1;l++,u(t,e,n).then((function(n){c||(c=!0,a[s]=n,--l||i(a))}),r)})),--l||i(a)}));return a.error&&r(a.value),t.promise},race:function(n){var e=this,t=J(e),i=t.reject,r=q((function(){var r=y(e.resolve);w(n,(function(n){u(r,e,n).then(t.resolve,i)}))}));return r.error&&i(r.value),t.promise}})},function(n,e,t){var i=t(0),r=t(56),a=t(11),o=t(8),l=t(84),s=t(165),c=t(22),d=t(35),u=t(145),p=t(106),m=t(166),h=i.TypeError,f=function(n,e){this.stopped=n,this.result=e},g=f.prototype;n.exports=function(n,e,t){var i,v,y,b,x,_,k,w=t&&t.that,T=!(!t||!t.AS_ENTRIES),I=!(!t||!t.IS_ITERATOR),z=!(!t||!t.INTERRUPTED),S=r(e,w),j=function(n){return i&&m(i,"normal",n),new f(!0,n)},P=function(n){return T?(o(n),z?S(n[0],n[1],j):S(n[0],n[1])):z?S(n,j):S(n)};if(I)i=n;else{if(!(v=p(n)))throw h(l(n)+" is not iterable");if(s(v)){for(y=0,b=c(n);b>y;y++)if((x=P(n[y]))&&d(g,x))return x;return new f(!1)}i=u(n,v)}for(_=i.next;!(k=a(_,i)).done;){try{x=P(k.value)}catch(n){m(i,"throw",n)}if("object"==typeof x&&x&&d(g,x))return x}return new f(!1)}},function(n,e,t){var i,r,a,o,l,s,c,d,u=t(0),p=t(56),m=t(38).f,h=t(169).set,f=t(170),g=t(243),v=t(244),y=t(89),b=u.MutationObserver||u.WebKitMutationObserver,x=u.document,_=u.process,k=u.Promise,w=m(u,"queueMicrotask"),T=w&&w.value;T||(i=function(){var n,e;for(y&&(n=_.domain)&&n.exit();r;){e=r.fn,r=r.next;try{e()}catch(n){throw r?o():a=void 0,n}}a=void 0,n&&n.enter()},f||y||v||!b||!x?!g&&k&&k.resolve?((c=k.resolve(void 0)).constructor=k,d=p(c.then,c),o=function(){d(i)}):y?o=function(){_.nextTick(i)}:(h=p(h,u),o=function(){h(i)}):(l=!0,s=x.createTextNode(""),new b(i).observe(s,{characterData:!0}),o=function(){s.data=l=!l})),n.exports=T||function(n){var e={fn:n,next:void 0};a&&(a.next=e),r||(r=e,o()),a=e}},function(n,e,t){var i=t(32),r=t(0);n.exports=/ipad|iphone|ipod/i.test(i)&&void 0!==r.Pebble},function(n,e,t){var i=t(32);n.exports=/web0s(?!.*chrome)/i.test(i)},function(n,e,t){var i=t(0);n.exports=function(n,e){var t=i.console;t&&t.error&&(1==arguments.length?t.error(n):t.error(n,e))}},function(n,e){n.exports=function(n){try{return{error:!1,value:n()}}catch(n){return{error:!0,value:n}}}},function(n,e){var t=function(){this.head=null,this.tail=null};t.prototype={add:function(n){var e={item:n,next:null};this.head?this.tail.next=e:this.head=e,this.tail=e},get:function(){var n=this.head;if(n)return this.head=n.next,this.tail===n&&(this.tail=null),n.item}},n.exports=t},function(n,e){n.exports="object"==typeof window},function(n,e,t){var i=t(1),r=t(250);i({target:"Object",stat:!0,forced:Object.assign!==r},{assign:r})},function(n,e,t){"use strict";var i=t(7),r=t(2),a=t(11),o=t(3),l=t(85),s=t(121),c=t(118),d=t(15),u=t(62),p=Object.assign,m=Object.defineProperty,h=r([].concat);n.exports=!p||o((function(){if(i&&1!==p({b:1},p(m({},"a",{enumerable:!0,get:function(){m(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var n={},e={},t=Symbol();return n[t]=7,"abcdefghijklmnopqrst".split("").forEach((function(n){e[n]=n})),7!=p({},n)[t]||"abcdefghijklmnopqrst"!=l(p({},e)).join("")}))?function(n,e){for(var t=d(n),r=arguments.length,o=1,p=s.f,m=c.f;r>o;)for(var f,g=u(arguments[o++]),v=p?h(l(g),p(g)):l(g),y=v.length,b=0;y>b;)f=v[b++],i&&!a(m,g,f)||(t[f]=g[f]);return t}:p},function(n,e,t){"use strict";var i=t(1),r=t(26),a=t(164),o=t(3),l=t(16),s=t(5),c=t(124),d=t(171),u=t(14);if(i({target:"Promise",proto:!0,real:!0,forced:!!a&&o((function(){a.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(n){var e=c(this,l("Promise")),t=s(n);return this.then(t?function(t){return d(e,n()).then((function(){return t}))}:n,t?function(t){return d(e,n()).then((function(){throw t}))}:n)}}),!r&&s(a)){var p=l("Promise").prototype.finally;a.prototype.finally!==p&&u(a.prototype,"finally",p,{unsafe:!0})}},function(n,e,t){"use strict";var i=t(123),r=t(76);n.exports=i?{}.toString:function(){return"[object "+r(this)+"]"}},function(n,e,t){var i=t(0),r=t(60),a=t(88),o=t(9),l=t(6)("species"),s=i.Array;n.exports=function(n){var e;return r(n)&&(e=n.constructor,(a(e)&&(e===s||r(e.prototype))||o(e)&&null===(e=e[l]))&&(e=void 0)),void 0===e?s:e}},function(n,e,t){"use strict";var i=t(1),r=t(255).left,a=t(46),o=t(51),l=t(89);i({target:"Array",proto:!0,forced:!a("reduce")||!l&&o>79&&o<83},{reduce:function(n){var e=arguments.length;return r(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){var i=t(0),r=t(37),a=t(15),o=t(62),l=t(22),s=i.TypeError,c=function(n){return function(e,t,i,c){r(t);var d=a(e),u=o(d),p=l(d),m=n?p-1:0,h=n?-1:1;if(i<2)for(;;){if(m in u){c=u[m],m+=h;break}if(m+=h,n?m<0:p<=m)throw s("Reduce of empty array with no initial value")}for(;n?m>=0:p>m;m+=h)m in u&&(c=t(c,u[m],m,d));return c}};n.exports={left:c(!1),right:c(!0)}},function(n,e,t){var i=t(1),r=t(182),a=t(3),o=t(9),l=t(257).onFreeze,s=Object.freeze;i({target:"Object",stat:!0,forced:a((function(){s(1)})),sham:!r},{freeze:function(n){return s&&o(n)?s(l(n)):n}})},function(n,e,t){var i=t(1),r=t(2),a=t(64),o=t(9),l=t(10),s=t(13).f,c=t(58),d=t(183),u=t(258),p=t(80),m=t(182),h=!1,f=p("meta"),g=0,v=function(n){s(n,f,{value:{objectID:"O"+g++,weakData:{}}})},y=n.exports={enable:function(){y.enable=function(){},h=!0;var n=c.f,e=r([].splice),t={};t[f]=1,n(t).length&&(c.f=function(t){for(var i=n(t),r=0,a=i.length;r<a;r++)if(i[r]===f){e(i,r,1);break}return i},i({target:"Object",stat:!0,forced:!0},{getOwnPropertyNames:d.f}))},fastKey:function(n,e){if(!o(n))return"symbol"==typeof n?n:("string"==typeof n?"S":"P")+n;if(!l(n,f)){if(!u(n))return"F";if(!e)return"E";v(n)}return n[f].objectID},getWeakData:function(n,e){if(!l(n,f)){if(!u(n))return!0;if(!e)return!1;v(n)}return n[f].weakData},onFreeze:function(n){return m&&h&&u(n)&&!l(n,f)&&v(n),n}};a[f]=!0},function(n,e,t){var i=t(3),r=t(9),a=t(25),o=t(259),l=Object.isExtensible,s=i((function(){l(1)}));n.exports=s||o?function(n){return!!r(n)&&((!o||"ArrayBuffer"!=a(n))&&(!l||l(n)))}:l},function(n,e,t){var i=t(3);n.exports=i((function(){if("function"==typeof ArrayBuffer){var n=new ArrayBuffer(8);Object.isExtensible(n)&&Object.defineProperty(n,"a",{value:8})}}))},function(n,e,t){var i=t(0);n.exports=i},function(n,e,t){"use strict";var i=t(0),r=t(56),a=t(11),o=t(15),l=t(262),s=t(165),c=t(88),d=t(22),u=t(69),p=t(145),m=t(106),h=i.Array;n.exports=function(n){var e=o(n),t=c(this),i=arguments.length,f=i>1?arguments[1]:void 0,g=void 0!==f;g&&(f=r(f,i>2?arguments[2]:void 0));var v,y,b,x,_,k,w=m(e),T=0;if(!w||this==h&&s(w))for(v=d(e),y=t?new this(v):h(v);v>T;T++)k=g?f(e[T],T):e[T],u(y,T,k);else for(_=(x=p(e,w)).next,y=t?new this:[];!(b=a(_,x)).done;T++)k=g?l(x,f,[b.value,T],!0):b.value,u(y,T,k);return y.length=T,y}},function(n,e,t){var i=t(8),r=t(166);n.exports=function(n,e,t,a){try{return a?e(i(t)[0],t[1]):e(t)}catch(e){r(n,"throw",e)}}},function(n,e,t){"use strict";var i=t(16),r=t(10),a=t(27),o=t(35),l=t(67),s=t(119),c=t(149),d=t(187),u=t(264),p=t(265),m=t(266),h=t(26);n.exports=function(n,e,t,f){var g=f?2:1,v=n.split("."),y=v[v.length-1],b=i.apply(null,v);if(b){var x=b.prototype;if(!h&&r(x,"cause")&&delete x.cause,!t)return b;var _=i("Error"),k=e((function(n,e){var t=d(f?e:n,void 0),i=f?new b(n):new b;return void 0!==t&&a(i,"message",t),m&&a(i,"stack",p(i.stack,2)),this&&o(x,this)&&c(i,this,k),arguments.length>g&&u(i,arguments[g]),i}));if(k.prototype=x,"Error"!==y&&(l?l(k,_):s(k,_,{name:!0})),s(k,b),!h)try{x.name!==y&&a(x,"name",y),x.constructor=k}catch(n){}return k}}},function(n,e,t){var i=t(9),r=t(27);n.exports=function(n,e){i(e)&&"cause"in e&&r(n,"cause",e.cause)}},function(n,e,t){var i=t(2)("".replace),r=String(Error("zxcasd").stack),a=/\n\s*at [^:]*:[^\n]*/,o=a.test(r);n.exports=function(n,e){if(o&&"string"==typeof n)for(;e--;)n=i(n,a,"");return n}},function(n,e,t){var i=t(3),r=t(48);n.exports=!i((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",r(1,7)),7!==n.stack)}))},function(n,e,t){"use strict";var i=t(7),r=t(3),a=t(8),o=t(34),l=t(187),s=Error.prototype.toString,c=r((function(){if(i){var n=o(Object.defineProperty({},"name",{get:function(){return this===n}}));if("true"!==s.call(n))return!0}return"2: 1"!==s.call({message:1,name:2})||"Error"!==s.call({})}));n.exports=c?function(){var n=a(this),e=l(n.name,"Error"),t=l(n.message);return e?t?e+": "+t:e:t}:s},function(n,e,t){var i=t(2),r=t(15),a=Math.floor,o=i("".charAt),l=i("".replace),s=i("".slice),c=/\$([$&'`]|\d{1,2}|<[^>]*>)/g,d=/\$([$&'`]|\d{1,2})/g;n.exports=function(n,e,t,i,u,p){var m=t+n.length,h=i.length,f=d;return void 0!==u&&(u=r(u),f=c),l(p,f,(function(r,l){var c;switch(o(l,0)){case"$":return"$";case"&":return n;case"`":return s(e,0,t);case"'":return s(e,m);case"<":c=u[s(l,1,-1)];break;default:var d=+l;if(0===d)return r;if(d>h){var p=a(d/10);return 0===p?r:p<=h?void 0===i[p-1]?o(l,1):i[p-1]+o(l,1):r}c=i[d-1]}return void 0===c?"":c}))}},function(n,e,t){var i=t(190),r=t(270);n.exports=function n(e,t,a,o,l){var s=-1,c=e.length;for(a||(a=r),l||(l=[]);++s<c;){var d=e[s];t>0&&a(d)?t>1?n(d,t-1,a,o,l):i(l,d):o||(l[l.length]=d)}return l}},function(n,e,t){var i=t(71),r=t(128),a=t(31),o=i?i.isConcatSpreadable:void 0;n.exports=function(n){return a(n)||r(n)||!!(o&&n&&n[o])}},function(n,e,t){var i=t(61),r=t(49);n.exports=function(n){return r(n)&&"[object Arguments]"==i(n)}},function(n,e,t){var i=t(71),r=Object.prototype,a=r.hasOwnProperty,o=r.toString,l=i?i.toStringTag:void 0;n.exports=function(n){var e=a.call(n,l),t=n[l];try{n[l]=void 0;var i=!0}catch(n){}var r=o.call(n);return i&&(e?n[l]=t:delete n[l]),r}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var i=t(275),r=t(331),a=t(136),o=t(31),l=t(342);n.exports=function(n){return"function"==typeof n?n:null==n?a:"object"==typeof n?o(n)?r(n[0],n[1]):i(n):l(n)}},function(n,e,t){var i=t(276),r=t(330),a=t(207);n.exports=function(n){var e=r(n);return 1==e.length&&e[0][2]?a(e[0][0],e[0][1]):function(t){return t===n||i(t,n,e)}}},function(n,e,t){var i=t(192),r=t(196);n.exports=function(n,e,t,a){var o=t.length,l=o,s=!a;if(null==n)return!l;for(n=Object(n);o--;){var c=t[o];if(s&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++o<l;){var d=(c=t[o])[0],u=n[d],p=c[1];if(s&&c[2]){if(void 0===u&&!(d in n))return!1}else{var m=new i;if(a)var h=a(u,p,d,n,e,m);if(!(void 0===h?r(p,u,3,a,m):h))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var i=t(95),r=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=i(e,n);return!(t<0)&&(t==e.length-1?e.pop():r.call(e,t,1),--this.size,!0)}},function(n,e,t){var i=t(95);n.exports=function(n){var e=this.__data__,t=i(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var i=t(95);n.exports=function(n){return i(this.__data__,n)>-1}},function(n,e,t){var i=t(95);n.exports=function(n,e){var t=this.__data__,r=i(t,n);return r<0?(++this.size,t.push([n,e])):t[r][1]=e,this}},function(n,e,t){var i=t(94);n.exports=function(){this.__data__=new i,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var i=t(94),r=t(129),a=t(131);n.exports=function(n,e){var t=this.__data__;if(t instanceof i){var o=t.__data__;if(!r||o.length<199)return o.push([n,e]),this.size=++t.size,this;t=this.__data__=new a(o)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var i=t(194),r=t(288),a=t(130),o=t(195),l=/^\[object .+?Constructor\]$/,s=Function.prototype,c=Object.prototype,d=s.toString,u=c.hasOwnProperty,p=RegExp("^"+d.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!a(n)||r(n))&&(i(n)?p:l).test(o(n))}},function(n,e,t){var i,r=t(289),a=(i=/[^.]+$/.exec(r&&r.keys&&r.keys.IE_PROTO||""))?"Symbol(src)_1."+i:"";n.exports=function(n){return!!a&&a in n}},function(n,e,t){var i=t(33)["__core-js_shared__"];n.exports=i},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var i=t(292),r=t(94),a=t(129);n.exports=function(){this.size=0,this.__data__={hash:new i,map:new(a||r),string:new i}}},function(n,e,t){var i=t(293),r=t(294),a=t(295),o=t(296),l=t(297);function s(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var i=n[e];this.set(i[0],i[1])}}s.prototype.clear=i,s.prototype.delete=r,s.prototype.get=a,s.prototype.has=o,s.prototype.set=l,n.exports=s},function(n,e,t){var i=t(96);n.exports=function(){this.__data__=i?i(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var i=t(96),r=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(i){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return r.call(e,n)?e[n]:void 0}},function(n,e,t){var i=t(96),r=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return i?void 0!==e[n]:r.call(e,n)}},function(n,e,t){var i=t(96);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=i&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var i=t(97);n.exports=function(n){var e=i(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var i=t(97);n.exports=function(n){return i(this,n).get(n)}},function(n,e,t){var i=t(97);n.exports=function(n){return i(this,n).has(n)}},function(n,e,t){var i=t(97);n.exports=function(n,e){var t=i(this,n),r=t.size;return t.set(n,e),this.size+=t.size==r?0:1,this}},function(n,e,t){var i=t(192),r=t(197),a=t(307),o=t(310),l=t(326),s=t(31),c=t(201),d=t(203),u="[object Object]",p=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,h,f){var g=s(n),v=s(e),y=g?"[object Array]":l(n),b=v?"[object Array]":l(e),x=(y="[object Arguments]"==y?u:y)==u,_=(b="[object Arguments]"==b?u:b)==u,k=y==b;if(k&&c(n)){if(!c(e))return!1;g=!0,x=!1}if(k&&!x)return f||(f=new i),g||d(n)?r(n,e,t,m,h,f):a(n,e,y,t,m,h,f);if(!(1&t)){var w=x&&p.call(n,"__wrapped__"),T=_&&p.call(e,"__wrapped__");if(w||T){var I=w?n.value():n,z=T?e.value():e;return f||(f=new i),h(I,z,t,m,f)}}return!!k&&(f||(f=new i),o(n,e,t,m,h,f))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,i=null==n?0:n.length;++t<i;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var i=t(71),r=t(308),a=t(193),o=t(197),l=t(309),s=t(132),c=i?i.prototype:void 0,d=c?c.valueOf:void 0;n.exports=function(n,e,t,i,c,u,p){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!u(new r(n),new r(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return a(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=l;case"[object Set]":var h=1&i;if(m||(m=s),n.size!=e.size&&!h)return!1;var f=p.get(n);if(f)return f==e;i|=2,p.set(n,e);var g=o(m(n),m(e),i,c,u,p);return p.delete(n),g;case"[object Symbol]":if(d)return d.call(n)==d.call(e)}return!1}},function(n,e,t){var i=t(33).Uint8Array;n.exports=i},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,i){t[++e]=[i,n]})),t}},function(n,e,t){var i=t(311),r=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,a,o,l){var s=1&t,c=i(n),d=c.length;if(d!=i(e).length&&!s)return!1;for(var u=d;u--;){var p=c[u];if(!(s?p in e:r.call(e,p)))return!1}var m=l.get(n),h=l.get(e);if(m&&h)return m==e&&h==n;var f=!0;l.set(n,e),l.set(e,n);for(var g=s;++u<d;){var v=n[p=c[u]],y=e[p];if(a)var b=s?a(y,v,p,e,n,l):a(v,y,p,n,e,l);if(!(void 0===b?v===y||o(v,y,t,a,l):b)){f=!1;break}g||(g="constructor"==p)}if(f&&!g){var x=n.constructor,_=e.constructor;x==_||!("constructor"in n)||!("constructor"in e)||"function"==typeof x&&x instanceof x&&"function"==typeof _&&_ instanceof _||(f=!1)}return l.delete(n),l.delete(e),f}},function(n,e,t){var i=t(312),r=t(313),a=t(200);n.exports=function(n){return i(n,a,r)}},function(n,e,t){var i=t(190),r=t(31);n.exports=function(n,e,t){var a=e(n);return r(n)?a:i(a,t(n))}},function(n,e,t){var i=t(314),r=t(315),a=Object.prototype.propertyIsEnumerable,o=Object.getOwnPropertySymbols,l=o?function(n){return null==n?[]:(n=Object(n),i(o(n),(function(e){return a.call(n,e)})))}:r;n.exports=l},function(n,e){n.exports=function(n,e){for(var t=-1,i=null==n?0:n.length,r=0,a=[];++t<i;){var o=n[t];e(o,t,n)&&(a[r++]=o)}return a}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var i=t(317),r=t(128),a=t(31),o=t(201),l=t(202),s=t(203),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=a(n),d=!t&&r(n),u=!t&&!d&&o(n),p=!t&&!d&&!u&&s(n),m=t||d||u||p,h=m?i(n.length,String):[],f=h.length;for(var g in n)!e&&!c.call(n,g)||m&&("length"==g||u&&("offset"==g||"parent"==g)||p&&("buffer"==g||"byteLength"==g||"byteOffset"==g)||l(g,f))||h.push(g);return h}},function(n,e){n.exports=function(n,e){for(var t=-1,i=Array(n);++t<n;)i[t]=e(t);return i}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var i=t(61),r=t(133),a=t(49),o={};o["[object Float32Array]"]=o["[object Float64Array]"]=o["[object Int8Array]"]=o["[object Int16Array]"]=o["[object Int32Array]"]=o["[object Uint8Array]"]=o["[object Uint8ClampedArray]"]=o["[object Uint16Array]"]=o["[object Uint32Array]"]=!0,o["[object Arguments]"]=o["[object Array]"]=o["[object ArrayBuffer]"]=o["[object Boolean]"]=o["[object DataView]"]=o["[object Date]"]=o["[object Error]"]=o["[object Function]"]=o["[object Map]"]=o["[object Number]"]=o["[object Object]"]=o["[object RegExp]"]=o["[object Set]"]=o["[object String]"]=o["[object WeakMap]"]=!1,n.exports=function(n){return a(n)&&r(n.length)&&!!o[i(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var i=t(191),r=e&&!e.nodeType&&e,a=r&&"object"==typeof n&&n&&!n.nodeType&&n,o=a&&a.exports===r&&i.process,l=function(){try{var n=a&&a.require&&a.require("util").types;return n||o&&o.binding&&o.binding("util")}catch(n){}}();n.exports=l}).call(this,t(150)(n))},function(n,e,t){var i=t(323),r=t(324),a=Object.prototype.hasOwnProperty;n.exports=function(n){if(!i(n))return r(n);var e=[];for(var t in Object(n))a.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var i=t(325)(Object.keys,Object);n.exports=i},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var i=t(327),r=t(129),a=t(328),o=t(205),l=t(329),s=t(61),c=t(195),d=c(i),u=c(r),p=c(a),m=c(o),h=c(l),f=s;(i&&"[object DataView]"!=f(new i(new ArrayBuffer(1)))||r&&"[object Map]"!=f(new r)||a&&"[object Promise]"!=f(a.resolve())||o&&"[object Set]"!=f(new o)||l&&"[object WeakMap]"!=f(new l))&&(f=function(n){var e=s(n),t="[object Object]"==e?n.constructor:void 0,i=t?c(t):"";if(i)switch(i){case d:return"[object DataView]";case u:return"[object Map]";case p:return"[object Promise]";case m:return"[object Set]";case h:return"[object WeakMap]"}return e}),n.exports=f},function(n,e,t){var i=t(41)(t(33),"DataView");n.exports=i},function(n,e,t){var i=t(41)(t(33),"Promise");n.exports=i},function(n,e,t){var i=t(41)(t(33),"WeakMap");n.exports=i},function(n,e,t){var i=t(206),r=t(200);n.exports=function(n){for(var e=r(n),t=e.length;t--;){var a=e[t],o=n[a];e[t]=[a,o,i(o)]}return e}},function(n,e,t){var i=t(196),r=t(332),a=t(339),o=t(134),l=t(206),s=t(207),c=t(98);n.exports=function(n,e){return o(n)&&l(e)?s(c(n),e):function(t){var o=r(t,n);return void 0===o&&o===e?a(t,n):i(e,o,3)}}},function(n,e,t){var i=t(208);n.exports=function(n,e,t){var r=null==n?void 0:i(n,e);return void 0===r?t:r}},function(n,e,t){var i=t(334),r=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,a=/\\(\\)?/g,o=i((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(r,(function(n,t,i,r){e.push(i?r.replace(a,"$1"):t||n)})),e}));n.exports=o},function(n,e,t){var i=t(335);n.exports=function(n){var e=i(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var i=t(131);function r(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var i=arguments,r=e?e.apply(this,i):i[0],a=t.cache;if(a.has(r))return a.get(r);var o=n.apply(this,i);return t.cache=a.set(r,o)||a,o};return t.cache=new(r.Cache||i),t}r.Cache=i,n.exports=r},function(n,e,t){var i=t(337);n.exports=function(n){return null==n?"":i(n)}},function(n,e,t){var i=t(71),r=t(338),a=t(31),o=t(135),l=i?i.prototype:void 0,s=l?l.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(a(e))return r(e,n)+"";if(o(e))return s?s.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,i=null==n?0:n.length,r=Array(i);++t<i;)r[t]=e(n[t],t,n);return r}},function(n,e,t){var i=t(340),r=t(341);n.exports=function(n,e){return null!=n&&r(n,e,i)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var i=t(209),r=t(128),a=t(31),o=t(202),l=t(133),s=t(98);n.exports=function(n,e,t){for(var c=-1,d=(e=i(e,n)).length,u=!1;++c<d;){var p=s(e[c]);if(!(u=null!=n&&t(n,p)))break;n=n[p]}return u||++c!=d?u:!!(d=null==n?0:n.length)&&l(d)&&o(p,d)&&(a(n)||r(n))}},function(n,e,t){var i=t(343),r=t(344),a=t(134),o=t(98);n.exports=function(n){return a(n)?i(o(n)):r(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var i=t(208);n.exports=function(n){return function(e){return i(e,n)}}},function(n,e,t){var i=t(136),r=t(346),a=t(348);n.exports=function(n,e){return a(r(n,e,i),n+"")}},function(n,e,t){var i=t(347),r=Math.max;n.exports=function(n,e,t){return e=r(void 0===e?n.length-1:e,0),function(){for(var a=arguments,o=-1,l=r(a.length-e,0),s=Array(l);++o<l;)s[o]=a[e+o];o=-1;for(var c=Array(e+1);++o<e;)c[o]=a[o];return c[e]=t(s),i(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var i=t(349),r=t(352)(i);n.exports=r},function(n,e,t){var i=t(350),r=t(351),a=t(136),o=r?function(n,e){return r(n,"toString",{configurable:!0,enumerable:!1,value:i(e),writable:!0})}:a;n.exports=o},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var i=t(41),r=function(){try{var n=i(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=r},function(n,e){var t=Date.now;n.exports=function(n){var e=0,i=0;return function(){var r=t(),a=16-(r-i);if(i=r,a>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var i=t(198),r=t(354),a=t(359),o=t(199),l=t(360),s=t(132);n.exports=function(n,e,t){var c=-1,d=r,u=n.length,p=!0,m=[],h=m;if(t)p=!1,d=a;else if(u>=200){var f=e?null:l(n);if(f)return s(f);p=!1,d=o,h=new i}else h=e?[]:m;n:for(;++c<u;){var g=n[c],v=e?e(g):g;if(g=t||0!==g?g:0,p&&v==v){for(var y=h.length;y--;)if(h[y]===v)continue n;e&&h.push(v),m.push(g)}else d(h,v,t)||(h!==m&&h.push(v),m.push(g))}return m}},function(n,e,t){var i=t(355);n.exports=function(n,e){return!!(null==n?0:n.length)&&i(n,e,0)>-1}},function(n,e,t){var i=t(356),r=t(357),a=t(358);n.exports=function(n,e,t){return e==e?a(n,e,t):i(n,r,t)}},function(n,e){n.exports=function(n,e,t,i){for(var r=n.length,a=t+(i?1:-1);i?a--:++a<r;)if(e(n[a],a,n))return a;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var i=t-1,r=n.length;++i<r;)if(n[i]===e)return i;return-1}},function(n,e){n.exports=function(n,e,t){for(var i=-1,r=null==n?0:n.length;++i<r;)if(t(e,n[i]))return!0;return!1}},function(n,e,t){var i=t(205),r=t(361),a=t(132),o=i&&1/a(new i([,-0]))[1]==1/0?function(n){return new i(n)}:r;n.exports=o},function(n,e){n.exports=function(){}},function(n,e,t){var i=t(204),r=t(49);n.exports=function(n){return r(n)&&i(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){var i=t(66).PROPER,r=t(3),a=t(153);n.exports=function(n){return r((function(){return!!a[n]()||"​᠎"!=="​᠎"[n]()||i&&a[n].name!==n}))}},function(n,e,t){var i=t(1),r=t(210);i({target:"Function",proto:!0,forced:Function.bind!==r},{bind:r})},function(n,e,t){var i=t(2);n.exports=i(1..valueOf)},function(n,e,t){var i=t(2),r=t(52),a=t(12),o=t(369),l=t(18),s=i(o),c=i("".slice),d=Math.ceil,u=function(n){return function(e,t,i){var o,u,p=a(l(e)),m=r(t),h=p.length,f=void 0===i?" ":a(i);return m<=h||""==f?p:((u=s(f,d((o=m-h)/f.length))).length>o&&(u=c(u,0,o)),n?p+u:u+p)}};n.exports={start:u(!1),end:u(!0)}},function(n,e,t){"use strict";var i=t(0),r=t(55),a=t(12),o=t(18),l=i.RangeError;n.exports=function(n){var e=a(o(this)),t="",i=r(n);if(i<0||i==1/0)throw l("Wrong number of repetitions");for(;i>0;(i>>>=1)&&(e+=e))1&i&&(t+=e);return t}},function(n,e,t){var i=t(32);n.exports=/Version\/10(?:\.\d+){1,2}(?: [\w./]+)?(?: Mobile\/\w+)? Safari\//.test(i)},function(n,e,t){"use strict";t(212)},function(n,e,t){"use strict";t(213)},function(n,e,t){"use strict";var i=t(1),r=t(2),a=t(37),o=t(15),l=t(22),s=t(12),c=t(3),d=t(234),u=t(46),p=t(374),m=t(375),h=t(51),f=t(376),g=[],v=r(g.sort),y=r(g.push),b=c((function(){g.sort(void 0)})),x=c((function(){g.sort(null)})),_=u("sort"),k=!c((function(){if(h)return h<70;if(!(p&&p>3)){if(m)return!0;if(f)return f<603;var n,e,t,i,r="";for(n=65;n<76;n++){switch(e=String.fromCharCode(n),n){case 66:case 69:case 70:case 72:t=3;break;case 68:case 71:t=4;break;default:t=2}for(i=0;i<47;i++)g.push({k:e+i,v:t})}for(g.sort((function(n,e){return e.v-n.v})),i=0;i<g.length;i++)e=g[i].k.charAt(0),r.charAt(r.length-1)!==e&&(r+=e);return"DGBEFHACIJK"!==r}}));i({target:"Array",proto:!0,forced:b||!x||!_||!k},{sort:function(n){void 0!==n&&a(n);var e=o(this);if(k)return void 0===n?v(e):v(e,n);var t,i,r=[],c=l(e);for(i=0;i<c;i++)i in e&&y(r,e[i]);for(d(r,function(n){return function(e,t){return void 0===t?-1:void 0===e?1:void 0!==n?+n(e,t)||0:s(e)>s(t)?1:-1}}(n)),t=r.length,i=0;i<t;)e[i]=r[i++];for(;i<c;)delete e[i++];return e}})},function(n,e,t){var i=t(32).match(/firefox\/(\d+)/i);n.exports=!!i&&+i[1]},function(n,e,t){var i=t(32);n.exports=/MSIE|Trident/.test(i)},function(n,e,t){var i=t(32).match(/AppleWebKit\/(\d+)\./);n.exports=!!i&&+i[1]},function(n,e,t){},function(n,e,t){},function(n,e,t){var i=t(1),r=t(3),a=t(19),o=t(38).f,l=t(7),s=r((function(){o(1)}));i({target:"Object",stat:!0,forced:!l||s,sham:!l},{getOwnPropertyDescriptor:function(n,e){return o(a(n),e)}})},function(n,e,t){var i=t(1),r=t(7),a=t(114).f;i({target:"Object",stat:!0,forced:Object.defineProperties!==a,sham:!r},{defineProperties:a})},function(n,e,t){t(1)({target:"Reflect",stat:!0},{ownKeys:t(120)})},function(n,e){},function(n,e){function t(n,e){for(var t=0,i=n.length-1;i>=0;i--){var r=n[i];"."===r?n.splice(i,1):".."===r?(n.splice(i,1),t++):t&&(n.splice(i,1),t--)}if(e)for(;t--;t)n.unshift("..");return n}function i(n,e){if(n.filter)return n.filter(e);for(var t=[],i=0;i<n.length;i++)e(n[i],i,n)&&t.push(n[i]);return t}e.resolve=function(){for(var n="",e=!1,r=arguments.length-1;r>=-1&&!e;r--){var a=r>=0?arguments[r]:process.cwd();if("string"!=typeof a)throw new TypeError("Arguments to path.resolve must be strings");a&&(n=a+"/"+n,e="/"===a.charAt(0))}return(e?"/":"")+(n=t(i(n.split("/"),(function(n){return!!n})),!e).join("/"))||"."},e.normalize=function(n){var a=e.isAbsolute(n),o="/"===r(n,-1);return(n=t(i(n.split("/"),(function(n){return!!n})),!a).join("/"))||a||(n="."),n&&o&&(n+="/"),(a?"/":"")+n},e.isAbsolute=function(n){return"/"===n.charAt(0)},e.join=function(){var n=Array.prototype.slice.call(arguments,0);return e.normalize(i(n,(function(n,e){if("string"!=typeof n)throw new TypeError("Arguments to path.join must be strings");return n})).join("/"))},e.relative=function(n,t){function i(n){for(var e=0;e<n.length&&""===n[e];e++);for(var t=n.length-1;t>=0&&""===n[t];t--);return e>t?[]:n.slice(e,t-e+1)}n=e.resolve(n).substr(1),t=e.resolve(t).substr(1);for(var r=i(n.split("/")),a=i(t.split("/")),o=Math.min(r.length,a.length),l=o,s=0;s<o;s++)if(r[s]!==a[s]){l=s;break}var c=[];for(s=l;s<r.length;s++)c.push("..");return(c=c.concat(a.slice(l))).join("/")},e.sep="/",e.delimiter=":",e.dirname=function(n){if("string"!=typeof n&&(n+=""),0===n.length)return".";for(var e=n.charCodeAt(0),t=47===e,i=-1,r=!0,a=n.length-1;a>=1;--a)if(47===(e=n.charCodeAt(a))){if(!r){i=a;break}}else r=!1;return-1===i?t?"/":".":t&&1===i?"/":n.slice(0,i)},e.basename=function(n,e){var t=function(n){"string"!=typeof n&&(n+="");var e,t=0,i=-1,r=!0;for(e=n.length-1;e>=0;--e)if(47===n.charCodeAt(e)){if(!r){t=e+1;break}}else-1===i&&(r=!1,i=e+1);return-1===i?"":n.slice(t,i)}(n);return e&&t.substr(-1*e.length)===e&&(t=t.substr(0,t.length-e.length)),t},e.extname=function(n){"string"!=typeof n&&(n+="");for(var e=-1,t=0,i=-1,r=!0,a=0,o=n.length-1;o>=0;--o){var l=n.charCodeAt(o);if(47!==l)-1===i&&(r=!1,i=o+1),46===l?-1===e?e=o:1!==a&&(a=1):-1!==e&&(a=-1);else if(!r){t=o+1;break}}return-1===e||-1===i||0===a||1===a&&e===i-1&&e===t+1?"":n.slice(e,i)};var r="b"==="ab".substr(-1)?function(n,e,t){return n.substr(e,t)}:function(n,e,t){return e<0&&(e=n.length+e),n.substr(e,t)}},function(n,e,t){"use strict";var i=/[|\\{}()[\]^$+*?.]/g;e.escapeRegExpChars=function(n){return n?String(n).replace(i,"\\$&"):""};var r={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&#34;","'":"&#39;"},a=/[&<>'"]/g;function o(n){return r[n]||n}e.escapeXML=function(n){return null==n?"":String(n).replace(a,o)},e.escapeXML.toString=function(){return Function.prototype.toString.call(this)+';\nvar _ENCODE_HTML_RULES = {\n      "&": "&amp;"\n    , "<": "&lt;"\n    , ">": "&gt;"\n    , \'"\': "&#34;"\n    , "\'": "&#39;"\n    }\n  , _MATCH_HTML = /[&<>\'"]/g;\nfunction encode_char(c) {\n  return _ENCODE_HTML_RULES[c] || c;\n};\n'},e.shallowCopy=function(n,e){for(var t in e=e||{})n[t]=e[t];return n},e.shallowCopyFromList=function(n,e,t){for(var i=0;i<t.length;i++){var r=t[i];void 0!==e[r]&&(n[r]=e[r])}return n},e.cache={_data:{},set:function(n,e){this._data[n]=e},get:function(n){return this._data[n]},remove:function(n){delete this._data[n]},reset:function(){this._data={}}}},function(n){n.exports=JSON.parse('{"name":"ejs","description":"Embedded JavaScript templates","keywords":["template","engine","ejs"],"version":"2.7.4","author":"Matthew Eernisse <mde@fleegix.org> (http://fleegix.org)","license":"Apache-2.0","main":"./lib/ejs.js","repository":{"type":"git","url":"git://github.com/mde/ejs.git"},"bugs":"https://github.com/mde/ejs/issues","homepage":"https://github.com/mde/ejs","dependencies":{},"devDependencies":{"browserify":"^13.1.1","eslint":"^4.14.0","git-directory-deploy":"^1.5.1","jake":"^10.3.1","jsdoc":"^3.4.0","lru-cache":"^4.0.1","mocha":"^5.0.5","uglify-js":"^3.3.16"},"engines":{"node":">=0.10.0"},"scripts":{"test":"mocha","postinstall":"node ./postinstall.js"}}')},function(n,e,t){"use strict";t(216)},function(n,e,t){"use strict";t(217)},function(n,e,t){"use strict";t.r(e);t(144),t(240),t(249),t(251);var i=t(90),r=(t(142),t(42),t(4),t(21),t(24),t(44),t(23),Object.freeze({}));function a(n){return null==n}function o(n){return null!=n}function l(n){return!0===n}function s(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return null!==n&&"object"==typeof n}var d=Object.prototype.toString;function u(n){return"[object Object]"===d.call(n)}function p(n){return"[object RegExp]"===d.call(n)}function m(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function h(n){return o(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function f(n){return null==n?"":Array.isArray(n)||u(n)&&n.toString===d?JSON.stringify(n,null,2):String(n)}function g(n){var e=parseFloat(n);return isNaN(e)?n:e}function v(n,e){for(var t=Object.create(null),i=n.split(","),r=0;r<i.length;r++)t[i[r]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}v("slot,component",!0);var y=v("key,ref,slot,slot-scope,is");function b(n,e){if(n.length){var t=n.indexOf(e);if(t>-1)return n.splice(t,1)}}var x=Object.prototype.hasOwnProperty;function _(n,e){return x.call(n,e)}function k(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var w=/-(\w)/g,T=k((function(n){return n.replace(w,(function(n,e){return e?e.toUpperCase():""}))})),I=k((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),z=/\B([A-Z])/g,S=k((function(n){return n.replace(z,"-$1").toLowerCase()}));var j=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var i=arguments.length;return i?i>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function P(n,e){e=e||0;for(var t=n.length-e,i=new Array(t);t--;)i[t]=n[t+e];return i}function E(n,e){for(var t in e)n[t]=e[t];return n}function q(n){for(var e={},t=0;t<n.length;t++)n[t]&&E(e,n[t]);return e}function O(n,e,t){}var A=function(n,e,t){return!1},$=function(n){return n};function C(n,e){if(n===e)return!0;var t=c(n),i=c(e);if(!t||!i)return!t&&!i&&String(n)===String(e);try{var r=Array.isArray(n),a=Array.isArray(e);if(r&&a)return n.length===e.length&&n.every((function(n,t){return C(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(r||a)return!1;var o=Object.keys(n),l=Object.keys(e);return o.length===l.length&&o.every((function(t){return C(n[t],e[t])}))}catch(n){return!1}}function L(n,e){for(var t=0;t<n.length;t++)if(C(n[t],e))return t;return-1}function M(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}var D=["component","directive","filter"],R=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],B={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:A,isReservedAttr:A,isUnknownElement:A,getTagNamespace:O,parsePlatformTagName:$,mustUseProp:A,async:!0,_lifecycleHooks:R},N=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function U(n,e,t,i){Object.defineProperty(n,e,{value:t,enumerable:!!i,writable:!0,configurable:!0})}var F=new RegExp("[^"+N.source+".$_\\d]");var H,G="__proto__"in{},K="undefined"!=typeof window,W="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,Q=W&&WXEnvironment.platform.toLowerCase(),V=K&&window.navigator.userAgent.toLowerCase(),J=V&&/msie|trident/.test(V),X=V&&V.indexOf("msie 9.0")>0,Y=V&&V.indexOf("edge/")>0,Z=(V&&V.indexOf("android"),V&&/iphone|ipad|ipod|ios/.test(V)||"ios"===Q),nn=(V&&/chrome\/\d+/.test(V),V&&/phantomjs/.test(V),V&&V.match(/firefox\/(\d+)/)),en={}.watch,tn=!1;if(K)try{var rn={};Object.defineProperty(rn,"passive",{get:function(){tn=!0}}),window.addEventListener("test-passive",null,rn)}catch(n){}var an=function(){return void 0===H&&(H=!K&&!W&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),H},on=K&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ln(n){return"function"==typeof n&&/native code/.test(n.toString())}var sn,cn="undefined"!=typeof Symbol&&ln(Symbol)&&"undefined"!=typeof Reflect&&ln(Reflect.ownKeys);sn="undefined"!=typeof Set&&ln(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var dn=O,un=0,pn=function(){this.id=un++,this.subs=[]};pn.prototype.addSub=function(n){this.subs.push(n)},pn.prototype.removeSub=function(n){b(this.subs,n)},pn.prototype.depend=function(){pn.target&&pn.target.addDep(this)},pn.prototype.notify=function(){var n=this.subs.slice();for(var e=0,t=n.length;e<t;e++)n[e].update()},pn.target=null;var mn=[];function hn(n){mn.push(n),pn.target=n}function fn(){mn.pop(),pn.target=mn[mn.length-1]}var gn=function(n,e,t,i,r,a,o,l){this.tag=n,this.data=e,this.children=t,this.text=i,this.elm=r,this.ns=void 0,this.context=a,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=o,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=l,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},vn={child:{configurable:!0}};vn.child.get=function(){return this.componentInstance},Object.defineProperties(gn.prototype,vn);var yn=function(n){void 0===n&&(n="");var e=new gn;return e.text=n,e.isComment=!0,e};function bn(n){return new gn(void 0,void 0,void 0,String(n))}function xn(n){var e=new gn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var _n=Array.prototype,kn=Object.create(_n);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=_n[n];U(kn,n,(function(){for(var t=[],i=arguments.length;i--;)t[i]=arguments[i];var r,a=e.apply(this,t),o=this.__ob__;switch(n){case"push":case"unshift":r=t;break;case"splice":r=t.slice(2)}return r&&o.observeArray(r),o.dep.notify(),a}))}));var wn=Object.getOwnPropertyNames(kn),Tn=!0;function In(n){Tn=n}var zn=function(n){this.value=n,this.dep=new pn,this.vmCount=0,U(n,"__ob__",this),Array.isArray(n)?(G?function(n,e){n.__proto__=e}(n,kn):function(n,e,t){for(var i=0,r=t.length;i<r;i++){var a=t[i];U(n,a,e[a])}}(n,kn,wn),this.observeArray(n)):this.walk(n)};function Sn(n,e){var t;if(c(n)&&!(n instanceof gn))return _(n,"__ob__")&&n.__ob__ instanceof zn?t=n.__ob__:Tn&&!an()&&(Array.isArray(n)||u(n))&&Object.isExtensible(n)&&!n._isVue&&(t=new zn(n)),e&&t&&t.vmCount++,t}function jn(n,e,t,i,r){var a=new pn,o=Object.getOwnPropertyDescriptor(n,e);if(!o||!1!==o.configurable){var l=o&&o.get,s=o&&o.set;l&&!s||2!==arguments.length||(t=n[e]);var c=!r&&Sn(t);Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=l?l.call(n):t;return pn.target&&(a.depend(),c&&(c.dep.depend(),Array.isArray(e)&&qn(e))),e},set:function(e){var i=l?l.call(n):t;e===i||e!=e&&i!=i||l&&!s||(s?s.call(n,e):t=e,c=!r&&Sn(e),a.notify())}})}}function Pn(n,e,t){if(Array.isArray(n)&&m(e))return n.length=Math.max(n.length,e),n.splice(e,1,t),t;if(e in n&&!(e in Object.prototype))return n[e]=t,t;var i=n.__ob__;return n._isVue||i&&i.vmCount?t:i?(jn(i.value,e,t),i.dep.notify(),t):(n[e]=t,t)}function En(n,e){if(Array.isArray(n)&&m(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||_(n,e)&&(delete n[e],t&&t.dep.notify())}}function qn(n){for(var e=void 0,t=0,i=n.length;t<i;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),Array.isArray(e)&&qn(e)}zn.prototype.walk=function(n){for(var e=Object.keys(n),t=0;t<e.length;t++)jn(n,e[t])},zn.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)Sn(n[e])};var On=B.optionMergeStrategies;function An(n,e){if(!e)return n;for(var t,i,r,a=cn?Reflect.ownKeys(e):Object.keys(e),o=0;o<a.length;o++)"__ob__"!==(t=a[o])&&(i=n[t],r=e[t],_(n,t)?i!==r&&u(i)&&u(r)&&An(i,r):Pn(n,t,r));return n}function $n(n,e,t){return t?function(){var i="function"==typeof e?e.call(t,t):e,r="function"==typeof n?n.call(t,t):n;return i?An(i,r):r}:e?n?function(){return An("function"==typeof e?e.call(this,this):e,"function"==typeof n?n.call(this,this):n)}:e:n}function Cn(n,e){var t=e?n?n.concat(e):Array.isArray(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function Ln(n,e,t,i){var r=Object.create(n||null);return e?E(r,e):r}On.data=function(n,e,t){return t?$n(n,e,t):e&&"function"!=typeof e?n:$n(n,e)},R.forEach((function(n){On[n]=Cn})),D.forEach((function(n){On[n+"s"]=Ln})),On.watch=function(n,e,t,i){if(n===en&&(n=void 0),e===en&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var r={};for(var a in E(r,n),e){var o=r[a],l=e[a];o&&!Array.isArray(o)&&(o=[o]),r[a]=o?o.concat(l):Array.isArray(l)?l:[l]}return r},On.props=On.methods=On.inject=On.computed=function(n,e,t,i){if(!n)return e;var r=Object.create(null);return E(r,n),e&&E(r,e),r},On.provide=$n;var Mn=function(n,e){return void 0===e?n:e};function Dn(n,e,t){if("function"==typeof e&&(e=e.options),function(n,e){var t=n.props;if(t){var i,r,a={};if(Array.isArray(t))for(i=t.length;i--;)"string"==typeof(r=t[i])&&(a[T(r)]={type:null});else if(u(t))for(var o in t)r=t[o],a[T(o)]=u(r)?r:{type:r};else 0;n.props=a}}(e),function(n,e){var t=n.inject;if(t){var i=n.inject={};if(Array.isArray(t))for(var r=0;r<t.length;r++)i[t[r]]={from:t[r]};else if(u(t))for(var a in t){var o=t[a];i[a]=u(o)?E({from:a},o):{from:o}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var i=e[t];"function"==typeof i&&(e[t]={bind:i,update:i})}}(e),!e._base&&(e.extends&&(n=Dn(n,e.extends,t)),e.mixins))for(var i=0,r=e.mixins.length;i<r;i++)n=Dn(n,e.mixins[i],t);var a,o={};for(a in n)l(a);for(a in e)_(n,a)||l(a);function l(i){var r=On[i]||Mn;o[i]=r(n[i],e[i],t,i)}return o}function Rn(n,e,t,i){if("string"==typeof t){var r=n[e];if(_(r,t))return r[t];var a=T(t);if(_(r,a))return r[a];var o=I(a);return _(r,o)?r[o]:r[t]||r[a]||r[o]}}function Bn(n,e,t,i){var r=e[n],a=!_(t,n),o=t[n],l=Hn(Boolean,r.type);if(l>-1)if(a&&!_(r,"default"))o=!1;else if(""===o||o===S(n)){var s=Hn(String,r.type);(s<0||l<s)&&(o=!0)}if(void 0===o){o=function(n,e,t){if(!_(e,"default"))return;var i=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return"function"==typeof i&&"Function"!==Un(e.type)?i.call(n):i}(i,r,n);var c=Tn;In(!0),Sn(o),In(c)}return o}var Nn=/^\s*function (\w+)/;function Un(n){var e=n&&n.toString().match(Nn);return e?e[1]:""}function Fn(n,e){return Un(n)===Un(e)}function Hn(n,e){if(!Array.isArray(e))return Fn(e,n)?0:-1;for(var t=0,i=e.length;t<i;t++)if(Fn(e[t],n))return t;return-1}function Gn(n,e,t){hn();try{if(e)for(var i=e;i=i.$parent;){var r=i.$options.errorCaptured;if(r)for(var a=0;a<r.length;a++)try{if(!1===r[a].call(i,n,e,t))return}catch(n){Wn(n,i,"errorCaptured hook")}}Wn(n,e,t)}finally{fn()}}function Kn(n,e,t,i,r){var a;try{(a=t?n.apply(e,t):n.call(e))&&!a._isVue&&h(a)&&!a._handled&&(a.catch((function(n){return Gn(n,i,r+" (Promise/async)")})),a._handled=!0)}catch(n){Gn(n,i,r)}return a}function Wn(n,e,t){if(B.errorHandler)try{return B.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Qn(e,null,"config.errorHandler")}Qn(n,e,t)}function Qn(n,e,t){if(!K&&!W||"undefined"==typeof console)throw n;console.error(n)}var Vn,Jn=!1,Xn=[],Yn=!1;function Zn(){Yn=!1;var n=Xn.slice(0);Xn.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&ln(Promise)){var ne=Promise.resolve();Vn=function(){ne.then(Zn),Z&&setTimeout(O)},Jn=!0}else if(J||"undefined"==typeof MutationObserver||!ln(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Vn="undefined"!=typeof setImmediate&&ln(setImmediate)?function(){setImmediate(Zn)}:function(){setTimeout(Zn,0)};else{var ee=1,te=new MutationObserver(Zn),ie=document.createTextNode(String(ee));te.observe(ie,{characterData:!0}),Vn=function(){ee=(ee+1)%2,ie.data=String(ee)},Jn=!0}function re(n,e){var t;if(Xn.push((function(){if(n)try{n.call(e)}catch(n){Gn(n,e,"nextTick")}else t&&t(e)})),Yn||(Yn=!0,Vn()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}var ae=new sn;function oe(n){!function n(e,t){var i,r,a=Array.isArray(e);if(!a&&!c(e)||Object.isFrozen(e)||e instanceof gn)return;if(e.__ob__){var o=e.__ob__.dep.id;if(t.has(o))return;t.add(o)}if(a)for(i=e.length;i--;)n(e[i],t);else for(r=Object.keys(e),i=r.length;i--;)n(e[r[i]],t)}(n,ae),ae.clear()}var le=k((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),i="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=i?n.slice(1):n,once:t,capture:i,passive:e}}));function se(n,e){function t(){var n=arguments,i=t.fns;if(!Array.isArray(i))return Kn(i,null,arguments,e,"v-on handler");for(var r=i.slice(),a=0;a<r.length;a++)Kn(r[a],null,n,e,"v-on handler")}return t.fns=n,t}function ce(n,e,t,i,r,o){var s,c,d,u;for(s in n)c=n[s],d=e[s],u=le(s),a(c)||(a(d)?(a(c.fns)&&(c=n[s]=se(c,o)),l(u.once)&&(c=n[s]=r(u.name,c,u.capture)),t(u.name,c,u.capture,u.passive,u.params)):c!==d&&(d.fns=c,n[s]=d));for(s in e)a(n[s])&&i((u=le(s)).name,e[s],u.capture)}function de(n,e,t){var i;n instanceof gn&&(n=n.data.hook||(n.data.hook={}));var r=n[e];function s(){t.apply(this,arguments),b(i.fns,s)}a(r)?i=se([s]):o(r.fns)&&l(r.merged)?(i=r).fns.push(s):i=se([r,s]),i.merged=!0,n[e]=i}function ue(n,e,t,i,r){if(o(e)){if(_(e,t))return n[t]=e[t],r||delete e[t],!0;if(_(e,i))return n[t]=e[i],r||delete e[i],!0}return!1}function pe(n){return s(n)?[bn(n)]:Array.isArray(n)?function n(e,t){var i,r,c,d,u=[];for(i=0;i<e.length;i++)a(r=e[i])||"boolean"==typeof r||(c=u.length-1,d=u[c],Array.isArray(r)?r.length>0&&(me((r=n(r,(t||"")+"_"+i))[0])&&me(d)&&(u[c]=bn(d.text+r[0].text),r.shift()),u.push.apply(u,r)):s(r)?me(d)?u[c]=bn(d.text+r):""!==r&&u.push(bn(r)):me(r)&&me(d)?u[c]=bn(d.text+r.text):(l(e._isVList)&&o(r.tag)&&a(r.key)&&o(t)&&(r.key="__vlist"+t+"_"+i+"__"),u.push(r)));return u}(n):void 0}function me(n){return o(n)&&o(n.text)&&!1===n.isComment}function he(n,e){if(n){for(var t=Object.create(null),i=cn?Reflect.ownKeys(n):Object.keys(n),r=0;r<i.length;r++){var a=i[r];if("__ob__"!==a){for(var o=n[a].from,l=e;l;){if(l._provided&&_(l._provided,o)){t[a]=l._provided[o];break}l=l.$parent}if(!l)if("default"in n[a]){var s=n[a].default;t[a]="function"==typeof s?s.call(e):s}else 0}}return t}}function fe(n,e){if(!n||!n.length)return{};for(var t={},i=0,r=n.length;i<r;i++){var a=n[i],o=a.data;if(o&&o.attrs&&o.attrs.slot&&delete o.attrs.slot,a.context!==e&&a.fnContext!==e||!o||null==o.slot)(t.default||(t.default=[])).push(a);else{var l=o.slot,s=t[l]||(t[l]=[]);"template"===a.tag?s.push.apply(s,a.children||[]):s.push(a)}}for(var c in t)t[c].every(ge)&&delete t[c];return t}function ge(n){return n.isComment&&!n.asyncFactory||" "===n.text}function ve(n){return n.isComment&&n.asyncFactory}function ye(n,e,t){var i,a=Object.keys(e).length>0,o=n?!!n.$stable:!a,l=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(o&&t&&t!==r&&l===t.$key&&!a&&!t.$hasNormal)return t;for(var s in i={},n)n[s]&&"$"!==s[0]&&(i[s]=be(e,s,n[s]))}else i={};for(var c in e)c in i||(i[c]=xe(e,c));return n&&Object.isExtensible(n)&&(n._normalized=i),U(i,"$stable",o),U(i,"$key",l),U(i,"$hasNormal",a),i}function be(n,e,t){var i=function(){var n=arguments.length?t.apply(null,arguments):t({}),e=(n=n&&"object"==typeof n&&!Array.isArray(n)?[n]:pe(n))&&n[0];return n&&(!e||1===n.length&&e.isComment&&!ve(e))?void 0:n};return t.proxy&&Object.defineProperty(n,e,{get:i,enumerable:!0,configurable:!0}),i}function xe(n,e){return function(){return n[e]}}function _e(n,e){var t,i,r,a,l;if(Array.isArray(n)||"string"==typeof n)for(t=new Array(n.length),i=0,r=n.length;i<r;i++)t[i]=e(n[i],i);else if("number"==typeof n)for(t=new Array(n),i=0;i<n;i++)t[i]=e(i+1,i);else if(c(n))if(cn&&n[Symbol.iterator]){t=[];for(var s=n[Symbol.iterator](),d=s.next();!d.done;)t.push(e(d.value,t.length)),d=s.next()}else for(a=Object.keys(n),t=new Array(a.length),i=0,r=a.length;i<r;i++)l=a[i],t[i]=e(n[l],l,i);return o(t)||(t=[]),t._isVList=!0,t}function ke(n,e,t,i){var r,a=this.$scopedSlots[n];a?(t=t||{},i&&(t=E(E({},i),t)),r=a(t)||("function"==typeof e?e():e)):r=this.$slots[n]||("function"==typeof e?e():e);var o=t&&t.slot;return o?this.$createElement("template",{slot:o},r):r}function we(n){return Rn(this.$options,"filters",n)||$}function Te(n,e){return Array.isArray(n)?-1===n.indexOf(e):n!==e}function Ie(n,e,t,i,r){var a=B.keyCodes[e]||t;return r&&i&&!B.keyCodes[e]?Te(r,i):a?Te(a,n):i?S(i)!==e:void 0===n}function ze(n,e,t,i,r){if(t)if(c(t)){var a;Array.isArray(t)&&(t=q(t));var o=function(o){if("class"===o||"style"===o||y(o))a=n;else{var l=n.attrs&&n.attrs.type;a=i||B.mustUseProp(e,l,o)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var s=T(o),c=S(o);s in a||c in a||(a[o]=t[o],r&&((n.on||(n.on={}))["update:"+o]=function(n){t[o]=n}))};for(var l in t)o(l)}else;return n}function Se(n,e){var t=this._staticTrees||(this._staticTrees=[]),i=t[n];return i&&!e||Pe(i=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,null,this),"__static__"+n,!1),i}function je(n,e,t){return Pe(n,"__once__"+e+(t?"_"+t:""),!0),n}function Pe(n,e,t){if(Array.isArray(n))for(var i=0;i<n.length;i++)n[i]&&"string"!=typeof n[i]&&Ee(n[i],e+"_"+i,t);else Ee(n,e,t)}function Ee(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function qe(n,e){if(e)if(u(e)){var t=n.on=n.on?E({},n.on):{};for(var i in e){var r=t[i],a=e[i];t[i]=r?[].concat(r,a):a}}else;return n}function Oe(n,e,t,i){e=e||{$stable:!t};for(var r=0;r<n.length;r++){var a=n[r];Array.isArray(a)?Oe(a,e,t):a&&(a.proxy&&(a.fn.proxy=!0),e[a.key]=a.fn)}return i&&(e.$key=i),e}function Ae(n,e){for(var t=0;t<e.length;t+=2){var i=e[t];"string"==typeof i&&i&&(n[e[t]]=e[t+1])}return n}function $e(n,e){return"string"==typeof n?e+n:n}function Ce(n){n._o=je,n._n=g,n._s=f,n._l=_e,n._t=ke,n._q=C,n._i=L,n._m=Se,n._f=we,n._k=Ie,n._b=ze,n._v=bn,n._e=yn,n._u=Oe,n._g=qe,n._d=Ae,n._p=$e}function Le(n,e,t,i,a){var o,s=this,c=a.options;_(i,"_uid")?(o=Object.create(i))._original=i:(o=i,i=i._original);var d=l(c._compiled),u=!d;this.data=n,this.props=e,this.children=t,this.parent=i,this.listeners=n.on||r,this.injections=he(c.inject,i),this.slots=function(){return s.$slots||ye(n.scopedSlots,s.$slots=fe(t,i)),s.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return ye(n.scopedSlots,this.slots())}}),d&&(this.$options=c,this.$slots=this.slots(),this.$scopedSlots=ye(n.scopedSlots,this.$slots)),c._scopeId?this._c=function(n,e,t,r){var a=Fe(o,n,e,t,r,u);return a&&!Array.isArray(a)&&(a.fnScopeId=c._scopeId,a.fnContext=i),a}:this._c=function(n,e,t,i){return Fe(o,n,e,t,i,u)}}function Me(n,e,t,i,r){var a=xn(n);return a.fnContext=t,a.fnOptions=i,e.slot&&((a.data||(a.data={})).slot=e.slot),a}function De(n,e){for(var t in e)n[T(t)]=e[t]}Ce(Le.prototype);var Re={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;Re.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},i=n.data.inlineTemplate;o(i)&&(t.render=i.render,t.staticRenderFns=i.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Ye)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,i,a){0;var o=i.data.scopedSlots,l=n.$scopedSlots,s=!!(o&&!o.$stable||l!==r&&!l.$stable||o&&n.$scopedSlots.$key!==o.$key||!o&&n.$scopedSlots.$key),c=!!(a||n.$options._renderChildren||s);n.$options._parentVnode=i,n.$vnode=i,n._vnode&&(n._vnode.parent=i);if(n.$options._renderChildren=a,n.$attrs=i.data.attrs||r,n.$listeners=t||r,e&&n.$options.props){In(!1);for(var d=n._props,u=n.$options._propKeys||[],p=0;p<u.length;p++){var m=u[p],h=n.$options.props;d[m]=Bn(m,h,e,n)}In(!0),n.$options.propsData=e}t=t||r;var f=n.$options._parentListeners;n.$options._parentListeners=t,Xe(n,t,f),c&&(n.$slots=fe(a,i.context),n.$forceUpdate());0}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,i=n.componentInstance;i._isMounted||(i._isMounted=!0,tt(i,"mounted")),n.data.keepAlive&&(t._isMounted?((e=i)._inactive=!1,rt.push(e)):et(i,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(t&&(e._directInactive=!0,nt(e)))return;if(!e._inactive){e._inactive=!0;for(var i=0;i<e.$children.length;i++)n(e.$children[i]);tt(e,"deactivated")}}(e,!0):e.$destroy())}},Be=Object.keys(Re);function Ne(n,e,t,i,s){if(!a(n)){var d=t.$options._base;if(c(n)&&(n=d.extend(n)),"function"==typeof n){var u;if(a(n.cid)&&void 0===(n=function(n,e){if(l(n.error)&&o(n.errorComp))return n.errorComp;if(o(n.resolved))return n.resolved;var t=Ge;t&&o(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t);if(l(n.loading)&&o(n.loadingComp))return n.loadingComp;if(t&&!o(n.owners)){var i=n.owners=[t],r=!0,s=null,d=null;t.$on("hook:destroyed",(function(){return b(i,t)}));var u=function(n){for(var e=0,t=i.length;e<t;e++)i[e].$forceUpdate();n&&(i.length=0,null!==s&&(clearTimeout(s),s=null),null!==d&&(clearTimeout(d),d=null))},p=M((function(t){n.resolved=Ke(t,e),r?i.length=0:u(!0)})),m=M((function(e){o(n.errorComp)&&(n.error=!0,u(!0))})),f=n(p,m);return c(f)&&(h(f)?a(n.resolved)&&f.then(p,m):h(f.component)&&(f.component.then(p,m),o(f.error)&&(n.errorComp=Ke(f.error,e)),o(f.loading)&&(n.loadingComp=Ke(f.loading,e),0===f.delay?n.loading=!0:s=setTimeout((function(){s=null,a(n.resolved)&&a(n.error)&&(n.loading=!0,u(!1))}),f.delay||200)),o(f.timeout)&&(d=setTimeout((function(){d=null,a(n.resolved)&&m(null)}),f.timeout)))),r=!1,n.loading?n.loadingComp:n.resolved}}(u=n,d)))return function(n,e,t,i,r){var a=yn();return a.asyncFactory=n,a.asyncMeta={data:e,context:t,children:i,tag:r},a}(u,e,t,i,s);e=e||{},Tt(n),o(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",i=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var r=e.on||(e.on={}),a=r[i],l=e.model.callback;o(a)?(Array.isArray(a)?-1===a.indexOf(l):a!==l)&&(r[i]=[l].concat(a)):r[i]=l}(n.options,e);var p=function(n,e,t){var i=e.options.props;if(!a(i)){var r={},l=n.attrs,s=n.props;if(o(l)||o(s))for(var c in i){var d=S(c);ue(r,s,c,d,!0)||ue(r,l,c,d,!1)}return r}}(e,n);if(l(n.options.functional))return function(n,e,t,i,a){var l=n.options,s={},c=l.props;if(o(c))for(var d in c)s[d]=Bn(d,c,e||r);else o(t.attrs)&&De(s,t.attrs),o(t.props)&&De(s,t.props);var u=new Le(t,s,a,i,n),p=l.render.call(null,u._c,u);if(p instanceof gn)return Me(p,t,u.parent,l,u);if(Array.isArray(p)){for(var m=pe(p)||[],h=new Array(m.length),f=0;f<m.length;f++)h[f]=Me(m[f],t,u.parent,l,u);return h}}(n,p,e,t,i);var m=e.on;if(e.on=e.nativeOn,l(n.options.abstract)){var f=e.slot;e={},f&&(e.slot=f)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<Be.length;t++){var i=Be[t],r=e[i],a=Re[i];r===a||r&&r._merged||(e[i]=r?Ue(a,r):a)}}(e);var g=n.options.name||s;return new gn("vue-component-"+n.cid+(g?"-"+g:""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:p,listeners:m,tag:s,children:i},u)}}}function Ue(n,e){var t=function(t,i){n(t,i),e(t,i)};return t._merged=!0,t}function Fe(n,e,t,i,r,d){return(Array.isArray(t)||s(t))&&(r=i,i=t,t=void 0),l(d)&&(r=2),function(n,e,t,i,r){if(o(t)&&o(t.__ob__))return yn();o(t)&&o(t.is)&&(e=t.is);if(!e)return yn();0;Array.isArray(i)&&"function"==typeof i[0]&&((t=t||{}).scopedSlots={default:i[0]},i.length=0);2===r?i=pe(i):1===r&&(i=function(n){for(var e=0;e<n.length;e++)if(Array.isArray(n[e]))return Array.prototype.concat.apply([],n);return n}(i));var s,d;if("string"==typeof e){var u;d=n.$vnode&&n.$vnode.ns||B.getTagNamespace(e),s=B.isReservedTag(e)?new gn(B.parsePlatformTagName(e),t,i,void 0,void 0,n):t&&t.pre||!o(u=Rn(n.$options,"components",e))?new gn(e,t,i,void 0,void 0,n):Ne(u,t,n,i,e)}else s=Ne(e,t,n,i);return Array.isArray(s)?s:o(s)?(o(d)&&function n(e,t,i){e.ns=t,"foreignObject"===e.tag&&(t=void 0,i=!0);if(o(e.children))for(var r=0,s=e.children.length;r<s;r++){var c=e.children[r];o(c.tag)&&(a(c.ns)||l(i)&&"svg"!==c.tag)&&n(c,t,i)}}(s,d),o(t)&&function(n){c(n.style)&&oe(n.style);c(n.class)&&oe(n.class)}(t),s):yn()}(n,e,t,i,r)}var He,Ge=null;function Ke(n,e){return(n.__esModule||cn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),c(n)?e.extend(n):n}function We(n){if(Array.isArray(n))for(var e=0;e<n.length;e++){var t=n[e];if(o(t)&&(o(t.componentOptions)||ve(t)))return t}}function Qe(n,e){He.$on(n,e)}function Ve(n,e){He.$off(n,e)}function Je(n,e){var t=He;return function i(){var r=e.apply(null,arguments);null!==r&&t.$off(n,i)}}function Xe(n,e,t){He=n,ce(e,t||{},Qe,Ve,Je,n),He=void 0}var Ye=null;function Ze(n){var e=Ye;return Ye=n,function(){Ye=e}}function nt(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function et(n,e){if(e){if(n._directInactive=!1,nt(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)et(n.$children[t]);tt(n,"activated")}}function tt(n,e){hn();var t=n.$options[e],i=e+" hook";if(t)for(var r=0,a=t.length;r<a;r++)Kn(t[r],n,null,n,i);n._hasHookEvent&&n.$emit("hook:"+e),fn()}var it=[],rt=[],at={},ot=!1,lt=!1,st=0;var ct=0,dt=Date.now;if(K&&!J){var ut=window.performance;ut&&"function"==typeof ut.now&&dt()>document.createEvent("Event").timeStamp&&(dt=function(){return ut.now()})}function pt(){var n,e;for(ct=dt(),lt=!0,it.sort((function(n,e){return n.id-e.id})),st=0;st<it.length;st++)(n=it[st]).before&&n.before(),e=n.id,at[e]=null,n.run();var t=rt.slice(),i=it.slice();st=it.length=rt.length=0,at={},ot=lt=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,et(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],i=t.vm;i._watcher===t&&i._isMounted&&!i._isDestroyed&&tt(i,"updated")}}(i),on&&B.devtools&&on.emit("flush")}var mt=0,ht=function(n,e,t,i,r){this.vm=n,r&&(n._watcher=this),n._watchers.push(this),i?(this.deep=!!i.deep,this.user=!!i.user,this.lazy=!!i.lazy,this.sync=!!i.sync,this.before=i.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++mt,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new sn,this.newDepIds=new sn,this.expression="","function"==typeof e?this.getter=e:(this.getter=function(n){if(!F.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=O)),this.value=this.lazy?void 0:this.get()};ht.prototype.get=function(){var n;hn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Gn(n,e,'getter for watcher "'+this.expression+'"')}finally{this.deep&&oe(n),fn(),this.cleanupDeps()}return n},ht.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},ht.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},ht.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(n){var e=n.id;if(null==at[e]){if(at[e]=!0,lt){for(var t=it.length-1;t>st&&it[t].id>n.id;)t--;it.splice(t+1,0,n)}else it.push(n);ot||(ot=!0,re(pt))}}(this)},ht.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||c(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'+this.expression+'"';Kn(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},ht.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},ht.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},ht.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||b(this.vm._watchers,this);for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1}};var ft={enumerable:!0,configurable:!0,get:O,set:O};function gt(n,e,t){ft.get=function(){return this[e][t]},ft.set=function(n){this[e][t]=n},Object.defineProperty(n,t,ft)}function vt(n){n._watchers=[];var e=n.$options;e.props&&function(n,e){var t=n.$options.propsData||{},i=n._props={},r=n.$options._propKeys=[];n.$parent&&In(!1);var a=function(a){r.push(a);var o=Bn(a,e,t,n);jn(i,a,o),a in n||gt(n,"_props",a)};for(var o in e)a(o);In(!0)}(n,e.props),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?O:j(e[t],n)}(n,e.methods),e.data?function(n){var e=n.$options.data;u(e=n._data="function"==typeof e?function(n,e){hn();try{return n.call(e,e)}catch(n){return Gn(n,e,"data()"),{}}finally{fn()}}(e,n):e||{})||(e={});var t=Object.keys(e),i=n.$options.props,r=(n.$options.methods,t.length);for(;r--;){var a=t[r];0,i&&_(i,a)||(o=void 0,36!==(o=(a+"").charCodeAt(0))&&95!==o&&gt(n,"_data",a))}var o;Sn(e,!0)}(n):Sn(n._data={},!0),e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),i=an();for(var r in e){var a=e[r],o="function"==typeof a?a:a.get;0,i||(t[r]=new ht(n,o||O,O,yt)),r in n||bt(n,r,a)}}(n,e.computed),e.watch&&e.watch!==en&&function(n,e){for(var t in e){var i=e[t];if(Array.isArray(i))for(var r=0;r<i.length;r++)kt(n,t,i[r]);else kt(n,t,i)}}(n,e.watch)}var yt={lazy:!0};function bt(n,e,t){var i=!an();"function"==typeof t?(ft.get=i?xt(e):_t(t),ft.set=O):(ft.get=t.get?i&&!1!==t.cache?xt(e):_t(t.get):O,ft.set=t.set||O),Object.defineProperty(n,e,ft)}function xt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),pn.target&&e.depend(),e.value}}function _t(n){return function(){return n.call(this,this)}}function kt(n,e,t,i){return u(t)&&(i=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,i)}var wt=0;function Tt(n){var e=n.options;if(n.super){var t=Tt(n.super);if(t!==n.superOptions){n.superOptions=t;var i=function(n){var e,t=n.options,i=n.sealedOptions;for(var r in t)t[r]!==i[r]&&(e||(e={}),e[r]=t[r]);return e}(n);i&&E(n.extendOptions,i),(e=n.options=Dn(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function It(n){this._init(n)}function zt(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,i=t.cid,r=n._Ctor||(n._Ctor={});if(r[i])return r[i];var a=n.name||t.options.name;var o=function(n){this._init(n)};return(o.prototype=Object.create(t.prototype)).constructor=o,o.cid=e++,o.options=Dn(t.options,n),o.super=t,o.options.props&&function(n){var e=n.options.props;for(var t in e)gt(n.prototype,"_props",t)}(o),o.options.computed&&function(n){var e=n.options.computed;for(var t in e)bt(n.prototype,t,e[t])}(o),o.extend=t.extend,o.mixin=t.mixin,o.use=t.use,D.forEach((function(n){o[n]=t[n]})),a&&(o.options.components[a]=o),o.superOptions=t.options,o.extendOptions=n,o.sealedOptions=E({},o.options),r[i]=o,o}}function St(n){return n&&(n.Ctor.options.name||n.tag)}function jt(n,e){return Array.isArray(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!p(n)&&n.test(e)}function Pt(n,e){var t=n.cache,i=n.keys,r=n._vnode;for(var a in t){var o=t[a];if(o){var l=o.name;l&&!e(l)&&Et(t,a,i,r)}}}function Et(n,e,t,i){var r=n[e];!r||i&&r.tag===i.tag||r.componentInstance.$destroy(),n[e]=null,b(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=wt++,e._isVue=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),i=e._parentVnode;t.parent=e.parent,t._parentVnode=i;var r=i.componentOptions;t.propsData=r.propsData,t._parentListeners=r.listeners,t._renderChildren=r.children,t._componentTag=r.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=Dn(Tt(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&Xe(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,i=t&&t.context;n.$slots=fe(e._renderChildren,i),n.$scopedSlots=r,n._c=function(e,t,i,r){return Fe(n,e,t,i,r,!1)},n.$createElement=function(e,t,i,r){return Fe(n,e,t,i,r,!0)};var a=t&&t.data;jn(n,"$attrs",a&&a.attrs||r,null,!0),jn(n,"$listeners",e._parentListeners||r,null,!0)}(e),tt(e,"beforeCreate"),function(n){var e=he(n.$options.inject,n);e&&(In(!1),Object.keys(e).forEach((function(t){jn(n,t,e[t])})),In(!0))}(e),vt(e),function(n){var e=n.$options.provide;e&&(n._provided="function"==typeof e?e.call(n):e)}(e),tt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(It),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=Pn,n.prototype.$delete=En,n.prototype.$watch=function(n,e,t){if(u(e))return kt(this,n,e,t);(t=t||{}).user=!0;var i=new ht(this,n,e,t);if(t.immediate){var r='callback for immediate watcher "'+i.expression+'"';hn(),Kn(e,this,[i.value],this,r),fn()}return function(){i.teardown()}}}(It),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var i=this;if(Array.isArray(n))for(var r=0,a=n.length;r<a;r++)i.$on(n[r],t);else(i._events[n]||(i._events[n]=[])).push(t),e.test(n)&&(i._hasHookEvent=!0);return i},n.prototype.$once=function(n,e){var t=this;function i(){t.$off(n,i),e.apply(t,arguments)}return i.fn=e,t.$on(n,i),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(Array.isArray(n)){for(var i=0,r=n.length;i<r;i++)t.$off(n[i],e);return t}var a,o=t._events[n];if(!o)return t;if(!e)return t._events[n]=null,t;for(var l=o.length;l--;)if((a=o[l])===e||a.fn===e){o.splice(l,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?P(t):t;for(var i=P(arguments,1),r='event handler for "'+n+'"',a=0,o=t.length;a<o;a++)Kn(t[a],e,i,e,r)}return e}}(It),function(n){n.prototype._update=function(n,e){var t=this,i=t.$el,r=t._vnode,a=Ze(t);t._vnode=n,t.$el=r?t.__patch__(r,n):t.__patch__(t.$el,n,e,!1),a(),i&&(i.__vue__=null),t.$el&&(t.$el.__vue__=t),t.$vnode&&t.$parent&&t.$vnode===t.$parent._vnode&&(t.$parent.$el=t.$el)},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){tt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||b(e.$children,n),n._watcher&&n._watcher.teardown();for(var t=n._watchers.length;t--;)n._watchers[t].teardown();n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),tt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(It),function(n){Ce(n.prototype),n.prototype.$nextTick=function(n){return re(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,i=t.render,r=t._parentVnode;r&&(e.$scopedSlots=ye(r.data.scopedSlots,e.$slots,e.$scopedSlots)),e.$vnode=r;try{Ge=e,n=i.call(e._renderProxy,e.$createElement)}catch(t){Gn(t,e,"render"),n=e._vnode}finally{Ge=null}return Array.isArray(n)&&1===n.length&&(n=n[0]),n instanceof gn||(n=yn()),n.parent=r,n}}(It);var qt=[String,RegExp,Array],Ot={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:qt,exclude:qt,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,i=this.keyToCache;if(t){var r=t.tag,a=t.componentInstance,o=t.componentOptions;n[i]={name:St(o),tag:r,componentInstance:a},e.push(i),this.max&&e.length>parseInt(this.max)&&Et(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Et(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){Pt(n,(function(n){return jt(e,n)}))})),this.$watch("exclude",(function(e){Pt(n,(function(n){return!jt(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=We(n),t=e&&e.componentOptions;if(t){var i=St(t),r=this.include,a=this.exclude;if(r&&(!i||!jt(r,i))||a&&i&&jt(a,i))return e;var o=this.cache,l=this.keys,s=null==e.key?t.Ctor.cid+(t.tag?"::"+t.tag:""):e.key;o[s]?(e.componentInstance=o[s].componentInstance,b(l,s),l.push(s)):(this.vnodeToCache=e,this.keyToCache=s),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return B}};Object.defineProperty(n,"config",e),n.util={warn:dn,extend:E,mergeOptions:Dn,defineReactive:jn},n.set=Pn,n.delete=En,n.nextTick=re,n.observable=function(n){return Sn(n),n},n.options=Object.create(null),D.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,E(n.options.components,Ot),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=P(arguments,1);return t.unshift(this),"function"==typeof n.install?n.install.apply(n,t):"function"==typeof n&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=Dn(this.options,n),this}}(n),zt(n),function(n){D.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&u(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&"function"==typeof t&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(It),Object.defineProperty(It.prototype,"$isServer",{get:an}),Object.defineProperty(It.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(It,"FunctionalRenderContext",{value:Le}),It.version="2.6.14";var At=v("style,class"),$t=v("input,textarea,option,select,progress"),Ct=v("contenteditable,draggable,spellcheck"),Lt=v("events,caret,typing,plaintext-only"),Mt=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),Dt="http://www.w3.org/1999/xlink",Rt=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},Bt=function(n){return Rt(n)?n.slice(6,n.length):""},Nt=function(n){return null==n||!1===n};function Ut(n){for(var e=n.data,t=n,i=n;o(i.componentInstance);)(i=i.componentInstance._vnode)&&i.data&&(e=Ft(i.data,e));for(;o(t=t.parent);)t&&t.data&&(e=Ft(e,t.data));return function(n,e){if(o(n)||o(e))return Ht(n,Gt(e));return""}(e.staticClass,e.class)}function Ft(n,e){return{staticClass:Ht(n.staticClass,e.staticClass),class:o(n.class)?[n.class,e.class]:e.class}}function Ht(n,e){return n?e?n+" "+e:n:e||""}function Gt(n){return Array.isArray(n)?function(n){for(var e,t="",i=0,r=n.length;i<r;i++)o(e=Gt(n[i]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):c(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var Kt={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Wt=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Qt=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Vt=function(n){return Wt(n)||Qt(n)};var Jt=Object.create(null);var Xt=v("text,number,password,search,email,tel,url");var Yt=Object.freeze({createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(Kt[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Zt={create:function(n,e){ni(e)},update:function(n,e){n.data.ref!==e.data.ref&&(ni(n,!0),ni(e))},destroy:function(n){ni(n,!0)}};function ni(n,e){var t=n.data.ref;if(o(t)){var i=n.context,r=n.componentInstance||n.elm,a=i.$refs;e?Array.isArray(a[t])?b(a[t],r):a[t]===r&&(a[t]=void 0):n.data.refInFor?Array.isArray(a[t])?a[t].indexOf(r)<0&&a[t].push(r):a[t]=[r]:a[t]=r}}var ei=new gn("",{},[]),ti=["create","activate","update","remove","destroy"];function ii(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&o(n.data)===o(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,i=o(t=n.data)&&o(t=t.attrs)&&t.type,r=o(t=e.data)&&o(t=t.attrs)&&t.type;return i===r||Xt(i)&&Xt(r)}(n,e)||l(n.isAsyncPlaceholder)&&a(e.asyncFactory.error))}function ri(n,e,t){var i,r,a={};for(i=e;i<=t;++i)o(r=n[i].key)&&(a[r]=i);return a}var ai={create:oi,update:oi,destroy:function(n){oi(n,ei)}};function oi(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,i,r,a=n===ei,o=e===ei,l=si(n.data.directives,n.context),s=si(e.data.directives,e.context),c=[],d=[];for(t in s)i=l[t],r=s[t],i?(r.oldValue=i.value,r.oldArg=i.arg,di(r,"update",e,n),r.def&&r.def.componentUpdated&&d.push(r)):(di(r,"bind",e,n),r.def&&r.def.inserted&&c.push(r));if(c.length){var u=function(){for(var t=0;t<c.length;t++)di(c[t],"inserted",e,n)};a?de(e,"insert",u):u()}d.length&&de(e,"postpatch",(function(){for(var t=0;t<d.length;t++)di(d[t],"componentUpdated",e,n)}));if(!a)for(t in l)s[t]||di(l[t],"unbind",n,n,o)}(n,e)}var li=Object.create(null);function si(n,e){var t,i,r=Object.create(null);if(!n)return r;for(t=0;t<n.length;t++)(i=n[t]).modifiers||(i.modifiers=li),r[ci(i)]=i,i.def=Rn(e.$options,"directives",i.name);return r}function ci(n){return n.rawName||n.name+"."+Object.keys(n.modifiers||{}).join(".")}function di(n,e,t,i,r){var a=n.def&&n.def[e];if(a)try{a(t.elm,n,t,i,r)}catch(i){Gn(i,t.context,"directive "+n.name+" "+e+" hook")}}var ui=[Zt,ai];function pi(n,e){var t=e.componentOptions;if(!(o(t)&&!1===t.Ctor.options.inheritAttrs||a(n.data.attrs)&&a(e.data.attrs))){var i,r,l=e.elm,s=n.data.attrs||{},c=e.data.attrs||{};for(i in o(c.__ob__)&&(c=e.data.attrs=E({},c)),c)r=c[i],s[i]!==r&&mi(l,i,r,e.data.pre);for(i in(J||Y)&&c.value!==s.value&&mi(l,"value",c.value),s)a(c[i])&&(Rt(i)?l.removeAttributeNS(Dt,Bt(i)):Ct(i)||l.removeAttribute(i))}}function mi(n,e,t,i){i||n.tagName.indexOf("-")>-1?hi(n,e,t):Mt(e)?Nt(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):Ct(e)?n.setAttribute(e,function(n,e){return Nt(e)||"false"===e?"false":"contenteditable"===n&&Lt(e)?e:"true"}(e,t)):Rt(e)?Nt(t)?n.removeAttributeNS(Dt,Bt(e)):n.setAttributeNS(Dt,e,t):hi(n,e,t)}function hi(n,e,t){if(Nt(t))n.removeAttribute(e);else{if(J&&!X&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var i=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",i)};n.addEventListener("input",i),n.__ieph=!0}n.setAttribute(e,t)}}var fi={create:pi,update:pi};function gi(n,e){var t=e.elm,i=e.data,r=n.data;if(!(a(i.staticClass)&&a(i.class)&&(a(r)||a(r.staticClass)&&a(r.class)))){var l=Ut(e),s=t._transitionClasses;o(s)&&(l=Ht(l,Gt(s))),l!==t._prevClass&&(t.setAttribute("class",l),t._prevClass=l)}}var vi,yi={create:gi,update:gi};function bi(n,e,t){var i=vi;return function r(){var a=e.apply(null,arguments);null!==a&&ki(n,r,t,i)}}var xi=Jn&&!(nn&&Number(nn[1])<=53);function _i(n,e,t,i){if(xi){var r=ct,a=e;e=a._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=r||n.timeStamp<=0||n.target.ownerDocument!==document)return a.apply(this,arguments)}}vi.addEventListener(n,e,tn?{capture:t,passive:i}:t)}function ki(n,e,t,i){(i||vi).removeEventListener(n,e._wrapper||e,t)}function wi(n,e){if(!a(n.data.on)||!a(e.data.on)){var t=e.data.on||{},i=n.data.on||{};vi=e.elm,function(n){if(o(n.__r)){var e=J?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}o(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),ce(t,i,_i,ki,bi,e.context),vi=void 0}}var Ti,Ii={create:wi,update:wi};function zi(n,e){if(!a(n.data.domProps)||!a(e.data.domProps)){var t,i,r=e.elm,l=n.data.domProps||{},s=e.data.domProps||{};for(t in o(s.__ob__)&&(s=e.data.domProps=E({},s)),l)t in s||(r[t]="");for(t in s){if(i=s[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),i===l[t])continue;1===r.childNodes.length&&r.removeChild(r.childNodes[0])}if("value"===t&&"PROGRESS"!==r.tagName){r._value=i;var c=a(i)?"":String(i);Si(r,c)&&(r.value=c)}else if("innerHTML"===t&&Qt(r.tagName)&&a(r.innerHTML)){(Ti=Ti||document.createElement("div")).innerHTML="<svg>"+i+"</svg>";for(var d=Ti.firstChild;r.firstChild;)r.removeChild(r.firstChild);for(;d.firstChild;)r.appendChild(d.firstChild)}else if(i!==l[t])try{r[t]=i}catch(n){}}}}function Si(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,i=n._vModifiers;if(o(i)){if(i.number)return g(t)!==g(e);if(i.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var ji={create:zi,update:zi},Pi=k((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var i=n.split(t);i.length>1&&(e[i[0].trim()]=i[1].trim())}})),e}));function Ei(n){var e=qi(n.style);return n.staticStyle?E(n.staticStyle,e):e}function qi(n){return Array.isArray(n)?q(n):"string"==typeof n?Pi(n):n}var Oi,Ai=/^--/,$i=/\s*!important$/,Ci=function(n,e,t){if(Ai.test(e))n.style.setProperty(e,t);else if($i.test(t))n.style.setProperty(S(e),t.replace($i,""),"important");else{var i=Mi(e);if(Array.isArray(t))for(var r=0,a=t.length;r<a;r++)n.style[i]=t[r];else n.style[i]=t}},Li=["Webkit","Moz","ms"],Mi=k((function(n){if(Oi=Oi||document.createElement("div").style,"filter"!==(n=T(n))&&n in Oi)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<Li.length;t++){var i=Li[t]+e;if(i in Oi)return i}}));function Di(n,e){var t=e.data,i=n.data;if(!(a(t.staticStyle)&&a(t.style)&&a(i.staticStyle)&&a(i.style))){var r,l,s=e.elm,c=i.staticStyle,d=i.normalizedStyle||i.style||{},u=c||d,p=qi(e.data.style)||{};e.data.normalizedStyle=o(p.__ob__)?E({},p):p;var m=function(n,e){var t,i={};if(e)for(var r=n;r.componentInstance;)(r=r.componentInstance._vnode)&&r.data&&(t=Ei(r.data))&&E(i,t);(t=Ei(n.data))&&E(i,t);for(var a=n;a=a.parent;)a.data&&(t=Ei(a.data))&&E(i,t);return i}(e,!0);for(l in u)a(m[l])&&Ci(s,l,"");for(l in m)(r=m[l])!==u[l]&&Ci(s,l,null==r?"":r)}}var Ri={create:Di,update:Di},Bi=/\s+/;function Ni(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(Bi).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" "+(n.getAttribute("class")||"")+" ";t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function Ui(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(Bi).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" "+(n.getAttribute("class")||"")+" ",i=" "+e+" ";t.indexOf(i)>=0;)t=t.replace(i," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function Fi(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&E(e,Hi(n.name||"v")),E(e,n),e}return"string"==typeof n?Hi(n):void 0}}var Hi=k((function(n){return{enterClass:n+"-enter",enterToClass:n+"-enter-to",enterActiveClass:n+"-enter-active",leaveClass:n+"-leave",leaveToClass:n+"-leave-to",leaveActiveClass:n+"-leave-active"}})),Gi=K&&!X,Ki="transition",Wi="transitionend",Qi="animation",Vi="animationend";Gi&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Ki="WebkitTransition",Wi="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Qi="WebkitAnimation",Vi="webkitAnimationEnd"));var Ji=K?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function Xi(n){Ji((function(){Ji(n)}))}function Yi(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),Ni(n,e))}function Zi(n,e){n._transitionClasses&&b(n._transitionClasses,e),Ui(n,e)}function nr(n,e,t){var i=tr(n,e),r=i.type,a=i.timeout,o=i.propCount;if(!r)return t();var l="transition"===r?Wi:Vi,s=0,c=function(){n.removeEventListener(l,d),t()},d=function(e){e.target===n&&++s>=o&&c()};setTimeout((function(){s<o&&c()}),a+1),n.addEventListener(l,d)}var er=/\b(transform|all)(,|$)/;function tr(n,e){var t,i=window.getComputedStyle(n),r=(i[Ki+"Delay"]||"").split(", "),a=(i[Ki+"Duration"]||"").split(", "),o=ir(r,a),l=(i[Qi+"Delay"]||"").split(", "),s=(i[Qi+"Duration"]||"").split(", "),c=ir(l,s),d=0,u=0;return"transition"===e?o>0&&(t="transition",d=o,u=a.length):"animation"===e?c>0&&(t="animation",d=c,u=s.length):u=(t=(d=Math.max(o,c))>0?o>c?"transition":"animation":null)?"transition"===t?a.length:s.length:0,{type:t,timeout:d,propCount:u,hasTransform:"transition"===t&&er.test(i[Ki+"Property"])}}function ir(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return rr(e)+rr(n[t])})))}function rr(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function ar(n,e){var t=n.elm;o(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var i=Fi(n.data.transition);if(!a(i)&&!o(t._enterCb)&&1===t.nodeType){for(var r=i.css,l=i.type,s=i.enterClass,d=i.enterToClass,u=i.enterActiveClass,p=i.appearClass,m=i.appearToClass,h=i.appearActiveClass,f=i.beforeEnter,v=i.enter,y=i.afterEnter,b=i.enterCancelled,x=i.beforeAppear,_=i.appear,k=i.afterAppear,w=i.appearCancelled,T=i.duration,I=Ye,z=Ye.$vnode;z&&z.parent;)I=z.context,z=z.parent;var S=!I._isMounted||!n.isRootInsert;if(!S||_||""===_){var j=S&&p?p:s,P=S&&h?h:u,E=S&&m?m:d,q=S&&x||f,O=S&&"function"==typeof _?_:v,A=S&&k||y,$=S&&w||b,C=g(c(T)?T.enter:T);0;var L=!1!==r&&!X,D=sr(O),R=t._enterCb=M((function(){L&&(Zi(t,E),Zi(t,P)),R.cancelled?(L&&Zi(t,j),$&&$(t)):A&&A(t),t._enterCb=null}));n.data.show||de(n,"insert",(function(){var e=t.parentNode,i=e&&e._pending&&e._pending[n.key];i&&i.tag===n.tag&&i.elm._leaveCb&&i.elm._leaveCb(),O&&O(t,R)})),q&&q(t),L&&(Yi(t,j),Yi(t,P),Xi((function(){Zi(t,j),R.cancelled||(Yi(t,E),D||(lr(C)?setTimeout(R,C):nr(t,l,R)))}))),n.data.show&&(e&&e(),O&&O(t,R)),L||D||R()}}}function or(n,e){var t=n.elm;o(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var i=Fi(n.data.transition);if(a(i)||1!==t.nodeType)return e();if(!o(t._leaveCb)){var r=i.css,l=i.type,s=i.leaveClass,d=i.leaveToClass,u=i.leaveActiveClass,p=i.beforeLeave,m=i.leave,h=i.afterLeave,f=i.leaveCancelled,v=i.delayLeave,y=i.duration,b=!1!==r&&!X,x=sr(m),_=g(c(y)?y.leave:y);0;var k=t._leaveCb=M((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),b&&(Zi(t,d),Zi(t,u)),k.cancelled?(b&&Zi(t,s),f&&f(t)):(e(),h&&h(t)),t._leaveCb=null}));v?v(w):w()}function w(){k.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),p&&p(t),b&&(Yi(t,s),Yi(t,u),Xi((function(){Zi(t,s),k.cancelled||(Yi(t,d),x||(lr(_)?setTimeout(k,_):nr(t,l,k)))}))),m&&m(t,k),b||x||k())}}function lr(n){return"number"==typeof n&&!isNaN(n)}function sr(n){if(a(n))return!1;var e=n.fns;return o(e)?sr(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function cr(n,e){!0!==e.data.show&&ar(e)}var dr=function(n){var e,t,i={},r=n.modules,c=n.nodeOps;for(e=0;e<ti.length;++e)for(i[ti[e]]=[],t=0;t<r.length;++t)o(r[t][ti[e]])&&i[ti[e]].push(r[t][ti[e]]);function d(n){var e=c.parentNode(n);o(e)&&c.removeChild(e,n)}function u(n,e,t,r,a,s,d){if(o(n.elm)&&o(s)&&(n=s[d]=xn(n)),n.isRootInsert=!a,!function(n,e,t,r){var a=n.data;if(o(a)){var s=o(n.componentInstance)&&a.keepAlive;if(o(a=a.hook)&&o(a=a.init)&&a(n,!1),o(n.componentInstance))return p(n,e),m(t,n.elm,r),l(s)&&function(n,e,t,r){var a,l=n;for(;l.componentInstance;)if(l=l.componentInstance._vnode,o(a=l.data)&&o(a=a.transition)){for(a=0;a<i.activate.length;++a)i.activate[a](ei,l);e.push(l);break}m(t,n.elm,r)}(n,e,t,r),!0}}(n,e,t,r)){var u=n.data,f=n.children,v=n.tag;o(v)?(n.elm=n.ns?c.createElementNS(n.ns,v):c.createElement(v,n),y(n),h(n,f,e),o(u)&&g(n,e),m(t,n.elm,r)):l(n.isComment)?(n.elm=c.createComment(n.text),m(t,n.elm,r)):(n.elm=c.createTextNode(n.text),m(t,n.elm,r))}}function p(n,e){o(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,f(n)?(g(n,e),y(n)):(ni(n),e.push(n))}function m(n,e,t){o(n)&&(o(t)?c.parentNode(t)===n&&c.insertBefore(n,e,t):c.appendChild(n,e))}function h(n,e,t){if(Array.isArray(e)){0;for(var i=0;i<e.length;++i)u(e[i],t,n.elm,null,!0,e,i)}else s(n.text)&&c.appendChild(n.elm,c.createTextNode(String(n.text)))}function f(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return o(n.tag)}function g(n,t){for(var r=0;r<i.create.length;++r)i.create[r](ei,n);o(e=n.data.hook)&&(o(e.create)&&e.create(ei,n),o(e.insert)&&t.push(n))}function y(n){var e;if(o(e=n.fnScopeId))c.setStyleScope(n.elm,e);else for(var t=n;t;)o(e=t.context)&&o(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e),t=t.parent;o(e=Ye)&&e!==n.context&&e!==n.fnContext&&o(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e)}function b(n,e,t,i,r,a){for(;i<=r;++i)u(t[i],a,n,e,!1,t,i)}function x(n){var e,t,r=n.data;if(o(r))for(o(e=r.hook)&&o(e=e.destroy)&&e(n),e=0;e<i.destroy.length;++e)i.destroy[e](n);if(o(e=n.children))for(t=0;t<n.children.length;++t)x(n.children[t])}function _(n,e,t){for(;e<=t;++e){var i=n[e];o(i)&&(o(i.tag)?(k(i),x(i)):d(i.elm))}}function k(n,e){if(o(e)||o(n.data)){var t,r=i.remove.length+1;for(o(e)?e.listeners+=r:e=function(n,e){function t(){0==--t.listeners&&d(n)}return t.listeners=e,t}(n.elm,r),o(t=n.componentInstance)&&o(t=t._vnode)&&o(t.data)&&k(t,e),t=0;t<i.remove.length;++t)i.remove[t](n,e);o(t=n.data.hook)&&o(t=t.remove)?t(n,e):e()}else d(n.elm)}function w(n,e,t,i){for(var r=t;r<i;r++){var a=e[r];if(o(a)&&ii(n,a))return r}}function T(n,e,t,r,s,d){if(n!==e){o(e.elm)&&o(r)&&(e=r[s]=xn(e));var p=e.elm=n.elm;if(l(n.isAsyncPlaceholder))o(e.asyncFactory.resolved)?S(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(l(e.isStatic)&&l(n.isStatic)&&e.key===n.key&&(l(e.isCloned)||l(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,h=e.data;o(h)&&o(m=h.hook)&&o(m=m.prepatch)&&m(n,e);var g=n.children,v=e.children;if(o(h)&&f(e)){for(m=0;m<i.update.length;++m)i.update[m](n,e);o(m=h.hook)&&o(m=m.update)&&m(n,e)}a(e.text)?o(g)&&o(v)?g!==v&&function(n,e,t,i,r){var l,s,d,p=0,m=0,h=e.length-1,f=e[0],g=e[h],v=t.length-1,y=t[0],x=t[v],k=!r;for(0;p<=h&&m<=v;)a(f)?f=e[++p]:a(g)?g=e[--h]:ii(f,y)?(T(f,y,i,t,m),f=e[++p],y=t[++m]):ii(g,x)?(T(g,x,i,t,v),g=e[--h],x=t[--v]):ii(f,x)?(T(f,x,i,t,v),k&&c.insertBefore(n,f.elm,c.nextSibling(g.elm)),f=e[++p],x=t[--v]):ii(g,y)?(T(g,y,i,t,m),k&&c.insertBefore(n,g.elm,f.elm),g=e[--h],y=t[++m]):(a(l)&&(l=ri(e,p,h)),a(s=o(y.key)?l[y.key]:w(y,e,p,h))?u(y,i,n,f.elm,!1,t,m):ii(d=e[s],y)?(T(d,y,i,t,m),e[s]=void 0,k&&c.insertBefore(n,d.elm,f.elm)):u(y,i,n,f.elm,!1,t,m),y=t[++m]);p>h?b(n,a(t[v+1])?null:t[v+1].elm,t,m,v,i):m>v&&_(e,p,h)}(p,g,v,t,d):o(v)?(o(n.text)&&c.setTextContent(p,""),b(p,null,v,0,v.length-1,t)):o(g)?_(g,0,g.length-1):o(n.text)&&c.setTextContent(p,""):n.text!==e.text&&c.setTextContent(p,e.text),o(h)&&o(m=h.hook)&&o(m=m.postpatch)&&m(n,e)}}}function I(n,e,t){if(l(t)&&o(n.parent))n.parent.data.pendingInsert=e;else for(var i=0;i<e.length;++i)e[i].data.hook.insert(e[i])}var z=v("attrs,class,staticClass,staticStyle,key");function S(n,e,t,i){var r,a=e.tag,s=e.data,c=e.children;if(i=i||s&&s.pre,e.elm=n,l(e.isComment)&&o(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(o(s)&&(o(r=s.hook)&&o(r=r.init)&&r(e,!0),o(r=e.componentInstance)))return p(e,t),!0;if(o(a)){if(o(c))if(n.hasChildNodes())if(o(r=s)&&o(r=r.domProps)&&o(r=r.innerHTML)){if(r!==n.innerHTML)return!1}else{for(var d=!0,u=n.firstChild,m=0;m<c.length;m++){if(!u||!S(u,c[m],t,i)){d=!1;break}u=u.nextSibling}if(!d||u)return!1}else h(e,c,t);if(o(s)){var f=!1;for(var v in s)if(!z(v)){f=!0,g(e,t);break}!f&&s.class&&oe(s.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,r){if(!a(e)){var s,d=!1,p=[];if(a(n))d=!0,u(e,p);else{var m=o(n.nodeType);if(!m&&ii(n,e))T(n,e,p,null,null,r);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),l(t)&&S(n,e,p))return I(e,p,!0),n;s=n,n=new gn(c.tagName(s).toLowerCase(),{},[],void 0,s)}var h=n.elm,g=c.parentNode(h);if(u(e,p,h._leaveCb?null:g,c.nextSibling(h)),o(e.parent))for(var v=e.parent,y=f(e);v;){for(var b=0;b<i.destroy.length;++b)i.destroy[b](v);if(v.elm=e.elm,y){for(var k=0;k<i.create.length;++k)i.create[k](ei,v);var w=v.data.hook.insert;if(w.merged)for(var z=1;z<w.fns.length;z++)w.fns[z]()}else ni(v);v=v.parent}o(g)?_([n],0,0):o(n.tag)&&x(n)}}return I(e,p,d),e.elm}o(n)&&x(n)}}({nodeOps:Yt,modules:[fi,yi,Ii,ji,Ri,K?{create:cr,activate:cr,remove:function(n,e){!0!==n.data.show?or(n,e):e()}}:{}].concat(ui)});X&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&yr(n,"input")}));var ur={inserted:function(n,e,t,i){"select"===t.tag?(i.elm&&!i.elm._vOptions?de(t,"postpatch",(function(){ur.componentUpdated(n,e,t)})):pr(n,e,t.context),n._vOptions=[].map.call(n.options,fr)):("textarea"===t.tag||Xt(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",gr),n.addEventListener("compositionend",vr),n.addEventListener("change",vr),X&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){pr(n,e,t.context);var i=n._vOptions,r=n._vOptions=[].map.call(n.options,fr);if(r.some((function(n,e){return!C(n,i[e])})))(n.multiple?e.value.some((function(n){return hr(n,r)})):e.value!==e.oldValue&&hr(e.value,r))&&yr(n,"change")}}};function pr(n,e,t){mr(n,e,t),(J||Y)&&setTimeout((function(){mr(n,e,t)}),0)}function mr(n,e,t){var i=e.value,r=n.multiple;if(!r||Array.isArray(i)){for(var a,o,l=0,s=n.options.length;l<s;l++)if(o=n.options[l],r)a=L(i,fr(o))>-1,o.selected!==a&&(o.selected=a);else if(C(fr(o),i))return void(n.selectedIndex!==l&&(n.selectedIndex=l));r||(n.selectedIndex=-1)}}function hr(n,e){return e.every((function(e){return!C(e,n)}))}function fr(n){return"_value"in n?n._value:n.value}function gr(n){n.target.composing=!0}function vr(n){n.target.composing&&(n.target.composing=!1,yr(n.target,"input"))}function yr(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function br(n){return!n.componentInstance||n.data&&n.data.transition?n:br(n.componentInstance._vnode)}var xr={model:ur,show:{bind:function(n,e,t){var i=e.value,r=(t=br(t)).data&&t.data.transition,a=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;i&&r?(t.data.show=!0,ar(t,(function(){n.style.display=a}))):n.style.display=i?a:"none"},update:function(n,e,t){var i=e.value;!i!=!e.oldValue&&((t=br(t)).data&&t.data.transition?(t.data.show=!0,i?ar(t,(function(){n.style.display=n.__vOriginalDisplay})):or(t,(function(){n.style.display="none"}))):n.style.display=i?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,i,r){r||(n.style.display=n.__vOriginalDisplay)}}},_r={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function kr(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?kr(We(e.children)):n}function wr(n){var e={},t=n.$options;for(var i in t.propsData)e[i]=n[i];var r=t._parentListeners;for(var a in r)e[T(a)]=r[a];return e}function Tr(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var Ir=function(n){return n.tag||ve(n)},zr=function(n){return"show"===n.name},Sr={name:"transition",props:_r,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(Ir)).length){0;var i=this.mode;0;var r=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return r;var a=kr(r);if(!a)return r;if(this._leaving)return Tr(n,r);var o="__transition-"+this._uid+"-";a.key=null==a.key?a.isComment?o+"comment":o+a.tag:s(a.key)?0===String(a.key).indexOf(o)?a.key:o+a.key:a.key;var l=(a.data||(a.data={})).transition=wr(this),c=this._vnode,d=kr(c);if(a.data.directives&&a.data.directives.some(zr)&&(a.data.show=!0),d&&d.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(a,d)&&!ve(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var u=d.data.transition=E({},l);if("out-in"===i)return this._leaving=!0,de(u,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),Tr(n,r);if("in-out"===i){if(ve(a))return c;var p,m=function(){p()};de(l,"afterEnter",m),de(l,"enterCancelled",m),de(u,"delayLeave",(function(n){p=n}))}}return r}}},jr=E({tag:String,moveClass:String},_r);function Pr(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function Er(n){n.data.newPos=n.elm.getBoundingClientRect()}function qr(n){var e=n.data.pos,t=n.data.newPos,i=e.left-t.left,r=e.top-t.top;if(i||r){n.data.moved=!0;var a=n.elm.style;a.transform=a.WebkitTransform="translate("+i+"px,"+r+"px)",a.transitionDuration="0s"}}delete jr.mode;var Or={Transition:Sr,TransitionGroup:{props:jr,beforeMount:function(){var n=this,e=this._update;this._update=function(t,i){var r=Ze(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,r(),e.call(n,t,i)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),i=this.prevChildren=this.children,r=this.$slots.default||[],a=this.children=[],o=wr(this),l=0;l<r.length;l++){var s=r[l];if(s.tag)if(null!=s.key&&0!==String(s.key).indexOf("__vlist"))a.push(s),t[s.key]=s,(s.data||(s.data={})).transition=o;else;}if(i){for(var c=[],d=[],u=0;u<i.length;u++){var p=i[u];p.data.transition=o,p.data.pos=p.elm.getBoundingClientRect(),t[p.key]?c.push(p):d.push(p)}this.kept=n(e,null,c),this.removed=d}return n(e,null,a)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Pr),n.forEach(Er),n.forEach(qr),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,i=t.style;Yi(t,e),i.transform=i.WebkitTransform=i.transitionDuration="",t.addEventListener(Wi,t._moveCb=function n(i){i&&i.target!==t||i&&!/transform$/.test(i.propertyName)||(t.removeEventListener(Wi,n),t._moveCb=null,Zi(t,e))})}})))},methods:{hasMove:function(n,e){if(!Gi)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){Ui(t,n)})),Ni(t,e),t.style.display="none",this.$el.appendChild(t);var i=tr(t);return this.$el.removeChild(t),this._hasMove=i.hasTransform}}}};It.config.mustUseProp=function(n,e,t){return"value"===t&&$t(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},It.config.isReservedTag=Vt,It.config.isReservedAttr=At,It.config.getTagNamespace=function(n){return Qt(n)?"svg":"math"===n?"math":void 0},It.config.isUnknownElement=function(n){if(!K)return!0;if(Vt(n))return!1;if(n=n.toLowerCase(),null!=Jt[n])return Jt[n];var e=document.createElement(n);return n.indexOf("-")>-1?Jt[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Jt[n]=/HTMLUnknownElement/.test(e.toString())},E(It.options.directives,xr),E(It.options.components,Or),It.prototype.__patch__=K?dr:O,It.prototype.$mount=function(n,e){return function(n,e,t){var i;return n.$el=e,n.$options.render||(n.$options.render=yn),tt(n,"beforeMount"),i=function(){n._update(n._render(),t)},new ht(n,i,O,{before:function(){n._isMounted&&!n._isDestroyed&&tt(n,"beforeUpdate")}},!0),t=!1,null==n.$vnode&&(n._isMounted=!0,tt(n,"mounted")),n}(this,n=n&&K?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},K&&setTimeout((function(){B.devtools&&on&&on.emit("init",It)}),0);var Ar=It;
/*!
  * vue-router v3.5.3
  * (c) 2021 Evan You
  * @license MIT
  */function $r(n,e){for(var t in e)n[t]=e[t];return n}var Cr=/[!'()*]/g,Lr=function(n){return"%"+n.charCodeAt(0).toString(16)},Mr=/%2C/g,Dr=function(n){return encodeURIComponent(n).replace(Cr,Lr).replace(Mr,",")};function Rr(n){try{return decodeURIComponent(n)}catch(n){0}return n}var Br=function(n){return null==n||"object"==typeof n?n:String(n)};function Nr(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),i=Rr(t.shift()),r=t.length>0?Rr(t.join("=")):null;void 0===e[i]?e[i]=r:Array.isArray(e[i])?e[i].push(r):e[i]=[e[i],r]})),e):e}function Ur(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return Dr(e);if(Array.isArray(t)){var i=[];return t.forEach((function(n){void 0!==n&&(null===n?i.push(Dr(e)):i.push(Dr(e)+"="+Dr(n)))})),i.join("&")}return Dr(e)+"="+Dr(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var Fr=/\/?$/;function Hr(n,e,t,i){var r=i&&i.options.stringifyQuery,a=e.query||{};try{a=Gr(a)}catch(n){}var o={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:a,params:e.params||{},fullPath:Qr(e,r),matched:n?Wr(n):[]};return t&&(o.redirectedFrom=Qr(t,r)),Object.freeze(o)}function Gr(n){if(Array.isArray(n))return n.map(Gr);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=Gr(n[t]);return e}return n}var Kr=Hr(null,{path:"/"});function Wr(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function Qr(n,e){var t=n.path,i=n.query;void 0===i&&(i={});var r=n.hash;return void 0===r&&(r=""),(t||"/")+(e||Ur)(i)+r}function Vr(n,e,t){return e===Kr?n===e:!!e&&(n.path&&e.path?n.path.replace(Fr,"")===e.path.replace(Fr,"")&&(t||n.hash===e.hash&&Jr(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&Jr(n.query,e.query)&&Jr(n.params,e.params))))}function Jr(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),i=Object.keys(e).sort();return t.length===i.length&&t.every((function(t,r){var a=n[t];if(i[r]!==t)return!1;var o=e[t];return null==a||null==o?a===o:"object"==typeof a&&"object"==typeof o?Jr(a,o):String(a)===String(o)}))}function Xr(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var i in t.instances){var r=t.instances[i],a=t.enteredCbs[i];if(r&&a){delete t.enteredCbs[i];for(var o=0;o<a.length;o++)r._isBeingDestroyed||a[o](r)}}}}var Yr={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,i=e.children,r=e.parent,a=e.data;a.routerView=!0;for(var o=r.$createElement,l=t.name,s=r.$route,c=r._routerViewCache||(r._routerViewCache={}),d=0,u=!1;r&&r._routerRoot!==r;){var p=r.$vnode?r.$vnode.data:{};p.routerView&&d++,p.keepAlive&&r._directInactive&&r._inactive&&(u=!0),r=r.$parent}if(a.routerViewDepth=d,u){var m=c[l],h=m&&m.component;return h?(m.configProps&&Zr(h,a,m.route,m.configProps),o(h,a,i)):o()}var f=s.matched[d],g=f&&f.components[l];if(!f||!g)return c[l]=null,o();c[l]={component:g},a.registerRouteInstance=function(n,e){var t=f.instances[l];(e&&t!==n||!e&&t===n)&&(f.instances[l]=e)},(a.hook||(a.hook={})).prepatch=function(n,e){f.instances[l]=e.componentInstance},a.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==f.instances[l]&&(f.instances[l]=n.componentInstance),Xr(s)};var v=f.props&&f.props[l];return v&&($r(c[l],{route:s,configProps:v}),Zr(g,a,s,v)),o(g,a,i)}};function Zr(n,e,t,i){var r=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,i);if(r){r=e.props=$r({},r);var a=e.attrs=e.attrs||{};for(var o in r)n.props&&o in n.props||(a[o]=r[o],delete r[o])}}function na(n,e,t){var i=n.charAt(0);if("/"===i)return n;if("?"===i||"#"===i)return e+n;var r=e.split("/");t&&r[r.length-1]||r.pop();for(var a=n.replace(/^\//,"").split("/"),o=0;o<a.length;o++){var l=a[o];".."===l?r.pop():"."!==l&&r.push(l)}return""!==r[0]&&r.unshift(""),r.join("/")}function ea(n){return n.replace(/\/+/g,"/")}var ta=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},ia=va,ra=ca,aa=function(n,e){return ua(ca(n,e),e)},oa=ua,la=ga,sa=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function ca(n,e){for(var t,i=[],r=0,a=0,o="",l=e&&e.delimiter||"/";null!=(t=sa.exec(n));){var s=t[0],c=t[1],d=t.index;if(o+=n.slice(a,d),a=d+s.length,c)o+=c[1];else{var u=n[a],p=t[2],m=t[3],h=t[4],f=t[5],g=t[6],v=t[7];o&&(i.push(o),o="");var y=null!=p&&null!=u&&u!==p,b="+"===g||"*"===g,x="?"===g||"*"===g,_=t[2]||l,k=h||f;i.push({name:m||r++,prefix:p||"",delimiter:_,optional:x,repeat:b,partial:y,asterisk:!!v,pattern:k?ma(k):v?".*":"[^"+pa(_)+"]+?"})}}return a<n.length&&(o+=n.substr(a)),o&&i.push(o),i}function da(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function ua(n,e){for(var t=new Array(n.length),i=0;i<n.length;i++)"object"==typeof n[i]&&(t[i]=new RegExp("^(?:"+n[i].pattern+")$",fa(e)));return function(e,i){for(var r="",a=e||{},o=(i||{}).pretty?da:encodeURIComponent,l=0;l<n.length;l++){var s=n[l];if("string"!=typeof s){var c,d=a[s.name];if(null==d){if(s.optional){s.partial&&(r+=s.prefix);continue}throw new TypeError('Expected "'+s.name+'" to be defined')}if(ta(d)){if(!s.repeat)throw new TypeError('Expected "'+s.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(s.optional)continue;throw new TypeError('Expected "'+s.name+'" to not be empty')}for(var u=0;u<d.length;u++){if(c=o(d[u]),!t[l].test(c))throw new TypeError('Expected all "'+s.name+'" to match "'+s.pattern+'", but received `'+JSON.stringify(c)+"`");r+=(0===u?s.prefix:s.delimiter)+c}}else{if(c=s.asterisk?encodeURI(d).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):o(d),!t[l].test(c))throw new TypeError('Expected "'+s.name+'" to match "'+s.pattern+'", but received "'+c+'"');r+=s.prefix+c}}else r+=s}return r}}function pa(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function ma(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function ha(n,e){return n.keys=e,n}function fa(n){return n&&n.sensitive?"":"i"}function ga(n,e,t){ta(e)||(t=e||t,e=[]);for(var i=(t=t||{}).strict,r=!1!==t.end,a="",o=0;o<n.length;o++){var l=n[o];if("string"==typeof l)a+=pa(l);else{var s=pa(l.prefix),c="(?:"+l.pattern+")";e.push(l),l.repeat&&(c+="(?:"+s+c+")*"),a+=c=l.optional?l.partial?s+"("+c+")?":"(?:"+s+"("+c+"))?":s+"("+c+")"}}var d=pa(t.delimiter||"/"),u=a.slice(-d.length)===d;return i||(a=(u?a.slice(0,-d.length):a)+"(?:"+d+"(?=$))?"),a+=r?"$":i&&u?"":"(?="+d+"|$)",ha(new RegExp("^"+a,fa(t)),e)}function va(n,e,t){return ta(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var i=0;i<t.length;i++)e.push({name:i,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return ha(n,e)}(n,e):ta(n)?function(n,e,t){for(var i=[],r=0;r<n.length;r++)i.push(va(n[r],e,t).source);return ha(new RegExp("(?:"+i.join("|")+")",fa(t)),e)}(n,e,t):function(n,e,t){return ga(ca(n,t),e,t)}(n,e,t)}ia.parse=ra,ia.compile=aa,ia.tokensToFunction=oa,ia.tokensToRegExp=la;var ya=Object.create(null);function ba(n,e,t){e=e||{};try{var i=ya[n]||(ya[n]=ia.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),i(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function xa(n,e,t,i){var r="string"==typeof n?{path:n}:n;if(r._normalized)return r;if(r.name){var a=(r=$r({},n)).params;return a&&"object"==typeof a&&(r.params=$r({},a)),r}if(!r.path&&r.params&&e){(r=$r({},r))._normalized=!0;var o=$r($r({},e.params),r.params);if(e.name)r.name=e.name,r.params=o;else if(e.matched.length){var l=e.matched[e.matched.length-1].path;r.path=ba(l,o,e.path)}else 0;return r}var s=function(n){var e="",t="",i=n.indexOf("#");i>=0&&(e=n.slice(i),n=n.slice(0,i));var r=n.indexOf("?");return r>=0&&(t=n.slice(r+1),n=n.slice(0,r)),{path:n,query:t,hash:e}}(r.path||""),c=e&&e.path||"/",d=s.path?na(s.path,c,t||r.append):c,u=function(n,e,t){void 0===e&&(e={});var i,r=t||Nr;try{i=r(n||"")}catch(n){i={}}for(var a in e){var o=e[a];i[a]=Array.isArray(o)?o.map(Br):Br(o)}return i}(s.query,r.query,i&&i.options.parseQuery),p=r.hash||s.hash;return p&&"#"!==p.charAt(0)&&(p="#"+p),{_normalized:!0,path:d,query:u,hash:p}}var _a,ka=function(){},wa={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,i=this.$route,r=t.resolve(this.to,i,this.append),a=r.location,o=r.route,l=r.href,s={},c=t.options.linkActiveClass,d=t.options.linkExactActiveClass,u=null==c?"router-link-active":c,p=null==d?"router-link-exact-active":d,m=null==this.activeClass?u:this.activeClass,h=null==this.exactActiveClass?p:this.exactActiveClass,f=o.redirectedFrom?Hr(null,xa(o.redirectedFrom),null,t):o;s[h]=Vr(i,f,this.exactPath),s[m]=this.exact||this.exactPath?s[h]:function(n,e){return 0===n.path.replace(Fr,"/").indexOf(e.path.replace(Fr,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(i,f);var g=s[h]?this.ariaCurrentValue:null,v=function(n){Ta(n)&&(e.replace?t.replace(a,ka):t.push(a,ka))},y={click:Ta};Array.isArray(this.event)?this.event.forEach((function(n){y[n]=v})):y[this.event]=v;var b={class:s},x=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:l,route:o,navigate:v,isActive:s[m],isExactActive:s[h]});if(x){if(1===x.length)return x[0];if(x.length>1||!x.length)return 0===x.length?n():n("span",{},x)}if("a"===this.tag)b.on=y,b.attrs={href:l,"aria-current":g};else{var _=function n(e){var t;if(e)for(var i=0;i<e.length;i++){if("a"===(t=e[i]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(_){_.isStatic=!1;var k=_.data=$r({},_.data);for(var w in k.on=k.on||{},k.on){var T=k.on[w];w in y&&(k.on[w]=Array.isArray(T)?T:[T])}for(var I in y)I in k.on?k.on[I].push(y[I]):k.on[I]=v;var z=_.data.attrs=$r({},_.data.attrs);z.href=l,z["aria-current"]=g}else b.on=y}return n(this.tag,b,this.$slots.default)}};function Ta(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var Ia="undefined"!=typeof window;function za(n,e,t,i,r){var a=e||[],o=t||Object.create(null),l=i||Object.create(null);n.forEach((function(n){!function n(e,t,i,r,a,o){var l=r.path,s=r.name;0;var c=r.pathToRegexpOptions||{},d=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return ea(e.path+"/"+n)}(l,a,c.strict);"boolean"==typeof r.caseSensitive&&(c.sensitive=r.caseSensitive);var u={path:d,regex:Sa(d,c),components:r.components||{default:r.component},alias:r.alias?"string"==typeof r.alias?[r.alias]:r.alias:[],instances:{},enteredCbs:{},name:s,parent:a,matchAs:o,redirect:r.redirect,beforeEnter:r.beforeEnter,meta:r.meta||{},props:null==r.props?{}:r.components?r.props:{default:r.props}};r.children&&r.children.forEach((function(r){var a=o?ea(o+"/"+r.path):void 0;n(e,t,i,r,u,a)}));t[u.path]||(e.push(u.path),t[u.path]=u);if(void 0!==r.alias)for(var p=Array.isArray(r.alias)?r.alias:[r.alias],m=0;m<p.length;++m){0;var h={path:p[m],children:r.children};n(e,t,i,h,a,u.path||"/")}s&&(i[s]||(i[s]=u))}(a,o,l,n,r)}));for(var s=0,c=a.length;s<c;s++)"*"===a[s]&&(a.push(a.splice(s,1)[0]),c--,s--);return{pathList:a,pathMap:o,nameMap:l}}function Sa(n,e){return ia(n,[],e)}function ja(n,e){var t=za(n),i=t.pathList,r=t.pathMap,a=t.nameMap;function o(n,t,o){var l=xa(n,t,!1,e),c=l.name;if(c){var d=a[c];if(!d)return s(null,l);var u=d.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof l.params&&(l.params={}),t&&"object"==typeof t.params)for(var p in t.params)!(p in l.params)&&u.indexOf(p)>-1&&(l.params[p]=t.params[p]);return l.path=ba(d.path,l.params),s(d,l,o)}if(l.path){l.params={};for(var m=0;m<i.length;m++){var h=i[m],f=r[h];if(Pa(f.regex,l.path,l.params))return s(f,l,o)}}return s(null,l)}function l(n,t){var i=n.redirect,r="function"==typeof i?i(Hr(n,t,null,e)):i;if("string"==typeof r&&(r={path:r}),!r||"object"!=typeof r)return s(null,t);var l=r,c=l.name,d=l.path,u=t.query,p=t.hash,m=t.params;if(u=l.hasOwnProperty("query")?l.query:u,p=l.hasOwnProperty("hash")?l.hash:p,m=l.hasOwnProperty("params")?l.params:m,c){a[c];return o({_normalized:!0,name:c,query:u,hash:p,params:m},void 0,t)}if(d){var h=function(n,e){return na(n,e.parent?e.parent.path:"/",!0)}(d,n);return o({_normalized:!0,path:ba(h,m),query:u,hash:p},void 0,t)}return s(null,t)}function s(n,t,i){return n&&n.redirect?l(n,i||t):n&&n.matchAs?function(n,e,t){var i=o({_normalized:!0,path:ba(t,e.params)});if(i){var r=i.matched,a=r[r.length-1];return e.params=i.params,s(a,e)}return s(null,e)}(0,t,n.matchAs):Hr(n,t,i,e)}return{match:o,addRoute:function(n,e){var t="object"!=typeof n?a[n]:void 0;za([e||n],i,r,a,t),t&&t.alias.length&&za(t.alias.map((function(n){return{path:n,children:[e]}})),i,r,a,t)},getRoutes:function(){return i.map((function(n){return r[n]}))},addRoutes:function(n){za(n,i,r,a)}}}function Pa(n,e,t){var i=e.match(n);if(!i)return!1;if(!t)return!0;for(var r=1,a=i.length;r<a;++r){var o=n.keys[r-1];o&&(t[o.name||"pathMatch"]="string"==typeof i[r]?Rr(i[r]):i[r])}return!0}var Ea=Ia&&window.performance&&window.performance.now?window.performance:Date;function qa(){return Ea.now().toFixed(3)}var Oa=qa();function Aa(){return Oa}function $a(n){return Oa=n}var Ca=Object.create(null);function La(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=$r({},window.history.state);return t.key=Aa(),window.history.replaceState(t,"",e),window.addEventListener("popstate",Ra),function(){window.removeEventListener("popstate",Ra)}}function Ma(n,e,t,i){if(n.app){var r=n.options.scrollBehavior;r&&n.app.$nextTick((function(){var a=function(){var n=Aa();if(n)return Ca[n]}(),o=r.call(n,e,t,i?a:null);o&&("function"==typeof o.then?o.then((function(n){Ha(n,a)})).catch((function(n){0})):Ha(o,a))}))}}function Da(){var n=Aa();n&&(Ca[n]={x:window.pageXOffset,y:window.pageYOffset})}function Ra(n){Da(),n.state&&n.state.key&&$a(n.state.key)}function Ba(n){return Ua(n.x)||Ua(n.y)}function Na(n){return{x:Ua(n.x)?n.x:window.pageXOffset,y:Ua(n.y)?n.y:window.pageYOffset}}function Ua(n){return"number"==typeof n}var Fa=/^#\d/;function Ha(n,e){var t,i="object"==typeof n;if(i&&"string"==typeof n.selector){var r=Fa.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(r){var a=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),i=n.getBoundingClientRect();return{x:i.left-t.left-e.x,y:i.top-t.top-e.y}}(r,a={x:Ua((t=a).x)?t.x:0,y:Ua(t.y)?t.y:0})}else Ba(n)&&(e=Na(n))}else i&&Ba(n)&&(e=Na(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var Ga,Ka=Ia&&((-1===(Ga=window.navigator.userAgent).indexOf("Android 2.")&&-1===Ga.indexOf("Android 4.0")||-1===Ga.indexOf("Mobile Safari")||-1!==Ga.indexOf("Chrome")||-1!==Ga.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function Wa(n,e){Da();var t=window.history;try{if(e){var i=$r({},t.state);i.key=Aa(),t.replaceState(i,"",n)}else t.pushState({key:$a(qa())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function Qa(n){Wa(n,!0)}function Va(n,e,t){var i=function(r){r>=n.length?t():n[r]?e(n[r],(function(){i(r+1)})):i(r+1)};i(0)}var Ja={redirected:2,aborted:4,cancelled:8,duplicated:16};function Xa(n,e){return Za(n,e,Ja.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return no.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function Ya(n,e){return Za(n,e,Ja.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function Za(n,e,t,i){var r=new Error(i);return r._isRouter=!0,r.from=n,r.to=e,r.type=t,r}var no=["params","query","hash"];function eo(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function to(n,e){return eo(n)&&n._isRouter&&(null==e||n.type===e)}function io(n){return function(e,t,i){var r=!1,a=0,o=null;ro(n,(function(n,e,t,l){if("function"==typeof n&&void 0===n.cid){r=!0,a++;var s,c=lo((function(e){var r;((r=e).__esModule||oo&&"Module"===r[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:_a.extend(e),t.components[l]=e,--a<=0&&i()})),d=lo((function(n){var e="Failed to resolve async component "+l+": "+n;o||(o=eo(n)?n:new Error(e),i(o))}));try{s=n(c,d)}catch(n){d(n)}if(s)if("function"==typeof s.then)s.then(c,d);else{var u=s.component;u&&"function"==typeof u.then&&u.then(c,d)}}})),r||i()}}function ro(n,e){return ao(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function ao(n){return Array.prototype.concat.apply([],n)}var oo="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function lo(n){var e=!1;return function(){for(var t=[],i=arguments.length;i--;)t[i]=arguments[i];if(!e)return e=!0,n.apply(this,t)}}var so=function(n,e){this.router=n,this.base=function(n){if(!n)if(Ia){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=Kr,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function co(n,e,t,i){var r=ro(n,(function(n,i,r,a){var o=function(n,e){"function"!=typeof n&&(n=_a.extend(n));return n.options[e]}(n,e);if(o)return Array.isArray(o)?o.map((function(n){return t(n,i,r,a)})):t(o,i,r,a)}));return ao(i?r.reverse():r)}function uo(n,e){if(e)return function(){return n.apply(e,arguments)}}so.prototype.listen=function(n){this.cb=n},so.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},so.prototype.onError=function(n){this.errorCbs.push(n)},so.prototype.transitionTo=function(n,e,t){var i,r=this;try{i=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var a=this.current;this.confirmTransition(i,(function(){r.updateRoute(i),e&&e(i),r.ensureURL(),r.router.afterHooks.forEach((function(n){n&&n(i,a)})),r.ready||(r.ready=!0,r.readyCbs.forEach((function(n){n(i)})))}),(function(n){t&&t(n),n&&!r.ready&&(to(n,Ja.redirected)&&a===Kr||(r.ready=!0,r.readyErrorCbs.forEach((function(e){e(n)}))))}))},so.prototype.confirmTransition=function(n,e,t){var i=this,r=this.current;this.pending=n;var a,o,l=function(n){!to(n)&&eo(n)&&(i.errorCbs.length?i.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},s=n.matched.length-1,c=r.matched.length-1;if(Vr(n,r)&&s===c&&n.matched[s]===r.matched[c])return this.ensureURL(),n.hash&&Ma(this.router,r,n,!1),l(((o=Za(a=r,n,Ja.duplicated,'Avoided redundant navigation to current location: "'+a.fullPath+'".')).name="NavigationDuplicated",o));var d=function(n,e){var t,i=Math.max(n.length,e.length);for(t=0;t<i&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),u=d.updated,p=d.deactivated,m=d.activated,h=[].concat(function(n){return co(n,"beforeRouteLeave",uo,!0)}(p),this.router.beforeHooks,function(n){return co(n,"beforeRouteUpdate",uo)}(u),m.map((function(n){return n.beforeEnter})),io(m)),f=function(e,t){if(i.pending!==n)return l(Ya(r,n));try{e(n,r,(function(e){!1===e?(i.ensureURL(!0),l(function(n,e){return Za(n,e,Ja.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(r,n))):eo(e)?(i.ensureURL(!0),l(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(l(Xa(r,n)),"object"==typeof e&&e.replace?i.replace(e):i.push(e)):t(e)}))}catch(n){l(n)}};Va(h,f,(function(){Va(function(n){return co(n,"beforeRouteEnter",(function(n,e,t,i){return function(n,e,t){return function(i,r,a){return n(i,r,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),a(n)}))}}(n,t,i)}))}(m).concat(i.router.resolveHooks),f,(function(){if(i.pending!==n)return l(Ya(r,n));i.pending=null,e(n),i.router.app&&i.router.app.$nextTick((function(){Xr(n)}))}))}))},so.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},so.prototype.setupListeners=function(){},so.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=Kr,this.pending=null};var po=function(n){function e(e,t){n.call(this,e,t),this._startLocation=mo(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,i=Ka&&t;i&&this.listeners.push(La());var r=function(){var t=n.current,r=mo(n.base);n.current===Kr&&r===n._startLocation||n.transitionTo(r,(function(n){i&&Ma(e,n,t,!0)}))};window.addEventListener("popstate",r),this.listeners.push((function(){window.removeEventListener("popstate",r)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var i=this,r=this.current;this.transitionTo(n,(function(n){Wa(ea(i.base+n.fullPath)),Ma(i.router,n,r,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var i=this,r=this.current;this.transitionTo(n,(function(n){Qa(ea(i.base+n.fullPath)),Ma(i.router,n,r,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(mo(this.base)!==this.current.fullPath){var e=ea(this.base+this.current.fullPath);n?Wa(e):Qa(e)}},e.prototype.getCurrentLocation=function(){return mo(this.base)},e}(so);function mo(n){var e=window.location.pathname,t=e.toLowerCase(),i=n.toLowerCase();return!n||t!==i&&0!==t.indexOf(ea(i+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var ho=function(n){function e(e,t,i){n.call(this,e,t),i&&function(n){var e=mo(n);if(!/^\/#/.test(e))return window.location.replace(ea(n+"/#"+e)),!0}(this.base)||fo()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=Ka&&e;t&&this.listeners.push(La());var i=function(){var e=n.current;fo()&&n.transitionTo(go(),(function(i){t&&Ma(n.router,i,e,!0),Ka||bo(i.fullPath)}))},r=Ka?"popstate":"hashchange";window.addEventListener(r,i),this.listeners.push((function(){window.removeEventListener(r,i)}))}},e.prototype.push=function(n,e,t){var i=this,r=this.current;this.transitionTo(n,(function(n){yo(n.fullPath),Ma(i.router,n,r,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var i=this,r=this.current;this.transitionTo(n,(function(n){bo(n.fullPath),Ma(i.router,n,r,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;go()!==e&&(n?yo(e):bo(e))},e.prototype.getCurrentLocation=function(){return go()},e}(so);function fo(){var n=go();return"/"===n.charAt(0)||(bo("/"+n),!1)}function go(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function vo(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function yo(n){Ka?Wa(vo(n)):window.location.hash=n}function bo(n){Ka?Qa(vo(n)):window.location.replace(vo(n))}var xo=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var i=this;this.transitionTo(n,(function(n){i.stack=i.stack.slice(0,i.index+1).concat(n),i.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var i=this;this.transitionTo(n,(function(n){i.stack=i.stack.slice(0,i.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var i=this.stack[t];this.confirmTransition(i,(function(){var n=e.current;e.index=t,e.updateRoute(i),e.router.afterHooks.forEach((function(e){e&&e(i,n)}))}),(function(n){to(n,Ja.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(so),_o=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=ja(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!Ka&&!1!==n.fallback,this.fallback&&(e="hash"),Ia||(e="abstract"),this.mode=e,e){case"history":this.history=new po(this,n.base);break;case"hash":this.history=new ho(this,n.base,this.fallback);break;case"abstract":this.history=new xo(this,n.base);break;default:0}},ko={currentRoute:{configurable:!0}};function wo(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}_o.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},ko.currentRoute.get=function(){return this.history&&this.history.current},_o.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof po||t instanceof ho){var i=function(n){t.setupListeners(),function(n){var i=t.current,r=e.options.scrollBehavior;Ka&&r&&"fullPath"in n&&Ma(e,n,i,!1)}(n)};t.transitionTo(t.getCurrentLocation(),i,i)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},_o.prototype.beforeEach=function(n){return wo(this.beforeHooks,n)},_o.prototype.beforeResolve=function(n){return wo(this.resolveHooks,n)},_o.prototype.afterEach=function(n){return wo(this.afterHooks,n)},_o.prototype.onReady=function(n,e){this.history.onReady(n,e)},_o.prototype.onError=function(n){this.history.onError(n)},_o.prototype.push=function(n,e,t){var i=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){i.history.push(n,e,t)}));this.history.push(n,e,t)},_o.prototype.replace=function(n,e,t){var i=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){i.history.replace(n,e,t)}));this.history.replace(n,e,t)},_o.prototype.go=function(n){this.history.go(n)},_o.prototype.back=function(){this.go(-1)},_o.prototype.forward=function(){this.go(1)},_o.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},_o.prototype.resolve=function(n,e,t){var i=xa(n,e=e||this.history.current,t,this),r=this.match(i,e),a=r.redirectedFrom||r.fullPath;return{location:i,route:r,href:function(n,e,t){var i="hash"===t?"#"+e:e;return n?ea(n+"/"+i):i}(this.history.base,a,this.mode),normalizedTo:i,resolved:r}},_o.prototype.getRoutes=function(){return this.matcher.getRoutes()},_o.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==Kr&&this.history.transitionTo(this.history.getCurrentLocation())},_o.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==Kr&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(_o.prototype,ko),_o.install=function n(e){if(!n.installed||_a!==e){n.installed=!0,_a=e;var t=function(n){return void 0!==n},i=function(n,e){var i=n.$options._parentVnode;t(i)&&t(i=i.data)&&t(i=i.registerRouteInstance)&&i(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,i(this,this)},destroyed:function(){i(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",Yr),e.component("RouterLink",wa);var r=e.config.optionMergeStrategies;r.beforeRouteEnter=r.beforeRouteLeave=r.beforeRouteUpdate=r.created}},_o.version="3.5.3",_o.isNavigationFailure=to,_o.NavigationFailureType=Ja,_o.START_LOCATION=Kr,Ia&&window.Vue&&window.Vue.use(_o);var To=_o;t(176),t(177),t(254),t(75),t(178),t(28),t(29),t(256);function Io(n){n.locales&&Object.keys(n.locales).forEach((function(e){n.locales[e].path=e})),Object.freeze(n)}t(70),t(92),t(126);function zo(n){return(zo="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n})(n)}var So=t(72),jo=(t(188),t(17),t(45),t(230),t(231),t(40),t(30),{NotFound:function(){return Promise.all([t.e(0),t.e(9)]).then(t.bind(null,761))},Layout:function(){return Promise.all([t.e(0),t.e(3)]).then(t.bind(null,760))}}),Po={"v-1ae7cc2e":function(){return t.e(11).then(t.bind(null,763))},"v-f3a4c5c8":function(){return t.e(12).then(t.bind(null,764))},"v-77b9adf3":function(){return t.e(13).then(t.bind(null,765))},"v-24c441a1":function(){return t.e(10).then(t.bind(null,766))},"v-550e6dee":function(){return t.e(5).then(t.bind(null,767))},"v-67450649":function(){return t.e(4).then(t.bind(null,768))},"v-5babeb51":function(){return t.e(7).then(t.bind(null,769))},"v-636c139f":function(){return t.e(14).then(t.bind(null,770))},"v-3f34187e":function(){return t.e(18).then(t.bind(null,771))},"v-1d67f52e":function(){return t.e(16).then(t.bind(null,772))},"v-6d870201":function(){return t.e(17).then(t.bind(null,773))},"v-1b38a950":function(){return t.e(15).then(t.bind(null,774))},"v-00fe5646":function(){return t.e(6).then(t.bind(null,775))},"v-01596c50":function(){return t.e(20).then(t.bind(null,776))},"v-65da262e":function(){return t.e(19).then(t.bind(null,777))},"v-7a4a7900":function(){return t.e(21).then(t.bind(null,778))},"v-5dae9843":function(){return t.e(23).then(t.bind(null,779))},"v-6fd0f028":function(){return t.e(24).then(t.bind(null,780))},"v-5e595d06":function(){return t.e(22).then(t.bind(null,781))},"v-5a5a4a6b":function(){return t.e(27).then(t.bind(null,782))},"v-088adbe6":function(){return t.e(25).then(t.bind(null,783))},"v-b5f9e054":function(){return t.e(26).then(t.bind(null,784))},"v-a670efd8":function(){return t.e(29).then(t.bind(null,785))},"v-73e22b34":function(){return t.e(30).then(t.bind(null,786))},"v-6d447165":function(){return t.e(32).then(t.bind(null,787))},"v-d552c9e2":function(){return t.e(28).then(t.bind(null,762))},"v-4849b605":function(){return t.e(31).then(t.bind(null,788))},"v-c07cc8f6":function(){return t.e(33).then(t.bind(null,789))},"v-70b16331":function(){return t.e(36).then(t.bind(null,790))},"v-60f6b60a":function(){return t.e(34).then(t.bind(null,791))},"v-91f7fbec":function(){return t.e(35).then(t.bind(null,792))},"v-233673d3":function(){return t.e(2).then(t.bind(null,793))}};function Eo(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var qo=/-(\w)/g,Oo=Eo((function(n){return n.replace(qo,(function(n,e){return e?e.toUpperCase():""}))})),Ao=/\B([A-Z])/g,$o=Eo((function(n){return n.replace(Ao,"-$1").toLowerCase()})),Co=Eo((function(n){return n.charAt(0).toUpperCase()+n.slice(1)}));function Lo(n,e){if(e)return n(e)?n(e):e.includes("-")?n(Co(Oo(e))):n(Co(e))||n($o(e))}var Mo=Object.assign({},jo,Po),Do=function(n){return Mo[n]},Ro=function(n){return Po[n]},Bo=function(n){return jo[n]},No=function(n){return Ar.component(n)};function Uo(n){return Lo(Ro,n)}function Fo(n){return Lo(Bo,n)}function Ho(n){return Lo(Do,n)}function Go(n){return Lo(No,n)}function Ko(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];return Promise.all(e.filter((function(n){return n})).map(function(){var n=Object(i.a)(regeneratorRuntime.mark((function n(e){var t;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(Go(e)||!Ho(e)){n.next=5;break}return n.next=3,Ho(e)();case 3:t=n.sent,Ar.component(e,t.default);case 5:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()))}function Wo(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var Qo=t(138),Vo=(t(189),t(110),t(53),t(218)),Jo=t.n(Vo),Xo=t(219),Yo=t.n(Xo),Zo={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(n){return"meta"===Object(Qo.a)(n,1)[0]})).map((function(n){var e=Object(Qo.a)(n,2);e[0];return e[1]})),this.$ssrContext){var n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map((function(n){var e="<meta";return Object.keys(n).forEach((function(t){e+=" ".concat(t,'="').concat(Yo()(n[t]),'"')})),e+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=el(this.$canonicalUrl)}var e},mounted:function(){this.currentMetaTags=Object(So.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var n=this.getMergedMetaTags();this.currentMetaTags=tl(n,this.currentMetaTags)},getMergedMetaTags:function(){var n=this.$page.frontmatter.meta||[];return Jo()([{name:"description",content:this.$description}],n,this.siteMeta,il)},updateCanonicalLink:function(){nl(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",el(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){tl(null,this.currentMetaTags),nl()}};function nl(){var n=document.querySelector("link[rel='canonical']");n&&n.remove()}function el(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return n?'<link href="'.concat(n,'" rel="canonical" />'):""}function tl(n,e){if(e&&Object(So.a)(e).filter((function(n){return n.parentNode===document.head})).forEach((function(n){return document.head.removeChild(n)})),n)return n.map((function(n){var e=document.createElement("meta");return Object.keys(n).forEach((function(t){e.setAttribute(t,n[t])})),document.head.appendChild(e),e}))}function il(n){for(var e=0,t=["name","property","itemprop"];e<t.length;e++){var i=t[e];if(n.hasOwnProperty(i))return n[i]+i}return JSON.stringify(n)}t(139);var rl=t(151),al={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(rl)()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var n=this,e=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(n){return e.some((function(e){return e.hash===n.hash}))})),i=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),r=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),a=window.innerHeight+i,o=0;o<t.length;o++){var l=t[o],s=t[o+1],c=0===o&&0===i||i>=l.parentElement.offsetTop+10&&(!s||i<s.parentElement.offsetTop-10),d=decodeURIComponent(this.$route.hash);if(c&&d!==decodeURIComponent(l.hash)){var u=l;if(a===r)for(var p=o+1;p<t.length;p++)if(d===decodeURIComponent(t[p].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(u.hash),(function(){n.$nextTick((function(){n.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},ol=(t(78),t(101)),ll=t.n(ol),sl={mounted:function(){var n=this;ll.a.configure({showSpinner:!1}),this.$router.beforeEach((function(n,e,t){n.path===e.path||Ar.component(n.name)||ll.a.start(),t()})),this.$router.afterEach((function(){ll.a.done(),n.isSidebarOpen=!1}))}};t(74),t(54),t(77),t(363);function cl(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}t(99);function dl(n,e){for(var t=0;t<e.length;t++){var i=e[t];i.enumerable=i.enumerable||!1,i.configurable=!0,"value"in i&&(i.writable=!0),Object.defineProperty(n,i.key,i)}}function ul(n,e,t){return e&&dl(n.prototype,e),t&&dl(n,t),Object.defineProperty(n,"prototype",{writable:!1}),n}t(364);var pl=function(){function n(){cl(this,n);this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}return ul(n,[{key:"show",value:function(n){var e=this,t=n.text,i=void 0===t?"":t,r=n.duration,a=void 0===r?3e3:r,o=document.createElement("div");o.className="message move-in",o.innerHTML='\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">'.concat(i,"</div>\n    "),this.containerEl.appendChild(o),a>0&&setTimeout((function(){e.close(o)}),a)}},{key:"close",value:function(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",(function(){n.remove()}))}}]),n}(),ml={mounted:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy:function(){var n=this;setTimeout((function(){(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach((function(e){document.querySelectorAll(e).forEach(n.generateCopyButton)}))}),1e3)},generateCopyButton:function(n){var e=this;if(!n.classList.contains("codecopy-enabled")){var t=document.createElement("i");t.className="code-copy",t.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',t.title="Copy to clipboard",t.addEventListener("click",(function(){e.copyToClipboard(n.innerText)})),n.appendChild(t),n.classList.add("codecopy-enabled")}},copyToClipboard:function(n){var e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);var t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy"),(new pl).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}};t(233),t(104),t(103),t(140),t(366);!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var i=document.head||document.getElementsByTagName("head")[0],r=document.createElement("style");r.type="text/css","top"===t&&i.firstChild?i.insertBefore(r,i.firstChild):i.appendChild(r),r.styleSheet?r.styleSheet.cssText=n:r.appendChild(document.createTextNode(n))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var hl={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},fl={},gl=function(n){return'<div id="app">\n'.concat(n,"\n</div>")},vl=function(n){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[n]?window.$VUEPRESS_DEMO_BLOCK[n]:hl[n]},yl=function n(e,t,i){var r=document.createElement(e);return t&&Object.keys(t).forEach((function(n){if(n.indexOf("data"))r[n]=t[n];else{var e=n.replace("data","");r.dataset[e]=t[n]}})),i&&i.forEach((function(e){var t=e.tag,i=e.attrs,a=e.children;r.appendChild(n(t,i,a))})),r},bl=function(n,e,t){var i,r=(i=n.querySelectorAll(".".concat(e)),Array.prototype.slice.call(i));return 1!==r.length||t?r:r[0]},xl=function(n,e){var t,i,r=n.match(/<style>([\s\S]+)<\/style>/),a=n.match(/<template>([\s\S]+)<\/template>/),o=n.match(/<script>([\s\S]+)<\/script>/),l={css:r&&r[1].replace(/^\n|\n$/g,""),html:a&&a[1].replace(/^\n|\n$/g,""),js:o&&o[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};l.htmlTpl=gl(l.html),l.jsTpl=(t=l.js,i=t.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(i,"\n})")),l.script=function(n,e){var t=n.split(/export\s+default/),i="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),r=window.Babel?window.Babel.transform(i,{presets:["es2015"]}).code:i,a=[eval][0](r);return a.template=e,a}(l.js,l.html);var s=vl("vue");return l.jsLib.unshift(s),l},_l=function(n,e){var t,i=n.match(/<style>([\s\S]+)<\/style>/),r=n.match(/<html>([\s\S]+)<\/html>/),a=n.match(/<script>([\s\S]+)<\/script>/),o={css:i&&i[1].replace(/^\n|\n$/g,""),html:r&&r[1].replace(/^\n|\n$/g,""),js:a&&a[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};return o.htmlTpl=o.html,o.jsTpl=o.js,o.script=(t=o.js,window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t),o},kl=function(n){return n=n.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),n+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function wl(){var n=bl(document,"vuepress-plugin-demo-block__wrapper",!0);n.length?n.forEach((function(n){if("true"!==n.dataset.created){n.style.display="block";var e=bl(n,"vuepress-plugin-demo-block__code"),t=bl(n,"vuepress-plugin-demo-block__display"),i=bl(n,"vuepress-plugin-demo-block__footer"),r=bl(t,"vuepress-plugin-demo-block__app"),a=decodeURIComponent(n.dataset.code),o=decodeURIComponent(n.dataset.config),l=decodeURIComponent(n.dataset.type);o=o?JSON.parse(o):{};var s=e.querySelector("div").clientHeight,c="react"===l?function(n,e){var t=(0,window.Babel.transform)(n,{presets:["es2015","react"]}).code,i="(function(exports){var module={};module.exports=exports;".concat(t,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),r=new Function("return ".concat(i))(),a={js:r,css:r.__style__||"",jsLib:e.jsLib||[],cssLib:e.cssLib||[],jsTpl:kl(n),htmlTpl:gl("")},o=vl("react"),l=vl("reactDOM");return a.jsLib.unshift(o,l),a}(a,o):"vanilla"===l?_l(a,o):xl(a,o),d=yl("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(i.appendChild(d),d.addEventListener("click",Tl.bind(null,d,s,e,i)),vl("jsfiddle")&&i.appendChild(function(n){var e=n.css,t=n.htmlTpl,i=n.jsTpl,r=n.jsLib,a=n.cssLib,o=r.concat(a).concat(vl("cssLib")).concat(vl("jsLib")).join(",");return yl("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:e}},{tag:"input",attrs:{type:"hidden",name:"html",value:t}},{tag:"input",attrs:{type:"hidden",name:"js",value:i}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:o}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}])}(c)),vl("codepen")&&i.appendChild(function(n){var e=n.css,t=n.htmlTpl,i=n.jsTpl,r=n.jsLib,a=n.cssLib,o=JSON.stringify({css:e,html:t,js:i,js_external:r.concat(vl("jsLib")).join(";"),css_external:a.concat(vl("cssLib")).join(";"),layout:vl("codepenLayout"),js_pre_processor:vl("codepenJsProcessor"),editors:vl("codepenEditors")});return yl("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:o}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(c)),void 0!==o.horizontal?o.horizontal:vl("horizontal")){n.classList.add("vuepress-plugin-demo-block__horizontal");var u=e.firstChild.cloneNode(!0);u.classList.add("vuepress-plugin-demo-block__h_code"),t.appendChild(u)}if(c.css&&function(n){if(!fl[n]){var e=yl("style",{innerHTML:n});document.body.appendChild(e),fl[n]=!0}}(c.css),"react"===l)ReactDOM.render(React.createElement(c.js),r);else if("vue"===l){var p=(new(Vue.extend(c.script))).$mount();r.appendChild(p.$el)}else"vanilla"===l&&(r.innerHTML=c.html,new Function("return (function(){".concat(c.script,"})()"))());n.dataset.created="true"}})):setTimeout((function(n){wl()}),300)}function Tl(n,e,t,i){var r="1"!==n.dataset.isExpand;t.style.height=r?"".concat(e,"px"):0,r?i.classList.add("vuepress-plugin-demo-block__show-link"):i.classList.remove("vuepress-plugin-demo-block__show-link"),n.dataset.isExpand=r?"1":"0"}var Il={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},wl()},updated:function(){wl()}},zl=(t(223),"auto"),Sl="zoom-in",jl="zoom-out",Pl="grab",El="move";function ql(n,e,t){var i=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],r={passive:!1};i?n.addEventListener(e,t,r):n.removeEventListener(e,t,r)}function Ol(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function Al(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function $l(n,e,t){!function(n){var e=Cl,t=Ll;if(n.transition){var i=n.transition;delete n.transition,n[e]=i}if(n.transform){var r=n.transform;delete n.transform,n[t]=r}}(e);var i=n.style,r={};for(var a in e)t&&(r[a]=i[a]||""),i[a]=e[a];return r}var Cl="transition",Ll="transform",Ml="transform",Dl="transitionend";var Rl=function(){},Bl={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Rl,onClose:Rl,onGrab:Rl,onMove:Rl,onRelease:Rl,onBeforeOpen:Rl,onBeforeClose:Rl,onBeforeGrab:Rl,onBeforeRelease:Rl,onImageLoading:Rl,onImageLoaded:Rl},Nl={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),Fl(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var i=this.lastScrollPosition.x-e,r=this.lastScrollPosition.y-t,a=this.options.scrollThreshold;(Math.abs(r)>=a||Math.abs(i)>=a)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(Ul(n)&&!Fl(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){Ul(n)&&!Fl(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,i=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,i)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,i=e.clientY;this.move(t,i)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function Ul(n){return 0===n.button}function Fl(n){return n.metaKey||n.ctrlKey}var Hl={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,$l(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),ql(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){$l(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},Gl="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},Kl=function(){function n(n,e){for(var t=0;t<e.length;t++){var i=e[t];i.enumerable=i.enumerable||!1,i.configurable=!0,"value"in i&&(i.writable=!0),Object.defineProperty(n,i.key,i)}}return function(e,t,i){return t&&n(e.prototype,t),i&&n(e,i),e}}(),Wl=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var i in t)Object.prototype.hasOwnProperty.call(t,i)&&(n[i]=t[i])}return n},Ql={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=Al(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,i=n.transitionDuration,r=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?Pl:jl,transition:Ml+"\n        "+i+"s\n        "+r,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=$l(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,$l(this.el,{transform:"none"})},grab:function(n,e,t){var i=Vl(),r=i.x-n,a=i.y-e;$l(this.el,{cursor:El,transform:"translate3d(\n        "+(this.translate.x+r)+"px, "+(this.translate.y+a)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var i=Vl(),r=i.x-n,a=i.y-e;$l(this.el,{transition:Ml,transform:"translate3d(\n        "+(this.translate.x+r)+"px, "+(this.translate.y+a)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){$l(this.el,this.styleClose)},restoreOpenStyle:function(){$l(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=Vl(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,i=this.instance.options,r=i.customSize,a=i.scaleBase;if(!r&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(r&&"object"===(void 0===r?"undefined":Gl(r)))return{x:r.width/this.rect.width,y:r.height/this.rect.height};var o=this.rect.width/2,l=this.rect.height/2,s=Vl(),c={x:s.x-o,y:s.y-l},d=c.x/o,u=c.y/l,p=a+Math.min(d,u);if(r&&"string"==typeof r){var m=t||this.el.naturalWidth,h=e||this.el.naturalHeight,f=parseFloat(r)*m/(100*this.rect.width),g=parseFloat(r)*h/(100*this.rect.height);if(p>f||p>g)return{x:f,y:g}}return{x:p,y:p}}};function Vl(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function Jl(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(i){ql(n,i,e[i],t)}))}var Xl=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(Ql),this.overlay=Object.create(Hl),this.handler=Object.create(Nl),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=Wl({},Bl,e),this.overlay.init(this),this.handler.init(this)}return Kl(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=Sl,ql(n,"click",this.handler.click),this.options.preloadImage&&Ol(Al(n)));return this}},{key:"config",value:function(n){return n?(Wl(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var i="string"==typeof n?document.querySelector(n):n;if("IMG"===i.tagName){if(this.options.onBeforeOpen(i),this.target.init(i,this),!this.options.preloadImage){var r=this.target.srcOriginal;null!=r&&(this.options.onImageLoading(i),Ol(r,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),ql(document,"scroll",this.handler.scroll),ql(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&ql(window,"resize",this.handler.resizeWindow);var a=function n(){ql(i,Dl,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&Jl(document,e.handler,!0),t(i)};return ql(i,Dl,a),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=zl,this.overlay.fadeOut(),this.target.zoomOut(),ql(document,"scroll",this.handler.scroll,!1),ql(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&ql(window,"resize",this.handler.resizeWindow,!1);var i=function i(){ql(t,Dl,i,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&Jl(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return ql(t,Dl,i),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,i=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var r=this.target.el;this.options.onBeforeGrab(r),this.released=!1,this.target.grab(n,e,t);var a=function n(){ql(r,Dl,n,!1),i(r)};return ql(r,Dl,a),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,i=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=El,this.target.move(n,e,t);var r=this.target.el,a=function n(){ql(r,Dl,n,!1),i(r)};return ql(r,Dl,a),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=zl,this.target.restoreOpenStyle();var i=function i(){ql(t,Dl,i,!1),n.lock=!1,n.released=!0,e(t)};return ql(t,Dl,i),this}}}]),n}(),Yl=".theme-vdoing-content img:not(.no-zoom)",Zl=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),ns=Number("500"),es=function(){function n(){cl(this,n),this.instance=new Xl(Zl)}return ul(n,[{key:"update",value:function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:Yl;"undefined"!=typeof window&&this.instance.listen(n)}},{key:"updateDelay",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:Yl,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:ns;setTimeout((function(){return n.update(e)}),t)}}]),n}(),ts=[Zo,al,sl,ml,Il,{watch:{"$page.path":function(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted:function(){this.$vuepress.zooming=new es,this.$vuepress.zooming.updateDelay()}}],is={name:"GlobalLayout",computed:{layout:function(){var n=this.getLayout();return Wo("layout",n),Ar.component(n)}},methods:{getLayout:function(){if(this.$page.path){var n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},rs=t(20),as=Object(rs.a)(is,(function(){var n=this.$createElement;return(this._self._c||n)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){var i;switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),(i=n[e]).push.apply(i,Object(So.a)(t));break;default:throw new Error("Unknown option name.")}}(as,"mixins",ts);var os=[{name:"v-1ae7cc2e",path:"/midware/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-1ae7cc2e").then(t)}},{path:"/midware/index.html",redirect:"/midware/"},{path:"/00.目录页/02.中间件.html",redirect:"/midware/"},{name:"v-f3a4c5c8",path:"/code/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-f3a4c5c8").then(t)}},{path:"/code/index.html",redirect:"/code/"},{path:"/00.目录页/03.编程.html",redirect:"/code/"},{name:"v-77b9adf3",path:"/life/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-77b9adf3").then(t)}},{path:"/life/index.html",redirect:"/life/"},{path:"/00.目录页/04.生活.html",redirect:"/life/"},{name:"v-24c441a1",path:"/db/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-24c441a1").then(t)}},{path:"/db/index.html",redirect:"/db/"},{path:"/00.目录页/01.数据库.html",redirect:"/db/"},{name:"v-550e6dee",path:"/pages/843f56/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-550e6dee").then(t)}},{path:"/pages/843f56/index.html",redirect:"/pages/843f56/"},{path:"/01.数据库/05.ELK/01.ES相关知识点整理.html",redirect:"/pages/843f56/"},{name:"v-67450649",path:"/pages/2f7a8b/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-67450649").then(t)}},{path:"/pages/2f7a8b/index.html",redirect:"/pages/2f7a8b/"},{path:"/01.数据库/05.ELK/04.ES_常见问题列表.html",redirect:"/pages/2f7a8b/"},{name:"v-5babeb51",path:"/pages/127f0c/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-5babeb51").then(t)}},{path:"/pages/127f0c/index.html",redirect:"/pages/127f0c/"},{path:"/01.数据库/05.ELK/02.ES架构规划.html",redirect:"/pages/127f0c/"},{name:"v-636c139f",path:"/pages/8d99ee/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-636c139f").then(t)}},{path:"/pages/8d99ee/index.html",redirect:"/pages/8d99ee/"},{path:"/01.数据库/05.ELK/03.ES_API集合.html",redirect:"/pages/8d99ee/"},{name:"v-3f34187e",path:"/pages/0765a4/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-3f34187e").then(t)}},{path:"/pages/0765a4/index.html",redirect:"/pages/0765a4/"},{path:"/02.中间件/01.K8S/03.Harbor镜像管理.html",redirect:"/pages/0765a4/"},{name:"v-1d67f52e",path:"/pages/cea09f/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-1d67f52e").then(t)}},{path:"/pages/cea09f/index.html",redirect:"/pages/cea09f/"},{path:"/02.中间件/01.K8S/01.K8S基础知识点整理.html",redirect:"/pages/cea09f/"},{name:"v-6d870201",path:"/pages/97264c/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-6d870201").then(t)}},{path:"/pages/97264c/index.html",redirect:"/pages/97264c/"},{path:"/02.中间件/01.K8S/02.Registry镜像管理.html",redirect:"/pages/97264c/"},{name:"v-1b38a950",path:"/pages/42d30d/",component:as,beforeEnter:function(n,e,t){Ko("post","v-1b38a950").then(t)}},{path:"/pages/42d30d/index.html",redirect:"/pages/42d30d/"},{path:"/01.数据库/05.ELK/85.ECE考试总结.html",redirect:"/pages/42d30d/"},{name:"v-00fe5646",path:"/pages/107c9b/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-00fe5646").then(t)}},{path:"/pages/107c9b/index.html",redirect:"/pages/107c9b/"},{path:"/03.编程/10.Python/01.Python基础知识点.html",redirect:"/pages/107c9b/"},{name:"v-01596c50",path:"/pages/63a425/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-01596c50").then(t)}},{path:"/pages/63a425/index.html",redirect:"/pages/63a425/"},{path:"/03.编程/10.Python/03.《从Python开始学编程》读书笔记.html",redirect:"/pages/63a425/"},{name:"v-65da262e",path:"/pages/5cdc7e/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-65da262e").then(t)}},{path:"/pages/5cdc7e/index.html",redirect:"/pages/5cdc7e/"},{path:"/02.中间件/01.K8S/80.华为CKA认证.html",redirect:"/pages/5cdc7e/"},{name:"v-7a4a7900",path:"/pages/349c56/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-7a4a7900").then(t)}},{path:"/pages/349c56/index.html",redirect:"/pages/349c56/"},{path:"/03.编程/80.数据结构和算法/70.883数据结构和算法.html",redirect:"/pages/349c56/"},{name:"v-5dae9843",path:"/pages/e5d8ed/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-5dae9843").then(t)}},{path:"/pages/e5d8ed/index.html",redirect:"/pages/e5d8ed/"},{path:"/04.生活/01.学习方法/02.《暗时间》读书笔记.html",redirect:"/pages/e5d8ed/"},{name:"v-6fd0f028",path:"/pages/3a1429/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-6fd0f028").then(t)}},{path:"/pages/3a1429/index.html",redirect:"/pages/3a1429/"},{path:"/04.生活/01.学习方法/03.《大脑强人》读书笔记.html",redirect:"/pages/3a1429/"},{name:"v-5e595d06",path:"/pages/8a0e8c/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-5e595d06").then(t)}},{path:"/pages/8a0e8c/index.html",redirect:"/pages/8a0e8c/"},{path:"/04.生活/01.学习方法/01.《原子习惯》读书笔记.html",redirect:"/pages/8a0e8c/"},{name:"v-5a5a4a6b",path:"/friends/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-5a5a4a6b").then(t)}},{path:"/friends/index.html",redirect:"/friends/"},{path:"/04.生活/99.友情链接.html",redirect:"/friends/"},{name:"v-088adbe6",path:"/pages/485aeb/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-088adbe6").then(t)}},{path:"/pages/485aeb/index.html",redirect:"/pages/485aeb/"},{path:"/04.生活/01.学习方法/04.《直线学习法》读书笔记.html",redirect:"/pages/485aeb/"},{name:"v-b5f9e054",path:"/pages/e80362/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-b5f9e054").then(t)}},{path:"/pages/e80362/index.html",redirect:"/pages/e80362/"},{path:"/04.生活/80.杂记/01.学习杂记.html",redirect:"/pages/e80362/"},{name:"v-a670efd8",path:"/pages/beb6c0bd8a66cea6/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-a670efd8").then(t)}},{path:"/pages/beb6c0bd8a66cea6/index.html",redirect:"/pages/beb6c0bd8a66cea6/"},{path:"/06.收藏夹/01.网站.html",redirect:"/pages/beb6c0bd8a66cea6/"},{name:"v-73e22b34",path:"/pages/47cf96/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-73e22b34").then(t)}},{path:"/pages/47cf96/index.html",redirect:"/pages/47cf96/"},{path:"/06.收藏夹/02.常用的前端轮子.html",redirect:"/pages/47cf96/"},{name:"v-6d447165",path:"/categories/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-6d447165").then(t)}},{path:"/categories/index.html",redirect:"/categories/"},{path:"/@pages/categoriesPage.html",redirect:"/categories/"},{name:"v-d552c9e2",path:"/about/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-d552c9e2").then(t)}},{path:"/about/index.html",redirect:"/about/"},{path:"/05.关于/01.关于.html",redirect:"/about/"},{name:"v-4849b605",path:"/archives/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-4849b605").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-c07cc8f6",path:"/tags/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-c07cc8f6").then(t)}},{path:"/tags/index.html",redirect:"/tags/"},{path:"/@pages/tagsPage.html",redirect:"/tags/"},{name:"v-70b16331",path:"/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-70b16331").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-60f6b60a",path:"/pages/f2e63f/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-60f6b60a").then(t)}},{path:"/pages/f2e63f/index.html",redirect:"/pages/f2e63f/"},{path:"/_posts/随笔/你知道的越多，不知道的也就越多.html",redirect:"/pages/f2e63f/"},{name:"v-91f7fbec",path:"/pages/cd8bde/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-91f7fbec").then(t)}},{path:"/pages/cd8bde/index.html",redirect:"/pages/cd8bde/"},{path:"/_posts/随笔/拥抱生活，拥抱快乐.html",redirect:"/pages/cd8bde/"},{name:"v-233673d3",path:"/pages/98f6c7/",component:as,beforeEnter:function(n,e,t){Ko("Layout","v-233673d3").then(t)}},{path:"/pages/98f6c7/index.html",redirect:"/pages/98f6c7/"},{path:"/03.编程/80.数据结构和算法/01.《数据结构与算法之美》读书笔记.html",redirect:"/pages/98f6c7/"},{path:"*",component:as}],ls={title:"",description:"",base:"/wiki/",headTags:[["link",{rel:"icon",href:"/wiki/img/favicon.ico"}],["meta",{name:"keywords",content:"个人技术博客,技术文档,学习,面试,vue,python,git,github,markdown"}],["meta",{name:"baidu-site-verification",content:"7F55weZDDc"}],["meta",{name:"theme-color",content:"#11a8cd"}],["meta",{name:"referrer",content:"no-referrer-when-downgrade"}],["link",{rel:"stylesheet",href:"https://at.alicdn.com/t/font_3077305_pt8umhrn4k9.css"}]],pages:[{title:"页面",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"02.中间件",imgUrl:"/img/ui.png",description:"html(5)/css(3)，前端页面相关技术"}},title:"页面",date:"2020-03-11T21:50:54.000Z",permalink:"/midware",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/02.%E4%B8%AD%E9%97%B4%E4%BB%B6.html",relativePath:"00.目录页/02.中间件.md",key:"v-1ae7cc2e",path:"/midware/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"技术",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"03.编程",imgUrl:"/img/other.png",description:"技术文档、教程、技巧、总结等文章"}},title:"技术",date:"2020-03-11T21:50:55.000Z",permalink:"/code",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/03.%E7%BC%96%E7%A8%8B.html",relativePath:"00.目录页/03.编程.md",key:"v-f3a4c5c8",path:"/code/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"更多",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"04.生活",imgUrl:"/img/more.png",description:"学习、面试、在线工具等更多文章和页面"}},title:"更多",date:"2020-03-11T21:50:56.000Z",permalink:"/life",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/04.%E7%94%9F%E6%B4%BB.html",relativePath:"00.目录页/04.生活.md",key:"v-77b9adf3",path:"/life/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"数据库",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"01.数据库",imgUrl:"/img/web.png",description:"Mysql、Postgresql、ElasticSearch等数据库技术"}},title:"数据库",date:"2020-03-11T21:50:53.000Z",permalink:"/db",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/01.%E6%95%B0%E6%8D%AE%E5%BA%93.html",relativePath:"00.目录页/01.数据库.md",key:"v-24c441a1",path:"/db/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"ES相关知识点整理",frontmatter:{title:"ES相关知识点整理",date:"2022-01-20T11:25:57.000Z",permalink:"/pages/843f56/",categories:["数据库","ELK"],tags:[null]},regularPath:"/01.%E6%95%B0%E6%8D%AE%E5%BA%93/05.ELK/01.ES%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86.html",relativePath:"01.数据库/05.ELK/01.ES相关知识点整理.md",key:"v-550e6dee",path:"/pages/843f56/",headers:[{level:2,title:"文档",slug:"文档",normalizedTitle:"文档",charIndex:78},{level:3,title:"JSON文档",slug:"json文档",normalizedTitle:"json文档",charIndex:315},{level:3,title:"文档的元数据",slug:"文档的元数据",normalizedTitle:"文档的元数据",charIndex:428},{level:2,title:"索引",slug:"索引",normalizedTitle:"索引",charIndex:470},{level:3,title:"索引的不同语意",slug:"索引的不同语意",normalizedTitle:"索引的不同语意",charIndex:788},{level:3,title:"Type",slug:"type",normalizedTitle:"type",charIndex:914},{level:2,title:"ES vs RDBMS的类比",slug:"es-vs-rdbms的类比",normalizedTitle:"es vs rdbms的类比",charIndex:1004},{level:2,title:"ES分布式特性",slug:"es分布式特性",normalizedTitle:"es分布式特性",charIndex:1600},{level:2,title:"节点",slug:"节点",normalizedTitle:"节点",charIndex:1462},{level:3,title:"Master Node",slug:"master-node",normalizedTitle:"master node",charIndex:2093},{level:3,title:"Master-eligible nodes & 选主流程",slug:"master-eligible-nodes-选主流程",normalizedTitle:"master-eligible nodes &amp; 选主流程",charIndex:null},{level:3,title:"集群状态",slug:"集群状态",normalizedTitle:"集群状态",charIndex:2251},{level:3,title:"Master Eligible Nodes & 选主的过程",slug:"master-eligible-nodes-选主的过程",normalizedTitle:"master eligible nodes &amp; 选主的过程",charIndex:null},{level:3,title:"Data Node",slug:"data-node",normalizedTitle:"data node",charIndex:3178},{level:3,title:"Coordinating Node",slug:"coordinating-node",normalizedTitle:"coordinating node",charIndex:1438},{level:3,title:"Ingress Node",slug:"ingress-node",normalizedTitle:"ingress node",charIndex:3777},{level:3,title:"其他的节点类型",slug:"其他的节点类型",normalizedTitle:"其他的节点类型",charIndex:4002},{level:3,title:"脑裂问题",slug:"脑裂问题",normalizedTitle:"脑裂问题",charIndex:4190},{level:3,title:"如何避免脑裂问题",slug:"如何避免脑裂问题",normalizedTitle:"如何避免脑裂问题",charIndex:4369},{level:3,title:"配置节点类型",slug:"配置节点类型",normalizedTitle:"配置节点类型",charIndex:4738},{level:2,title:"分片(Primary Shard&Replica Shard)",slug:"分片-primary-shard-replica-shard",normalizedTitle:"分片(primary shard&amp;replica shard)",charIndex:null},{level:2,title:"倒排索引",slug:"倒排索引",normalizedTitle:"倒排索引",charIndex:870},{level:3,title:"正排索引和倒排索引示例",slug:"正排索引和倒排索引示例",normalizedTitle:"正排索引和倒排索引示例",charIndex:5039},{level:3,title:"倒排索引概念",slug:"倒排索引概念",normalizedTitle:"倒排索引概念",charIndex:5057},{level:2,title:"分词器",slug:"分词器",normalizedTitle:"分词器",charIndex:5218},{level:3,title:"什么是Analysis",slug:"什么是analysis",normalizedTitle:"什么是analysis",charIndex:5644},{level:3,title:"Analysis与Analyzer什么关系",slug:"analysis与analyzer什么关系",normalizedTitle:"analysis与analyzer什么关系",charIndex:5710},{level:3,title:"什么时候需要使用Analyzer分词器",slug:"什么时候需要使用analyzer分词器",normalizedTitle:"什么时候需要使用analyzer分词器",charIndex:5798},{level:3,title:"Analyzer由哪些部分组成",slug:"analyzer由哪些部分组成",normalizedTitle:"analyzer由哪些部分组成",charIndex:5901},{level:3,title:"Character Filters",slug:"character-filters",normalizedTitle:"character filters",charIndex:5940},{level:3,title:"Tokenizer",slug:"tokenizer",normalizedTitle:"tokenizer",charIndex:5978},{level:3,title:"Token Filters",slug:"token-filters",normalizedTitle:"token filters",charIndex:7464},{level:3,title:"Elasticsearch的内置分词器有哪些",slug:"elasticsearch的内置分词器有哪些",normalizedTitle:"elasticsearch的内置分词器有哪些",charIndex:7939},{level:3,title:"Standard Analyer",slug:"standard-analyer",normalizedTitle:"standard analyer",charIndex:8247},{level:3,title:"Simple Analyzer",slug:"simple-analyzer",normalizedTitle:"simple analyzer",charIndex:8365},{level:3,title:"Whitespace Analyzer",slug:"whitespace-analyzer",normalizedTitle:"whitespace analyzer",charIndex:8481},{level:3,title:"Stop Analyzer",slug:"stop-analyzer",normalizedTitle:"stop analyzer",charIndex:8592},{level:3,title:"Keyword Analyzer",slug:"keyword-analyzer",normalizedTitle:"keyword analyzer",charIndex:8723},{level:3,title:"Pattern Analyzer",slug:"pattern-analyzer",normalizedTitle:"pattern analyzer",charIndex:8764},{level:3,title:"Language Analyzer",slug:"language-analyzer",normalizedTitle:"language analyzer",charIndex:8785},{level:3,title:"ICU Analyzer",slug:"icu-analyzer",normalizedTitle:"icu analyzer",charIndex:8939},{level:2,title:"搜索相关性Relevance",slug:"搜索相关性relevance",normalizedTitle:"搜索相关性relevance",charIndex:9168},{level:2,title:"Mapping",slug:"mapping",normalizedTitle:"mapping",charIndex:747},{level:3,title:"什么是mapping",slug:"什么是mapping",normalizedTitle:"什么是mapping",charIndex:9389},{level:3,title:"ES中字段的数据类型有哪些",slug:"es中字段的数据类型有哪些",normalizedTitle:"es中字段的数据类型有哪些",charIndex:9647},{level:3,title:"什么是多字段类型",slug:"什么是多字段类型",normalizedTitle:"什么是多字段类型",charIndex:9837},{level:3,title:"什么是精确值，什么是全文本",slug:"什么是精确值-什么是全文本",normalizedTitle:"什么是精确值，什么是全文本",charIndex:10739},{level:3,title:"什么是Dynamic Mapping",slug:"什么是dynamic-mapping",normalizedTitle:"什么是dynamic mapping",charIndex:10926},{level:3,title:"Dynamic Mapping中类型如何自动识别",slug:"dynamic-mapping中类型如何自动识别",normalizedTitle:"dynamic mapping中类型如何自动识别",charIndex:11144},{level:3,title:"能否更改mapping的字段类型",slug:"能否更改mapping的字段类型",normalizedTitle:"能否更改mapping的字段类型",charIndex:11394},{level:3,title:"控制Dynamic Mappings",slug:"控制dynamic-mappings",normalizedTitle:"控制dynamic mappings",charIndex:11731},{level:2,title:"Index Template",slug:"index-template",normalizedTitle:"index template",charIndex:12220},{level:3,title:"什么是Index Template",slug:"什么是index-template",normalizedTitle:"什么是index template",charIndex:12239},{level:3,title:"Index Template的工作方式",slug:"index-template的工作方式",normalizedTitle:"index template的工作方式",charIndex:12435},{level:2,title:"Dynamic Template",slug:"dynamic-template",normalizedTitle:"dynamic template",charIndex:13143},{level:3,title:"什么是Dynamic Template",slug:"什么是dynamic-template",normalizedTitle:"什么是dynamic template",charIndex:13164},{level:3,title:"匹配规则参数",slug:"匹配规则参数",normalizedTitle:"匹配规则参数",charIndex:13879},{level:2,title:"聚合分析",slug:"聚合分析",normalizedTitle:"聚合分析",charIndex:13992},{level:3,title:"什么是聚合Aggregation",slug:"什么是聚合aggregation",normalizedTitle:"什么是聚合aggregation",charIndex:14001},{level:3,title:"聚合的分类",slug:"聚合的分类",normalizedTitle:"聚合的分类",charIndex:14161},{level:3,title:"聚合的作用范围及排序",slug:"聚合的作用范围及排序",normalizedTitle:"聚合的作用范围及排序",charIndex:14334},{level:3,title:"聚合的精准度问题",slug:"聚合的精准度问题",normalizedTitle:"聚合的精准度问题",charIndex:14430},{level:3,title:"Aggregation的语法",slug:"aggregation的语法",normalizedTitle:"aggregation的语法",charIndex:15191},{level:3,title:"Bucket & Metric",slug:"bucket-metric",normalizedTitle:"bucket &amp; metric",charIndex:null},{level:3,title:"Buket",slug:"buket",normalizedTitle:"buket",charIndex:15334},{level:3,title:"Metric",slug:"metric",normalizedTitle:"metric",charIndex:14209},{level:2,title:"Term Query",slug:"term-query",normalizedTitle:"term query",charIndex:16743},{level:2,title:"基于全文的查询",slug:"基于全文的查询",normalizedTitle:"基于全文的查询",charIndex:17832},{level:3,title:"Match Query查询过程",slug:"match-query查询过程",normalizedTitle:"match query查询过程",charIndex:18121},{level:2,title:"结构化搜索",slug:"结构化搜索",normalizedTitle:"结构化搜索",charIndex:18143},{level:3,title:"什么是结构化数据",slug:"什么是结构化数据",normalizedTitle:"什么是结构化数据",charIndex:18181},{level:3,title:"ES中的结构化搜索",slug:"es中的结构化搜索",normalizedTitle:"es中的结构化搜索",charIndex:18348},{level:2,title:"搜索相关性算分",slug:"搜索相关性算分",normalizedTitle:"搜索相关性算分",charIndex:18517},{level:3,title:"相关性和相关性算分",slug:"相关性和相关性算分",normalizedTitle:"相关性和相关性算分",charIndex:18529},{level:3,title:"词频 TF",slug:"词频-tf",normalizedTitle:"词频 tf",charIndex:18719},{level:3,title:"逆文档频率 IDF",slug:"逆文档频率-idf",normalizedTitle:"逆文档频率 idf",charIndex:18919},{level:3,title:"TF-IDF的评分公式",slug:"tf-idf的评分公式",normalizedTitle:"tf-idf的评分公式",charIndex:19117},{level:3,title:"BM 25",slug:"bm-25",normalizedTitle:"bm 25",charIndex:18709},{level:3,title:"自定义相似性",slug:"自定义相似性",normalizedTitle:"自定义相似性",charIndex:19234},{level:3,title:"Explain API查看TF-IDF",slug:"explain-api查看tf-idf",normalizedTitle:"explain api查看tf-idf",charIndex:19265},{level:3,title:"Boosting Relevance",slug:"boosting-relevance",normalizedTitle:"boosting relevance",charIndex:19581},{level:2,title:"Query Context & Filter Context",slug:"query-context-filter-context",normalizedTitle:"query context &amp; filter context",charIndex:null},{level:2,title:"布尔查询",slug:"布尔查询",normalizedTitle:"布尔查询",charIndex:20061},{level:2,title:"Disjunction Max Query",slug:"disjunction-max-query",normalizedTitle:"disjunction max query",charIndex:20551},{level:2,title:"Multi Match",slug:"multi-match",normalizedTitle:"multi match",charIndex:21360},{level:2,title:"Search Template",slug:"search-template",normalizedTitle:"search template",charIndex:21756},{level:2,title:"Suggester API",slug:"suggester-api",normalizedTitle:"suggester api",charIndex:21940},{level:3,title:"什么是搜索建议",slug:"什么是搜索建议",normalizedTitle:"什么是搜索建议",charIndex:21958},{level:3,title:"精准度和召回率",slug:"精准度和召回率",normalizedTitle:"精准度和召回率",charIndex:22132},{level:3,title:"ES Suggester API",slug:"es-suggester-api",normalizedTitle:"es suggester api",charIndex:22269},{level:3,title:"Term Suggester 和Phrase Suggester",slug:"term-suggester-和phrase-suggester",normalizedTitle:"term suggester 和phrase suggester",charIndex:22471},{level:3,title:"Completion Suggester",slug:"completion-suggester",normalizedTitle:"completion suggester",charIndex:22775},{level:3,title:"Context Suggester",slug:"context-suggester",normalizedTitle:"context suggester",charIndex:22449},{level:2,title:"文档分布式存储",slug:"文档分布式存储",normalizedTitle:"文档分布式存储",charIndex:23129},{level:3,title:"概述文档存储",slug:"概述文档存储",normalizedTitle:"概述文档存储",charIndex:23186},{level:3,title:"文档到分片路由算法",slug:"文档到分片路由算法",normalizedTitle:"文档到分片路由算法",charIndex:23443},{level:3,title:"倒排索引不可变性",slug:"倒排索引不可变性",normalizedTitle:"倒排索引不可变性",charIndex:23772},{level:3,title:"Lucene Index",slug:"lucene-index",normalizedTitle:"lucene index",charIndex:24006},{level:3,title:"什么是Refresh",slug:"什么是refresh",normalizedTitle:"什么是refresh",charIndex:24305},{level:3,title:"什么是Transaction Log",slug:"什么是transaction-log",normalizedTitle:"什么是transaction log",charIndex:24548},{level:3,title:"什么是Flush",slug:"什么是flush",normalizedTitle:"什么是flush",charIndex:24784},{level:3,title:"什么是Merge",slug:"什么是merge",normalizedTitle:"什么是merge",charIndex:25115},{level:3,title:"整个索引文档到分片的流程",slug:"整个索引文档到分片的流程",normalizedTitle:"整个索引文档到分片的流程",charIndex:25345},{level:2,title:"logstash是什么",slug:"logstash是什么",normalizedTitle:"logstash是什么",charIndex:27075},{level:2,title:"logstash中的基本概念",slug:"logstash中的基本概念",normalizedTitle:"logstash中的基本概念",charIndex:27228},{level:2,title:"logstash架构简介",slug:"logstash架构简介",normalizedTitle:"logstash架构简介",charIndex:27464},{level:2,title:"Input Plugins",slug:"input-plugins",normalizedTitle:"input plugins",charIndex:27615},{level:2,title:"Codec Plugins",slug:"codec-plugins",normalizedTitle:"codec plugins",charIndex:27803},{level:2,title:"Filter Plugins",slug:"filter-plugins",normalizedTitle:"filter plugins",charIndex:27998},{level:2,title:"Output Plugins",slug:"output-plugins",normalizedTitle:"output plugins",charIndex:28183},{level:2,title:"Queue",slug:"queue",normalizedTitle:"queue",charIndex:28422},{level:2,title:"Input Plugin -File",slug:"input-plugin-file",normalizedTitle:"input plugin -file",charIndex:28921},{level:2,title:"Code Plugin - Multiline",slug:"code-plugin-multiline",normalizedTitle:"code plugin - multiline",charIndex:29089},{level:2,title:"Filter Plugin",slug:"filter-plugin",normalizedTitle:"filter plugin",charIndex:27998},{level:3,title:"Filter Plugin -Mutate",slug:"filter-plugin-mutate",normalizedTitle:"filter plugin -mutate",charIndex:29455}],headersStr:"文档 JSON文档 文档的元数据 索引 索引的不同语意 Type ES vs RDBMS的类比 ES分布式特性 节点 Master Node Master-eligible nodes & 选主流程 集群状态 Master Eligible Nodes & 选主的过程 Data Node Coordinating Node Ingress Node 其他的节点类型 脑裂问题 如何避免脑裂问题 配置节点类型 分片(Primary Shard&Replica Shard) 倒排索引 正排索引和倒排索引示例 倒排索引概念 分词器 什么是Analysis Analysis与Analyzer什么关系 什么时候需要使用Analyzer分词器 Analyzer由哪些部分组成 Character Filters Tokenizer Token Filters Elasticsearch的内置分词器有哪些 Standard Analyer Simple Analyzer Whitespace Analyzer Stop Analyzer Keyword Analyzer Pattern Analyzer Language Analyzer ICU Analyzer 搜索相关性Relevance Mapping 什么是mapping ES中字段的数据类型有哪些 什么是多字段类型 什么是精确值，什么是全文本 什么是Dynamic Mapping Dynamic Mapping中类型如何自动识别 能否更改mapping的字段类型 控制Dynamic Mappings Index Template 什么是Index Template Index Template的工作方式 Dynamic Template 什么是Dynamic Template 匹配规则参数 聚合分析 什么是聚合Aggregation 聚合的分类 聚合的作用范围及排序 聚合的精准度问题 Aggregation的语法 Bucket & Metric Buket Metric Term Query 基于全文的查询 Match Query查询过程 结构化搜索 什么是结构化数据 ES中的结构化搜索 搜索相关性算分 相关性和相关性算分 词频 TF 逆文档频率 IDF TF-IDF的评分公式 BM 25 自定义相似性 Explain API查看TF-IDF Boosting Relevance Query Context & Filter Context 布尔查询 Disjunction Max Query Multi Match Search Template Suggester API 什么是搜索建议 精准度和召回率 ES Suggester API Term Suggester 和Phrase Suggester Completion Suggester Context Suggester 文档分布式存储 概述文档存储 文档到分片路由算法 倒排索引不可变性 Lucene Index 什么是Refresh 什么是Transaction Log 什么是Flush 什么是Merge 整个索引文档到分片的流程 logstash是什么 logstash中的基本概念 logstash架构简介 Input Plugins Codec Plugins Filter Plugins Output Plugins Queue Input Plugin -File Code Plugin - Multiline Filter Plugin Filter Plugin -Mutate",content:'ELK中相关概念知识点整理\n\n----------------------------------------\n\n\n# Elasticsearch\n\n\n# 文档\n\n * ES是面向文档的，文档是所有可搜索数据的最小单位。\n   \n   比如日志文件中的一条日志项；一本电影的具体信息；一张唱片的具体信息；MP3播放器里的一首歌；一篇PDF文档中的具体内容。\n\n * 文档会被序列化JSON格式，保存在ES中。\n   \n   JSON对象由字段组成；每个字段都有对应的字段类型(字符串、数值、布尔、日期、二进制、范围类型)\n\n * 每个文档都有一个Unique ID\n   \n   我们可以自己指定；或者通过ES自动生成。\n\n\n# JSON文档\n\n * 一篇文档包含了一系列的字段。类似数据库表中一条记录。\n\n * JSON文档，格式灵活，不需要预先定义格式。\n   \n   字段的类型可以指定或者通过ES自动推算\n   \n   支持数组、支持嵌套\n\n\n# 文档的元数据\n\n元数据是用来标注文档的相关信息的。\n\n * _index 文档所属的索引名\n * _type 文档所属的类型名\n * _id 文档唯一的ID\n * _source 文档的原始Json数据\n * _all 整合所有字段内容到该字段，已废弃\n * _version 文档的版本信息\n * _score 相关性打分\n\n\n# 索引\n\nindex 索引是文档的容器，是一类文档的结合。\n\n * index 体现了逻辑空间的概念：每个索引都有自己的mapping定义，用于定义包含的文档的字段名和字段类型。\n * shard体现了物理空间的概念：索引中的数据分散在shard分片上。\n\n索引的mapping与settings\n\n * Mapping定义文档字段的类型\n * setting定义不同的数据分布\n\n\n# 索引的不同语意\n\n * 名词：一个ES集群中，可以创建很多个不同的索引\n * 动词：保存一个文档到ES的过程也叫做索引(indexing)，也就是在ES中，创建一个倒排索引的过程。\n * 名词：通常可以理解为一个B树索引，在ES中是倒排索引。\n\n\n# Type\n\n * 在7.0之前，一个index可以设置多个Types\n * 6.0开始，Type已经被废弃。从7.0开始，一个索引只能创建一个Type 也就是"_doc"\n\n\n# ES vs RDBMS的类比\n\n传统关系型数据库和ES的区别有：\n\n * ES的相关性、高性能全文检索\n * RDMS 用于事务性，join\n\nRDBMS                          ELASTICSEARCH\nrow                            document\ntable                          index\ncolumn                         filed\nschema                         mapping\n分布式Mysql中设置的分片数量               setting\nSQL                            dsl\n分布式mysql分片路由信息保存在ZK中           master node(表现为ES的一个JAVA进程)\n有点类似访问代理                       Coordinating Node\nmysql的节点实例                     data node(表现为ES的一个JAVA进程)\nset集群中的master节点上的主的schema的分片   主分片(一个lucene实例)\nset集群中的slave节点上的从的schema的分片    副本分片\n\n\n# ES分布式特性\n\n * 高可用\n   \n   服务可用性：允许有节点停止服务\n   \n   数据可用性：部分节点丢失，不会丢失数据\n\n * 可扩展性\n   \n   请求量提升、数据的不断增长（将数据分布到所有节点上）\n\n * ES的分布式架构的好处\n   \n   存储的水平扩容。\n   \n   提高系统的可用性，部分节点停止服务，这个集群的服务不受影响。\n\n * ES的分布式架构\n   \n   不同的集群通过不同的名字来区分，默认名字"elasticsearch"。\n   \n   通过配置文件修改，或者在命令行中 -E cluster.name=geektime进行设定。\n   \n   一个集群可以有一个或多个节点。\n\n\n# 节点\n\n * 节点是一个ES的实例\n   \n   本质上就是一个JAVA进程；一台机器上可以运行多个ES进程，但是生产环境一般建议一台机器上只运行一个ES实例。\n\n * 每一个节点都有名字，通过配置文件配置，或者启动的时候使用 -E node.name=node1指定。\n\n * 每一个节点在启动之后，会分配一个UID，保存在data目录下。\n\n\n# Master Node\n\n * Master Node的职责\n   \n   处理创建，删除索引等请求 / 决定分片被分配到哪个节点 / 负责索引的创建与删除\n   \n   维护并且更新Cluster State\n   \n   每个节点上都保存了集群的状态，只有master节点才能修改集群的状态信息。\n   \n   集群状态，维护了一个集群中必要的信息（所有的节点信息、所有的索引和其相关的Mapping与Setting信息、分片的路由信息）\n\n * Master Node的最佳实践\n   \n   * Master节点非常重要，在部署上需要考虑解决单点的问题。\n   \n   * 为一个集群设置多个Master节点 / 每个节点只承担Master的单一角色。\n     \n     由于索引和搜索数据都是CPU、内存、IO密集型的，可能会对数据节点的资源造成较大压力。因此，在较大规模的集群里，最好要设置单独的仅主节点角色。（这点PB级集群调优时重点关注）\n     \n     不要将主节点同时充当协调节点的角色，因为：对于稳定的集群来说，主节点的角色功能越单一越好。\n\n\n# Master-eligible nodes & 选主流程\n\n * 一个集群，支持配置多个Master Eligible节点。这些节点可以在必要的时候(如Master节点出现故障，网络故障时)参与选主流程，成为Master节点。\n\n * 每个节点启动后，默认就是一个Master eligible节点。\n   \n   可以设置node.master:false禁止\n\n * 当集群内第一个Master eligible节点启动时候，它会将自己选举成Master节点。\n\n * Master-eligible节点可以参加选主流程，成为Master节点\n\n\n# 集群状态\n\n * 集群状态信息，维护了一个集群中，必要的信息\n   \n   所有的节点信息；所有的索引和其相关的mapping与setting信息；分片的路由信息\n\n * 在每个节点上都保存了集群的状态信息\n\n * 但是，只有master节点才能修改集群的状态信息，并负责同步给其他节点\n   \n   因为，任意节点能修改信息会导致cluster state信息的不一致\n\n\n# Master Eligible Nodes & 选主的过程\n\n * 互相ping对方，node id低的会成为被选举的节点\n * 其他节点会加入集群，但是不承担master节点的角色。一旦发现被选中的主节点丢失，就会选举出新的master节点\n\n\n# Data Node\n\n数据节点：保存包含索引文档的分片数据，执行CRUD、搜索、聚合相关的操作。属于：内存、CPU、IO密集型，对硬件资源要求高。\n\n * 可以保存数据的节点，叫做Data Node\n   \n   节点启动后，默认就是数据节点。可以设置node.data:false禁止\n\n * Data Node的职责\n   \n   保存分片数据。在数据扩展上启动了至关重要的作用(由Master Node决定如何把分片分发到数据节点上)\n\n * 通过增加数据节点\n   \n   可以解决数据水平扩展和解决数据单点问题\n\n\n# Coordinating Node\n\n搜索请求在两个阶段中执行（query 和 fetch），这两个阶段由接收客户端请求的节点 - 协调节点协调。\n\n在请求阶段，协调节点将请求转发到保存数据的数据节点。 每个数据节点在本地执行请求并将其结果返回给协调节点。\n\n在收集fetch阶段，协调节点将每个数据节点的结果汇集为单个全局结果集。\n\n * 处理请求的节点，叫做Coordinating Node\n   \n   路由请求到正确的节点，例如对于创建索引的请求，需要路由到Master节点上去执行。\n\n * 所有节点默认都是Coordinating Node\n\n * 通过将其他类型设置成False，使其成为Dedicated Coordinating Node\n\n\n# Ingress Node\n\ningest 节点可以看作是数据前置处理转换的节点，支持 pipeline管道 设置，可以使用 ingest 对数据进行过滤、转换等操作，类似于 logstash 中 filter 的作用，功能相当强大。\n\n可以把Ingest节点的功能抽象为：大数据处理环节的“ETL”——抽取、转换、加载。\n\n比如如何在数据写入阶段修改字段名(不是修改字段值)？比如在批量写入数据的时候，每条document插入实时的时间戳？\n\n\n# 其他的节点类型\n\n * Hot & Warm Node\n   \n   不同硬件配置的Data Node,用来实现Hot&Warm架构，降低集群部署的成本。\n\n * Machine Learing Node\n   \n   负责跑机器学习的Job，用来做异常检测\n\n * Tribe Node\n   \n   从5.3开始使用Cross Cluster Search工具了\n\n\n# 脑裂问题\n\nSplit-Brain，分布式系统的经典网络问题，当出现问题，一个节点和其他节点无法连接\n\n * node2和node3会重新选举master\n * node1自己还是作为master，组成一个集群，同时更新cluster state\n * 导致2个master，维护不同的cluster state。当网络恢复的时候，无法选择正确恢复\n\n\n# 如何避免脑裂问题\n\n * 限定一个选举条件，设置quorum(仲裁)，只有在Master eligible节点大于quorum的时候，才能进行选举。\n   \n   Quorum = (master节点总数/2) +1\n   \n   当3个master eligible的时候，设置discovery.minimum_master_nodes为2，即可避免脑裂\n\n * 从7.0开始，无需这个配置\n   \n   * 移除了minimum_master_nodes参数，让ES自己选择可以形成仲裁的节点\n   * 典型的主节点选举现在只需要很短的时间就可以完成了。集群的伸缩变得更安全、更容易，并且可能造成丢失数据的系统配置选项更少了。\n   * 节点更清楚地记录它们的状态，有助于诊断为什么它们不能加入集群或为什么无法选举出主节点\n\n\n# 配置节点类型\n\n * 开发环境中一个节点可以承担多种角色\n * 生产环境中，应该设置单一的角色的节点(dedicated node)\n\n\n\n\n# 分片(Primary Shard&Replica Shard)\n\n主分片，用以解决数据水平扩展的问题。通过主分片，可以将数据库分布到集群内的所有节点之上。\n\n * 一个分片是一个运行的lucene的实例\n * 主分片数在索引创建时指定，后续不允许修改，除非Reindex\n\n副本，用以解决数据高可用的问题。分片是主分片的拷贝\n\n * 副本分片数，可以动态的调整\n * 增加副本数，还可以在一定程度上提高服务的可用性（读取的吞吐）\n\n\n# 倒排索引\n\n\n# 正排索引和倒排索引示例\n\n\n\n\n# 倒排索引概念\n\n通俗来讲，正向索引是通过key来找value，反向索引则是通过value来找key。从图书来看，正排索引是目录页；倒排索引是索引页。\n\nES分别为JSON文档中的每一个字段都建立了一个倒排索引。\n\n可以指定对某些字段不做索引，优点是可以节省存储空间；缺点是字段无法被搜索。\n\nTerm(单词)：一段文本经过分词器后就会输出一串单词，这一个一个的词就是term。\n\nTerm Dictionary(单词字典)：里面维护的是term，是term的集合。\n\nTerm Index(单词索引)：为了更快的找到某个单词，我们为单词建立索引。\n\nPosting List(倒排列表)：倒排列表记录了出现过某个单词的所有文档的文档列表(文档ID)及单词在该文档中出现的位置信息，每条记录称为一个倒排项(posting)。根据倒排列表，即可获知哪些文档包含某个单词。（实际的倒排列表中并不只是存储了文档ID那么简单，还有一些其他的信息）\n\n倒排索引项(posting)中有如下的信息：\n\n文档ID；\n\n词频TF，该单词在文档中出现的次数，用于相关性评分；\n\n位置(Position)，单词在文档中分词的位置。用于语句搜索(phrase query)。\n\n偏移(Offset)，记录单词的开始结束位置，实现高亮显示。\n\n# ES查询的大致过程如下\n\n\n\n\n# 分词器\n\n\n# 什么是Analysis\n\nAnalysis 文本分析就是把全文本转换一系列单词(term/token)的过程，也叫做分词。\n\n\n# Analysis与Analyzer什么关系\n\nAnalysis 是通过Analyzer来实现的，有ES内置的Analyzer分词器，或者也可以使用按需定制化的分词器。\n\n\n# 什么时候需要使用Analyzer分词器\n\n * 在数据写入的时候使用Analyzer分词器，转换词条。\n * 在匹配Query语句的时候也需要指定相同的Analyzer分词器，来对查询语句进行分析\n\n\n# Analyzer由哪些部分组成\n\nAnalyzer主要是由三个部分组成的。\n\nCharacter Filters(针对原始文本处理，例如去除html)\n\nTokenizer(按照规则切分为单词)\n\nToken Filter(将切分的单词进行加工，小写，删除stopwords，增加同义词)\n\n\n\n\n# Character Filters\n\n * 在tokenizer之前对文本进行处理，例如增加删除及替换字符。可以配置多个Character Filters。会影响Tokenizer的position和offset信息。\n * 一些自带的caharacter filters\n   * HTML strip 去除html标签\n   * Mapping 字符串替换\n   * Pattern replace 正则匹配替换\n\nPOST _analyze\n{\n  "tokenizer":"keyword",\n  "char_filter":["html_strip"],\n  "text": "<b>hello world</b>"\n}\n#使用char filter进行替换\nPOST _analyze\n{\n  "tokenizer": "standard",\n  "char_filter": [\n      {\n        "type" : "mapping",\n        "mappings" : [ "- => _"]\n      }\n    ],\n  "text": "123-456, I-test! test-990 650-555-1234"\n}\n#char filter 替换表情符号\nPOST _analyze\n{\n  "tokenizer": "standard",\n  "char_filter": [\n      {\n        "type" : "mapping",\n        "mappings" : [ ":) => happy", ":( => sad"]\n      }\n    ],\n    "text": ["I am felling :)", "Feeling :( today"]\n}\n#正则表达式\nGET _analyze\n{\n  "tokenizer": "standard",\n  "char_filter": [\n      {\n        "type" : "pattern_replace",\n        "pattern" : "http://(.*)",\n        "replacement" : "$1"\n      }\n    ],\n    "text" : "http://www.elastic.co"\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# Tokenizer\n\n * 将原始的文本按照一定的规则，切分为词(term or token)\n\n * es内置的tokenizers\n   \n   whitespace / standard / uax_url_email / pattern / keyword / path hierarchy\n\n * 可以用Java开发插件，实现自己的Tokenizer\n\nPOST _analyze\n{\n  "tokenizer":"path_hierarchy",\n  "text":"/user/ymruan/a/b/c/d/e"\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# Token Filters\n\n * 将Tokenizer输出的单词(term)，进行增加，修改，删除\n\n * 自带的Token Filters\n   \n   lowercase / stop / synonym(添加近义词)\n\nGET _analyze\n{\n  "tokenizer": "whitespace",\n  "filter": ["stop","snowball"],\n  "text": ["The rain in Spain falls mainly on the plain."]\n}\n#remove 加入lowercase后，The被当成 stopword删除\nGET _analyze\n{\n  "tokenizer": "whitespace",\n  "filter": ["lowercase","stop","snowball"],\n  "text": ["The gilrs in China are playing this game!"]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# Elasticsearch的内置分词器有哪些\n\n * standard analyzer 默认分词器，按词进行切分，小写处理\n * simple analyzer 按照非字母切分(符号被过滤)，小写处理\n * stop analyzer 小写处理，停用词过滤(the,a,is)\n * whitespace analyzer 按照空格切分，不转小写\n * keyword analyzer 不分词，直接将输入当作输出\n * patter analyzer 正则表达式，默认\\W+ (非字符分隔)\n * Lanaguage 提供了30多种常见语言的分词器\n * Customer Analyzer自定义分词器\n\n\n# Standard Analyer\n\n这是ES的默认分词器。\n\nCharacter Filters：没有说明\n\nTokenizer：是按词来切分\n\nToken Filter：将切分后的单词转为小写，stopwords是默认关闭的\n\n\n# Simple Analyzer\n\nCharacter Filters：没有说明\n\nTokenizer：非字母的都会被去除，例如数字，符号等\n\nToken Filter：将切分后的单词转为小写，stopwords是默认关闭的\n\n\n# Whitespace Analyzer\n\nCharacter Filters：没有说明\n\nTokenizer：按照空格来切分单词\n\nToken Filter：将切分后的单词转为小写，stopwords是默认关闭的\n\n\n# Stop Analyzer\n\nCharacter Filters：没有说明\n\nTokenizer：非字母的都会被去除，例如数字，符号等\n\nToken Filter：将切分后的单词转为小写，多了stop filter，会把the/a/is等修饰性词语去除\n\n\n# Keyword Analyzer\n\n不分词，直接将输入当一个term输出\n\n\n# Pattern Analyzer\n\n\n# Language Analyzer\n\n在english的分词器中\n\nCharacter Filters：没有说明\n\nTokenizer：分词后的单词，去除掉复数，ing，等时态信息\n\nToken Filter：将切分后的单词转为小写，多了stop filter，会把the/a/is等修饰性词语去除\n\n\n# ICU Analyzer\n\nCharacter Filters：Normalization\n\nTokenizer：ICU tokenizer\n\nToken Filter: Normalization、Folding、Collation、Transform\n\n这个Analyzer不是ES自带的，需要另行安装。\n\n$ elasticsearch-plugin install analysis-icu\n\n提供了Unicode的支持，更好的支持亚洲语言\n\n\n# 搜索相关性Relevance\n\n衡量搜索的相关性中有个概念：Precision(查准率)和Recall(查全率)\n\n * Precision (查准率) 尽可能返回较少的无关的文档，也就是查询出来的所有的正确的结果除以全部返回的结果。\n * Recall (查全率) 尽量返回较多的相关文档，也就是查询出来的所有的正确的结果除以所有应该返回的正确结果的数量。\n * Ranking 是否能够按照相关度进行排序\n\n\n# Mapping\n\n\n# 什么是mapping\n\n就是关系型数据库中定义schema。Mapping会把JSON文档映射成Lucene所需要的扁平格式。\n\n一个Mapping属于一个索引的Type；每个文档都属于一个Type；一个Type有一个mapping定义；从7.0开始，不需要在mapping定义中指定Type信息。\n\n作用如下：\n\n * 定义索引中的字段的名称\n * 定义字段的数据类型，例如字符串、数字、布尔....\n * 字段，倒排索引的相关配置，(Analyzed or Not Analyed, Analyzer)\n\n\n# ES中字段的数据类型有哪些\n\n * 简单类型\n   * Text/keyword\n   * date\n   * integer / Floating\n   * boolean\n   * IPV4 & IPV6\n * 复杂类型 - 对象和嵌套对象\n   * 对象类型、嵌套类型\n * 特殊类型\n   * geo_point & geo_shape / percolator\n\n\n# 什么是多字段类型\n\n当我们写入一个文档到ES中，如果这个文档中的某个字段是text类型的话，那么ES默认会给我们增加一个keyword子字段。可以通过mapping信息查看到。\n\n同样我们可以手动显性的去mapping定义一个多字段的类型，在子字段中配置不同的analyzer。例如：针对不同语言；针对pinyin字段的搜索；还支持为搜索和索引指定不通过的anayzer。\n\nPUT products\n{\n    "mappins":{\n        "properties":{\n            "company":{\n                "type":"text",\n                "fields":{\n                    "keyword":{\n                        "type":"keyword",\n                        "ignore_above":256\n                    }\n                }\n            }\n            "comment":{\n                "type":"text",\n                "fields":{\n                    "english_comment":{\n                        "type":"text",\n                        "analyzer":"english",\n                        "search_analyzer":"english"\n                    }\n                }\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 什么是精确值，什么是全文本\n\nexact values中包括数字、日志、具体一个字符串。可以使用es中的keyword来实现。\n\nfull text中针对的是全文本，非结构化的文本数据，例如es中的text。\n\n精确值(exact value)当设置为keyword的时候，是不会再对这个进行分词的，无论这个有多长。ES会为这个精确值所在的字段类创建一个倒排索引。\n\n\n# 什么是Dynamic Mapping\n\n简而言之，ES自己根据文档的各个字段的信息，来自动决定创建各个字段的类型。\n\n * 在写入文档的时候，如果索引不存在，会自动创建索引\n * Dynamic Mapping的机制，使得我们无需手动定义Mappings。ES会自动根据文档信息，推算出字段的类型。\n * 但是有时候会推算的不对，例如地理位置信息\n * 当类型如果设置不对的时候，会导致一些功能无法正常运行，例如Range查询\n\n\n# Dynamic Mapping中类型如何自动识别\n\nJSON类型   ELASTICSEARCH类型\n字符串      * 匹配日期格式，设置成date\n         * 配置数字设置为float或long，该选项默认关闭\n         * 设置为Text，并且增加keyword子字段\n布尔值      boolean\n浮点数      float\n整数       long\n对象       Object\n数组       由第一个非空数值的类型所决定\n空值       忽略\n\n\n# 能否更改mapping的字段类型\n\n新增加字段的情况下：\n\n * 当Dynamic设置为true的时候，一旦有新增字段的文档写入，Mapping也同时会被更新。\n\n * 当Dynamic设置为false的时候，Mapping不会被更新，新增字段的数据无法被索引，但是信息会出现在_source中。\n\n * 当Dynamic设置成Stric的时候，文档写入失败。\n\n对已有的字段，修改字段类型的情况下：\n\n * 对已有的字段，一旦已经有数据写入，就不再支持修改字段定义。\n\n * Lucene实现的倒排索引，一旦生成后，就不允许被修改了。\n\n原因：\n\n * 如果修改了字段的数据类型，会导致已被索引的无法被搜索.\n\n * 但是如果是增加新的字段，就不会有这样的影响。\n\n\n# 控制Dynamic Mappings\n\n通过设置dynamic mappings的值来影响以下三个方面：\n\n * 新增加的文档，是否可以被索引查询出来\n * 新增加的字段，是否可以被索引查询出来\n * 如果存在新增字段，索引的mapping信息是否会被更新\n\n             "TRUE"   "FALSE"   "STRICT"\n文档可以被索引      YES      YES       NO\n字段可以被索引      YES      NO        NO\nmapping被索引   YES      NO        NO\n\n值得注意的是，\n\n * 当dynamic被设置成flase的时候，存在新增的字段的数据写入，该数据可以被索引，但是新增字段被丢弃。\n * 当设置成strict模式的时候，数据写入直接出错。\n\n修改dynamic mappings的方法：\n\n#修改为dynamic false\nPUT dynamic_mapping_test/_mapping\n{\n  "dynamic": false\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# Index Template\n\n\n# 什么是Index Template\n\nindex templates 帮助我们设置mappings和settings，并按照一定的规则，自动匹配到新创建的索引之上。\n\n * 模板仅在一个索引被新创建的时候，才会产生作用。修改模板不会影响已经创建的索引\n * 我们可以设定多个索引模板，这些设置会被"merge"在一起\n * 可以指定"order"的数值，控制"merging"的过程\n\n\n# Index Template的工作方式\n\n当一个索引被新创建的时候\n\n * 应用es默认的settings和mappings\n * 应用order数值低的index template中的设定\n * 应用order高的index template中的设定，之前的设定会被覆盖\n * 应用创建索引的时候，用户所指定的settings和mappings，并覆盖之前模板中的设定\n\n#Create a default template\nPUT _template/template_default\n{\n  "index_patterns": ["*"],\n  "order" : 0,\n  "version": 1,\n  "settings": {\n    "number_of_shards": 1,\n    "number_of_replicas":1\n  }\n}\nPUT /_template/template_test\n{\n    "index_patterns" : ["test*"],\n    "order" : 1,\n    "settings" : {\n    \t"number_of_shards": 1,\n        "number_of_replicas" : 2\n    },\n    "mappings" : {\n    \t"date_detection": false,\n    \t"numeric_detection": true\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# Dynamic Template\n\n\n# 什么是Dynamic Template\n\n根据ES识别的数据类型，结合字段名称，来动态设定字段类型\n\n * 所有的字符串类型都设定成keyword，或者关闭keyword字段\n * is开头的字段都设置成boolean\n * long_开头的都设置成long类型\n\nDynamic Template是定义在某个索引的mapping中；template有一个名称；匹配规则是一个数组；为匹配到字段设置mapping\n\nPUT my_index\n{\n  "mappings": {\n    "dynamic_templates": [\n            {\n        "strings_as_boolean": {\n          "match_mapping_type":   "string",\n          "match":"is*",\n          "mapping": {\n            "type": "boolean"\n          }\n        }\n      },\n      {\n        "strings_as_keywords": {\n          "match_mapping_type":   "string",\n          "mapping": {\n            "type": "keyword"\n          }\n        }\n      }\n    ]\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 匹配规则参数\n\n * match_mapping_type：匹配自动识别的字段类型，如string，boolean等\n * match，unmatch：匹配字段名\n * path_match，path_unmatch\n\n\n# 聚合分析\n\n\n# 什么是聚合Aggregation\n\n * ES除搜索以外，还提供针对ES数据统计分析的功能\n   \n   实时性高；Hadoop (T+1)\n\n * 通过聚合，我们会得到一个数据的概览，是分析和总结全套的数据，而不是寻找单个文档。\n\n * 高性能，只需要一条语句，就可以从ES得到分析结果\n\n * 聚合支持嵌套\n\n\n# 聚合的分类\n\n * Bucket Aggregation 一些列满足特定条件的文档的集合\n * Metric Aggregation 一些数学运算，可以对文档字段进行统计分析\n * Pipeline Aggregation 对其他的聚合结果进行二次聚合\n * Matrix Aggregation 支持对多个字段的操作并提供一个结果矩阵\n\n\n# 聚合的作用范围及排序\n\nES聚合分析的默认作用范围是query的查询结果集。\n\n同时ES还支持以下方式改变聚合的作用范围：\n\nfilter /post_filter / global\n\n\n# 聚合的精准度问题\n\n# 分布式系统的近似统计算法\n\n\n\n# Terms聚合分析解析\n\nterms聚合分析的执行流程:\n\n\n\nterms不正确的案例：\n\n\n\nterms aggregation的返回值\n\n * size是最终返回多少个buckt的数量\n * shard_size是每个bucket在一个shard 上返回的bucket的总数。然后，每个shard上的结果，会在coordinate节点上再做一次汇总，返回总数。\n * doc_count_error_upper_bound：被遗漏的term分桶，包含的文档，有可能的最大值。\n * sum_other_doc_count：除了返回结果bucket的terms以外，其他terms的文档总数(总数-返回的总数)\n * show_term_doc_count_error： 还会对每个 bucket都显示一个错误数，表示最大可能的误差情况。\n\n# 解决terms不准的问题\n\n提升shard_size的参数\n\nterms聚合分析不准的原因，数据分散在多个分片上，coordinating node无法获取数据全貌。\n\n * 解决方案1：当数据量不大的时候，设置primary shard为1；实现准确性。\n * 解决方案2：在分布式数据上，设置shard_size参数，提高精确度。(原理是：每次从shard上额外多获取数据，提升准确率)\n\nshard_size设定\n\n * 调整shard size大小，降低doc_count_error_upper_bound来提升准确度\n   \n   增加了整体计算量，提高了准确度，但会降低相应时间\n\n * shard size默认大小设定\n   \n   shard size = size * 1.5 +10\n\n\n# Aggregation的语法\n\nAggregation属于search的一部分。一般情况下，建议将其size指定为0。\n\n类似的语法如下：\n\n\n\n\n# Bucket & Metric\n\nmetric类似SQL中的count统计，bucket类似SQL中的group by.\n\n\n\n\n# Buket\n\n我们可以对一类商品分为高档、中档、低档三类，再对高档商品中分为好评、中评、差评。\n\nES中提供很多类型的Bucket，帮助我们用多种方式划分文档。例如Term & Range(时间/年龄区间/地理位置)。\n\n#按照目的地进行分桶统计\nGET kibana_sample_data_flights/_search\n{\n\t"size": 0,\n\t"aggs":{\n\t\t"flight_dest":{\n\t\t\t"terms":{\n\t\t\t\t"field":"DestCountry"\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# Metric\n\n * Metric会基于数据集计算结果，除了支持在字段上进行计算，统一也支持在脚本(painless script)产生的结果之上进行计算\n\n * 大多数Metric是数学计算，仅输出一个值\n   \n   min/max/sum/avg/cardinality\n\n * 部分metric支持输出多个数值\n   \n   stats/percentiles/percentile_ranks\n\n#查看航班目的地的统计信息，增加平均，最高最低价格\nGET kibana_sample_data_flights/_search\n{\n\t"size": 0,\n\t"aggs":{\n\t\t"flight_dest":{\n\t\t\t"terms":{\n\t\t\t\t"field":"DestCountry"\n\t\t\t},\n\t\t\t"aggs":{\n\t\t\t\t"avg_price":{\n\t\t\t\t\t"avg":{\n\t\t\t\t\t\t"field":"AvgTicketPrice"\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t"max_price":{\n\t\t\t\t\t"max":{\n\t\t\t\t\t\t"field":"AvgTicketPrice"\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t"min_price":{\n\t\t\t\t\t"min":{\n\t\t\t\t\t\t"field":"AvgTicketPrice"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n#嵌套查询价格统计信息+天气信息\nGET kibana_sample_data_flights/_search\n{\n\t"size": 0,\n\t"aggs":{\n\t\t"flight_dest":{\n\t\t\t"terms":{\n\t\t\t\t"field":"DestCountry"\n\t\t\t},\n\t\t\t"aggs":{\n\t\t\t\t"stats_price":{\n\t\t\t\t\t"stats":{\n\t\t\t\t\t\t"field":"AvgTicketPrice"\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t"wather":{\n\t\t\t\t  "terms": {\n\t\t\t\t    "field": "DestWeather",\n\t\t\t\t    "size": 5\n\t\t\t\t  }\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\n\n# Term Query\n\n这里的term query，单指的是在request body search 中的term查询。精确值匹配\n\nTerm是表达语意的最少单位。搜索和利用统计语言模型进行自然语言处理都需要处理Term。\n\n特点：\n\n * Term level Query:Term Query/ Range Query /Exists Query /Prefix Query /Wildcard Query\n * 在ES中，Term查询，对输入不做分词处理。会将输入作为一个整体，在倒排索引中查找准确的词项，并且使用相关度算分公式为每个包含该词项的文档进行相关度算分。输入什么就是查询什么， 例如输入"Apple"，就是查询的是"Apple"，而"apple"是不会放结果的。而text类型中，会对数据进行分词处理，改为小写。如果term查询中，不是全小写，很有可能是查询不到的。\n * 可以通过Constant Score将查询转换成一个Filtering，避免算分，并利用缓存，提高性能。\n\n如何解决term 查询搜索不到结果的问题：\n\n * 如果针对的是text类型的字段的查询，那么term 查询的时候，使用字段名称.keyword，使用keyword会对查询进行完全匹配，插入的值是什么，查询的时候输入什么，就会查询出来。\n   \n   POST /products/_search\n   {\n     "query": {\n       "term": {\n         "productID": {\n           "value": "XHDK-A-1293-#fJ3"\n         }\n       }\n     }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   \n\n * term查询的时候，输入的查询条件中，查询字段的值，自己手动改为小写来查询。\n   \n   POST /products/_search\n   {\n     "query": {\n       "term": {\n         "desc": {\n           //"value": "iPhone"\n           "value":"iphone"\n         }\n       }\n     }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   \n\n\n# 基于全文的查询\n\n和term query不同的是，这是基于全文的查询，查询的时候会对输入的查询条件的值，进行分词处理。\n\n有Match Query / Match Phrase Query / Query String Query。\n\n特定是：\n\n * 索引和搜索的时候都会进行分词，查询字符串先传递到一个合适的分词器，然后生成一个供查询的词项列表。\n * 查询的时候，先会对输入的查询进行分词，然后每个词项逐个进行底层的查询，最终将结果进行合并。并为每个文档生成一个算分。例如查询"Matrix reloaded"，会查到包括Matrix或者reload的所有结果。\n\n\n# Match Query查询过程\n\n\n\n\n# 结构化搜索\n\n或者可以理解为term查询，或者说是对精确值的查询。\n\n\n# 什么是结构化数据\n\n * 结构化搜索是指对结构化数据的搜索\n   \n   比如日期，布尔类型和数字都是结构化的\n\n * 文本也可以是结构化的\n   \n   * 比如彩色笔可以由离散的颜色集合\n   * 一个博客可能被标记了标签\n   * 电商网站上的商品都有UPCS或者其他的唯一标识，它们都需要遵从严格的规定结构化的格式。\n\n\n# ES中的结构化搜索\n\n结构化数据：\n\n布尔，时间，日期和数字这类结构化数据：有精确的格式，我们可以对这些格式进行逻辑操作。包括比较数字或时间的范围，或判定两个值的大小。\n\n结构化文本：\n\n结构化的文本可以做精确匹配或部分匹配。\n\n结构化结果：\n\n结构化结果只有"是"或"否"两个值；根据场景需要，可以决定结构化搜索是否需要打分。\n\n\n# 搜索相关性算分\n\n\n# 相关性和相关性算分\n\n相关性也就是Relevance,值得注意的是也只有match 查询才有算分的过程，term 查询没有。\n\n * 搜索的相关性算分，描述了一个文档和查询语句匹配的程度。ES会对每个匹配查询条件的结果进行算分 _score\n * 打分的本质是排序，需要把最符合用户需求的文档排在前面。在ES 5之前，默认的相关性算分采用TF-IDF，现在采用BM 25\n\n\n# 词频 TF\n\n * Term Freuency：检索词在一篇文档中出现的频率\n   \n   也就是检索词出现的次数除以文档的总字数\n\n * 度量一条查询和结果文档相关性的简单方法：简单将搜索中每一个词的TF进行相加\n   \n   TF(区块链)+TF(的)+TF(应用)\n\n * Stop Word\n   \n   "的"在文档中出现了很多次，但是对贡献相关度几乎没有用处，不应该考虑其TF值\n\n\n# 逆文档频率 IDF\n\n * DF：检索词在所有文档中出现的频率\n   \n   这里中，"区块链"在相对比较少的文档中出现；"应用"在相对比较多的文件中出现；"stop word"在大量的文档中出现\n\n * Inverse Document Frequency：简单说是 log(全部文档数/检索词出现过的文档总数)\n\n * TF-IDF本质上就是将TF求和变成了加权求和\n   \n   \n\n\n# TF-IDF的评分公式\n\n\n\n注意的一个参数boosting，可以用来影响算分。\n\n\n# BM 25\n\n从ES 5开始，默认算法改为BM 25\n\n和经典的TF-IDF相比，当TF无限增加时，BM 25算分会趋于一个数值。\n\n\n\n\n# 自定义相似性\n\n可以自己定义similarity\n\n\n\n\n# Explain API查看TF-IDF\n\n在Request body search中打开"explain":true的方法来查看Explain的API\n\nPOST /testscore/_search\n{\n  "explain": true,\n  "query": {\n    "match": {\n      "content":"you"\n      //"content": "elasticsearch"\n      //"content":"the"\n      //"content": "the elasticsearch"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# Boosting Relevance\n\n * Boosting是控制相关度的一种手段，可以在索引、字段或查询子条件中进行设置\n * 参数boost的含义\n   * 当boost>1的时候，打分的相关度相对性提升\n   * 当0<boost<1的时候，打分的权重相对性降低\n   * 当boost<0的时候，贡献负分\n\n\n\n\n# Query Context & Filter Context\n\nES中查询子句可以分为query context（查询上下文）和filter context（过滤上下文）。\n\n查询上下文，是指其使用的查询子句回答了"此文档与此查询子句的匹配程度"的问题。除了决定文档是否匹配外，查询子句还会计算一个_score，表示此文档和其他文档的匹配程度。\n\n过滤器上下文，是指在过滤器上下文中，一个查询子句回答了"此文档是否匹配此查询子句？"的问题。该答案是简单的"YES"或"NO"，没有scores算分的。过滤器上下文主要是用来过滤结构性数据，（一般里面嵌套term 查询）。同时频繁使用的过滤器，将被ES自动缓存，以提升性能。\n\n\n# 布尔查询\n\n一般应用于多字符串，多字段的复合查询。也就是说查询条件有很多个，有要过滤字段的(filter context)，有要匹配字符串的(query context)。\n\n一个bool查询，是一个或多个查询子句的组合。\n\n总共包括4种子句。其中2种会影响算分，2种不影响算分。\n\n相关性并不只是全文本检索的专利。也适用于yes | no的子句，匹配的子句越多，相关性评分越高。如果多条查询子句被合并为一条复合查询语句，例如bool查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。\n\nTerm query只是不会去分词，还是会算分的。\n\nMUST子句       QUERY CONTEXT\n             必须匹配，贡献算分\nshould子句     query context\n             选择性匹配，贡献算分\nmust_not子句   Filter Context\n             必须不能匹配，不贡献算分\nfilter子句     Filter Context\n             必须匹配，但是不贡献算分\n\n\n# Disjunction Max Query\n\nDisjunction Max Query应用于单字符串多字段查询的场景，很多搜索引擎都是这样的。允许你只是输入一个字符串，要多所有的字段进行查询。\n\n下面例子中，只是输入了一个"Brown fox"的字符串，需要对所有的字段做匹配。我们这里使用的是bool查询中的should。\n\nPOST /blogs/_search\n{\n    "query": {\n        "bool": {\n            "should": [\n                { "match": { "title": "Brown fox" }},\n                { "match": { "body":  "Brown fox" }}\n            ]\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nshould的算分过程\n\n * 对每个文档中，执行查询should语句中的两个查询\n * 加和这两个查询的评分\n * 乘以匹配语句的总数\n * 除以所有语句的总数\n\nDisjunction Max Query将任何与任一查询匹配的文档作为结果返回。采用字段上最匹配的评分最终评分返回。\n\nPOST blogs/_search\n{\n    "query": {\n        "dis_max": {\n            "queries": [\n                { "match": { "title": "Quick pets" }},\n                { "match": { "body":  "Quick pets" }}\n            ]\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# Multi Match\n\nmulti match同样运用于单字符串多字段的查询。\n\n单字符串多字段查询中场景归纳：\n\n * 最佳字段(Best Fields)\n   \n   当字段之间相互竞争，又相互关联。例如title和body这样的字段，评分来自最匹配字段。\n\n * 多数字段(Most Fields)\n   \n   处理英文内容时：一种常见的手段是，在主字段(english analyzer)，抽取词干，加入同义词，以匹配更多的文档。相同的文本，加入子字段(standard analyzer)，以提供更加精确的匹配。其他字段作为匹配文档提高相关度的信号。匹配字段越多越好。\n\n * 混合字段(Cross Field)\n   \n   对于某些实体，例如人名，地址，图书信息。需要在多个字段中确定信息，单个字段之鞥呢作为整体的一部分。希望在任何这些列出的字段中找出尽可能的词。\n\n\n# Search Template\n\nES中的search template是用来解耦程序的开发和ES搜索DSL的，这样开发工程师、搜索工程师、性能工程师可以各司其职。\n\n在开发初期，虽然可以明确查询参数，但是往往还不能最终定义查询的DSL的具体结构。\n\n可以定义一个search template，这样前端的开发工程师直接调用这个模板来查看到查询结果就可以了。\n\n\n# Suggester API\n\n\n# 什么是搜索建议\n\n现代的搜索引擎，一般的都会提供Suggest as you type的功能。\n\n帮助用户在输入搜索的过程中，进行自动补全或者纠错。通过协助用户输入更加精准的关键词，提高后续搜索阶段文档匹配的程度。\n\n在Google上搜索的时候，一开始会自动补全。当输入到一定长度，如因为单词拼写错误无法补全，就会开始提示相似的词或句子。\n\n\n# 精准度和召回率\n\n * 精准度\n   \n   completion > phrase > term\n\n * 召回率\n   \n   term > phrase > completion\n\n * 性能\n   \n   completion > phrase > term\n\n\n# ES Suggester API\n\n * 搜索引擎中类似的功能，在ES中是通过Suggester API实现的。\n * 原理：将输入的文本分解为Token，然后在搜索的字典里查找相似的Term并返回。\n * 根据不同的使用场景，ES设计了4种类别的Suggesters\n   * Term & Phrase Suggester\n   * Complete & Context Suggester\n\n\n# Term Suggester 和Phrase Suggester\n\nterm suggester按照"suggest_mode"可以分为如下几种：\n\n * Missing：如索引中已经存在，就不提供建议\n * Popular：推荐出现频率更加高的词\n * Always：无论是否存在，都提供建议。\n\nPhrase Suggester在term suggester上增加了一些额外的逻辑。\n\n例如下面的参数：\n\n * Suggest Mode: missing，popular，always\n * Max Errors：最多可以拼错的Terms数\n * Confidence：限制返回结果数，默认是1\n\n\n# Completion Suggester\n\n * completion Suggester提供了"自动完成"(auto complete)的功能。用户每输入一个字符，就需要即时发送一个查询请求到后端查找匹配项。\n\n * 对性能要求比较苛刻。ES采用了不同的数据结构，并非通过倒排索引来完成的。而是将Analyze的数据编码成FST和索引一起存放。FST会被ES整个加载进内存，速度很快。\n\n * FST只能用于前缀查找\n\n\n# Context Suggester\n\ncontext suggester是completion suggester的扩展，\n\n可以在搜索中加入更多的上下文信息，例如，输入"star"\n\n * 咖啡相关：建议"starbucks"\n * 电影相关的："star wars"\n\n\n# 文档分布式存储\n\n整理这个章节的目的在于，需要去深入了解文档的查询和索引的过程，文档是如何按分片存储的。\n\n\n# 概述文档存储\n\n文档会存储在具体的某个主分片和副本分片上：例如文档1，会存储在PO和RO分片上。\n\n文档到分片的映射算法：\n\n * 确保文档能均匀分布在所用分片上，充分利用硬件资源，避免部分机器空闲，部分机器繁忙\n * 潜在的算法：\n   * 方案一：随机/Round Robin。当查询文档的时候，这个时候分片数很多，需要多次查询才可能查询到。\n   * 方案二：维护文档到分片的映射关系。当文档数据量大的时候，维护成本高。\n   * 方案三：实时计算，通过文档1，自动算出，需要去哪个分片上获取文档。\n\n\n# 文档到分片路由算法\n\nshard=hash(_routing)%number_of_primary_shards\n\n * Hash算法确保文档均匀分散到分片中\n * 默认的_routing值是文档id\n * 可以自行制定routing数值，例如用相同国家的商品，都分配到指定的shard\n * 设置Index Setting后，Primary数，不能随意修改的根本原因。\n\n参考如下的写入文档的时候，指定_routing的值\n\nPUT posts/_doc/100?routing=bigdata\n{\n  "title":"Mastering Elasticsearch",\n  "body":"Let\'s Rock"\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 倒排索引不可变性\n\n * 倒排索引采用的是immutable design，一旦生成，不可更改\n * 不可变性，带来的好处有：\n   * 无需考虑并发写文件的问题，避免了锁机制带来的性能问题\n   * 一旦读入内核的文件系统缓存，便留在那里。只要文件系统缓存有足够的空间，大部分请求就会直接请求内存，不会命中磁盘，提升了很大的性能。\n   * 缓存容易生产和维护/数据可以被压缩\n * 不可变性，带来挑战：如果需要让一个新的文档可以被索引，需要常见整个索引。\n\n\n# Lucene Index\n\n在lucene中有两个概念，一个是segment，一个是index。多个segment组成了一个index。\n\n * 在lucene中，单个倒排索引文件被称为segment。segment是自包含的，不可变更的。多个segments汇总在一起，称为Lucene的index，其对应的就是ES的shard。\n * 当有新文档写入的时候，会生成新的segment，查询时会同时查询所有的segments，并且对结果汇总。lucene中有一个文件，用来记录所有segments信息，叫做commit point。\n * 删除的文档信息，保存在".del"文件中。\n\n\n\n\n# 什么是Refresh\n\n * 将index buffer写入segment的过程叫做refresh。refresh不执行fsync操作。\n * Refresh频率：默认1秒发生一次，可通过index.refresh_interval配置。Refresh后，数据就可以被搜索到了。这也是为什么ES被称为近实时搜索。\n * 如果系统有大量的数据写入的时候，那就会产生很多的segment。\n * index buffer被占满时，会触发Refresh，默认值是JVM的10%。\n\n\n\n\n# 什么是Transaction Log\n\n * Segment写入磁盘的过程相对耗时，借助文件系统缓存，Refresh时，先将Segment写入缓存以开放查询。\n * 为了保证数据不会丢失。所以在Index文档的时候，同时写Transaction Log，高版本开始，Transaction Log默认落盘。每个分片由一个Transaction Log。\n * 在ES Refresh时，Index Buffer被清空，Transaction Log不会清空。\n\n\n\n\n# 什么是Flush\n\nFlush主要是指调用fsync将缓存中的segments写入磁盘的过程。\n\n具体描述ES Flush和lucene commit的过程如下：\n\n * ES写入index buffer，当默认间隔1S或者index buffer到JVM的10%的时候，触发refresh将buffer写入到lucene中的segment。（写入index buffer的时候同时会写transaction log）refresh后的index buffer为清空了。\n * 默认经过30分钟或者transaction log满512M的时候，调用fsync将缓存中的segments写入到磁盘中。最后会清空删除相应的transaction log。\n\n\n# 什么是Merge\n\n * 当segment很多的时候，需要被定期被合并。\n   \n   这个时候可以调用merge的API来实现，这样会减少segments的数量，并且会删除已经删除的文档。\n\n * ES和lucene会自动进行merge操作，我们也可以通过手动调用POST my_index/_forcemerge来执行。\n\n * ES有一个后台进程专门负责segment的合并，它会把小segments合并成更大的segments，然后反复这样。\n\n\n# 整个索引文档到分片的流程\n\n 1. 客户端发起数据写入请求，对你写的这条数据根据_routing规则选择发给哪个Shard。\n    \n    * 确认Index Request中是否设置了使用哪个Filed的值作为路由参数\n    * 如果没有设置，则使用Mapping中的配置\n    * 如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的Hash值选择出Shard，最后从集群的Meta中找出出该Shard的Primary节点。\n\n 2. 写入请求到达Shard后，先把数据写入到内存（buffer）中，同时会写入一条日志到translog日志文件中去。\n    \n    * 当写入请求到shard后，首先是写Lucene，其实就是创建索引。\n    * 索引创建好后并不是马上生成segment，这个时候索引数据还在缓存中，这里的缓存是lucene的缓存，并非Elasticsearch缓存，lucene缓存中的数据是不可被查询的。\n\n 3. 执行refresh操作：从内存buffer中将数据写入os cache(操作系统的内存)，产生一个segment file文件，buffer清空。\n    \n    * 写入os cache的同时，建立倒排索引，这时数据就可以供客户端进行访问了。\n    * 默认是每隔1秒refresh一次的，所以es是准实时的，因为写入的数据1秒之后才能被看到。\n    * buffer内存占满的时候也会执行refresh操作，buffer默认值是JVM内存的10%。\n    * 通过es的restful api或者java api，手动执行一次refresh操作，就是手动将buffer中的数据刷入os cache中，让数据立马就可以被搜索到。\n    * 若要优化索引速度, 而不注重实时性, 可以降低刷新频率。\n\n 4. translog会每隔5秒或者在一个变更请求完成之后，将translog从缓存刷入磁盘。\n    \n    * translog是存储在os cache中，每个分片有一个，如果节点宕机会有5秒数据丢失，但是性能比较好，最多丢5秒的数据。\n    * 可以将translog设置成每次写操作必须是直接fsync到磁盘，但是性能会差很多。\n    * 可以通过配置增加transLog刷磁盘的频率来增加数据可靠性，最小可配置100ms，但不建议这么做，因为这会对性能有非常大的影响。\n\n 5. 每30分钟或者当tanslog的大小达到512M时候，就会执行commit操作（flush操作），将os cache中所有的数据全以segment file的形式，持久到磁盘上去。\n    \n    * 第一步，就是将buffer中现有数据refresh到os cache中去。\n    * 清空buffer 然后强行将os cache中所有的数据全都一个一个的通过segmentfile的形式，持久到磁盘上去。\n    * 将commit point这个文件更新到磁盘中，每个Shard都有一个提交点(commit point), 其中保存了当前Shard成功写入磁盘的所有segment。\n    * 把translog文件删掉清空，再开一个空的translog文件。\n    * flush参数设置有，index.translog.flush_threshold_period , index.translog.flush_threshold_size，控制每收到多少条数据后flush一次, index.translog.flush_threshold_ops。\n\n 6. Segment的merge操作：\n    \n    * 随着时间，磁盘上的segment越来越多，需要定期进行合并。\n    * Es和Lucene 会自动进行merge操作，合并segment和删除已经删除的文档。\n    * 我们可以手动进行merge：POST index/_forcemerge。一般不需要，这是一个比较消耗资源的操作。\n\n\n# Logstash\n\n\n# logstash是什么\n\nlogstash是ETL工具，数据收集处理引擎，java写的。支持200多个插件。\n\nlogstash支持从文件、HTTP、数据库、Kafka等数据源中读取数据，经过定义的规则进行加工清洗和处理，然后最终吐到mogodb、es、HDFS的数据分析器中进行分析和查询。\n\n\n\n\n# logstash中的基本概念\n\npipeline\n\n * 包含了input - filter - output 三个阶段的处理流程\n * 可以对插件进行生命周期管理\n * 内置队列的管理\n\nlogstash event\n\n * event是数据在内部流转的时候，具体表现形式。数据在input阶段被转换为event，在output阶段被转化为目标格式的数据。\n * event其实就是一个java object，在配置文件中，可以对event的属性进行增删改查。\n\n\n# logstash架构简介\n\nCodec(code / decode)：将原始数据decode成event；将event encode成目标数据。\n\n从input -> 配置code成event -> 经过filter过滤 -> 然后将event encode -> 通过output输出。\n\n\n\n\n# Input Plugins\n\n一个pipeline可以有多个input插件\n\n * stdin/file\n * beats/log4j/elasticsearch/JDBC/kafka/babbitmq/redis\n * JMX/HTTP/Websocket/UDP/TCP\n * Google Cloud Storages/S3\n * Github/Twitter\n\n\n# Codec Plugins\n\n将原始数据decode成event；将event encode成目标数据\n\n内置的codec plugins有 https://www.elastic.co/guide/en/logstash/7.2/codec-plugins.html\n\n * line/multiline\n\n * json/avro/cef\n\n * dots/rubydebug\n\n\n# Filter Plugins\n\n处理event\n\n内置的filter plugins有： https://www.elastic.co/guide/en/logstash/7.2/filter-plugins.html\n\n * mutate 操作event的字段\n\n * metrics aggregates metrics\n\n * ruby 执行ruby代码\n\n\n# Output Plugins\n\n将Event发送到特定的目的地，是pipeline的最后一个阶段。\n\n常见的output plugins有, https://www.elastic.co/guide/en/logstash/7.2/output-plugins.html\n\n * elasticsearch\n * email/pageduty\n * influxdb/kafka/mogodb/opentsdb/zabbix\n * http/tcp/websocket\n\n\n# Queue\n\n在logstash中可以配置queue.type为persisted的方式，也就是说在会在当前文件夹下会存储当前正在处理的数据，当初队列来使用。即input已经取到该数据，而filter和output还没处理的数据。\n\n如果设置了persisted的方式，那么在logstash重启了以后，也不会丢失正在处理的数据，数据会在存储在磁盘上，这样可以继续进行filter和output。\n\n当已经output到ES完成后，会删除掉persisted磁盘中的数据。\n\n\n\nIn memory queue:\n\n * 进程crash的时候，机器会宕机，都会引起数据的丢失\n\nPersistent Queue:\n\n * queue.type.persisted（默认是memory）,可以设置queue.max_bytes: 4gb 。\n   \n   机器宕机，数据也不会丢失；数据保证会被消费；可以替代kafka等消息队列缓冲区的作用\n   \n   https://www.elastic.co/guide/en/logstash/7.2/persistent-queues.html\n\n\n# Input Plugin -File\n\n * 支持从文件中读取数据，如日志文件\n\n * 文件读取需要解决的问题\n   \n   只被读取一次，重启后需要从上次读取的位置继续(通过sincedb实现)\n\n * 读取到文件新内容，发现新文件\n\n * 文件发生归档操作(文档位置发生变化，日志rotation)，不能影响当前的内容读取\n\n\n# Code Plugin - Multiline\n\n也就是对于多行的情况下，如何匹配是否属于上一个event还是下一个event。\n\n设置参数：\n\npattern：设置行匹配的正则表达式\n\nwhat：如果匹配成功，那么匹配行属于上一个事件还是下一个事件，可选Previous / Next\n\nnegate true /false：是否对pattern结果取反，可选为true或flase\n\n\n# Filter Plugin\n\nfilter plugin可以对logstash event进行各种处理，例如解析，删除字段，类型转换\n\n * date：日期解析\n * dissect：分隔符解析\n * grok：正则匹配解析\n * mutate：处理字段。重命名，删除，替换\n * ruby：利用ruby代码来动态修改event\n\n\n# Filter Plugin -Mutate\n\n对字段做各种操作\n\n * convert 类型转换\n\n * gsub 字符串替换\n\n * split / join / merge字符串切割，数组合并字符串，数组合并数组\n\n * rename 字段重命名\n\n * update / replace 字段内容更新替换\n\n * remove_field 字段删除',normalizedContent:'elk中相关概念知识点整理\n\n----------------------------------------\n\n\n# elasticsearch\n\n\n# 文档\n\n * es是面向文档的，文档是所有可搜索数据的最小单位。\n   \n   比如日志文件中的一条日志项；一本电影的具体信息；一张唱片的具体信息；mp3播放器里的一首歌；一篇pdf文档中的具体内容。\n\n * 文档会被序列化json格式，保存在es中。\n   \n   json对象由字段组成；每个字段都有对应的字段类型(字符串、数值、布尔、日期、二进制、范围类型)\n\n * 每个文档都有一个unique id\n   \n   我们可以自己指定；或者通过es自动生成。\n\n\n# json文档\n\n * 一篇文档包含了一系列的字段。类似数据库表中一条记录。\n\n * json文档，格式灵活，不需要预先定义格式。\n   \n   字段的类型可以指定或者通过es自动推算\n   \n   支持数组、支持嵌套\n\n\n# 文档的元数据\n\n元数据是用来标注文档的相关信息的。\n\n * _index 文档所属的索引名\n * _type 文档所属的类型名\n * _id 文档唯一的id\n * _source 文档的原始json数据\n * _all 整合所有字段内容到该字段，已废弃\n * _version 文档的版本信息\n * _score 相关性打分\n\n\n# 索引\n\nindex 索引是文档的容器，是一类文档的结合。\n\n * index 体现了逻辑空间的概念：每个索引都有自己的mapping定义，用于定义包含的文档的字段名和字段类型。\n * shard体现了物理空间的概念：索引中的数据分散在shard分片上。\n\n索引的mapping与settings\n\n * mapping定义文档字段的类型\n * setting定义不同的数据分布\n\n\n# 索引的不同语意\n\n * 名词：一个es集群中，可以创建很多个不同的索引\n * 动词：保存一个文档到es的过程也叫做索引(indexing)，也就是在es中，创建一个倒排索引的过程。\n * 名词：通常可以理解为一个b树索引，在es中是倒排索引。\n\n\n# type\n\n * 在7.0之前，一个index可以设置多个types\n * 6.0开始，type已经被废弃。从7.0开始，一个索引只能创建一个type 也就是"_doc"\n\n\n# es vs rdbms的类比\n\n传统关系型数据库和es的区别有：\n\n * es的相关性、高性能全文检索\n * rdms 用于事务性，join\n\nrdbms                          elasticsearch\nrow                            document\ntable                          index\ncolumn                         filed\nschema                         mapping\n分布式mysql中设置的分片数量               setting\nsql                            dsl\n分布式mysql分片路由信息保存在zk中           master node(表现为es的一个java进程)\n有点类似访问代理                       coordinating node\nmysql的节点实例                     data node(表现为es的一个java进程)\nset集群中的master节点上的主的schema的分片   主分片(一个lucene实例)\nset集群中的slave节点上的从的schema的分片    副本分片\n\n\n# es分布式特性\n\n * 高可用\n   \n   服务可用性：允许有节点停止服务\n   \n   数据可用性：部分节点丢失，不会丢失数据\n\n * 可扩展性\n   \n   请求量提升、数据的不断增长（将数据分布到所有节点上）\n\n * es的分布式架构的好处\n   \n   存储的水平扩容。\n   \n   提高系统的可用性，部分节点停止服务，这个集群的服务不受影响。\n\n * es的分布式架构\n   \n   不同的集群通过不同的名字来区分，默认名字"elasticsearch"。\n   \n   通过配置文件修改，或者在命令行中 -e cluster.name=geektime进行设定。\n   \n   一个集群可以有一个或多个节点。\n\n\n# 节点\n\n * 节点是一个es的实例\n   \n   本质上就是一个java进程；一台机器上可以运行多个es进程，但是生产环境一般建议一台机器上只运行一个es实例。\n\n * 每一个节点都有名字，通过配置文件配置，或者启动的时候使用 -e node.name=node1指定。\n\n * 每一个节点在启动之后，会分配一个uid，保存在data目录下。\n\n\n# master node\n\n * master node的职责\n   \n   处理创建，删除索引等请求 / 决定分片被分配到哪个节点 / 负责索引的创建与删除\n   \n   维护并且更新cluster state\n   \n   每个节点上都保存了集群的状态，只有master节点才能修改集群的状态信息。\n   \n   集群状态，维护了一个集群中必要的信息（所有的节点信息、所有的索引和其相关的mapping与setting信息、分片的路由信息）\n\n * master node的最佳实践\n   \n   * master节点非常重要，在部署上需要考虑解决单点的问题。\n   \n   * 为一个集群设置多个master节点 / 每个节点只承担master的单一角色。\n     \n     由于索引和搜索数据都是cpu、内存、io密集型的，可能会对数据节点的资源造成较大压力。因此，在较大规模的集群里，最好要设置单独的仅主节点角色。（这点pb级集群调优时重点关注）\n     \n     不要将主节点同时充当协调节点的角色，因为：对于稳定的集群来说，主节点的角色功能越单一越好。\n\n\n# master-eligible nodes & 选主流程\n\n * 一个集群，支持配置多个master eligible节点。这些节点可以在必要的时候(如master节点出现故障，网络故障时)参与选主流程，成为master节点。\n\n * 每个节点启动后，默认就是一个master eligible节点。\n   \n   可以设置node.master:false禁止\n\n * 当集群内第一个master eligible节点启动时候，它会将自己选举成master节点。\n\n * master-eligible节点可以参加选主流程，成为master节点\n\n\n# 集群状态\n\n * 集群状态信息，维护了一个集群中，必要的信息\n   \n   所有的节点信息；所有的索引和其相关的mapping与setting信息；分片的路由信息\n\n * 在每个节点上都保存了集群的状态信息\n\n * 但是，只有master节点才能修改集群的状态信息，并负责同步给其他节点\n   \n   因为，任意节点能修改信息会导致cluster state信息的不一致\n\n\n# master eligible nodes & 选主的过程\n\n * 互相ping对方，node id低的会成为被选举的节点\n * 其他节点会加入集群，但是不承担master节点的角色。一旦发现被选中的主节点丢失，就会选举出新的master节点\n\n\n# data node\n\n数据节点：保存包含索引文档的分片数据，执行crud、搜索、聚合相关的操作。属于：内存、cpu、io密集型，对硬件资源要求高。\n\n * 可以保存数据的节点，叫做data node\n   \n   节点启动后，默认就是数据节点。可以设置node.data:false禁止\n\n * data node的职责\n   \n   保存分片数据。在数据扩展上启动了至关重要的作用(由master node决定如何把分片分发到数据节点上)\n\n * 通过增加数据节点\n   \n   可以解决数据水平扩展和解决数据单点问题\n\n\n# coordinating node\n\n搜索请求在两个阶段中执行（query 和 fetch），这两个阶段由接收客户端请求的节点 - 协调节点协调。\n\n在请求阶段，协调节点将请求转发到保存数据的数据节点。 每个数据节点在本地执行请求并将其结果返回给协调节点。\n\n在收集fetch阶段，协调节点将每个数据节点的结果汇集为单个全局结果集。\n\n * 处理请求的节点，叫做coordinating node\n   \n   路由请求到正确的节点，例如对于创建索引的请求，需要路由到master节点上去执行。\n\n * 所有节点默认都是coordinating node\n\n * 通过将其他类型设置成false，使其成为dedicated coordinating node\n\n\n# ingress node\n\ningest 节点可以看作是数据前置处理转换的节点，支持 pipeline管道 设置，可以使用 ingest 对数据进行过滤、转换等操作，类似于 logstash 中 filter 的作用，功能相当强大。\n\n可以把ingest节点的功能抽象为：大数据处理环节的“etl”——抽取、转换、加载。\n\n比如如何在数据写入阶段修改字段名(不是修改字段值)？比如在批量写入数据的时候，每条document插入实时的时间戳？\n\n\n# 其他的节点类型\n\n * hot & warm node\n   \n   不同硬件配置的data node,用来实现hot&warm架构，降低集群部署的成本。\n\n * machine learing node\n   \n   负责跑机器学习的job，用来做异常检测\n\n * tribe node\n   \n   从5.3开始使用cross cluster search工具了\n\n\n# 脑裂问题\n\nsplit-brain，分布式系统的经典网络问题，当出现问题，一个节点和其他节点无法连接\n\n * node2和node3会重新选举master\n * node1自己还是作为master，组成一个集群，同时更新cluster state\n * 导致2个master，维护不同的cluster state。当网络恢复的时候，无法选择正确恢复\n\n\n# 如何避免脑裂问题\n\n * 限定一个选举条件，设置quorum(仲裁)，只有在master eligible节点大于quorum的时候，才能进行选举。\n   \n   quorum = (master节点总数/2) +1\n   \n   当3个master eligible的时候，设置discovery.minimum_master_nodes为2，即可避免脑裂\n\n * 从7.0开始，无需这个配置\n   \n   * 移除了minimum_master_nodes参数，让es自己选择可以形成仲裁的节点\n   * 典型的主节点选举现在只需要很短的时间就可以完成了。集群的伸缩变得更安全、更容易，并且可能造成丢失数据的系统配置选项更少了。\n   * 节点更清楚地记录它们的状态，有助于诊断为什么它们不能加入集群或为什么无法选举出主节点\n\n\n# 配置节点类型\n\n * 开发环境中一个节点可以承担多种角色\n * 生产环境中，应该设置单一的角色的节点(dedicated node)\n\n\n\n\n# 分片(primary shard&replica shard)\n\n主分片，用以解决数据水平扩展的问题。通过主分片，可以将数据库分布到集群内的所有节点之上。\n\n * 一个分片是一个运行的lucene的实例\n * 主分片数在索引创建时指定，后续不允许修改，除非reindex\n\n副本，用以解决数据高可用的问题。分片是主分片的拷贝\n\n * 副本分片数，可以动态的调整\n * 增加副本数，还可以在一定程度上提高服务的可用性（读取的吞吐）\n\n\n# 倒排索引\n\n\n# 正排索引和倒排索引示例\n\n\n\n\n# 倒排索引概念\n\n通俗来讲，正向索引是通过key来找value，反向索引则是通过value来找key。从图书来看，正排索引是目录页；倒排索引是索引页。\n\nes分别为json文档中的每一个字段都建立了一个倒排索引。\n\n可以指定对某些字段不做索引，优点是可以节省存储空间；缺点是字段无法被搜索。\n\nterm(单词)：一段文本经过分词器后就会输出一串单词，这一个一个的词就是term。\n\nterm dictionary(单词字典)：里面维护的是term，是term的集合。\n\nterm index(单词索引)：为了更快的找到某个单词，我们为单词建立索引。\n\nposting list(倒排列表)：倒排列表记录了出现过某个单词的所有文档的文档列表(文档id)及单词在该文档中出现的位置信息，每条记录称为一个倒排项(posting)。根据倒排列表，即可获知哪些文档包含某个单词。（实际的倒排列表中并不只是存储了文档id那么简单，还有一些其他的信息）\n\n倒排索引项(posting)中有如下的信息：\n\n文档id；\n\n词频tf，该单词在文档中出现的次数，用于相关性评分；\n\n位置(position)，单词在文档中分词的位置。用于语句搜索(phrase query)。\n\n偏移(offset)，记录单词的开始结束位置，实现高亮显示。\n\n# es查询的大致过程如下\n\n\n\n\n# 分词器\n\n\n# 什么是analysis\n\nanalysis 文本分析就是把全文本转换一系列单词(term/token)的过程，也叫做分词。\n\n\n# analysis与analyzer什么关系\n\nanalysis 是通过analyzer来实现的，有es内置的analyzer分词器，或者也可以使用按需定制化的分词器。\n\n\n# 什么时候需要使用analyzer分词器\n\n * 在数据写入的时候使用analyzer分词器，转换词条。\n * 在匹配query语句的时候也需要指定相同的analyzer分词器，来对查询语句进行分析\n\n\n# analyzer由哪些部分组成\n\nanalyzer主要是由三个部分组成的。\n\ncharacter filters(针对原始文本处理，例如去除html)\n\ntokenizer(按照规则切分为单词)\n\ntoken filter(将切分的单词进行加工，小写，删除stopwords，增加同义词)\n\n\n\n\n# character filters\n\n * 在tokenizer之前对文本进行处理，例如增加删除及替换字符。可以配置多个character filters。会影响tokenizer的position和offset信息。\n * 一些自带的caharacter filters\n   * html strip 去除html标签\n   * mapping 字符串替换\n   * pattern replace 正则匹配替换\n\npost _analyze\n{\n  "tokenizer":"keyword",\n  "char_filter":["html_strip"],\n  "text": "<b>hello world</b>"\n}\n#使用char filter进行替换\npost _analyze\n{\n  "tokenizer": "standard",\n  "char_filter": [\n      {\n        "type" : "mapping",\n        "mappings" : [ "- => _"]\n      }\n    ],\n  "text": "123-456, i-test! test-990 650-555-1234"\n}\n#char filter 替换表情符号\npost _analyze\n{\n  "tokenizer": "standard",\n  "char_filter": [\n      {\n        "type" : "mapping",\n        "mappings" : [ ":) => happy", ":( => sad"]\n      }\n    ],\n    "text": ["i am felling :)", "feeling :( today"]\n}\n#正则表达式\nget _analyze\n{\n  "tokenizer": "standard",\n  "char_filter": [\n      {\n        "type" : "pattern_replace",\n        "pattern" : "http://(.*)",\n        "replacement" : "$1"\n      }\n    ],\n    "text" : "http://www.elastic.co"\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n\n\n\n# tokenizer\n\n * 将原始的文本按照一定的规则，切分为词(term or token)\n\n * es内置的tokenizers\n   \n   whitespace / standard / uax_url_email / pattern / keyword / path hierarchy\n\n * 可以用java开发插件，实现自己的tokenizer\n\npost _analyze\n{\n  "tokenizer":"path_hierarchy",\n  "text":"/user/ymruan/a/b/c/d/e"\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# token filters\n\n * 将tokenizer输出的单词(term)，进行增加，修改，删除\n\n * 自带的token filters\n   \n   lowercase / stop / synonym(添加近义词)\n\nget _analyze\n{\n  "tokenizer": "whitespace",\n  "filter": ["stop","snowball"],\n  "text": ["the rain in spain falls mainly on the plain."]\n}\n#remove 加入lowercase后，the被当成 stopword删除\nget _analyze\n{\n  "tokenizer": "whitespace",\n  "filter": ["lowercase","stop","snowball"],\n  "text": ["the gilrs in china are playing this game!"]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# elasticsearch的内置分词器有哪些\n\n * standard analyzer 默认分词器，按词进行切分，小写处理\n * simple analyzer 按照非字母切分(符号被过滤)，小写处理\n * stop analyzer 小写处理，停用词过滤(the,a,is)\n * whitespace analyzer 按照空格切分，不转小写\n * keyword analyzer 不分词，直接将输入当作输出\n * patter analyzer 正则表达式，默认\\w+ (非字符分隔)\n * lanaguage 提供了30多种常见语言的分词器\n * customer analyzer自定义分词器\n\n\n# standard analyer\n\n这是es的默认分词器。\n\ncharacter filters：没有说明\n\ntokenizer：是按词来切分\n\ntoken filter：将切分后的单词转为小写，stopwords是默认关闭的\n\n\n# simple analyzer\n\ncharacter filters：没有说明\n\ntokenizer：非字母的都会被去除，例如数字，符号等\n\ntoken filter：将切分后的单词转为小写，stopwords是默认关闭的\n\n\n# whitespace analyzer\n\ncharacter filters：没有说明\n\ntokenizer：按照空格来切分单词\n\ntoken filter：将切分后的单词转为小写，stopwords是默认关闭的\n\n\n# stop analyzer\n\ncharacter filters：没有说明\n\ntokenizer：非字母的都会被去除，例如数字，符号等\n\ntoken filter：将切分后的单词转为小写，多了stop filter，会把the/a/is等修饰性词语去除\n\n\n# keyword analyzer\n\n不分词，直接将输入当一个term输出\n\n\n# pattern analyzer\n\n\n# language analyzer\n\n在english的分词器中\n\ncharacter filters：没有说明\n\ntokenizer：分词后的单词，去除掉复数，ing，等时态信息\n\ntoken filter：将切分后的单词转为小写，多了stop filter，会把the/a/is等修饰性词语去除\n\n\n# icu analyzer\n\ncharacter filters：normalization\n\ntokenizer：icu tokenizer\n\ntoken filter: normalization、folding、collation、transform\n\n这个analyzer不是es自带的，需要另行安装。\n\n$ elasticsearch-plugin install analysis-icu\n\n提供了unicode的支持，更好的支持亚洲语言\n\n\n# 搜索相关性relevance\n\n衡量搜索的相关性中有个概念：precision(查准率)和recall(查全率)\n\n * precision (查准率) 尽可能返回较少的无关的文档，也就是查询出来的所有的正确的结果除以全部返回的结果。\n * recall (查全率) 尽量返回较多的相关文档，也就是查询出来的所有的正确的结果除以所有应该返回的正确结果的数量。\n * ranking 是否能够按照相关度进行排序\n\n\n# mapping\n\n\n# 什么是mapping\n\n就是关系型数据库中定义schema。mapping会把json文档映射成lucene所需要的扁平格式。\n\n一个mapping属于一个索引的type；每个文档都属于一个type；一个type有一个mapping定义；从7.0开始，不需要在mapping定义中指定type信息。\n\n作用如下：\n\n * 定义索引中的字段的名称\n * 定义字段的数据类型，例如字符串、数字、布尔....\n * 字段，倒排索引的相关配置，(analyzed or not analyed, analyzer)\n\n\n# es中字段的数据类型有哪些\n\n * 简单类型\n   * text/keyword\n   * date\n   * integer / floating\n   * boolean\n   * ipv4 & ipv6\n * 复杂类型 - 对象和嵌套对象\n   * 对象类型、嵌套类型\n * 特殊类型\n   * geo_point & geo_shape / percolator\n\n\n# 什么是多字段类型\n\n当我们写入一个文档到es中，如果这个文档中的某个字段是text类型的话，那么es默认会给我们增加一个keyword子字段。可以通过mapping信息查看到。\n\n同样我们可以手动显性的去mapping定义一个多字段的类型，在子字段中配置不同的analyzer。例如：针对不同语言；针对pinyin字段的搜索；还支持为搜索和索引指定不通过的anayzer。\n\nput products\n{\n    "mappins":{\n        "properties":{\n            "company":{\n                "type":"text",\n                "fields":{\n                    "keyword":{\n                        "type":"keyword",\n                        "ignore_above":256\n                    }\n                }\n            }\n            "comment":{\n                "type":"text",\n                "fields":{\n                    "english_comment":{\n                        "type":"text",\n                        "analyzer":"english",\n                        "search_analyzer":"english"\n                    }\n                }\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 什么是精确值，什么是全文本\n\nexact values中包括数字、日志、具体一个字符串。可以使用es中的keyword来实现。\n\nfull text中针对的是全文本，非结构化的文本数据，例如es中的text。\n\n精确值(exact value)当设置为keyword的时候，是不会再对这个进行分词的，无论这个有多长。es会为这个精确值所在的字段类创建一个倒排索引。\n\n\n# 什么是dynamic mapping\n\n简而言之，es自己根据文档的各个字段的信息，来自动决定创建各个字段的类型。\n\n * 在写入文档的时候，如果索引不存在，会自动创建索引\n * dynamic mapping的机制，使得我们无需手动定义mappings。es会自动根据文档信息，推算出字段的类型。\n * 但是有时候会推算的不对，例如地理位置信息\n * 当类型如果设置不对的时候，会导致一些功能无法正常运行，例如range查询\n\n\n# dynamic mapping中类型如何自动识别\n\njson类型   elasticsearch类型\n字符串      * 匹配日期格式，设置成date\n         * 配置数字设置为float或long，该选项默认关闭\n         * 设置为text，并且增加keyword子字段\n布尔值      boolean\n浮点数      float\n整数       long\n对象       object\n数组       由第一个非空数值的类型所决定\n空值       忽略\n\n\n# 能否更改mapping的字段类型\n\n新增加字段的情况下：\n\n * 当dynamic设置为true的时候，一旦有新增字段的文档写入，mapping也同时会被更新。\n\n * 当dynamic设置为false的时候，mapping不会被更新，新增字段的数据无法被索引，但是信息会出现在_source中。\n\n * 当dynamic设置成stric的时候，文档写入失败。\n\n对已有的字段，修改字段类型的情况下：\n\n * 对已有的字段，一旦已经有数据写入，就不再支持修改字段定义。\n\n * lucene实现的倒排索引，一旦生成后，就不允许被修改了。\n\n原因：\n\n * 如果修改了字段的数据类型，会导致已被索引的无法被搜索.\n\n * 但是如果是增加新的字段，就不会有这样的影响。\n\n\n# 控制dynamic mappings\n\n通过设置dynamic mappings的值来影响以下三个方面：\n\n * 新增加的文档，是否可以被索引查询出来\n * 新增加的字段，是否可以被索引查询出来\n * 如果存在新增字段，索引的mapping信息是否会被更新\n\n             "true"   "false"   "strict"\n文档可以被索引      yes      yes       no\n字段可以被索引      yes      no        no\nmapping被索引   yes      no        no\n\n值得注意的是，\n\n * 当dynamic被设置成flase的时候，存在新增的字段的数据写入，该数据可以被索引，但是新增字段被丢弃。\n * 当设置成strict模式的时候，数据写入直接出错。\n\n修改dynamic mappings的方法：\n\n#修改为dynamic false\nput dynamic_mapping_test/_mapping\n{\n  "dynamic": false\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# index template\n\n\n# 什么是index template\n\nindex templates 帮助我们设置mappings和settings，并按照一定的规则，自动匹配到新创建的索引之上。\n\n * 模板仅在一个索引被新创建的时候，才会产生作用。修改模板不会影响已经创建的索引\n * 我们可以设定多个索引模板，这些设置会被"merge"在一起\n * 可以指定"order"的数值，控制"merging"的过程\n\n\n# index template的工作方式\n\n当一个索引被新创建的时候\n\n * 应用es默认的settings和mappings\n * 应用order数值低的index template中的设定\n * 应用order高的index template中的设定，之前的设定会被覆盖\n * 应用创建索引的时候，用户所指定的settings和mappings，并覆盖之前模板中的设定\n\n#create a default template\nput _template/template_default\n{\n  "index_patterns": ["*"],\n  "order" : 0,\n  "version": 1,\n  "settings": {\n    "number_of_shards": 1,\n    "number_of_replicas":1\n  }\n}\nput /_template/template_test\n{\n    "index_patterns" : ["test*"],\n    "order" : 1,\n    "settings" : {\n    \t"number_of_shards": 1,\n        "number_of_replicas" : 2\n    },\n    "mappings" : {\n    \t"date_detection": false,\n    \t"numeric_detection": true\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# dynamic template\n\n\n# 什么是dynamic template\n\n根据es识别的数据类型，结合字段名称，来动态设定字段类型\n\n * 所有的字符串类型都设定成keyword，或者关闭keyword字段\n * is开头的字段都设置成boolean\n * long_开头的都设置成long类型\n\ndynamic template是定义在某个索引的mapping中；template有一个名称；匹配规则是一个数组；为匹配到字段设置mapping\n\nput my_index\n{\n  "mappings": {\n    "dynamic_templates": [\n            {\n        "strings_as_boolean": {\n          "match_mapping_type":   "string",\n          "match":"is*",\n          "mapping": {\n            "type": "boolean"\n          }\n        }\n      },\n      {\n        "strings_as_keywords": {\n          "match_mapping_type":   "string",\n          "mapping": {\n            "type": "keyword"\n          }\n        }\n      }\n    ]\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 匹配规则参数\n\n * match_mapping_type：匹配自动识别的字段类型，如string，boolean等\n * match，unmatch：匹配字段名\n * path_match，path_unmatch\n\n\n# 聚合分析\n\n\n# 什么是聚合aggregation\n\n * es除搜索以外，还提供针对es数据统计分析的功能\n   \n   实时性高；hadoop (t+1)\n\n * 通过聚合，我们会得到一个数据的概览，是分析和总结全套的数据，而不是寻找单个文档。\n\n * 高性能，只需要一条语句，就可以从es得到分析结果\n\n * 聚合支持嵌套\n\n\n# 聚合的分类\n\n * bucket aggregation 一些列满足特定条件的文档的集合\n * metric aggregation 一些数学运算，可以对文档字段进行统计分析\n * pipeline aggregation 对其他的聚合结果进行二次聚合\n * matrix aggregation 支持对多个字段的操作并提供一个结果矩阵\n\n\n# 聚合的作用范围及排序\n\nes聚合分析的默认作用范围是query的查询结果集。\n\n同时es还支持以下方式改变聚合的作用范围：\n\nfilter /post_filter / global\n\n\n# 聚合的精准度问题\n\n# 分布式系统的近似统计算法\n\n\n\n# terms聚合分析解析\n\nterms聚合分析的执行流程:\n\n\n\nterms不正确的案例：\n\n\n\nterms aggregation的返回值\n\n * size是最终返回多少个buckt的数量\n * shard_size是每个bucket在一个shard 上返回的bucket的总数。然后，每个shard上的结果，会在coordinate节点上再做一次汇总，返回总数。\n * doc_count_error_upper_bound：被遗漏的term分桶，包含的文档，有可能的最大值。\n * sum_other_doc_count：除了返回结果bucket的terms以外，其他terms的文档总数(总数-返回的总数)\n * show_term_doc_count_error： 还会对每个 bucket都显示一个错误数，表示最大可能的误差情况。\n\n# 解决terms不准的问题\n\n提升shard_size的参数\n\nterms聚合分析不准的原因，数据分散在多个分片上，coordinating node无法获取数据全貌。\n\n * 解决方案1：当数据量不大的时候，设置primary shard为1；实现准确性。\n * 解决方案2：在分布式数据上，设置shard_size参数，提高精确度。(原理是：每次从shard上额外多获取数据，提升准确率)\n\nshard_size设定\n\n * 调整shard size大小，降低doc_count_error_upper_bound来提升准确度\n   \n   增加了整体计算量，提高了准确度，但会降低相应时间\n\n * shard size默认大小设定\n   \n   shard size = size * 1.5 +10\n\n\n# aggregation的语法\n\naggregation属于search的一部分。一般情况下，建议将其size指定为0。\n\n类似的语法如下：\n\n\n\n\n# bucket & metric\n\nmetric类似sql中的count统计，bucket类似sql中的group by.\n\n\n\n\n# buket\n\n我们可以对一类商品分为高档、中档、低档三类，再对高档商品中分为好评、中评、差评。\n\nes中提供很多类型的bucket，帮助我们用多种方式划分文档。例如term & range(时间/年龄区间/地理位置)。\n\n#按照目的地进行分桶统计\nget kibana_sample_data_flights/_search\n{\n\t"size": 0,\n\t"aggs":{\n\t\t"flight_dest":{\n\t\t\t"terms":{\n\t\t\t\t"field":"destcountry"\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# metric\n\n * metric会基于数据集计算结果，除了支持在字段上进行计算，统一也支持在脚本(painless script)产生的结果之上进行计算\n\n * 大多数metric是数学计算，仅输出一个值\n   \n   min/max/sum/avg/cardinality\n\n * 部分metric支持输出多个数值\n   \n   stats/percentiles/percentile_ranks\n\n#查看航班目的地的统计信息，增加平均，最高最低价格\nget kibana_sample_data_flights/_search\n{\n\t"size": 0,\n\t"aggs":{\n\t\t"flight_dest":{\n\t\t\t"terms":{\n\t\t\t\t"field":"destcountry"\n\t\t\t},\n\t\t\t"aggs":{\n\t\t\t\t"avg_price":{\n\t\t\t\t\t"avg":{\n\t\t\t\t\t\t"field":"avgticketprice"\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t"max_price":{\n\t\t\t\t\t"max":{\n\t\t\t\t\t\t"field":"avgticketprice"\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t"min_price":{\n\t\t\t\t\t"min":{\n\t\t\t\t\t\t"field":"avgticketprice"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n#嵌套查询价格统计信息+天气信息\nget kibana_sample_data_flights/_search\n{\n\t"size": 0,\n\t"aggs":{\n\t\t"flight_dest":{\n\t\t\t"terms":{\n\t\t\t\t"field":"destcountry"\n\t\t\t},\n\t\t\t"aggs":{\n\t\t\t\t"stats_price":{\n\t\t\t\t\t"stats":{\n\t\t\t\t\t\t"field":"avgticketprice"\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t"wather":{\n\t\t\t\t  "terms": {\n\t\t\t\t    "field": "destweather",\n\t\t\t\t    "size": 5\n\t\t\t\t  }\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n\n\n\n# term query\n\n这里的term query，单指的是在request body search 中的term查询。精确值匹配\n\nterm是表达语意的最少单位。搜索和利用统计语言模型进行自然语言处理都需要处理term。\n\n特点：\n\n * term level query:term query/ range query /exists query /prefix query /wildcard query\n * 在es中，term查询，对输入不做分词处理。会将输入作为一个整体，在倒排索引中查找准确的词项，并且使用相关度算分公式为每个包含该词项的文档进行相关度算分。输入什么就是查询什么， 例如输入"apple"，就是查询的是"apple"，而"apple"是不会放结果的。而text类型中，会对数据进行分词处理，改为小写。如果term查询中，不是全小写，很有可能是查询不到的。\n * 可以通过constant score将查询转换成一个filtering，避免算分，并利用缓存，提高性能。\n\n如何解决term 查询搜索不到结果的问题：\n\n * 如果针对的是text类型的字段的查询，那么term 查询的时候，使用字段名称.keyword，使用keyword会对查询进行完全匹配，插入的值是什么，查询的时候输入什么，就会查询出来。\n   \n   post /products/_search\n   {\n     "query": {\n       "term": {\n         "productid": {\n           "value": "xhdk-a-1293-#fj3"\n         }\n       }\n     }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   \n\n * term查询的时候，输入的查询条件中，查询字段的值，自己手动改为小写来查询。\n   \n   post /products/_search\n   {\n     "query": {\n       "term": {\n         "desc": {\n           //"value": "iphone"\n           "value":"iphone"\n         }\n       }\n     }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   \n\n\n# 基于全文的查询\n\n和term query不同的是，这是基于全文的查询，查询的时候会对输入的查询条件的值，进行分词处理。\n\n有match query / match phrase query / query string query。\n\n特定是：\n\n * 索引和搜索的时候都会进行分词，查询字符串先传递到一个合适的分词器，然后生成一个供查询的词项列表。\n * 查询的时候，先会对输入的查询进行分词，然后每个词项逐个进行底层的查询，最终将结果进行合并。并为每个文档生成一个算分。例如查询"matrix reloaded"，会查到包括matrix或者reload的所有结果。\n\n\n# match query查询过程\n\n\n\n\n# 结构化搜索\n\n或者可以理解为term查询，或者说是对精确值的查询。\n\n\n# 什么是结构化数据\n\n * 结构化搜索是指对结构化数据的搜索\n   \n   比如日期，布尔类型和数字都是结构化的\n\n * 文本也可以是结构化的\n   \n   * 比如彩色笔可以由离散的颜色集合\n   * 一个博客可能被标记了标签\n   * 电商网站上的商品都有upcs或者其他的唯一标识，它们都需要遵从严格的规定结构化的格式。\n\n\n# es中的结构化搜索\n\n结构化数据：\n\n布尔，时间，日期和数字这类结构化数据：有精确的格式，我们可以对这些格式进行逻辑操作。包括比较数字或时间的范围，或判定两个值的大小。\n\n结构化文本：\n\n结构化的文本可以做精确匹配或部分匹配。\n\n结构化结果：\n\n结构化结果只有"是"或"否"两个值；根据场景需要，可以决定结构化搜索是否需要打分。\n\n\n# 搜索相关性算分\n\n\n# 相关性和相关性算分\n\n相关性也就是relevance,值得注意的是也只有match 查询才有算分的过程，term 查询没有。\n\n * 搜索的相关性算分，描述了一个文档和查询语句匹配的程度。es会对每个匹配查询条件的结果进行算分 _score\n * 打分的本质是排序，需要把最符合用户需求的文档排在前面。在es 5之前，默认的相关性算分采用tf-idf，现在采用bm 25\n\n\n# 词频 tf\n\n * term freuency：检索词在一篇文档中出现的频率\n   \n   也就是检索词出现的次数除以文档的总字数\n\n * 度量一条查询和结果文档相关性的简单方法：简单将搜索中每一个词的tf进行相加\n   \n   tf(区块链)+tf(的)+tf(应用)\n\n * stop word\n   \n   "的"在文档中出现了很多次，但是对贡献相关度几乎没有用处，不应该考虑其tf值\n\n\n# 逆文档频率 idf\n\n * df：检索词在所有文档中出现的频率\n   \n   这里中，"区块链"在相对比较少的文档中出现；"应用"在相对比较多的文件中出现；"stop word"在大量的文档中出现\n\n * inverse document frequency：简单说是 log(全部文档数/检索词出现过的文档总数)\n\n * tf-idf本质上就是将tf求和变成了加权求和\n   \n   \n\n\n# tf-idf的评分公式\n\n\n\n注意的一个参数boosting，可以用来影响算分。\n\n\n# bm 25\n\n从es 5开始，默认算法改为bm 25\n\n和经典的tf-idf相比，当tf无限增加时，bm 25算分会趋于一个数值。\n\n\n\n\n# 自定义相似性\n\n可以自己定义similarity\n\n\n\n\n# explain api查看tf-idf\n\n在request body search中打开"explain":true的方法来查看explain的api\n\npost /testscore/_search\n{\n  "explain": true,\n  "query": {\n    "match": {\n      "content":"you"\n      //"content": "elasticsearch"\n      //"content":"the"\n      //"content": "the elasticsearch"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# boosting relevance\n\n * boosting是控制相关度的一种手段，可以在索引、字段或查询子条件中进行设置\n * 参数boost的含义\n   * 当boost>1的时候，打分的相关度相对性提升\n   * 当0<boost<1的时候，打分的权重相对性降低\n   * 当boost<0的时候，贡献负分\n\n\n\n\n# query context & filter context\n\nes中查询子句可以分为query context（查询上下文）和filter context（过滤上下文）。\n\n查询上下文，是指其使用的查询子句回答了"此文档与此查询子句的匹配程度"的问题。除了决定文档是否匹配外，查询子句还会计算一个_score，表示此文档和其他文档的匹配程度。\n\n过滤器上下文，是指在过滤器上下文中，一个查询子句回答了"此文档是否匹配此查询子句？"的问题。该答案是简单的"yes"或"no"，没有scores算分的。过滤器上下文主要是用来过滤结构性数据，（一般里面嵌套term 查询）。同时频繁使用的过滤器，将被es自动缓存，以提升性能。\n\n\n# 布尔查询\n\n一般应用于多字符串，多字段的复合查询。也就是说查询条件有很多个，有要过滤字段的(filter context)，有要匹配字符串的(query context)。\n\n一个bool查询，是一个或多个查询子句的组合。\n\n总共包括4种子句。其中2种会影响算分，2种不影响算分。\n\n相关性并不只是全文本检索的专利。也适用于yes | no的子句，匹配的子句越多，相关性评分越高。如果多条查询子句被合并为一条复合查询语句，例如bool查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。\n\nterm query只是不会去分词，还是会算分的。\n\nmust子句       query context\n             必须匹配，贡献算分\nshould子句     query context\n             选择性匹配，贡献算分\nmust_not子句   filter context\n             必须不能匹配，不贡献算分\nfilter子句     filter context\n             必须匹配，但是不贡献算分\n\n\n# disjunction max query\n\ndisjunction max query应用于单字符串多字段查询的场景，很多搜索引擎都是这样的。允许你只是输入一个字符串，要多所有的字段进行查询。\n\n下面例子中，只是输入了一个"brown fox"的字符串，需要对所有的字段做匹配。我们这里使用的是bool查询中的should。\n\npost /blogs/_search\n{\n    "query": {\n        "bool": {\n            "should": [\n                { "match": { "title": "brown fox" }},\n                { "match": { "body":  "brown fox" }}\n            ]\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nshould的算分过程\n\n * 对每个文档中，执行查询should语句中的两个查询\n * 加和这两个查询的评分\n * 乘以匹配语句的总数\n * 除以所有语句的总数\n\ndisjunction max query将任何与任一查询匹配的文档作为结果返回。采用字段上最匹配的评分最终评分返回。\n\npost blogs/_search\n{\n    "query": {\n        "dis_max": {\n            "queries": [\n                { "match": { "title": "quick pets" }},\n                { "match": { "body":  "quick pets" }}\n            ]\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# multi match\n\nmulti match同样运用于单字符串多字段的查询。\n\n单字符串多字段查询中场景归纳：\n\n * 最佳字段(best fields)\n   \n   当字段之间相互竞争，又相互关联。例如title和body这样的字段，评分来自最匹配字段。\n\n * 多数字段(most fields)\n   \n   处理英文内容时：一种常见的手段是，在主字段(english analyzer)，抽取词干，加入同义词，以匹配更多的文档。相同的文本，加入子字段(standard analyzer)，以提供更加精确的匹配。其他字段作为匹配文档提高相关度的信号。匹配字段越多越好。\n\n * 混合字段(cross field)\n   \n   对于某些实体，例如人名，地址，图书信息。需要在多个字段中确定信息，单个字段之鞥呢作为整体的一部分。希望在任何这些列出的字段中找出尽可能的词。\n\n\n# search template\n\nes中的search template是用来解耦程序的开发和es搜索dsl的，这样开发工程师、搜索工程师、性能工程师可以各司其职。\n\n在开发初期，虽然可以明确查询参数，但是往往还不能最终定义查询的dsl的具体结构。\n\n可以定义一个search template，这样前端的开发工程师直接调用这个模板来查看到查询结果就可以了。\n\n\n# suggester api\n\n\n# 什么是搜索建议\n\n现代的搜索引擎，一般的都会提供suggest as you type的功能。\n\n帮助用户在输入搜索的过程中，进行自动补全或者纠错。通过协助用户输入更加精准的关键词，提高后续搜索阶段文档匹配的程度。\n\n在google上搜索的时候，一开始会自动补全。当输入到一定长度，如因为单词拼写错误无法补全，就会开始提示相似的词或句子。\n\n\n# 精准度和召回率\n\n * 精准度\n   \n   completion > phrase > term\n\n * 召回率\n   \n   term > phrase > completion\n\n * 性能\n   \n   completion > phrase > term\n\n\n# es suggester api\n\n * 搜索引擎中类似的功能，在es中是通过suggester api实现的。\n * 原理：将输入的文本分解为token，然后在搜索的字典里查找相似的term并返回。\n * 根据不同的使用场景，es设计了4种类别的suggesters\n   * term & phrase suggester\n   * complete & context suggester\n\n\n# term suggester 和phrase suggester\n\nterm suggester按照"suggest_mode"可以分为如下几种：\n\n * missing：如索引中已经存在，就不提供建议\n * popular：推荐出现频率更加高的词\n * always：无论是否存在，都提供建议。\n\nphrase suggester在term suggester上增加了一些额外的逻辑。\n\n例如下面的参数：\n\n * suggest mode: missing，popular，always\n * max errors：最多可以拼错的terms数\n * confidence：限制返回结果数，默认是1\n\n\n# completion suggester\n\n * completion suggester提供了"自动完成"(auto complete)的功能。用户每输入一个字符，就需要即时发送一个查询请求到后端查找匹配项。\n\n * 对性能要求比较苛刻。es采用了不同的数据结构，并非通过倒排索引来完成的。而是将analyze的数据编码成fst和索引一起存放。fst会被es整个加载进内存，速度很快。\n\n * fst只能用于前缀查找\n\n\n# context suggester\n\ncontext suggester是completion suggester的扩展，\n\n可以在搜索中加入更多的上下文信息，例如，输入"star"\n\n * 咖啡相关：建议"starbucks"\n * 电影相关的："star wars"\n\n\n# 文档分布式存储\n\n整理这个章节的目的在于，需要去深入了解文档的查询和索引的过程，文档是如何按分片存储的。\n\n\n# 概述文档存储\n\n文档会存储在具体的某个主分片和副本分片上：例如文档1，会存储在po和ro分片上。\n\n文档到分片的映射算法：\n\n * 确保文档能均匀分布在所用分片上，充分利用硬件资源，避免部分机器空闲，部分机器繁忙\n * 潜在的算法：\n   * 方案一：随机/round robin。当查询文档的时候，这个时候分片数很多，需要多次查询才可能查询到。\n   * 方案二：维护文档到分片的映射关系。当文档数据量大的时候，维护成本高。\n   * 方案三：实时计算，通过文档1，自动算出，需要去哪个分片上获取文档。\n\n\n# 文档到分片路由算法\n\nshard=hash(_routing)%number_of_primary_shards\n\n * hash算法确保文档均匀分散到分片中\n * 默认的_routing值是文档id\n * 可以自行制定routing数值，例如用相同国家的商品，都分配到指定的shard\n * 设置index setting后，primary数，不能随意修改的根本原因。\n\n参考如下的写入文档的时候，指定_routing的值\n\nput posts/_doc/100?routing=bigdata\n{\n  "title":"mastering elasticsearch",\n  "body":"let\'s rock"\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 倒排索引不可变性\n\n * 倒排索引采用的是immutable design，一旦生成，不可更改\n * 不可变性，带来的好处有：\n   * 无需考虑并发写文件的问题，避免了锁机制带来的性能问题\n   * 一旦读入内核的文件系统缓存，便留在那里。只要文件系统缓存有足够的空间，大部分请求就会直接请求内存，不会命中磁盘，提升了很大的性能。\n   * 缓存容易生产和维护/数据可以被压缩\n * 不可变性，带来挑战：如果需要让一个新的文档可以被索引，需要常见整个索引。\n\n\n# lucene index\n\n在lucene中有两个概念，一个是segment，一个是index。多个segment组成了一个index。\n\n * 在lucene中，单个倒排索引文件被称为segment。segment是自包含的，不可变更的。多个segments汇总在一起，称为lucene的index，其对应的就是es的shard。\n * 当有新文档写入的时候，会生成新的segment，查询时会同时查询所有的segments，并且对结果汇总。lucene中有一个文件，用来记录所有segments信息，叫做commit point。\n * 删除的文档信息，保存在".del"文件中。\n\n\n\n\n# 什么是refresh\n\n * 将index buffer写入segment的过程叫做refresh。refresh不执行fsync操作。\n * refresh频率：默认1秒发生一次，可通过index.refresh_interval配置。refresh后，数据就可以被搜索到了。这也是为什么es被称为近实时搜索。\n * 如果系统有大量的数据写入的时候，那就会产生很多的segment。\n * index buffer被占满时，会触发refresh，默认值是jvm的10%。\n\n\n\n\n# 什么是transaction log\n\n * segment写入磁盘的过程相对耗时，借助文件系统缓存，refresh时，先将segment写入缓存以开放查询。\n * 为了保证数据不会丢失。所以在index文档的时候，同时写transaction log，高版本开始，transaction log默认落盘。每个分片由一个transaction log。\n * 在es refresh时，index buffer被清空，transaction log不会清空。\n\n\n\n\n# 什么是flush\n\nflush主要是指调用fsync将缓存中的segments写入磁盘的过程。\n\n具体描述es flush和lucene commit的过程如下：\n\n * es写入index buffer，当默认间隔1s或者index buffer到jvm的10%的时候，触发refresh将buffer写入到lucene中的segment。（写入index buffer的时候同时会写transaction log）refresh后的index buffer为清空了。\n * 默认经过30分钟或者transaction log满512m的时候，调用fsync将缓存中的segments写入到磁盘中。最后会清空删除相应的transaction log。\n\n\n# 什么是merge\n\n * 当segment很多的时候，需要被定期被合并。\n   \n   这个时候可以调用merge的api来实现，这样会减少segments的数量，并且会删除已经删除的文档。\n\n * es和lucene会自动进行merge操作，我们也可以通过手动调用post my_index/_forcemerge来执行。\n\n * es有一个后台进程专门负责segment的合并，它会把小segments合并成更大的segments，然后反复这样。\n\n\n# 整个索引文档到分片的流程\n\n 1. 客户端发起数据写入请求，对你写的这条数据根据_routing规则选择发给哪个shard。\n    \n    * 确认index request中是否设置了使用哪个filed的值作为路由参数\n    * 如果没有设置，则使用mapping中的配置\n    * 如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的hash值选择出shard，最后从集群的meta中找出出该shard的primary节点。\n\n 2. 写入请求到达shard后，先把数据写入到内存（buffer）中，同时会写入一条日志到translog日志文件中去。\n    \n    * 当写入请求到shard后，首先是写lucene，其实就是创建索引。\n    * 索引创建好后并不是马上生成segment，这个时候索引数据还在缓存中，这里的缓存是lucene的缓存，并非elasticsearch缓存，lucene缓存中的数据是不可被查询的。\n\n 3. 执行refresh操作：从内存buffer中将数据写入os cache(操作系统的内存)，产生一个segment file文件，buffer清空。\n    \n    * 写入os cache的同时，建立倒排索引，这时数据就可以供客户端进行访问了。\n    * 默认是每隔1秒refresh一次的，所以es是准实时的，因为写入的数据1秒之后才能被看到。\n    * buffer内存占满的时候也会执行refresh操作，buffer默认值是jvm内存的10%。\n    * 通过es的restful api或者java api，手动执行一次refresh操作，就是手动将buffer中的数据刷入os cache中，让数据立马就可以被搜索到。\n    * 若要优化索引速度, 而不注重实时性, 可以降低刷新频率。\n\n 4. translog会每隔5秒或者在一个变更请求完成之后，将translog从缓存刷入磁盘。\n    \n    * translog是存储在os cache中，每个分片有一个，如果节点宕机会有5秒数据丢失，但是性能比较好，最多丢5秒的数据。\n    * 可以将translog设置成每次写操作必须是直接fsync到磁盘，但是性能会差很多。\n    * 可以通过配置增加translog刷磁盘的频率来增加数据可靠性，最小可配置100ms，但不建议这么做，因为这会对性能有非常大的影响。\n\n 5. 每30分钟或者当tanslog的大小达到512m时候，就会执行commit操作（flush操作），将os cache中所有的数据全以segment file的形式，持久到磁盘上去。\n    \n    * 第一步，就是将buffer中现有数据refresh到os cache中去。\n    * 清空buffer 然后强行将os cache中所有的数据全都一个一个的通过segmentfile的形式，持久到磁盘上去。\n    * 将commit point这个文件更新到磁盘中，每个shard都有一个提交点(commit point), 其中保存了当前shard成功写入磁盘的所有segment。\n    * 把translog文件删掉清空，再开一个空的translog文件。\n    * flush参数设置有，index.translog.flush_threshold_period , index.translog.flush_threshold_size，控制每收到多少条数据后flush一次, index.translog.flush_threshold_ops。\n\n 6. segment的merge操作：\n    \n    * 随着时间，磁盘上的segment越来越多，需要定期进行合并。\n    * es和lucene 会自动进行merge操作，合并segment和删除已经删除的文档。\n    * 我们可以手动进行merge：post index/_forcemerge。一般不需要，这是一个比较消耗资源的操作。\n\n\n# logstash\n\n\n# logstash是什么\n\nlogstash是etl工具，数据收集处理引擎，java写的。支持200多个插件。\n\nlogstash支持从文件、http、数据库、kafka等数据源中读取数据，经过定义的规则进行加工清洗和处理，然后最终吐到mogodb、es、hdfs的数据分析器中进行分析和查询。\n\n\n\n\n# logstash中的基本概念\n\npipeline\n\n * 包含了input - filter - output 三个阶段的处理流程\n * 可以对插件进行生命周期管理\n * 内置队列的管理\n\nlogstash event\n\n * event是数据在内部流转的时候，具体表现形式。数据在input阶段被转换为event，在output阶段被转化为目标格式的数据。\n * event其实就是一个java object，在配置文件中，可以对event的属性进行增删改查。\n\n\n# logstash架构简介\n\ncodec(code / decode)：将原始数据decode成event；将event encode成目标数据。\n\n从input -> 配置code成event -> 经过filter过滤 -> 然后将event encode -> 通过output输出。\n\n\n\n\n# input plugins\n\n一个pipeline可以有多个input插件\n\n * stdin/file\n * beats/log4j/elasticsearch/jdbc/kafka/babbitmq/redis\n * jmx/http/websocket/udp/tcp\n * google cloud storages/s3\n * github/twitter\n\n\n# codec plugins\n\n将原始数据decode成event；将event encode成目标数据\n\n内置的codec plugins有 https://www.elastic.co/guide/en/logstash/7.2/codec-plugins.html\n\n * line/multiline\n\n * json/avro/cef\n\n * dots/rubydebug\n\n\n# filter plugins\n\n处理event\n\n内置的filter plugins有： https://www.elastic.co/guide/en/logstash/7.2/filter-plugins.html\n\n * mutate 操作event的字段\n\n * metrics aggregates metrics\n\n * ruby 执行ruby代码\n\n\n# output plugins\n\n将event发送到特定的目的地，是pipeline的最后一个阶段。\n\n常见的output plugins有, https://www.elastic.co/guide/en/logstash/7.2/output-plugins.html\n\n * elasticsearch\n * email/pageduty\n * influxdb/kafka/mogodb/opentsdb/zabbix\n * http/tcp/websocket\n\n\n# queue\n\n在logstash中可以配置queue.type为persisted的方式，也就是说在会在当前文件夹下会存储当前正在处理的数据，当初队列来使用。即input已经取到该数据，而filter和output还没处理的数据。\n\n如果设置了persisted的方式，那么在logstash重启了以后，也不会丢失正在处理的数据，数据会在存储在磁盘上，这样可以继续进行filter和output。\n\n当已经output到es完成后，会删除掉persisted磁盘中的数据。\n\n\n\nin memory queue:\n\n * 进程crash的时候，机器会宕机，都会引起数据的丢失\n\npersistent queue:\n\n * queue.type.persisted（默认是memory）,可以设置queue.max_bytes: 4gb 。\n   \n   机器宕机，数据也不会丢失；数据保证会被消费；可以替代kafka等消息队列缓冲区的作用\n   \n   https://www.elastic.co/guide/en/logstash/7.2/persistent-queues.html\n\n\n# input plugin -file\n\n * 支持从文件中读取数据，如日志文件\n\n * 文件读取需要解决的问题\n   \n   只被读取一次，重启后需要从上次读取的位置继续(通过sincedb实现)\n\n * 读取到文件新内容，发现新文件\n\n * 文件发生归档操作(文档位置发生变化，日志rotation)，不能影响当前的内容读取\n\n\n# code plugin - multiline\n\n也就是对于多行的情况下，如何匹配是否属于上一个event还是下一个event。\n\n设置参数：\n\npattern：设置行匹配的正则表达式\n\nwhat：如果匹配成功，那么匹配行属于上一个事件还是下一个事件，可选previous / next\n\nnegate true /false：是否对pattern结果取反，可选为true或flase\n\n\n# filter plugin\n\nfilter plugin可以对logstash event进行各种处理，例如解析，删除字段，类型转换\n\n * date：日期解析\n * dissect：分隔符解析\n * grok：正则匹配解析\n * mutate：处理字段。重命名，删除，替换\n * ruby：利用ruby代码来动态修改event\n\n\n# filter plugin -mutate\n\n对字段做各种操作\n\n * convert 类型转换\n\n * gsub 字符串替换\n\n * split / join / merge字符串切割，数组合并字符串，数组合并数组\n\n * rename 字段重命名\n\n * update / replace 字段内容更新替换\n\n * remove_field 字段删除',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"ES_常见问题列表",frontmatter:{title:"ES_常见问题列表",date:"2022-01-20T20:57:04.000Z",permalink:"/pages/2f7a8b/",categories:["数据库","ELK"],tags:[null]},regularPath:"/01.%E6%95%B0%E6%8D%AE%E5%BA%93/05.ELK/04.ES_%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%88%97%E8%A1%A8.html",relativePath:"01.数据库/05.ELK/04.ES_常见问题列表.md",key:"v-67450649",path:"/pages/2f7a8b/",headers:[{level:2,title:"ILM的配置",slug:"ilm的配置",normalizedTitle:"ilm的配置",charIndex:15},{level:3,title:"什么是ILM",slug:"什么是ilm",normalizedTitle:"什么是ilm",charIndex:26},{level:3,title:"提出问题:为什么需要ILM",slug:"提出问题-为什么需要ilm",normalizedTitle:"提出问题:为什么需要ilm",charIndex:221},{level:3,title:"理论上如何解决这个问题",slug:"理论上如何解决这个问题",normalizedTitle:"理论上如何解决这个问题",charIndex:374},{level:3,title:"ES如何实现新增分片的",slug:"es如何实现新增分片的",normalizedTitle:"es如何实现新增分片的",charIndex:525},{level:3,title:"ES迁移分片到warm和cold节点",slug:"es迁移分片到warm和cold节点",normalizedTitle:"es迁移分片到warm和cold节点",charIndex:734},{level:3,title:"ILM的基本概念",slug:"ilm的基本概念",normalizedTitle:"ilm的基本概念",charIndex:859},{level:3,title:"Hot Phase阶段",slug:"hot-phase阶段",normalizedTitle:"hot phase阶段",charIndex:1270},{level:3,title:"Warm Phase阶段",slug:"warm-phase阶段",normalizedTitle:"warm phase阶段",charIndex:1543},{level:3,title:"Cold Phase阶段",slug:"cold-phase阶段",normalizedTitle:"cold phase阶段",charIndex:1784},{level:2,title:"ILM API",slug:"ilm-api",normalizedTitle:"ilm api",charIndex:1872},{level:3,title:"创建policy的API",slug:"创建policy的api",normalizedTitle:"创建policy的api",charIndex:1883},{level:3,title:"创建index template",slug:"创建index-template",normalizedTitle:"创建index template",charIndex:1896},{level:3,title:"实际操作",slug:"实际操作",normalizedTitle:"实际操作",charIndex:3684},{level:2,title:"Logstash导入ES",slug:"logstash导入es",normalizedTitle:"logstash导入es",charIndex:6271},{level:3,title:"Mysql中全量导入ES",slug:"mysql中全量导入es",normalizedTitle:"mysql中全量导入es",charIndex:6288},{level:3,title:"Oracle中全量导入ES",slug:"oracle中全量导入es",normalizedTitle:"oracle中全量导入es",charIndex:8146},{level:3,title:"ES环境的优化",slug:"es环境的优化",normalizedTitle:"es环境的优化",charIndex:10016},{level:2,title:"ES中多表关联如何解决",slug:"es中多表关联如何解决",normalizedTitle:"es中多表关联如何解决",charIndex:10702},{level:3,title:"引入问题",slug:"引入问题",normalizedTitle:"引入问题",charIndex:10769},{level:3,title:"基础认知",slug:"基础认知",normalizedTitle:"基础认知",charIndex:10842},{level:3,title:"ES场景中如何解决这种关联关系",slug:"es场景中如何解决这种关联关系",normalizedTitle:"es场景中如何解决这种关联关系",charIndex:11565},{level:3,title:"小结",slug:"小结",normalizedTitle:"小结",charIndex:13332},{level:2,title:"ES有哪些应用场景",slug:"es有哪些应用场景",normalizedTitle:"es有哪些应用场景",charIndex:13754},{level:2,title:"ES与关系型数据库的抽象类比",slug:"es与关系型数据库的抽象类比",normalizedTitle:"es与关系型数据库的抽象类比",charIndex:13911},{level:2,title:"正排索引和倒排索引",slug:"正排索引和倒排索引",normalizedTitle:"正排索引和倒排索引",charIndex:14451},{level:3,title:"倒排索引的数据结构",slug:"倒排索引的数据结构",normalizedTitle:"倒排索引的数据结构",charIndex:14501},{level:3,title:"为什么需要倒排索引呢?",slug:"为什么需要倒排索引呢",normalizedTitle:"为什么需要倒排索引呢?",charIndex:14734},{level:3,title:"什么是倒排索引呢？",slug:"什么是倒排索引呢",normalizedTitle:"什么是倒排索引呢？",charIndex:15333},{level:3,title:"倒排索引的总结",slug:"倒排索引的总结",normalizedTitle:"倒排索引的总结",charIndex:19027},{level:2,title:"ES Top10监控指标",slug:"es-top10监控指标",normalizedTitle:"es top10监控指标",charIndex:19371},{level:3,title:"监控ES集群的重要性",slug:"监控es集群的重要性",normalizedTitle:"监控es集群的重要性",charIndex:19388},{level:3,title:"集群健康维度：分片和节点",slug:"集群健康维度-分片和节点",normalizedTitle:"集群健康维度：分片和节点",charIndex:19606},{level:3,title:"搜索性能维度：请求率和延迟",slug:"搜索性能维度-请求率和延迟",normalizedTitle:"搜索性能维度：请求率和延迟",charIndex:20724},{level:3,title:"索引性能维度：刷新(refresh)和合并(merge)时间",slug:"索引性能维度-刷新-refresh-和合并-merge-时间",normalizedTitle:"索引性能维度：刷新(refresh)和合并(merge)时间",charIndex:21747},{level:3,title:"节点运行状况维度：内存，磁盘和CPU指标",slug:"节点运行状况维度-内存-磁盘和cpu指标",normalizedTitle:"节点运行状况维度：内存，磁盘和cpu指标",charIndex:23107},{level:3,title:"JVM运行状况维度：堆，GC和池大小(Pool Size)",slug:"jvm运行状况维度-堆-gc和池大小-pool-size",normalizedTitle:"jvm运行状况维度：堆，gc和池大小(pool size)",charIndex:23878},{level:3,title:"ES Top10监控指标",slug:"es-top10监控指标-2",normalizedTitle:"es top10监控指标",charIndex:19371},{level:2,title:"seed_host/initial master区别联系",slug:"seed-host-initial-master区别联系",normalizedTitle:"seed_host/initial master区别联系",charIndex:26491},{level:3,title:"腾讯大佬的灵魂9问",slug:"腾讯大佬的灵魂9问",normalizedTitle:"腾讯大佬的灵魂9问",charIndex:26524},{level:3,title:"解答如下",slug:"解答如下",normalizedTitle:"解答如下",charIndex:26942},{level:2,title:"network.host/discovery.seed_hosts等区别和联系",slug:"network-host-discovery-seed-hosts等区别和联系",normalizedTitle:"network.host/discovery.seed_hosts等区别和联系",charIndex:27164},{level:3,title:"提问：",slug:"提问",normalizedTitle:"提问：",charIndex:27284},{level:3,title:"解答：",slug:"解答",normalizedTitle:"解答：",charIndex:27458},{level:2,title:"常见大佬博客整理",slug:"常见大佬博客整理",normalizedTitle:"常见大佬博客整理",charIndex:28683},{level:2,title:"三节点(含master)集群高可用测试",slug:"三节点-含master-集群高可用测试",normalizedTitle:"三节点(含master)集群高可用测试",charIndex:28951},{level:2,title:"Elasticsearch不适合做什么",slug:"elasticsearch不适合做什么",normalizedTitle:"elasticsearch不适合做什么",charIndex:30342},{level:2,title:"Elasticsearch 25个默认值",slug:"elasticsearch-25个默认值",normalizedTitle:"elasticsearch 25个默认值",charIndex:30692},{level:3,title:"参数类型以及静态和动态参数的区别",slug:"参数类型以及静态和动态参数的区别",normalizedTitle:"参数类型以及静态和动态参数的区别",charIndex:30813},{level:3,title:"ES集群bool类型默认支持最大子句个数",slug:"es集群bool类型默认支持最大子句个数",normalizedTitle:"es集群bool类型默认支持最大子句个数",charIndex:31281},{level:3,title:"ES集群数据节点支持默认分片数个数",slug:"es集群数据节点支持默认分片数个数",normalizedTitle:"es集群数据节点支持默认分片数个数",charIndex:31565},{level:3,title:"ES集群index_buffer默认比例是多少",slug:"es集群index-buffer默认比例是多少",normalizedTitle:"es集群index_buffer默认比例是多少",charIndex:32004},{level:3,title:"ES默认磁盘使用率85%不再支持写入数据吗？",slug:"es默认磁盘使用率85-不再支持写入数据吗",normalizedTitle:"es默认磁盘使用率85%不再支持写入数据吗？",charIndex:32516},{level:3,title:"ES集群默认的gc方式",slug:"es集群默认的gc方式",normalizedTitle:"es集群默认的gc方式",charIndex:32942},{level:3,title:"ES索引默认主分片分配大小",slug:"es索引默认主分片分配大小",normalizedTitle:"es索引默认主分片分配大小",charIndex:33664},{level:3,title:"ES索引默认压缩算法是什么",slug:"es索引默认压缩算法是什么",normalizedTitle:"es索引默认压缩算法是什么",charIndex:33954},{level:3,title:"ES索引默认副本分片数",slug:"es索引默认副本分片数",normalizedTitle:"es索引默认副本分片数",charIndex:34156},{level:3,title:"ES索引默认的刷新频率",slug:"es索引默认的刷新频率",normalizedTitle:"es索引默认的刷新频率",charIndex:34300},{level:3,title:"ES索引terms默认最大支持的长度是什么?",slug:"es索引terms默认最大支持的长度是什么",normalizedTitle:"es索引terms默认最大支持的长度是什么?",charIndex:34435},{level:3,title:"ES索引默认分页返回最大条数?",slug:"es索引默认分页返回最大条数",normalizedTitle:"es索引默认分页返回最大条数?",charIndex:34558},{level:3,title:"ES索引默认管道有必要设置吗？",slug:"es索引默认管道有必要设置吗",normalizedTitle:"es索引默认管道有必要设置吗？",charIndex:34780},{level:3,title:"ES索引Mapping默认支持最大字段数?",slug:"es索引mapping默认支持最大字段数",normalizedTitle:"es索引mapping默认支持最大字段数?",charIndex:35018},{level:3,title:"ES索引Mapping字段默认的最大深度？",slug:"es索引mapping字段默认的最大深度",normalizedTitle:"es索引mapping字段默认的最大深度？",charIndex:35162},{level:3,title:"ES索引Mapping nested默认支持大小？",slug:"es索引mapping-nested默认支持大小",normalizedTitle:"es索引mapping nested默认支持大小？",charIndex:35354},{level:3,title:"ES索引动态Mapping条件下，匹配的字符串默认匹配的是？",slug:"es索引动态mapping条件下-匹配的字符串默认匹配的是",normalizedTitle:"es索引动态mapping条件下，匹配的字符串默认匹配的是？",charIndex:35865},{level:3,title:"ES默认的评分机制是什么？",slug:"es默认的评分机制是什么",normalizedTitle:"es默认的评分机制是什么？",charIndex:36456},{level:3,title:"ES keyword类型默认支持的字符数是多少?",slug:"es-keyword类型默认支持的字符数是多少",normalizedTitle:"es keyword类型默认支持的字符数是多少?",charIndex:36595},{level:3,title:"为什么说，ES默认不适用别名，不算入门ES",slug:"为什么说-es默认不适用别名-不算入门es",normalizedTitle:"为什么说，es默认不适用别名，不算入门es",charIndex:36797},{level:3,title:"ES集群节点默认属性值",slug:"es集群节点默认属性值",normalizedTitle:"es集群节点默认属性值",charIndex:36933},{level:3,title:"ES客户端请求的节点默认是？",slug:"es客户端请求的节点默认是",normalizedTitle:"es客户端请求的节点默认是？",charIndex:37130},{level:3,title:"ES默认分词器？",slug:"es默认分词器",normalizedTitle:"es默认分词器？",charIndex:37313},{level:3,title:"ES聚合默认UTC时间，可以修改吗？",slug:"es聚合默认utc时间-可以修改吗",normalizedTitle:"es聚合默认utc时间，可以修改吗？",charIndex:37541},{level:3,title:"ES默认堆内存大小？",slug:"es默认堆内存大小",normalizedTitle:"es默认堆内存大小？",charIndex:37931},{level:3,title:"ES JDK 什么版本开始默认自带的？",slug:"es-jdk-什么版本开始默认自带的",normalizedTitle:"es jdk 什么版本开始默认自带的？",charIndex:38057},{level:3,title:"索引(动态/静态)设置参考：",slug:"索引-动态-静态-设置参考",normalizedTitle:"索引(动态/静态)设置参考：",charIndex:38133},{level:2,title:"ES集群状态变成非绿怎么办？",slug:"es集群状态变成非绿怎么办",normalizedTitle:"es集群状态变成非绿怎么办？",charIndex:38236},{level:3,title:"集群状态的含义是什么",slug:"集群状态的含义是什么",normalizedTitle:"集群状态的含义是什么",charIndex:38281},{level:3,title:"排查实战思路",slug:"排查实战思路",normalizedTitle:"排查实战思路",charIndex:38350},{level:3,title:"到底索引的哪个分片出现了红色或黄色问题？",slug:"到底索引的哪个分片出现了红色或黄色问题",normalizedTitle:"到底索引的哪个分片出现了红色或黄色问题？",charIndex:38539},{level:3,title:"到底什么原因导致了集群变成红色或黄色呢？",slug:"到底什么原因导致了集群变成红色或黄色呢",normalizedTitle:"到底什么原因导致了集群变成红色或黄色呢？",charIndex:38598},{level:3,title:'扩展思考：类似"current_state":"unassigned"， 未分配还有哪些?',slug:"扩展思考-类似-current-state-unassigned-未分配还有哪些",normalizedTitle:"扩展思考：类似&quot;current_state&quot;:&quot;unassigned&quot;， 未分配还有哪些?",charIndex:null},{level:2,title:"文档打标签",slug:"文档打标签",normalizedTitle:"文档打标签",charIndex:40192},{level:2,title:"索引的分片规划",slug:"索引的分片规划",normalizedTitle:"索引的分片规划",charIndex:41812},{level:3,title:"球友提问",slug:"球友提问",normalizedTitle:"球友提问",charIndex:41824},{level:3,title:"星主回复",slug:"星主回复",normalizedTitle:"星主回复",charIndex:42031},{level:2,title:"ES磁盘空间满",slug:"es磁盘空间满",normalizedTitle:"es磁盘空间满",charIndex:42283},{level:3,title:"故障现象",slug:"故障现象",normalizedTitle:"故障现象",charIndex:42295},{level:3,title:"故障分析",slug:"故障分析",normalizedTitle:"故障分析",charIndex:42375},{level:3,title:"故障处理",slug:"故障处理",normalizedTitle:"故障处理",charIndex:42469},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:19032},{level:2,title:"Reindex 高性能实践",slug:"reindex-高性能实践",normalizedTitle:"reindex 高性能实践",charIndex:42826},{level:3,title:"问题描述",slug:"问题描述",normalizedTitle:"问题描述",charIndex:42844},{level:3,title:"Reindex简介",slug:"reindex简介",normalizedTitle:"reindex简介",charIndex:42941},{level:3,title:"原因分析",slug:"原因分析",normalizedTitle:"原因分析",charIndex:43258},{level:3,title:"Reindex提升迁移效率的方案",slug:"reindex提升迁移效率的方案",normalizedTitle:"reindex提升迁移效率的方案",charIndex:43420},{level:3,title:"借助scroll的sliced提升写入效率",slug:"借助scroll的sliced提升写入效率",normalizedTitle:"借助scroll的sliced提升写入效率",charIndex:44154},{level:3,title:"ES副本数设置为0",slug:"es副本数设置为0",normalizedTitle:"es副本数设置为0",charIndex:44822},{level:3,title:"增加refresh间隔",slug:"增加refresh间隔",normalizedTitle:"增加refresh间隔",charIndex:45080},{level:3,title:"设置task",slug:"设置task",normalizedTitle:"设置task",charIndex:45226},{level:3,title:"小结",slug:"小结-2",normalizedTitle:"小结",charIndex:13332},{level:2,title:"修改未分片，重新分片",slug:"修改未分片-重新分片",normalizedTitle:"修改未分片，重新分片",charIndex:46044},{level:2,title:"删除索引数据，不删除模型",slug:"删除索引数据-不删除模型",normalizedTitle:"删除索引数据，不删除模型",charIndex:46120},{level:2,title:"FRA",slug:"fra",normalizedTitle:"fra",charIndex:46190},{level:3,title:"filebeat和logstash有什么区别联系",slug:"filebeat和logstash有什么区别联系",normalizedTitle:"filebeat和logstash有什么区别联系",charIndex:46198},{level:3,title:"ES的堆内存该设置多大，为什么?",slug:"es的堆内存该设置多大-为什么",normalizedTitle:"es的堆内存该设置多大，为什么?",charIndex:46234},{level:3,title:"什么是堆内存",slug:"什么是堆内存",normalizedTitle:"什么是堆内存",charIndex:46283},{level:3,title:"堆内存的作用是什么？",slug:"堆内存的作用是什么",normalizedTitle:"堆内存的作用是什么？",charIndex:46472},{level:3,title:"堆内存如何配置？",slug:"堆内存如何配置",normalizedTitle:"堆内存如何配置？",charIndex:46773},{level:3,title:"堆内存配置建议",slug:"堆内存配置建议",normalizedTitle:"堆内存配置建议",charIndex:46938},{level:3,title:"堆内存为什么不能超过物理机内存的一半？",slug:"堆内存为什么不能超过物理机内存的一半",normalizedTitle:"堆内存为什么不能超过物理机内存的一半？",charIndex:47138},{level:3,title:"堆内存为什么不能超过32GB?",slug:"堆内存为什么不能超过32gb",normalizedTitle:"堆内存为什么不能超过32gb?",charIndex:47610},{level:3,title:"我是内存土豪怎么办？",slug:"我是内存土豪怎么办",normalizedTitle:"我是内存土豪怎么办？",charIndex:48148},{level:3,title:"堆内存优化建议",slug:"堆内存优化建议",normalizedTitle:"堆内存优化建议",charIndex:48817},{level:3,title:"最新认知",slug:"最新认知",normalizedTitle:"最新认知",charIndex:49284},{level:3,title:"参考URL",slug:"参考url",normalizedTitle:"参考url",charIndex:10301}],headersStr:'ILM的配置 什么是ILM 提出问题:为什么需要ILM 理论上如何解决这个问题 ES如何实现新增分片的 ES迁移分片到warm和cold节点 ILM的基本概念 Hot Phase阶段 Warm Phase阶段 Cold Phase阶段 ILM API 创建policy的API 创建index template 实际操作 Logstash导入ES Mysql中全量导入ES Oracle中全量导入ES ES环境的优化 ES中多表关联如何解决 引入问题 基础认知 ES场景中如何解决这种关联关系 小结 ES有哪些应用场景 ES与关系型数据库的抽象类比 正排索引和倒排索引 倒排索引的数据结构 为什么需要倒排索引呢? 什么是倒排索引呢？ 倒排索引的总结 ES Top10监控指标 监控ES集群的重要性 集群健康维度：分片和节点 搜索性能维度：请求率和延迟 索引性能维度：刷新(refresh)和合并(merge)时间 节点运行状况维度：内存，磁盘和CPU指标 JVM运行状况维度：堆，GC和池大小(Pool Size) ES Top10监控指标 seed_host/initial master区别联系 腾讯大佬的灵魂9问 解答如下 network.host/discovery.seed_hosts等区别和联系 提问： 解答： 常见大佬博客整理 三节点(含master)集群高可用测试 Elasticsearch不适合做什么 Elasticsearch 25个默认值 参数类型以及静态和动态参数的区别 ES集群bool类型默认支持最大子句个数 ES集群数据节点支持默认分片数个数 ES集群index_buffer默认比例是多少 ES默认磁盘使用率85%不再支持写入数据吗？ ES集群默认的gc方式 ES索引默认主分片分配大小 ES索引默认压缩算法是什么 ES索引默认副本分片数 ES索引默认的刷新频率 ES索引terms默认最大支持的长度是什么? ES索引默认分页返回最大条数? ES索引默认管道有必要设置吗？ ES索引Mapping默认支持最大字段数? ES索引Mapping字段默认的最大深度？ ES索引Mapping nested默认支持大小？ ES索引动态Mapping条件下，匹配的字符串默认匹配的是？ ES默认的评分机制是什么？ ES keyword类型默认支持的字符数是多少? 为什么说，ES默认不适用别名，不算入门ES ES集群节点默认属性值 ES客户端请求的节点默认是？ ES默认分词器？ ES聚合默认UTC时间，可以修改吗？ ES默认堆内存大小？ ES JDK 什么版本开始默认自带的？ 索引(动态/静态)设置参考： ES集群状态变成非绿怎么办？ 集群状态的含义是什么 排查实战思路 到底索引的哪个分片出现了红色或黄色问题？ 到底什么原因导致了集群变成红色或黄色呢？ 扩展思考：类似"current_state":"unassigned"， 未分配还有哪些? 文档打标签 索引的分片规划 球友提问 星主回复 ES磁盘空间满 故障现象 故障分析 故障处理 总结 Reindex 高性能实践 问题描述 Reindex简介 原因分析 Reindex提升迁移效率的方案 借助scroll的sliced提升写入效率 ES副本数设置为0 增加refresh间隔 设置task 小结 修改未分片，重新分片 删除索引数据，不删除模型 FRA filebeat和logstash有什么区别联系 ES的堆内存该设置多大，为什么? 什么是堆内存 堆内存的作用是什么？ 堆内存如何配置？ 堆内存配置建议 堆内存为什么不能超过物理机内存的一半？ 堆内存为什么不能超过32GB? 我是内存土豪怎么办？ 堆内存优化建议 最新认知 参考URL',content:'# ES常见问题列表\n\n\n# ILM的配置\n\n\n# 什么是ILM\n\nILM全名叫做index lifecycle managemenet，也就是所谓的索引生命周期管理。包含了索引的从诞生(hot)，warm节点，cold节点，直至delete。\n\n\n\n从ES6.6开始引入集成，是ES最佳实践的集大成者。并且以hot-warm为基础，自动化的对索引进行管理。解放使用者的心智，提升集群稳定性，达到一劳永逸的效果。\n\n图解生命周期\n\n\n\n\n# 提出问题:为什么需要ILM\n\n从ES最佳实践的角度来看：\n\n * 单分片大小需要控制在50GB以内\n   * 推荐日志场景在30GB\n   * 搜索场景在10GB\n * 索引分片数为数据节点的倍数\n\n在一个数据不断增长的场景下，不管是索引的分片数，还是单个分片可能的大小，都很难进行准确的估算的。\n\n\n# 理论上如何解决这个问题\n\n我们主要需要解决的是，让shard数随着数据量的不断增长，而进行同步的增长。增长的方式有，分裂(从原有的分片中分裂出1个分片来)；还有新增分片的方式。\n\n考虑到ES默认使用的对log_id进行hashcode路由的方式来分片存储的，采用新增分片的方式，比较合适。\n\n\n\n\n# ES如何实现新增分片的\n\n新增的分片实际上是，新增了新的索引。新的索引和老的索引都有着相同的索引的别名，我们写数据的时候，是通过索引别名的方式来写数据的。\n\n当老的索引达到某个阀值的时候，触发ES执行rollover，老的索引的别名参数设置为is_write_index:false，新的索引的别名参数设置为is_write_index:true。这样就把数据写到了新的索引上面了。\n\n\n\n具体场景如下：\n\n\n\n\n# ES迁移分片到warm和cold节点\n\nILM的功能，同样可以使得某个索引的分片达到某个阀值后，将索引的分片往warm或cold节点上进行迁移。\n\nHot节点是高配的机器，可以读写索引。而Warm和cold节点是低配的机器，只读索引的状态。\n\n\n# ILM的基本概念\n\nPolicy: 顾名思义，就是一条索引的生命周期的策略。\n\nPhase: 指的是索引的不同的生命周期的阶段，包括有Hot阶段、Warm阶段、Cold阶段、Delete阶段。\n\nAction: 是指在各个阶段，有哪些可以执行的操作。\n\n如下图所示：\n\n\n\n在Hot阶段，我们需要手动执行Create一个索引，达到某个阀值后，会触发Rollover滚动。\n\n在Warm阶段，可以执行Allocate分配操作，将索引的分片从hot的节点属性上迁移到warm的节点属性上；将之前的索引设置为Read-Only的状态；可以执行Force Merge的操作，手动触发合并以减少索引的每个分片中的段数，并释放已删除文档使用的空间；Shrik操作是指，减少索引中的主分片数量。\n\n在Cold阶段，只有一个Allocate的操作。\n\n在Delete阶段，直接将对应的之前的Cold阶段到期的索引，直接删除掉了。\n\n\n# Hot Phase阶段\n\n 1. 我们需要先定义了一个index template，其中定义了索引的别名，匹配索引的模式，匹配上的生命周期的policy名称。\n 2. 手动创建一个对应的索引，这个索引除了要满足匹配的索引模式外，还应该是后缀是以中横线-，加上一串数字结尾的索引的名字。\n 3. 我们往别名中写入文档，当索引的文档数或索引的大小，或再者是时间达到一定条件后，新建索引。\n 4. Rollover的时候，实际上会把新索引中的别名参数is_write_index:true，而老的索引is_write_index:false。\n\n\n# Warm Phase阶段\n\n 1. 从Hot阶段allocate至Warm阶段的时候，Hot 和warm都需要提前给ES的节点打上标签。\n 2. 相应的allocate会去执行index.routing.allocation.require.*的配置。\n 3. 会将allocate后的老的索引，设置为Read-only，也就是所谓的index.blocks.read_only:true的配置。\n 4. 可以执行Force Merge\n 5. 可以执行Shrink操作。\n\n\n# Cold Phase阶段\n\n同样的需要有对应的打上cold标签的节点。\n\n同时也是会去执行index.routing.allocation.require.*的操作。\n\n\n# ILM API\n\n涉及创建policy的API和创建index template的API。\n\n\n# 创建policy的API\n\nPUT /_ilm/policy/nginx_ilm_policy\n{\n  "policy": {\n    "phases": {\n      "hot": {\n        "actions": {\n          "rollover": {\n            "max_docs": "10",\n            "max_age": "20d",\n            "max_size": "50gb"\n          }\n        }\n      },\n      "warm": {\n        "min_age": "5s",\n        "actions": {\n          "allocate": {\n            "include": {\n              "box_type": "warm"\n            },\n            "number_of_replicas": 0\n          },\n          "forcemerge": {\n            "max_num_segments": 1\n          },\n          "shrink": {\n            "number_of_shards": 1\n          }\n        }\n      },\n      "cold": {\n        "min_age": "1d",\n        "actions": {\n          "allocate": {\n            "require": {\n              "box_type": "cold"\n            }\n          }\n        }\n      },\n      "delete": {\n        "min_age": "40s",\n        "actions": {\n          "delete": {}\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n"rollover"中有三个可以触发的参数，最大的索引的文档数，最大的时间，还有就是索引最大的大小。\n\n"warm"里面的"min_age"，如果前面"hot"阶段有rollover的话，就是从rollover后开始的计算。如果前面"hot"阶段没有rollover的话，那么就是从开始创建索引的时候开始计算的(从头开始计算的)。\n\n\n# 创建index template\n\nPUT /_template/nginx_ilm_template\n{\n  "index_patterns": ["nginx_logs-*"],                 \n  "settings": {\n    "number_of_shards": 1,\n    "number_of_replicas": 0,\n    "index.lifecycle.name": "nginx_ilm_policy",      \n    "index.lifecycle.rollover_alias": "nginx_logs",\n    "index.routing.allocation.include.box_type": "hot"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n"index_patterns"是索引的匹配模式；"index.lifecycle.rollover_alias"是针对这个策略的索引的别名，这里有个问题，不同的索引的别名，需要创建不同的policy，应该是在7.9版本中的data stream中解决了这个问题。\n\n\n# 实际操作\n\n 1. 准备3节点的ES集群，每个节点的标签分别是hot/warm/cold\n\n 2. 临时设置ES中定时器的时长间隔\n    \n    PUT _cluster/settings\n    {\n      "persistent": {\n        "indices.lifecycle.poll_interval":"1s"\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. 设置一个索引7天后删除的策略和一个自定义的个性化的策略\n    \n    策略1：\n    \n    PUT /_ilm/policy/delete_after_7d_ilm_policy\n    {\n      "policy": {\n        "phases": {\n          "hot": {\n            "actions": {\n              \n            }\n          },\n          \n          "delete": {\n            "min_age": "7d",\n            "actions": {\n              "delete": {}\n            }\n          }\n        }\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n    \n    策略2：\n    \n    PUT /_ilm/policy/nginx_ilm_policy\n    {\n      "policy": {\n        "phases": {\n          "hot": {\n            "actions": {\n              "rollover": {\n                "max_docs": "10"\n              }\n            }\n          },\n          "warm": {\n            "min_age": "5s",\n            "actions": {\n              "allocate": {\n                "include": {\n                  "box_type": "warm"\n                }\n              }\n            }\n          },\n          "delete": {\n            "min_age": "40s",\n            "actions": {\n              "delete": {}\n            }\n          }\n        }\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    \n\n 4. 创建ILM的 policy策略\n    \n    PUT /_template/nginx_ilm_template\n    {\n      "index_patterns": ["nginx_logs-*"],                 \n      "settings": {\n        "number_of_shards": 1,\n        "number_of_replicas": 0,\n        "index.lifecycle.name": "nginx_ilm_policy",      \n        "index.lifecycle.rollover_alias": "nginx_logs",\n        "index.routing.allocation.include.box_type": "hot"\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n\n 5. 手动create一个索引\n    \n    DELETE nginx_logs*\n    GET _alias/nginx_logs\n    PUT nginx_logs-000001\n    {\n      "aliases": {\n        "nginx_logs": {\n          "is_write_index":true\n        }\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n\n 6. 往索引别名中写入超过10个文档数据\n    \n    POST nginx_logs/_doc\n    {\n      "name":"abbc"\n    }\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 7. 观察索引的各个阶段的状态\n    \n    GET _cat/indices/nginx_logs*?v\n    \n    \n    1\n    \n\n\n# Logstash导入ES\n\n\n# Mysql中全量导入ES\n\n 1. 工具准备\n    \n    logstash的版本必须和es的版本是一致的，另外还需要连接mysql的jar包。\n    \n    logstash的JVM配置成30G, ES的JVM配置成30G。\n\n 2. 新增同步的conf文件\n    \n    input {\n            jdbc {\n                    #驱动包路径\n                    jdbc_driver_library => "/app/docker/logstash-7.8.1/logstash-core/lib/jars/mysql-connector-java-5.1.40.jar"\n                    #驱动类\n                    jdbc_driver_class => "com.mysql.jdbc.Driver"\n                    #jdbc url\n                    jdbc_connection_string => "jdbc:mysql://192.168.1.100:3389/test1"\n                    jdbc_user => "xxx"\n                    jdbc_password => "xxx"\n                    #schedule => "* * * * *"\n                    statement => "select * from table1"\n                    #增量标识字段名\n                    tracking_column => "table_ID"\n                    #是否使用字段值作为增量标识\n                    use_column_value => true\n                    #源表字段名导入ES后是否忽略大小写\n                    lowercase_column_names=> false\n                    #分页\n                    jdbc_paging_enabled => "true"\n                    #每页数据量\n                    jdbc_page_size => "500000"\n                    #默认时区\n                    jdbc_default_timezone => "UTC"\n            }\n    }\n    output{\n            elasticsearch {\n                    hosts =>["http://192.168.1.200:9200"]\n                    index => "mysql_t0_es"\n                    document_id => "%{table_ID}"\n                    user => "xxxx"\n                    password => "xxxx"\n            }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    \n\n 3. 启动logstash并查看相应的同步日志\n    \n    ./logstash -f ../config/oracle2es.conf >> log   -w 3 &\n    \n    \n    1\n    \n\n\n# Oracle中全量导入ES\n\n 1. 工具准备\n    \n    logstash的版本必须和es的版本是一致的，另外还需要连接mysql的jar包。logstash的JVM配置成30G, ES的JVM配置成30G。\n\n 2. 新增同步的conf文件\n    \n    input {\n            jdbc {\n                    #驱动包路径\n                    jdbc_driver_library => "/app/ctgcloud/logstash-7.2.1/logstash-core/lib/jars/ojdbc6-11.2.0.3.jar"\n                    #驱动类\n                    jdbc_driver_class => "Java::oracle.jdbc.driver.OracleDriver"\n                    #jdbc url\n                    jdbc_connection_string => "jdbc:oracle:thin:@192.168.2.100:1521/testdb"\n                    jdbc_user => "xxxx"\n                    jdbc_password => "xxxx"\n                    #schedule => "* * * * *"\n                    statement => "select * from table2"\n                    #增量标识字段名\n                    tracking_column => "table2_ID"\n                    #是否使用字段值作为增量标识\n                    use_column_value => true\n                    #源表字段名导入ES后是否忽略大小写\n                    lowercase_column_names=> false\n                    #分页\n                    jdbc_paging_enabled => "true"\n                    #每页数据量\n                    jdbc_page_size => "500000"\n                    #默认时区\n                    jdbc_default_timezone => "UTC"\n            }\n    }\n    output{\n            elasticsearch {\n                    hosts =>["http://192.168.1.200:9200"]\n                    index => "table2_es_oracle2"\n                    document_id => "%{table2_ID}"\n                    user => "xxxx"\n                    password => "xxx"\n            }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    \n\n 3. 启动logstash并查看相应的同步日志\n    \n    ./logstash -f ../config/oracle2es.conf >> log   -w 3 &\n    \n    \n    1\n    \n\n\n# ES环境的优化\n\n修改"refresh_interval"，暂时设置副本分片为0，修改translog的配置，修改ES的线程池和队列。\n\nPUT table2_es_oracle2/_settings\n{\n "refresh_interval": "-1" ,\n "number_of_replicas":0,\n "translog": {\n        "flush_threshold_size": "51200mb",         \n        "durability": "async"\n      }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n参考URL：\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html\n\nhttps://cloud.tencent.com/developer/article/1696747?fromSource=gwzcw.1293314.1293314.1293314&cps_key=ad1dd5b36e1c498308f7302ab4cdabb7\n\nhttps://t.zsxq.com/BQRvfea\n\n单次bulk限制在10M左右，ES目标索引的mapping规则，最好根据实际业务定义模板，ES自定义的有一半机率影响性能。\n\n> ● 计算资源评估：2C8G 的配置大概能支持 5k doc/s 的写入，32C64G 的配置大概能支撑 5w doc/s的写入能力；\n\n\n# ES中多表关联如何解决\n\n整个文档，摘录来自"铭毅天下"的博客。原始的地址如下：\n\n> Elasticsearh多表关联设计\n\n\n# 引入问题\n\n多表关联通常是指：1对多，或者多对多关系在ES中的呈现。\n\n比如：博客和评论的关系，用户和爱好的关系，主表和子表的关系。\n\n\n\n\n# 基础认知\n\n# 关系型数据库\n\n关系型数据库是专门为关系设计的，有如下特点：\n\n * 可以通过主键唯一地标识每个实体(如Mysql中的行)。\n * 实体规范化。唯一实体的数据只存储一次，而相关实体只存储它的主键。只能在一个具体位置修改这个实体的数据。\n * 实体可以进行关联查询，可以跨实体搜索。\n * 支持AICD特性，即：单个实体的变化是原子的、一致的、隔离的和持久的。\n * 大多数关系型数据库支持跨多个实体的ACID事务。\n\n关系型数据库的缺陷：\n\n * 第一：全文检索有限的支持能力。这点，postgresql已部分支持，但相对有限。\n * 第二：多表关联查询的耗时很长，甚至不可用。之前系统开发中使用过Mysql8个表做关联查询，一次查询等待十分钟以上，甚至不可用。\n\n# Elasticsearch\n\nES和大多数NoSQL数据库类似，是扁平化的。索引是独立文档的集合体。文档是否匹配搜索请求取决于它是否包含所有的所需信息和关联程度。\n\nES中单个文档的数据变更是满足ACID的，但是如果涉及多个文档时的删除，修改时，则不支持事务。当一个事务中的部分文档更新失败的时候，是无法将所有涉及到的事务内的文档更新操作都回滚到之前状态的。\n\n扁平化有如下的优势：\n\n * 索引过程是快速和无锁的。\n * 搜索过程是快速和无锁的。\n * 因为每个文档相互都是独立的，大规模数据可以在多个节点上进行分布。\n\n# Mysql VS Elasticsearch\n\nmysql擅长关系管理，而ES擅长的是检索。\n\nMedcl也曾强调："如果可能，尽量在设计时使用扁平的文档模型。"ES的关联存储、检索、聚合操作势必会有非常大的性能开销。\n\n\n\n\n# ES场景中如何解决这种关联关系\n\n关联关系仍然非常重要。某些时候，我们需要缩小扁平化和现实世界关系模型的差异。有如下的四种常用的方法，用来在ES中进行关联数据的管理。\n\n# 应用端关联\n\n这是普遍使用的技术，即在应用接口层面来处理关联关系。\n\n针对上面的问题，来如下实践：\n\n1. 存储层面：独立两个索引存储\n2. 实际业务层面分两次请求：\n第一次查询返回：Top5中文姓名和成绩；\n根据第一次查询的结果中，取出中文姓名。\n到第二个表中进行查询，返回对应的Top5中文姓名和英文姓名。\n最后将第一次查询结果和第二次查询结果组合后，返回给用户。\n即：实际业务层面是进行两次查询，统一返回给用户。用户是无感知的。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n适用场景：数据量少的业务场景。\n\n优点：数据量少时，用户体验好。\n\n缺点：数据量大，两次查询耗时肯定会比较长，影响用户体验。\n\n引申场景：关系型数据库和ES结合，各取所长。将关系型数据库全量同步到ES存储，不做冗余处理。\n\n如前所述：ES擅长的是检索，而MySQL才擅长关系管理。所以可以考虑二者结合，使用ES多索引建立相同的别名，针对别名检索到对应ID后再回MySQL查询，业务层面通过关联ID join 出需要的数据。\n\n# 宽表冗余存储\n\n对每个文档保持一定数量的冗余数据可以在需要访问时避免进行关联。\n\n这点通过logstash同步关联数据到ES时，通常会建议：先通过视图对MySQL数据做好多表关联，然后同步视图数据到ES。此处的视图就是宽表。\n\n针对最开始提出的问题：姓名、英文名、成绩两张表合为一张表存储。\n\n适用场景：一对多或者多对多关联。\n\n优点：速度快。因为每个文档都包含了所需的所有信息，当这些信息需要在查询进行匹配时，并不需要进行昂贵的关联操作。\n\n缺点：索引更新或删除数据，应用程序不得不处理宽表的冗余数据；由于冗余数据，导致某些搜索和聚合操作可能无法按照预期工作。\n\n# 嵌套文档nested存储\n\nNested类型是ES mapping定义的集合类型之一，它解决了原有object类型扁平化的字段属性，导致查询错误的问题，是支持独立检索的类型。\n\n举例：有一个文档描述了一个帖子和一个包含帖子上所有评论的内部对象评论。可以借助于Nested实现。\n\n实践注意1：当使用嵌套文档时，使用通用的查询方式是无法访问到的，必须使用合适的查询方式(nested query、nested filter、nested facet等)，很多场景下，使用嵌套文档的复杂度在于索引阶段对关联关系的组织拼装。\n\n实践注意2：\n\nindex.mapping.nested_fields.limit 缺省值为50\n即：一个索引中最大允许拥有50个nested类型的数据。\nindex.mapping.mested_object.limit 缺省值是10000。\n即：1个文档中所有nested类型json对象数据的总量是10000。\n\n\n1\n2\n3\n4\n\n\n适用场景：对少量，子文档偶尔更新、查询频繁的场景。\n\n如果需要索引对象数组并保持数组中每个对象的独立性，则应使用嵌套Nested数据类型而不是对象Object数据类型。\n\n优点：nested文档可以将父子关系的两部分数据(举例：博客+评论)关联起来，可以基于nested类型做任何的查询。\n\n缺点：查询相对慢，更新子文档需要更新整篇文档。\n\n# 父子文档存储\n\n注意：6.X之前的版本的父子文档存储在相同索引的不同type中。而6.X之上的版本，单索引下已不存在多type的概念。父子文档Join的都是基于相同索引相同type实现的。\n\nJoin类型是ES mapping定义的类型之一，用于在同一索引的文档中创建父/子关系。关系部分定义文档中的一组可能关系，每个关系是父名称和子名称。\n\n适用场景：子文档数据量要明显多于父文档的数据量，存在1对多的关系；子文档更新频繁的场景。\n\n举例：1个产品和供应商之间是1对N的关联关系。\n\n当使用父子文档时，使用has_child或has_parent做父子关联查询。\n\n优点：父子文档可以独立更新。\n\n缺点：维护Join关系需要占据部分内存，查询较Nested更耗资源。\n\n\n# 小结\n\nNested Object和父子关系的这两种的区别如下：\n\n对比   NESTED OBJECT      PARENT/CHILD\n优点   文档存储在一起，因此读取性能高    父子文档可以独立更新，互不影响\n缺点   更新父或子文档时需要更新整个文档   为了维护Join的关系，需要占用部分内存读取性能较差\n场景   子文档偶尔更新，查询频繁       子文档更新频繁\n\n注意1：在ES开发实战中对于多表关联的设计要突破关系型数据库设计的思维定式。\n\n注意2：不建议在ES做Join操作，父子能实现部分功能，但是它的开销比较大，如果有可能，尽量在设计时使用扁平的文档模型。\n\n注意3：尽量将业务转化为没有关联关系的文档形式，在文档建模处多下功夫，以提升检索效率。\n\n注意4：Nested&Join父子类型，在选项时必须考虑性能问题。nested类型检索使得检索效率慢几倍，父子Join类型检索会使得检索效率慢几百倍。\n\n\n# ES有哪些应用场景\n\nES的主要应用分为两大类：\n\n * 搜索类(带上聚合)，考虑事务性，频繁更新，与现有数据库进行同步，通过ES进行查询聚合。\n * 日志类，包括日志收集，指标性收集，通过beats等工具收集到kafka等Q中，通过logstash进行转换，输送到ES中，然后通过Kibana进行展示。\n\n\n# ES与关系型数据库的抽象类比\n\nRDBMS                          ELASTICSEARCH\nrow                            document\ntable                          index\ncolumn                         filed\nschema                         mapping\n分布式MySQL设置的分片数量                setting\nSQL                            dsl\n分片路由信息保存在ZK中                   master node(表现为ES的一个JAVA进程)\n有点类似dbproxy                    Coordinating Node\nmysql的节点实例                     data node(表现为ES的一个JAVA进程)\nset集群中的master节点上的主的schema的分片   主分片(一个lucene实例)\nset集群中的slave节点上的从的schema的分片    副本分片\n\n\n# 正排索引和倒排索引\n\n这里我们主要是研究三个问题：\n\n * 正排索引和倒排索引的关系是什么？\n * 倒排索引的数据结构究竟长得什么样子？\n * 倒排索引的物理存储究竟是什么样子？\n\n\n# 倒排索引的数据结构\n\n> 原文的地址为：https://www.cnblogs.com/zlslch/p/6440114.html和https://www.jianshu.com/p/104f322de27c\n\n见其名知其意，有倒排索引，对应肯定有正向索引。\n\n正向索引(forward index)，反向索引(inverted index)，更熟悉的名字是倒排索引。\n\n\n# 为什么需要倒排索引呢?\n\n在搜索引擎中每个文件都对应一个文件ID，文件内容被表示为一系列关键词的集合(实际上在搜索引擎库中，关键词也已经转换为关键词ID)。例如"文档1"经过分词，提取了20个关键词，每个关键词都会记录它在文档中的出现次数和出现位置。\n\n得到的正向索引的结构如下：\n\n"文档1"的ID > 单词1：出现次数，出现位置列表；单词2：出现次数，出现位置列表；......\n\n"文档2"的ID > 此文档初选的关联词列表。\n\n如下图所示，我们一般也是通过key，去找vaule。\n\n\n\n当用户在主页上搜索关键词"华为手机"的时候，假设只存在正向索引(forward index)，那么就需要扫描索引库中的所有文档，找出所有包含关键词"华为手机"的文档，再根据打分模型进行打分，排出名次后呈现给用户。因为互联网上收录在搜索引擎中的文档的数目是个天文数字，这样的索引结构根本无法满足实时返回排名结果的要求。\n\n所以，搜索引擎会将正向索引重新构建为倒排索引，即把文件ID对应到关键词的映射转换为关键词到文件ID的映射(1对N转换为N对M)，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词。\n\n得到倒排索引的结构如下：\n\n"关键词1"："文档1"的ID，"文档2"的ID，.......\n\n"关键词2"：带有此关键词的文档ID列表。\n\n如下图所示，从词的关键字去找文档。\n\n\n\n\n# 什么是倒排索引呢？\n\n# 单词-文档矩阵\n\n单词-文档矩阵是表达两者之间所具有的一种包含关系的概念模型，下图展示了其含义。每列代表一个文档，每行代表一个单词，打对勾的位置代表包含关系。\n\n\n\n从纵向即文档这个维度来看，每列代表文档包含了哪些单词，比如文档1包含了词汇1和词汇4，而不包含其他单词。从横向即单词这个维度来看，每行代表了哪些文档包含了某个单词。比如对于词汇1来说，文档1和文档4中出现过单词1，而其他文档不包含词汇1。矩阵中其他的行列也可以作此种解读。\n\n搜索引擎的索引其实就是实现的"单词-文档矩阵"的具体数据结构。可以有不同的方式来实现上述概念模型，比如"倒排索引"、"签名文件"、"后缀树"等方式。但是各项实验数据表名，"倒排索引"是实现单词到文档映射关系的最佳实现方式，所以本文档主要介绍"倒排索引"的技术细节。\n\n# 倒排索引基本概念\n\n文档(Document)：一般搜索引擎的处理对象是互联网网页，而文档这个概念要更宽泛一些，代表以文本形式存在的存储对象，相比网页来说，涵盖更多种形式，比如word，PDF，HTML，XML等不同格式的文件都可以称之为文档。再比如一封邮件，一条短信，一条微博也可以称之为文档。在本书后续内容，很多情况下会使用文档来表征文本信息。\n\n文档集合(Document Collection)：由若干文档构成额集合称之为文档集合。比如海量的互联网网页或大量的电子邮件都是文档集合的具体例子。\n\n文档编号(Document ID)：在搜索引擎内部，会将文档集合内每个文档赋予一个唯一的内部编号，以此编号来作为这个文档的唯一标识，这样方便内部处理，每个文档的内部编号即称之为"文档编号"，后文有时会用DocID来便捷地代表文档编号。\n\n单词编号(Word ID)：与文档编号类似，搜索引擎内部以唯一的编号来表征某个单词，单词编号可以作为某个单词的唯一表征。\n\n倒排索引(Inverted Index)：倒排索引是实现"单词-文档矩阵"的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成："单词词典"和"倒排文件"。\n\n单词词典(Lexicon)：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向"倒排列表"的指针。\n\n倒排列表(PostingList)：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。\n\n倒排文件(Inverted File)：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称为倒排文件，倒排文件是存储倒排索引的物理文件。\n\n上述的所有的这些概念之间的关系，可以通过下面的图示比较清晰的看出来。\n\n\n\n# 倒排索引简单实例\n\n倒排索引从逻辑结果和基本思路上来讲非常简单。下面我们通过具体的实例来进行说明，使得读者能够对倒排索引有一个宏观而直接的感受。\n\n假设文档集合包含五个文档，每个文档内容如下图所示，在图中左端一栏是每个文档对应的文档编号。我们的任务就是对这个文档集合建立倒排索引。\n\n\n\n中文和英文等语言不同，单词之间没有明确分隔符号，所以首先要用分词系统关键文档自动切分为单词序列。这样每个文档就转换为由单词序列构成的数据流，为了系统后续处理方便，需要对每个不同的单词赋予唯一的单词编号，同时记录下哪些文档包含这个单词，在如此处理结束后，我们可以得到最简单的倒排索引。在下面的图示中，"单词ID"一栏记录了每个单词的单词编号，第二栏是对应的单词，第三栏即每个单词对应的倒排列表。比如单词"谷歌"，其单词编号为1，倒排列表为{1,2,3,4,5}，说明文档集合中每个文档都包含了这个单词。\n\n\n\n上面的图所示的倒排索引是最简单的，是因为这个索引系统只记载了哪些文档包含某个单词，而事实上，索引提供还可以记录除此之外的更多信息。下面的图示是一个相对复杂些的倒排索引，与上面的图的基本索引系统相比，在单词对应的倒排列表中不仅记录了文档编号，还记载了单词频率信息(TF)，即这个单词在某个文档中的出现的次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。在下面的图示的例子中，单词"创始人"的单词编号是7，对应的倒排列表内容为：(3:1)，其中的3代表文档编号为3的文档包含这个单词，数字1代表词频信息，即这个单词在3号文档中只出现过1此，其他单词对应的倒排列表所代表含义与此相同。\n\n\n\n实用的倒排索引还可以记载更多的信息，下图所示的索引系统除了记录文档编号和单词频率信息外，额外记载了两类信息，即每个单词对应的"文档频率信息"(对应下图中的第三栏)，以及在倒排列表中记录单词在某个文档出现的位置信息。\n\n\n\n"文档频率信息"代表了文档集合中有多少个文档包含某个单词，之所以要记录这个信息，其原因与单词频率信息一样，这个信息在搜索结果排序计算中是非常重要的一个因子。而单词在某个文档中出现的位置信息并非索引系统一定要记录的，在死机的索引系统里可以包含，也可以选择不包含这个信息，之所以如此，因为这个信息对于搜索系统来说并非必须的，位置信息只有在支持"短语查询"的时候才能排上用场。\n\n以单词"拉斯"为例，其单词编号为8，文档频率为2，代表整个文档集合中有两个文档包含这个单词，对应的倒排列表为：{(3;1;<4>)},(5;1;<4>)}，其函数以为在文档3和文档5出现过这个单词，单词频率都为1，单词"拉斯"在两个文档中的出现未知都是4，即文档中第四个单词是"拉斯"。\n\n上图所示的倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此，区别无非是采取哪些具体的数据结构来实现上述逻辑结构。\n\n有了这个索引系统，搜索引擎可以很方便地响应用户的请求，比如用户输入查询此"Facebook"，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息既可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程。\n\n# 单词词典\n\n单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里查询，就能够获得相应的倒排列表，并以此作为后续排序的基础。\n\n对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。\n\n# 哈希加链表\n\n下图就是这种词典结构的示意图。这种词典结构主要由两个部分构成：\n\n主体部分是哈希表，每个哈希表项保存一个指针，指针指向冲突链表，在冲突链表里，相同哈希值的单词形成链表结构。之所以会有冲突链表，是因为两个单词获得相同的哈希值，如果是这样，在哈希方法里被称作是一次冲突，可以将相同哈希值的单词存储在链表里，以供后续查找。\n\n\n\n在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档的时候，对于某个在文档中出现的单词T，首先利用哈希函数获得其哈希值，之后根据哈希值对应的哈希表项读取其中保存的指针，就找到了对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解析的文档里已经出现过。如果在冲突链表里没有发现这个单词，说明单词是首次碰到，则将其加入冲突链表里。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就建立起来了。\n\n在响应用户查询请求时，其过程与建立词典类似，不同点在于即使词典里没出现过某个单词，也不会添加到词典内。以上图为例，假设用户输入的查询请求为单词3，对这个单词进行哈希，定位到哈希表内的2号槽，从其保留的指针可以获得冲突链表，依次将单词3和冲突链表内的单词比较，发现单词3在冲突链表内，于是找到这个单词，之后可以读出这个单词对应的倒排列表来进行后续的工作，如果没有找到这个单词，说明文档集合内没有任何文档包含单词，则搜索结果为空。\n\n# 树形结构\n\nB树(或者B+树)是另外一种高效查找结构，下图是一个B树结构示意图。B树与哈希方式查找不同，需要字典项能够按照大小排序(数字或字符序)，而哈希方式则无需数据满足此项要求。\n\nB树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到根据词典项比较大小进行导航的作用，最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。\n\n\n\n\n# 倒排索引的总结\n\n第一个图示就是5篇文档。\n\n\n\n第二个图示就是类似的Posting List\n\n\n\n单词ID：记录每个单词的单词编号。\n\n单词：对应的单词。\n\n文档频率：代表文档集合中有多少个文档包含某个单词。\n\n倒排列表：包含单词ID及其他必要信息。\n\nDocID：单词出现的文档ID。\n\nTF：单词在某个文档中出现的次数。\n\nPOS：单词在文档中出现的位置。\n\n以单词"加盟"为例，其单词编号为6，文档频率为3，代表整个文档集合中有三个文档包含这个单词，对应的倒排列表{(2;1;<4>),(3;1;<7>),(5;1;<5>)}，含义是在文档2,3,5出现过这个单词，在每个文档的出现过1次，单词"加盟"在第一个文档的POS为4，即文档的第四个单词是"加盟"，其他类似。\n\n\n# ES Top10监控指标\n\n\n# 监控ES集群的重要性\n\nES具有通用性，可扩展性和实用性的特点，集群的基础架构必须满足如上特性。合理的集群架构能支撑其数据存储及并发响应需求。相反，不合理的集群基础架构和错误配置可能导致集群性能下降、集群无法响应甚至集群崩溃。\n\n适当地监视集群可以帮助你实时监控集群规模，并且可以有效地处理所有数据请求。\n\n本文将从五个不同的维度来看待集群，并从这些维度中提炼出监控的关键指标，并探讨通过观察这些指标可以避免哪些潜在问题。\n\n\n\n\n# 集群健康维度：分片和节点\n\n集群、索引、分片、副本的定义不再阐述。分片数的多少对集群性能的影响至关重要。分片数量设置过多或过低都会引发一些问题。\n\n> 分片数量过多，则批量写入/查询请求被分隔为过多的子写入/查询，导致该索引的写入、查询拒绝率上升。\n> \n> 对于数据量较大的索引，当分片数量过小的时候，无法充分利用节点资源，造成机器资源利用率不高或不均衡，影响写入/查询的效率。\n\n通过GET _cluster/health监视群集时，可以查询集群的状态、节点数和活动分片计数的信息。还可以查看重新定位分片，初始化分片和未分配分片的计数。\n\nGET _cluster/health\n{\n  "cluster_name" : "elasticsearch",\n  "status" : "yellow",\n  "timed_out" : false,\n  "number_of_nodes" : 1,\n  "number_of_data_nodes" : 1,\n  "active_primary_shards" : 127,\n  "active_shards" : 127,\n  "relocating_shards" : 0,\n  "initializing_shards" : 0,\n  "unassigned_shards" : 120,\n  "delayed_unassigned_shards" : 0,\n  "number_of_pending_tasks" : 0,\n  "number_of_in_flight_fetch" : 0,\n  "task_max_waiting_in_queue_millis" : 0,\n  "active_shards_percent_as_number" : 51.417004048582996\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n集群运行的重要指标：\n\n * status: 集群的状态。红色：部分主分片未分配。黄色：部分副本分片未分配。绿色：所有分片分配ok。\n * nodes: 节点。包括集群中的节点总数，并包括成功和失败节点的计数。ount of activate\n * shards: 活动分片计数。集群中活动分片的数量。\n * Relocating Shards: 重定位分片。由于节点丢失而移动的分片计数。\n * Initializing Shards: 初始化分片。由于添加索引而初始化的分片计数。\n * Unassgned Shards: 未分配的分片。尚未创建或分配副本的分片计数。\n\n\n# 搜索性能维度：请求率和延迟\n\n我们可以通过测量系统处理请求的速率和每个请求的使用时间来衡量集群的有效性。\n\n当集群收到请求时，可能需要跨多个节点访问多个分片中的数据。系统处理和返回请求的速率，当前正在进行的请求数，以及请求的持续时间等核心指标是衡量集群健康重要因素。\n\n请求过程本身分为两个阶段：\n\n * 第一个是查询阶段(query phrase)，集群将请求分发到索引中的每个分片(主分片或副本分片)。\n * 第二个是获取阶段(fetch phrase)，查询结果被收集，处理并返回给用户。\n\n通过GET index_a/_stats查看对应目标索引状态。大致截取了下面的信息：\n\n      "search" : {\n        "open_contexts" : 0,\n        "query_total" : 10,\n        "query_time_in_millis" : 0,\n        "query_current" : 0,\n        "fetch_total" : 1,\n        "fetch_time_in_millis" : 0,\n        "fetch_current" : 0,\n        "scroll_total" : 5,\n        "scroll_time_in_millis" : 15850,\n        "scroll_current" : 0,\n        "suggest_total" : 0,\n        "suggest_time_in_millis" : 0,\n        "suggest_current" : 0\n      },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n请求检索性能相关的重要指标如下：\n\n * query_current: 当前正在进行的查询数。集群当前正在处理的查询计数。\n * fetch_current: 当前正在进行的fetch次数。集群中正在进行的fetch计数。\n * query_total: 查询总数。集群处理的所有查询的聚合数。\n * fetch_total: 提取总数。集群处理的所有fetch的聚合数。\n * fetch_time_in_millis: fetch 所花费的总时间。所有fetch消耗的总时间(以毫秒为单位)\n\n\n# 索引性能维度：刷新(refresh)和合并(merge)时间\n\n文档的增、删、改操作，集群需要不断更新其索引，然后在所有节点上刷新它们。所有这些都由集群负责，作为用户，除了配置refresh interval之外，我们对此过程的控制有限。\n\n增、删、改批处理操作，会形成新段(segment)并刷新到磁盘，并且由于每个段消耗资源，因此将较小的段合并为更大的段对于性能非常重要。同上类似，这由集群本身管理。\n\n监视文档的**索引速度(indexing rate)和合并时间(merge time)**有助于在开始影响集群性能之前提前识别异常和相关问题。将这些指标与每个节点的运行状况并行考虑，这些指标为系统内的潜在问题提供了重要线索，为性能优化提供重要参考。\n\n可以通过GET /_nodes/stats获取索引性能指标，并可以在节点，索引或分配级别进行汇总。\n\n "merges" : {\n          "current" : 0,\n          "current_docs" : 0,\n          "current_size_in_bytes" : 0,\n          "total" : 245,\n          "total_time_in_millis" : 58332,\n          "total_docs" : 1351279,\n          "total_size_in_bytes" : 640703378,\n          "total_stopped_time_in_millis" : 0,\n          "total_throttled_time_in_millis" : 0,\n          "total_auto_throttle_in_bytes" : 2663383040\n        },\n        "refresh" : {\n          "total" : 2955,\n          "total_time_in_millis" : 244217,\n          "listeners" : 0\n        },\n        "flush" : {\n          "total" : 127,\n          "periodic" : 0,\n          "total_time_in_millis" : 13137\n        },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n索引性能维度相关重要指标：\n\n * refresh.total: 总刷新计数。刷新总数的计数。\n * refresh.total_time_in_millis: 刷新总时间。汇总所有花在刷新的时间(以毫秒为单位进行测量)\n * merges.current_docs: 目前的合并。合并目前正在处理中。\n * merges.total_docs: 合并总数。合并总数的计数。\n * merges.total_stopped_time_in_millis: 合并花费的总时间。合并段的所有时间的聚合。\n\n\n# 节点运行状况维度：内存，磁盘和CPU指标\n\n每个节点都运行物理硬件上，需要访问系统内存，磁盘存储和CPU周期，以便管理其控制下的数据并响应对集群的请求。\n\nES是一个严重依赖内存以实现性能的系统，因此密切关注内存使用情况与每个节点的运行状况和性能相关。改进指标的相关配置更改也可能会对内存分配和使用产生负面影响，因此记住从整体上查看系统运行状况非常重要。\n\n监视节点的CPU使用情况并查找峰值有助于识别节点中的低效进程或潜在问题。CPU性能与Java虚拟机(JVM)的垃圾收集过程密切相关。\n\n磁盘高读写可能导致系统性能问题。由于访问磁盘在时间上是一个"昂贵"的过程，因此应尽可能减少磁盘IO。\n\n通过如下命令行可以实现节点级别度量指标，并反映运行它的实例或计算机的性能。\n\nGET /_cat/nodes?v&h=id,disk.total,disk.used,disk.avail,disk.used_percent,ram.current,ram.percent,ram.max,cpu\n\n\n1\n\n\n节点运行的重要指标：\n\n * disk.total: 总磁盘容量。节点主机上的总磁盘容量。\n * disk.used: 总磁盘使用量。节点主机上的磁盘使用总量。\n * avail disk: 可用磁盘空间总量。\n * disk.avail disk.used_percent: 使用的磁盘百分比。已使用的磁盘百分比。\n * ram：当前的RAM使用情况。当前内存使用量(测量单位)。\n * percent ram: RAM百分比。正在使用的内存百分比。\n * max: 最大RAM。节点主机上的内存总量。\n * CPU：中央处理器。正在使用的CPU百分比。\n\n实际业务场景中推荐使用：Elastic-HQ，cerebro监控。\n\n\n# JVM运行状况维度：堆，GC和池大小(Pool Size)\n\n作为基于Java的应用程序，ES在Java虚拟机(JVM)中运行。JVM在其"堆"分配中管理其内存，并通过garbage collection进行垃圾回收处理。\n\n如果应用程序的需求超过堆的容量，则应用程序开始强制使用连接的存储介质上的交换空间。虽然这可以防止系统崩溃，但它可能会对集群的性能造成严重破坏。监视可用堆空间以确保系统具有足够的容量对于集群的健康至关重要。\n\nJVM内存分配给不同的内存池。我们需要密切注意这些池中每个池，以确保它们得到充分利用并且没有被超限利用的风险。\n\n垃圾收集器(GC)很像物理垃圾收集服务。我们希望让它定期运行，并确保系统不会让它过载。理想情况下，GC性能视图应类似均衡波浪线大小的常规执行。尖峰和异常可以成为更深层次问题的指标。\n\n可以通过GET /_nodes/stats命令检索JVM度量标准。\n\n  "jvm" : {\n        "timestamp" : 1557582707194,\n        "uptime_in_millis" : 22970151,\n        "mem" : {\n          "heap_used_in_bytes" : 843509048,\n          "heap_used_percent" : 40,\n          "heap_committed_in_bytes" : 2077753344,\n          "heap_max_in_bytes" : 2077753344,\n          "non_heap_used_in_bytes" : 156752056,\n          "non_heap_committed_in_bytes" : 167890944,\n          "pools" : {\n            "young" : {\n              "used_in_bytes" : 415298464,\n              "max_in_bytes" : 558432256,\n              "peak_used_in_bytes" : 558432256,\n              "peak_max_in_bytes" : 558432256\n            },\n            "survivor" : {\n              "used_in_bytes" : 12172632,\n              "max_in_bytes" : 69730304,\n              "peak_used_in_bytes" : 69730304,\n              "peak_max_in_bytes" : 69730304\n            },\n            "old" : {\n              "used_in_bytes" : 416031952,\n              "max_in_bytes" : 1449590784,\n              "peak_used_in_bytes" : 416031952,\n              "peak_max_in_bytes" : 1449590784\n            }\n          }\n        },\n        "threads" : {\n          "count" : 116,\n          "peak_count" : 119\n        },\n        "gc" : {\n          "collectors" : {\n            "young" : {\n              "collection_count" : 260,\n              "collection_time_in_millis" : 3463\n            },\n            "old" : {\n              "collection_count" : 2,\n              "collection_time_in_millis" : 125\n            }\n          }\n        },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\nJVM运行的重要指标如下：\n\n * mem: 内存使用情况。堆和非堆进程和池的使用情况统计信息。\n * threads: 当前使用的线程和最大数量。\n * gc：垃圾收集。计算垃圾收集所花费的总时间。\n\n\n# ES Top10监控指标\n\n经过上面的分析，Top10监控指标如下。\n\n 1.  Cluster Health - Nodes and Shards\n 2.  Search Performance - Request Latency and\n 3.  Search Performance - Request Rate\n 4.  Indexing Performance - Merge Times\n 5.  Indexing Performance - Refresh Times\n 6.  Node Health - Memory Usage\n 7.  Node Health - Disk I/O\n 8.  Node Heatlh - CPU\n 9.  JVM Health - Heap Usage and Garbage Collection\n 10. JVM health - JVM Pool Size\n\n对于将ES作为解决方案的任何公司而言，投资全面的监控策略至关重要。有效的监控可以节省公司因非响应或无法修复的集群问题而导致的停机时间成本和经济成本。\n\n\n# seed_host/initial master区别联系\n\n\n# 腾讯大佬的灵魂9问\n\n还是没太搞懂seed_hosts和cluster.inital_master_nodes的区别。\n\n 1. seed_hosts里面一定是配置master eligible节点吗？\n 2. 还是说data节点也可以。\n 3. ES是如何发现潜在机器的？\n 4. initial_master一定是master eligible节点吗？\n 5. 集群初始启动时，这几个节点一定都要在吗？\n 6. 初始化的时候是不是可以配置一个，然后集群初始化后，再加master eligible节点也可以的呢？\n 7. 多加几个以后，把initial_master里的几个去掉是不是也可以了？\n 8. 如果一个集群当前master是7，那它的quorum是4.ES是支持慢慢去掉节点，quorum慢慢降低的吗？\n 9. 假如慢慢去掉了3个节点，原集群正常工作，那这三个节点重启后网络分区在一起了，那会不会自己形成集群呢？\n\n\n# 解答如下\n\n1-3问：seed_host是7.x之前的参数，initial master是之后的。seed推荐是master-eligible，不是一定的。\n\n4-6问：initial node需要是master eligible。第一次，新集群，需要配置。以后不需要，也不需要再改动。\n\n9问：7之前的版本有个最小master eligible控制，防止脑裂。7以后参数被废除了，看官方文档是说现在很智能选举很快，不用担心这种情况。\n\n\n# network.host/discovery.seed_hosts等区别和联系\n\n> https://wx.zsxq.com/dweb2/index/group/225224548581?from=mweb&type=detail\n\n\n# 提问：\n\n如何理解es7中下面几个配置使得属性解释。\n\nnetwork.bind_host\n\nnetwork.publish_host\n\nnetwork.host\n\ndiscovery.seed_hosts\n\ncluster.initial_master_nodes\n\n还有，network.host配置成ip和0.0.0.0有什么区别？\n\n\n# 解答：\n\n# 这几个参数是什么意思呢？\n\n先将network.host、network.publish_host、network.bind_host，这几个属性解答下。\n\nnetwork.host兼具有publish_host和bind_host两者的功能。\n\n注意：0.0.0.0的意思是：0.0.0.0是可以接受的IP地址，将绑定到所有网络接口。这里的IP建议设置为内网地址，不要设置0.0.0.0，0.0.0.0会暴露你的地址至公网，非常不安全，如果非要使用：建议加一层代理。\n\npublish_host means:"Call me on this number",是ES与其他集群机器通信的地址。\n\nbind_host means:"I\'ll answer on this number", 这是设置控制ES侦听的网络地址的接口。\n\n实际业务场景中，我们只使用了network.host，其他没有配置。可以将publish_host和bind_host设置为不同的值，并且在某些情况下非常有用。\n\n# 有啥实际应用的场景吗？\n\n具体场景：我在数据中心有一个本地网络，我运行由不同节点组成的es集群。每个机器都有两个IP地址，一个用于从外部计算机到达，另一个用于本地连接到同一网络中的其他计算机。\n\n内部ip(eth1)用于让不同的ES节点相互通信，发现等。\n\n外部ip(eth0)是我的web应用程序(在另外一个网络中)发出请求的地址。\n\n所以，在这种场景下，我们的web应用程序位于另外一个网络中，可以从bind_host地址访问ES群集，而ES使用publish_host与集群其他节点通信。\n\n# discovery.seed_hosts参数理解\n\ndiscovery.seed_hosts，在6.X/5.X中对应的名字是，discovery.zen.ping.unicast.hosts。对比：除了名称不同，解释部分一模一样。\n\nIn order to join a cluster, a node needs to konw the hostname or IP \naddress of at least some of the other nodes in the cluster\n\n\n1\n2\n\n\n如果多节点集群，discovery.seed_hosts应该配置为候选主节点。\n\n# cluster.initial_master_nodes\n\n这也是7.X的特性，区别于之前设置min_master_count候选主节点的个数。\n\n白话文：设置候选主机节点的主机名称列表。\n\n当第一次启动全新的ES 集群时，会出现一个集群引导步骤，该步骤确定在第一次选举中计票的主要合格节点集。\n\n你在生产模式下启动全新集群时，必须明确列出符合条件的节点的名称或IP地址，这些节点的投票应在第一次选举中计算。使用这个参数来设置此列表。\n\n\n# 常见大佬博客整理\n\n 1. wood大叔(Elasticsearch 中文社区)，文档地址：\n    \n    > ES中文社区\n\n 2. 彬哥(Elastic认证中国第一人，普翔科技CTO)\n    \n    > [rockybean]\n\n 3. 张超（奇安信搜索架构师，《Elasticsearch 源码解析与实战》作者）\n    \n    > easyice\n\n 4. 死磕Elasticsearch\n    \n    > 铭毅天下\n\n 5. Elastic官方技术博客\n    \n    > Elastic官方技术博客\n\n\n# 三节点(含master)集群高可用测试\n\n现有192.168.3.26/192.168.3.27/192.168.3.28，三台主机。这三台主机上都部署了es和kibana。\n\n这三台机器会承担ES的所有角色，包含master节点角色。\n\n 1. 先停26上master节点\n    \n    目前master节点是26，通过26的kibana可以正常查询节点状态和查看写入一个索引文档。\n    \n    GET _cat/nodes?v\n    GET _cat/indices?v\n    POST tes/_doc\n    {\n      "id": 111,\n      "name": "lili"\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n    \n    接下来kill掉26上的es 进程。\n    \n    从27的kibana上来看，现在集群为2个节点，27和28两个节点。\n    \n    再次执行上面的查看集群状态，写入索引数据，依旧正常。\n\n 2. 停掉27节点\n    \n    现在master节点在28上，我们先手动停止27上的ES进程。\n    \n    登录28上的kibana来查看上面的集群状态，写入索引数据测试。\n    \n    执行如下命令报错：\n    \n    GET _cat/nodes\n    GET _cat/health\n    GET _cluster/health\n    POST tes/_doc\n    {\n      "id": 111,\n      "name": "lili"\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    报错信息：\n    \n    报错：\n    {\n      "statusCode": 504,\n      "error": "Gateway Time-out",\n      "message": "Client request timeout"\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n    \n    查看28上面的ES日志报错信息如下：\n    \n     [cluster_block_exception] blocked by: [SERVICE_UNAVAILABLE/2/no master]\n    \n    \n    1\n    \n    \n    但是执行查询索引信息，虽然慢性，还是有查询返回的。\n    \n    GET kibana_sample_data_flights/_search\n    GET tes/_search\n    \n    \n    1\n    2\n    \n\n先将27节点上面的ES 进行恢复。现在整个集群都恢复正常了，现在master节点还是在28上面。接下来停28，这个原本是master节点的上面的ES实例，再次执行上面的查看集群的状态和执行写入文档信息，查看索引信息。结果和上面是一样的，可以查一些文档索引信息，但是查集群的信息报错。\n\n\n# Elasticsearch不适合做什么\n\n 1. 不支持事务。对事务要求高的金融、银行业务选项慎重。\n 2. 近实时非准实时。写入到可被检索最快1s。对实时要求高选项要注意。\n 3. 聚合是非精确的。每个分片取TopN导致结果不准确。对精确值要求高的选项需要注意。\n 4. ES数据预处理功能相对受限(尽管可以结合：ingest和logstash filter)，但实时流队列第三方平台的要么定义开发，要么借助第三方：如kafka stream实现。选型需要注意。\n 5. 多表关联不适合，选项需要注意。传统数据的多表关联操作，在ES中处理非常麻烦(尽管有宽表，nested，join等方案)。原因在于：传统数据库设计的初衷在于特定字段的关键词匹配查询；而ES倒排索引的设计更擅长全文检索。\n\n\n# Elasticsearch 25个默认值\n\nES中存在很多的默认值，这些默认值对于架构选型、开发实战、运维排查性能问题等都有很好的借鉴价值。\n\n需要分析常用的默认值的适用场景、参数、默认值大小、静态/动态参数类型、实战建议等知识点。\n\n\n# 参数类型以及静态和动态参数的区别\n\n# 参数类型\n\n参数类型分为：集群级别参数、索引级别、Mapping级别参数等。\n\n# 集群级别参数\n\n举例1：cluster.max_shards_per_node\n\n前缀是：cluster.*，修改针对集群生效。\n\n举例2：indices.query.bool.max_clause_count\n\n需要在：elasticesarch.yml配置文件中设置，重启ES生效\n\n# 索引级别参数\n\n举例：index.number_of_shards\n\n前缀是：index.*，修改针对索引生效\n\n# 区分静态参数和动态参数\n\nElasticsearch主分片数在索引创建之后，不可以修改(除非reindex)，index.number_of_shards是静态参数。\n\n但副本分片数，可以动态的借助：update-index-settings API任意调整。index.number_of_replicas是动态参数。\n\n以下内容分别从：集群层面、索引层面、映射层面、其他常用逐步展开讲解。\n\n\n# ES集群bool类型默认支持最大子句个数\n\n * 适用场景：N多子句的bool组合查询，实现类型规则过滤的功能。\n * 参数：indices.query.bool.max_clause_count。\n * 参数类型：静态参数(需要在elasticsearch.yml中设置)\n * 默认最大值：1024.\n * 限制原因：为了防止搜索子句过多而占用过多的CPU和内存，导致集群性能下降。\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/search-settings.html\n\n\n# ES集群数据节点支持默认分片数个数\n\n * 适用场景：大数据量的集群分片选项。\n * 参数：cluster.max_shards_per_node\n * 默认最大值：1000 (7.x版本后)\n * 扩展知识：超大规模集群会遇到这个问题\n\n每个节点可以存储的分片数和可用的堆内存大小成正比关系。\n\nElastic官方博客文章建议：堆内存和分片的配置比例为1:20，举例：30GB堆内存，最多可有600个分片。\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.0/misc-cluster.html#cluster-shard-limit\n\nhttps://github.com/elastic/kibana/issues/35529\n\n不合理分片可能问题：\n\n分片数量过多，写入放大，导致bulk queue打满，拒绝率上升；\n\n一定数据量后，分片数量过少，无法充分利用多节点资源，机器资源不均衡。\n\n\n# ES集群index_buffer默认比例是多少\n\n * 适用场景：堆内存中索引缓冲区用于存储新索引的文档。填满后，缓冲区中的文档将写入磁盘上的某个段。它在节点上的所有分片之间划分。\n\n * 参数：indices.memory.index_buffer_size/indices.memory.min_index_buffer_size/indices.memory.max_index_buffer_size\n\n * 参数类型：静态参数(需要在elasticsearch.yml中设置)\n\n * 默认值：\n   \n   indices.memory.index_buffer_size: 10%\n   \n   indices.memory.min_buffer_size: 48Mb\n\n * 使用建议：\n   \n   必须在集群中的每个数据节点上进行配置。\n   \n   写入优化中首选的优化参数之一，有助于提高写入性能和稳定性。\n   \n   https://www.elastic.co/guide/en/elasticsearch/reference/current/indexing-buffer.html\n\n\n# ES默认磁盘使用率85%不再支持写入数据吗？\n\n * 使用场景：基于磁盘分配分配的参数之一，控制磁盘的使用率低警戒水位线值。\n\n * 参数: cluster.routing.allocation.disk.watermark.low/high/flood_stage\n\n * 默认值：\n   \n   cluster.routing.allocation.disk.watermark.low: 85%\n   \n   cluster.routing.allocation.disk.watermark.high: 90%\n   \n   cluster.routing.allocation.disk.watermark.flood_stage: 95%\n\n * 参数类型：集群动态参数\n\n * 使用建议\n   \n   85%：禁止写入；90%：索引分片迁移到其他可用节点；95%：索引只读。\n   \n   磁盘使用率也是监控的一个核心指标之一。\n\n\n# ES集群默认的gc方式\n\n * 适用场景：写入到可搜索的最小时间间隔(单位s)\n\n * 默认参数：\n   \n   -XX:+UserConcMarkSweepGC\n   -XX:CMSInitiatingOccupancyFraction=75\n   -XX:+UserCMSInitiatingOcupancyOnly\n   \n   \n   1\n   2\n   3\n   \n\n * 使用建议：\n   \n   官方建议：\n   \n   目前，我们仍然认为CMS垃圾收集器是大多数部署的最佳选择，但是自ES6.5.0(如果在JDK 11或更高版本上运行)以来，我们现在也支持G1GC。\n   \n   https://github.com/elastic/elasticsearch/issues/44321\n   \n   配置位置：jvm.options,优化参考wood大叔建议：更改为\n   \n   -XX:+UseG1GC\n   -XX:MaxGCPauseMillis=50\n   \n   \n   1\n   2\n   \n   \n   其中-XX:MaxGCPauseMillis是控制预期的最高GC时长，默认值为200ms，如果线上业务特性对于GC停顿非常敏感，可以适当设置低一些。但是这个值如果设置过小，可能会带来比较高的cpu消耗。\n   \n   G1对于集群正常运作的情况下减轻G1停顿对服务时延的影响还是很有效的，但是如果是GC导致集群卡死，那么很有可能换G1也无法根本上解决问题。通常都是集群的数据模型或者Query需要优化。\n   \n   https://elasticsearch.cn/question/4589\n\n\n# ES索引默认主分片分配大小\n\n * 适用场景：数据存储\n\n * 参数：index.number_of_shards\n\n * 参数类型：静态参数\n\n * 默认值：1 (7.X版本，早期版本是5)；单索引最大支持分片数：1024.\n\n * 使用建议：\n   \n   只能在创建索引时设置此值。\n   \n   单索引1024个最大分片数的限制是一项安全限制，可防止因资源分配问题导致集群不稳定。\n   \n   可通过在每个节点上指定export ES_JAVA_OPTS = "-Des.index.max_number_of_shards = 128"系统属性来修改此限制。\n\n\n# ES索引默认压缩算法是什么\n\n * 适用场景：写入数据压缩\n\n * 参数：index.codec\n\n * 参数类型：静态参数\n\n * 默认值：LZ4\n\n * 使用建议：\n   \n   可以将其设置为best_compression，它使用DEFLATE以获得更高的压缩率，但代价是存储字段的性能较慢。\n   \n   不追求压缩效率，追求磁盘占用比低的用户推荐best_compression压缩\n\n\n# ES索引默认副本分片数\n\n * 适用场景：确保业务数据的高可用性\n\n * 参数：index.number_of_replicas\n\n * 参数类型：动态参数\n\n * 默认值：1\n\n * 使用建议：\n   \n   根据业务需要合理设置副本，基于数据安全性考虑，建议副本至少设置1.\n\n\n# ES索引默认的刷新频率\n\n * 适用场景：确保业务数据的高可用性\n * 参数：index.refresh_interval\n * 参数类型：动态参数\n * 默认最小值：1s\n * 使用建议：对于实时性要求不高且想优化写入的业务场景，建议根据业务实际调大刷新频率\n\n\n# ES索引terms默认最大支持的长度是什么?\n\n * 适用场景：Terms query\n * 参数：index.max_terms_count\n * 参数类型：动态参数\n * 默认最大值：65536\n * 使用建议：一般不会超过此最大值\n\n\n# ES索引默认分页返回最大条数?\n\n * 适用场景：搜索的深度翻页\n\n * 参数：index.max_result_window\n\n * 参数类型：动态参数\n\n * 默认最大值：10000\n\n * 使用建议：\n   \n   深度翻页的机制，决定了越往后越慢。除非特殊业务需求，不建议修改默认值，可以参考百度和google的实现。\n   \n   全部数据遍历推荐scroll API. 仅支持向后翻页推荐：Search After API\n\n\n# ES索引默认管道有必要设置吗？\n\n * 适用场景：索引默认写入数据环节加上ETL操作。\n\n * 参数：index.default_pipeline\n\n * 参数类型：动态参数\n\n * 默认值：自定义关东\n\n * 使用建议：\n   \n   结合实际业务需要，一些基础需要ETL的功能建议加上。\n   \n   如果不加index.default_pipeline也可以，update_by_query + 自定义pipeline结合也能实现。不过第一种方法更周全、简练。\n\n\n# ES索引Mapping默认支持最大字段数?\n\n * 使用场景：防止索引Mapping横向无限增大，导致内存泄漏等异常。\n * 参数：index.mapping.total_fields.limit\n * 参数类型：动态参数\n * 默认最大值：1000\n * 使用建议：不建议修改\n\n\n# ES索引Mapping字段默认的最大深度？\n\n * 使用场景：防止索引Mapping纵向无限增大，导致异常。\n * 参数：index.mapping.depth.limit\n * 参数类型：动态参数\n * 默认最大值：20\n * 使用建议：不建议修改\n * 计算依据：例如，如果所有字段都在根对象级别定义，则深度为1。如果有一个对象映射，则深度为2，依次类推。默认值为20。\n\n\n# ES索引Mapping nested默认支持大小？\n\n * 适用场景：nested类型选型。\n\n * 参数：\n   \n   index.mapping.nested_fields.limit\n   \n   一个索引最大支持的nested类型个数\n   \n   index.mapping.nested_objects.limit\n   \n   一个nested类型支持的最大对象数\n\n * 参数类型：动态参数\n\n * 默认值：\n   \n   index.mapping.nested_fields.limit: 50\n   \n   index.mapping.nested_objects.limit: 10000\n\n * 使用建议：\n   \n   nested的可能的性能问题不容小觑。nested本质：每个嵌套对象都被索引为一个单独的Lucene文档。如果我们为包含100个用户对象的的那个文档建立索引，则将创建101个Lucene文档。\n   \n   nested较父子文档不同之处：\n   \n   如果子文档频繁更新，建议使用父子文档。\n   \n   如果子文档不频繁更新，查询频繁建议nested类型。\n\n\n# ES索引动态Mapping条件下，匹配的字符串默认匹配的是？\n\n * 适用场景：不提前设置Mapping精准字段的场景。\n\n * 默认类型：text + keyword类型。\n\n * 实战举例如下：\n   \n   {\n     "my_index_0001" : {\n       "mappings" : {\n         "properties" : {\n           "cont" : {\n             "type" : "text",\n             "fields" : {\n               "keyword" : {\n                 "type" : "keyword",\n                 "ignore_above" : 256\n               }\n             }\n           }\n         }\n       }\n     }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   17\n   \n\n * 实际建议：建议结合业务需要，提前精准设置Mapping，并优化数据建模。\n\n\n# ES默认的评分机制是什么？\n\n * 默认值：BM 25\n\n * 除非业务需要，否则不建议修改。\n   \n   https://www.elastic.co/guide/en/elasticsearch/reference/current/similarity.html\n\n\n# ES keyword类型默认支持的字符数是多少?\n\n * ES 5.X版本以后，keyword支持的最大长度为32766个UTF-8字符，text丢字符长度没有限制。\n * 设置ignore_above后，超过给定长度后的数据将不被索引，无法通过term精确匹配检索返回结果。\n\nhttps://blog.csdn.net/laoyang360/article/details/78207980\n\n\n# 为什么说，ES默认不适用别名，不算入门ES\n\n一句话概括：别名可以零停机改造(经典技巧，无缝切换)\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/6.8/indices-aliases.html\n\n\n# ES集群节点默认属性值\n\n * 默认：候选主节点、数据节点、Ingest节点、协调节点、机器学习节点(如果付费)的角色\n * 建议：集群规模到达一定量级后，一定要独立设置专有的主节点、协调节点、数据节点。角色划分清楚。\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html\n\n\n# ES客户端请求的节点默认是？\n\n * 如果不明确指定协调节点，默认请求的节点充当协调节点的角色。\n * 每个节点都隐式地是一个协调节点。协调节点：需要具有足够的内存和CPU才能处理收集阶段。\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html\n\n\n# ES默认分词器？\n\n * 适用场景：不明确指定分词器的场景。\n\n * 默认类型：analyzer分词器。\n\n * 实战举例如下：\n   \n   POST /_analyze\n   {\n      "text": "fdfdfdf",\n      "analyzer": "standard"\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n * 实战建议：_analyze API在聚集分词问题中的作用巨大。\n\n\n# ES聚合默认UTC时间，可以修改吗？\n\n * 可以聚合时候修改，设置时区time_zone即可解决。\n\n * "+08:00"：代表东8区。\n   \n   GET my_index/_search?size=0\n   {\n     "aggs": {\n       "by_day": {\n         "date_histogram": {\n           "field":     "date",\n           "calendar_interval":  "day",\n           "time_zone": "+08:00"\n         }\n       }\n     }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   \n\n\n# ES默认堆内存大小？\n\n * 默认值：2gB，建议一定结合实际机器环境修改。\n * ES建议独立机器环境部署，不和其他进程：如logstash，hadoop，redis等共享机器资源。\n * JVM设置建议：min(31GB，机器内存的一半)\n\n\n# ES JDK 什么版本开始默认自带的？\n\n7.0版本。7.0版本之后开始默认捆绑了JDK(安装包里自带JDK)，因此我们可以不单独安装JDK。\n\n\n# 索引(动态/静态)设置参考：\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html\n\n\n# ES集群状态变成非绿怎么办？\n\nES集群处于不同的状态下，会以不同的颜色来呈现。\n\n\n# 集群状态的含义是什么\n\n红色：至少一个主分片未分配成功；\n\n黄色：至少一个副本分片未分配成功；\n\n绿色：全部主&副本都分配成功。\n\n\n# 排查实战思路\n\n# 查看集群状态\n\nGET _cluster/health\n\n# 到底哪个节点出现了红色或黄色问题呢？\n\nGET _cluster/health?level=indices\n\n如下的方式，更明快直接，来找到对应的索引。\n\nGET /_cat/indices?v&health=yellow\n\nGET /_cat/indices?v&health=red\n\n\n# 到底索引的哪个分片出现了红色或黄色问题？\n\nGET _cluster/health?level=shards\n\n\n# 到底什么原因导致了集群变成红色或黄色呢？\n\nGET _cluster/allocation/explain\n\n返回核心信息解读举例：\n\n"current_state" : "unassigned",——未分配\n  "unassigned_info" : {\n    "reason" : "INDEX_CREATED",——原因，索引创建阶段\n    "at" : "2020-01-29T07:32:39.041Z",\n    "last_allocation_status" : "no"\n  },\n\n  "explanation" : """node does not match index setting [index.routing.allocation.require] filters [box_type:"hot"]"""\n        }——根本原因，shard分片过滤类型不一致\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 扩展思考：类似"current_state":"unassigned"， 未分配还有哪些?\n\nGET _cat/shards?h=index,shard,prirep,state,unassigned.reason\n\n> 官方文档\n\n未分配状态及原因解读：\n\n 1.  INDEX_CREATED : unassigned as a result of an API creation of an index.\n 2.  Cluster_Recovered：unassigned as a result of a full cluster recover.\n 3.  Index_Reopened: unassigned as a result of opening a closed index.\n 4.  Dangling_index_imported: unassigned as a result of importing a dangling index.\n 5.  New_index_Restored: unassigned as a result of restoring into a new index.\n 6.  Existing_index_Restored: unassigned as a result of restoring into a closed index\n 7.  Replica_ADDED: unassigned as a result of explicit addition of a replica.\n 8.  Allocation_Failed: unassigned as a result of a failed allocation of the shard.\n 9.  Node_Left: unassigned as a result of the node hosting it leaving the cluster.\n 10. Reroute_cancelled: unassigned as a result of explicit cannel reroute command.\n 11. Reinitialized: when a shard moves from started back to initializing, for example, with shadow replicas.\n 12. Reallocated_Replica: A better replica location is identified and casuses the existing replica allocation to be cancelled.\n\n\n# 文档打标签\n\n作业： 写入如下数据：\n\n1、“庆俞年”更新！李国庆用章了，俞渝被“任命”新职位 \n2、当当大戏“庆俞年”上演第二集\n3、当当网创始人李国庆和俞渝之间的权力之争，在昨日如宫斗剧一般再度上演。\n4.这时候距离李国庆和妻子俞渝分居，已经过去了8个月。\n\n\n1\n2\n3\n4\n\n\n注意：写入时做匹配。\n\n包含： 庆俞年的打上“庆俞年”的tag, 包含李国庆的打上"李国庆"的tag, 包含当当的打上"当当"的tag 两个或者更多包含的tag组成数组。\n\n\n1\n\n\n举例如下:\n\n举例：1、“庆俞年”更新！李国庆用章了，俞渝被“任命”新职位 的tags为：［"庆俞年" "李国庆］ 请思考并回答您的实现？？\n\n\n1\n\n\n答案如下：\n\n可以使用ingest中的append的proessor来实现，if去判断string中是否存在该字段名，如果存在，则添加tags字段，里面是一个数组。\n\n但是，这种方式，只是适合于文档不多，不大的情况下的使用。如果文档很多的话，文档很长的话，估计会存在效率上的问题。最好的方式是，使用合适的中文分词器，结合自定义分词来实现。\n\nPOST _ingest/pipeline/_simulate\n{\n  "pipeline": {\n    "processors": [\n      {\n        "append": {\n          "field": "tags",\n          "value": "庆俞年",\n          "if": "ctx.text.contains(\'庆俞年\')"\n        }\n      },\n      {\n        "append": {\n          "field": "tags",\n          "value": "李国庆",\n          "if": "ctx.text.contains(\'李国庆\')"\n        }\n      },\n      {\n        "append": {\n          "field": "tags",\n          "value": "当当",\n          "if": "ctx.text.contains(\'当当\')"\n        }\n      },\n      {\n        "set": {\n          "field": "tags",\n          "value": "{tags.0}",\n          "if": "ctx.tags != null && ctx.tags instanceof List && ctx.tags.length == 1"\n        }\n      }\n    ]\n  },\n  "docs": [\n    {\n      "_source": {\n        "text": "当当网创始人李国庆和俞渝之间的权力之争，在昨日如宫斗剧一般再度上演."\n      }\n    }]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n那个set 的processor的目的，只是在于，将只有一个字段匹配上的时候，这个新增的字段，是一个text类型，而不是数组类型。\n\n其中ctx.text.contains()方法，是painless中可以使用的java 方法。\n\n> https://www.elastic.co/guide/en/elasticsearch/painless/7.2/painless-api-reference-shared.html\n\n\n# 索引的分片规划\n\n\n# 球友提问\n\n"在做集群规划的时候，大都会提到分片数，分片大小等参数。请问除了数据量的大小，数据条数方面有没有最佳实践？一个分片，一个索引，一个节点，可以塞下多少条数据？"\n\n"还有假如数据量已知的情况下，要分索引存储，那是索引数多一些好，还是每个索引的分片多一些好？例如，现在根据每个分片可以承载的数据大小，划分出需要1000个分片，是一个索引1000分片，还是10个索引，每个索引100个分片合适呢？"\n\n\n# 星主回复\n\n需要注意的是，单个分片支持的文档的个数是2的32次幂减1，大于42亿多。这只是理论值，实际上使用中，会远远低于这个值的。这点非常重要，这是lucene底层的索引机制所决定的。\n\n通常为了照顾到各个节点的数据分布均匀，规划的主分片的数量是数据节点的整数倍。我看有的书上提到的是1.5~3倍。实际业务场景中，基本是整数1倍。举个例子，5个数据节点，那就是5个分片。\n\n> https://blog.csdn.net/laoyang360/article/details/78080602\n\n\n# ES磁盘空间满\n\n\n# 故障现象\n\n 1. kibana中查询对应的ES日志，没有更新，只有早上3点有数据。\n\n 2. 查看相应的应用中，发现有索引read-only的错误。\n\n\n# 故障分析\n\n联想到当索引处于read-only状态时，肯定是ES本身将索引处于保护的状态了，很有可能是这个时候磁盘的空间不足了。\n\n接下来查看ES集群中，各个节点的磁盘空间大小。\n\n\n# 故障处理\n\n 1. 在kibana中删除过期的索引。\n\n 2. 将ES节点所有的索引设置中的"read_only_alow_delete"设置为"false"\n    \n    > 参考如下URL https://www.jianshu.com/p/90cec7c6523f\n\n\n# 总结\n\n 1. 在ES集群中查看各个节点的磁盘大小，使用率的情况。\n    \n    GET _cat/allocation?v&pretty\n    \n    \n    1\n    \n\n 2. 在ES集群中查看节点的详细配置信息。\n    \n    使用如下的DSL，可以查看到ES节点的几乎所有配置，各种参数/路径配置等信息。\n    \n    GET _nodes\n\n 3. 后期考虑开启ILM，自动清理ES索引。\n\n\n# Reindex 高性能实践\n\n\n# 问题描述\n\nreindex索引的时候，发现非常缓慢。整体的ES的集群发现并没有太多的瓶颈。\n\n在数据量几十个G或者上百个G的index中，执行reindex的时候，有啥性能提升的配置？\n\n\n# Reindex简介\n\n5.X版本后新增Reindex。Reindex可以直接在ES集群里面对数据进行重建，如果我们索引的mapping因为修改而需要重建，又或者索引setting修改，需要重建的时候，借助reindex可以很方便的异步进行重建，并且支持持跨集群间的数据迁移。\n\n比如按天创建的索引可以定期重建合并到以月为单位的索引里面去。当然索引里面要启用_source 。\n\nPOST _reindex\n{\n  "source": {\n    "index": "twitter"\n  },\n  "dest": {\n    "index": "new_twitter"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 原因分析\n\nreindex的核心是做跨索引、跨集群的数据迁移。\n\n慢的原因及优化思路无非包括：\n\n * 批量大小值可能太小。例如需要结合堆内存、线程池调整大小。\n * reindex的底层是scroll实现，借助scroll并行优化方式，提升效率；\n * 跨索引、跨集群的核心是写入数据，考虑写入优化角度提升效率。\n\n\n# Reindex提升迁移效率的方案\n\n# 提升批量写入大小值\n\n默认情况下，_reindex使用1000进行批量操作，我们可以在source中调整batch_size。\n\nPOST _reindex\n{\n  "source": {\n    "index": "source",\n    "size": 5000\n  },\n  "dest": {\n    "index": "dest",\n    "routing": "=cat"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n批量大小设置的依据：\n\n 1. 使用批量索引请求以获得最佳性能\n    \n    批量大小取决于数据、分析和集群配置，但一个好的起点是每批处理5-15MB。\n    \n    注意，这是物理大小。文档数量不是度量批量大小的好指标。例如，如果每批索引1000个文档。\n    \n    * 每个1kb的1000个文档是1mb.\n    * 每个100kb的1000个文档是100MB.\n    \n    这些是完全不同的体积大小。\n\n 2. 逐渐递增文档容量大小的方式调试\n    \n    * 从大约5-15MB的大容量开始，慢慢增加，直到看不到性能的提升。然后开始增加批量写入的并发性(多线程等)。\n    * 使用kibana、cerebro或iostat、top和ps等工具监视节点，以查看资源何时开始出现瓶颈。如果我们开始接收到EsRejectedExecutionException，那么这个时候我们的集群就不能再跟上了，或者说至少有一个资源达到了瓶颈。要么减少并发性，或者提供更多有限的资源(例如从机械硬盘切换到SSD固态硬盘)，要么添加更多节点。\n\n\n# 借助scroll的sliced提升写入效率\n\nreindex支持sliced scroll以并行化重建索引过程。这种并行化可以提高效率，并提供一种方便的方法将请求分解为更小的部分。\n\n# sliced原理\n\n 1. 用过scroll接口吧，很慢？如果你数据量很大，用scroll遍历数据那确实是接受不了，现在scroll接口可以并发来进行数据遍历了。\n 2. 每个scroll请求，可以分成多个slice请求，可以理解为切片，各slice独立并行，利用scroll重建或者遍历要快很多倍。\n\n# slicing使用举例\n\nslicing的设定分为两种方式：手动设置分片、自动设置分片。\n\nPOST _reindex?slices=5&refresh\n{\n  "source": {\n    "index": "twitter"\n  },\n  "dest": {\n    "index": "new_twitter"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nslices大小设置注意事项：\n\n 1. slices大小的设置可以手动指定，或者设置slices为auto，auto的含义是：针对单索引，slices大小=分片数；针对多索引，slices = 分片的最小值。\n 2. 当slices的数量等于索引中的分片数量时，查询性能最高效。slices大小大于分片数，非但不会提升效率，反而会增加开销。\n 3. 如果这个slices数字很大(例如500)，建议选择一个较低的数字，因为过大的slices会影响性能。\n\n\n# ES副本数设置为0\n\n如果要进行大量批量导入，请考虑通过设置index.number_of_replicas来禁用副本：0。\n\n主要原因在于：复制文档时，将整个文档发送到副本节点，并逐字重复索引过程。这意味着每个副本都将执行分析，索引和潜在合并过程。\n\n相反，如果你使用零副本进行索引，然后在提取完成时启用副本，则恢复过程本质上是逐字节的网络传输。这比复制索引过程更有效。\n\nPUT /my_logs/_settings\n{\n    "number_of_replicas": 1\n}\n\n\n1\n2\n3\n4\n\n\n\n# 增加refresh间隔\n\n如果我们的搜索结果不需要接近实时的准确性，可以考虑先不急于索引刷新refresh。可以将每个索引的refresh_interval到30s。\n\nPUT /my_logs/_settings\n{ "refresh_interval": -1 }\n\n\n1\n2\n\n\n\n# 设置task\n\n可以设置task，后台去执行。\n\nPOST _reindex?slices=auto&wait_for_completion=false\n{\n  "source":{\n    "index":"pn_num_use_rec_es_hn",\n    "type":"_doc",\n    "size": 10000\n  },\n  "dest":{\n    "index":"sr_pn_app_11_mkt_res_num_use_rec",\n    "type":"mkt_res_num_use_rec"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看task的任务执行情况：\n\nGET _tasks?detailed=true&actions=*reindex\n\n\n1\n\n\n停止cancel掉task:\n\nPOST _tasks/mwkNLS9LTn-2wHt0Oy1EwQ:12512020/_cancel\n\n\n1\n\n\n\n# 小结\n\n这样调整以后，会比默认设置reindex速度提升10倍+。\n\n类似这种问题，多从官网、原理甚至源码角度思考。\n\n参考：\n\n> https://www.elastic.co/guide/en/elasticsearch/guide/master/indexing-performance.html\n> \n> https://www.elastic.co/guide/en/elasticsearch/reference/7.2/docs-reindex.html\n> \n> https://www.elastic.co/guide/en/elasticsearch/reference/7.2/tasks.html\n> \n> https://blog.csdn.net/laoyang360/article/details/81589459\n\n\n# 修改未分片，重新分片\n\nhttps://blog.csdn.net/qq_21383435/article/details/109136021\n\n\n# 删除索引数据，不删除模型\n\nhttps://cloud.tencent.com/developer/article/1737025\n\n\n# FRA\n\n\n# filebeat和logstash有什么区别联系\n\n\n\n\n1\n\n\n\n# ES的堆内存该设置多大，为什么?\n\n宿主机内存的一半和31GB，两个值中，取最小值。\n\n\n\n\n# 什么是堆内存\n\nJava中的堆是JVM所管理的最大的一块内存空间，主要用于存放各种类的实例对象。在Java中，堆被划分成两个不同的区域：新生代(Young)、老年代(Old)。\n\n新生代(Young)又被划分为三个区域：Eden、From Survivor、To Survivor。\n\n这样划分的目的是为了使JVM能够更好的管理堆内存中的对象，包括内存的分片以及回收。\n\n\n# 堆内存的作用是什么？\n\n在虚拟机启动时创建。\n\n堆内存的唯一目的就是创建对象实例，所有的对象实例和数组都要在堆上分配。\n\n堆是由垃圾回收来负责的，因此也叫做"GC堆"，垃圾回收采用分代算法，堆由此分为新生代和老生代。\n\n堆的优势是可以动态地分配内存大小，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，Java的垃圾收集器会自动收走这些不再使用的数据。\n\n但缺点是，由于要在运行时动态分配内存，存取速度较慢。当堆内存因为满了无法扩展的时候就会抛出java.lang.OutOfMemoryError:Java heap space异常。出现这种情况的解决办法具体参见java调优。\n\n\n# 堆内存如何配置？\n\n默认情况下，ES JVM使用堆内存最小和最大大小为2GB(5.X版本以上)。\n\n早期版本默认1GB，官网指出：这明显不够。\n\n在转移到生产环境时，配置足够容量的堆大小以确保ES功能和性能是必要的。\n\nES将通过Xms(最小堆大小)和Xmx(最大堆大小)设置来分配jvm.options中指定的整个堆。\n\n\n# 堆内存配置建议\n\n * 将最小堆大小(Xms)和最大堆大小(Xmx)设置为彼此相等。\n * ES可用的堆越多，可用于缓存的内存就越多。但请注意，太多的堆内存可能会使得我们长时间垃圾收集暂停。\n * 将Xmx设置为不超过物理内存的50%，以确保有足够的物理内存留给内核文件系统缓存。\n * 不要将Xmx设置为JVM超过32GB\n * 一般取宿主机内存大小的一半和31GB中的，两个值的最小值。\n\n\n# 堆内存为什么不能超过物理机内存的一半？\n\n堆内存对于ES绝对重要。它被许多内存数据结构用来提供快速操作。但还有另外一个非常重要的内存使用者：Lucene.\n\nLucene旨在利用底层操作系统来缓存内存中的数据结构。Lucene段(segment)存储在单个文件中。因为段是一成不变的，所以这些文件永远不会改变。这使得它们非常容易缓存，并且底层操作系统将愉快地将热段(hot segments)保留在内存中以便更快地访问。这些段包括倒排索引(用于全文搜索)和文档值(用于聚合)。\n\nLucene的性能依赖于与操作系统的这种交互。但是如果我们把所有可用的内存都给了ES的堆内存，那么Lucene就不会有任何剩余的内存。这会严重影响性能。\n\n标准建议是将可用内存的50%提供给ES堆，而将其他50%空闲。它不会被闲置，Lucene会高兴地吞噬掉剩下的东西。\n\n如果我们不需要在字段上做聚合操作(例如，我们不需要fielddata)，则可以考虑进一步降低堆。堆越小，我们可以从ES(更快的GC)和Lucene(更多内存缓存)中获得更好的性能。\n\n\n# 堆内存为什么不能超过32GB?\n\n在Java中，所有对象都分配在堆上并由指针引用。普通的对象指针(OOP)指向这些对象，传统上它们是CPU本地字的大小：32位或64位，取决于处理器。\n\n对于32位系统，这意味着最大堆大小为4GB。对于64位系统，堆大小可能会变得更大，但是64位指针的开销意味着仅仅因为指针较大而存在更多的浪费空间。并且比浪费的空间更糟糕的是，当在主存储器和各种缓存(LLC,L1等等)之间移动值时，较大的指针消耗更多的带宽。\n\nJava使用称为压缩OOPS的技巧来解决这个问题。而不是指向内存中的确切字节位置，而是表示指针引用对象偏移量。这意味着一个32位指针可以引用40亿个对象，而不是40亿个字节。最终，这意味着堆可以增长到约32GB的物理尺寸，同时仍然使用32位指针。\n\n一旦穿越了这个神奇的32GB的边界，指针就会切换回普通的对象指针。每个指针的大小增加，使用更多的CPU内存带宽，并且实际上会丢失内存。实际上，在使用压缩OOPS获得32GB以下堆的相同有效内存之前，需要大约40-50GB的分配堆。\n\n以上小结为：即使我们有足够的内存空间，尽量避免跨越32GB的堆边界。\n\n否则会导致浪费了内存，降低了CPU的性能，并使GC在大堆中挣扎。\n\n\n# 我是内存土豪怎么办？\n\n首先，我们建议避免使用这种大型机器。\n\n但是，如果已经有了这些机器，我们有三种实用的选择：\n\n 1. 我们是否主要进行全文检索？\n    \n    考虑给ES提供4-32GB，并让Lucene通过操作系统文件系统缓存使用剩余的内存。所有内存都会缓存段，并导致快速全文检索。\n\n 2. 我们是否做很多排序/聚合？\n    \n    大部分聚合数字、日期、地理位置和not_analyzed字符串？那么这个时候，我们的聚合将在内存缓存的文档值上完成。\n    \n    分配4-32GB的内存给ES堆内存，剩下的让操作系统在内存中缓存doc值。\n\n 3. 我们是否对分析过的字符串进行很多排序/聚合？(例如对于字标记或SigTerms等)？\n    \n    * 这种情况下，我们需要使用fielddata，这意味着我们需要堆空间。\n    * 考虑在一台机器上运行两个或多个节点，而不是只有一个节点，配置数量巨大的RAM。\n    * 尽管如此，仍然坚持50%的规则。\n    \n    小节：\n    \n    * 如果我们的机器具有128GB的RAM，请运行两个节点，每个节点的容量低于32GB。这意味着小于64GB将用于堆，而Lucene将剩余64GB以上。\n    * 如果我们选择此选项，需要在我们的一台机器上的几个ES实例中设置 cluster.routing.allocation.same_shard.host: true。这将阻止主副本分片共享同一台物理机(因为这会消除副本高可用的好处)\n\n\n# 堆内存优化建议\n\n方式一：最好的办法是在系统上完全禁用交换分区。\n\n下面可以暂时关闭swap:\n\nsudo swapoff -a\n\n\n1\n\n\n要永久禁用它，需要编辑我们的/etc/fstab\n\n方式二：控制操作系统尝试交换内存的积极性。\n\n如果完全禁用交换不是一种选择，我们可以尝试降低swappiness。该值控制操作系统尝试交换内存的积极性。这可以防止在正常情况下交换，但仍然允许操作系统在紧急内存情况下进行交换。\n\n对于大多数Linux系统，这是使用sysctl值配置的：\n\nvm.swappiness = 1\n\n\n1\n\n\n1的swappiness优于0，因为在某些内核版本上，swappiness为0可以调用OOM杀手。\n\n方式三：mlockall允许JVM锁定其内存并防止其被操作系统交换\n\n最后，如果两种方法都不可行，则应启用mlockall文件。这允许JVM锁定其内存并防止其被操作系统交换。在我们的elasticsearch.yml中，设置如下：\n\nbootstrap.mlockall: true\n\n\n1\n\n\n\n# 最新认知\n\nwood大叔：事实上，给ES分配的内存有一个魔法上限值26GB。这样可以确保启用zero based Compressed OOPS，这样性能才是最佳的。\n\n\n# 参考URL\n\n> 铭毅天下\n> \n> JVM之压缩指针——Compressed oops\n> \n> elastic中文社区',normalizedContent:'# es常见问题列表\n\n\n# ilm的配置\n\n\n# 什么是ilm\n\nilm全名叫做index lifecycle managemenet，也就是所谓的索引生命周期管理。包含了索引的从诞生(hot)，warm节点，cold节点，直至delete。\n\n\n\n从es6.6开始引入集成，是es最佳实践的集大成者。并且以hot-warm为基础，自动化的对索引进行管理。解放使用者的心智，提升集群稳定性，达到一劳永逸的效果。\n\n图解生命周期\n\n\n\n\n# 提出问题:为什么需要ilm\n\n从es最佳实践的角度来看：\n\n * 单分片大小需要控制在50gb以内\n   * 推荐日志场景在30gb\n   * 搜索场景在10gb\n * 索引分片数为数据节点的倍数\n\n在一个数据不断增长的场景下，不管是索引的分片数，还是单个分片可能的大小，都很难进行准确的估算的。\n\n\n# 理论上如何解决这个问题\n\n我们主要需要解决的是，让shard数随着数据量的不断增长，而进行同步的增长。增长的方式有，分裂(从原有的分片中分裂出1个分片来)；还有新增分片的方式。\n\n考虑到es默认使用的对log_id进行hashcode路由的方式来分片存储的，采用新增分片的方式，比较合适。\n\n\n\n\n# es如何实现新增分片的\n\n新增的分片实际上是，新增了新的索引。新的索引和老的索引都有着相同的索引的别名，我们写数据的时候，是通过索引别名的方式来写数据的。\n\n当老的索引达到某个阀值的时候，触发es执行rollover，老的索引的别名参数设置为is_write_index:false，新的索引的别名参数设置为is_write_index:true。这样就把数据写到了新的索引上面了。\n\n\n\n具体场景如下：\n\n\n\n\n# es迁移分片到warm和cold节点\n\nilm的功能，同样可以使得某个索引的分片达到某个阀值后，将索引的分片往warm或cold节点上进行迁移。\n\nhot节点是高配的机器，可以读写索引。而warm和cold节点是低配的机器，只读索引的状态。\n\n\n# ilm的基本概念\n\npolicy: 顾名思义，就是一条索引的生命周期的策略。\n\nphase: 指的是索引的不同的生命周期的阶段，包括有hot阶段、warm阶段、cold阶段、delete阶段。\n\naction: 是指在各个阶段，有哪些可以执行的操作。\n\n如下图所示：\n\n\n\n在hot阶段，我们需要手动执行create一个索引，达到某个阀值后，会触发rollover滚动。\n\n在warm阶段，可以执行allocate分配操作，将索引的分片从hot的节点属性上迁移到warm的节点属性上；将之前的索引设置为read-only的状态；可以执行force merge的操作，手动触发合并以减少索引的每个分片中的段数，并释放已删除文档使用的空间；shrik操作是指，减少索引中的主分片数量。\n\n在cold阶段，只有一个allocate的操作。\n\n在delete阶段，直接将对应的之前的cold阶段到期的索引，直接删除掉了。\n\n\n# hot phase阶段\n\n 1. 我们需要先定义了一个index template，其中定义了索引的别名，匹配索引的模式，匹配上的生命周期的policy名称。\n 2. 手动创建一个对应的索引，这个索引除了要满足匹配的索引模式外，还应该是后缀是以中横线-，加上一串数字结尾的索引的名字。\n 3. 我们往别名中写入文档，当索引的文档数或索引的大小，或再者是时间达到一定条件后，新建索引。\n 4. rollover的时候，实际上会把新索引中的别名参数is_write_index:true，而老的索引is_write_index:false。\n\n\n# warm phase阶段\n\n 1. 从hot阶段allocate至warm阶段的时候，hot 和warm都需要提前给es的节点打上标签。\n 2. 相应的allocate会去执行index.routing.allocation.require.*的配置。\n 3. 会将allocate后的老的索引，设置为read-only，也就是所谓的index.blocks.read_only:true的配置。\n 4. 可以执行force merge\n 5. 可以执行shrink操作。\n\n\n# cold phase阶段\n\n同样的需要有对应的打上cold标签的节点。\n\n同时也是会去执行index.routing.allocation.require.*的操作。\n\n\n# ilm api\n\n涉及创建policy的api和创建index template的api。\n\n\n# 创建policy的api\n\nput /_ilm/policy/nginx_ilm_policy\n{\n  "policy": {\n    "phases": {\n      "hot": {\n        "actions": {\n          "rollover": {\n            "max_docs": "10",\n            "max_age": "20d",\n            "max_size": "50gb"\n          }\n        }\n      },\n      "warm": {\n        "min_age": "5s",\n        "actions": {\n          "allocate": {\n            "include": {\n              "box_type": "warm"\n            },\n            "number_of_replicas": 0\n          },\n          "forcemerge": {\n            "max_num_segments": 1\n          },\n          "shrink": {\n            "number_of_shards": 1\n          }\n        }\n      },\n      "cold": {\n        "min_age": "1d",\n        "actions": {\n          "allocate": {\n            "require": {\n              "box_type": "cold"\n            }\n          }\n        }\n      },\n      "delete": {\n        "min_age": "40s",\n        "actions": {\n          "delete": {}\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n"rollover"中有三个可以触发的参数，最大的索引的文档数，最大的时间，还有就是索引最大的大小。\n\n"warm"里面的"min_age"，如果前面"hot"阶段有rollover的话，就是从rollover后开始的计算。如果前面"hot"阶段没有rollover的话，那么就是从开始创建索引的时候开始计算的(从头开始计算的)。\n\n\n# 创建index template\n\nput /_template/nginx_ilm_template\n{\n  "index_patterns": ["nginx_logs-*"],                 \n  "settings": {\n    "number_of_shards": 1,\n    "number_of_replicas": 0,\n    "index.lifecycle.name": "nginx_ilm_policy",      \n    "index.lifecycle.rollover_alias": "nginx_logs",\n    "index.routing.allocation.include.box_type": "hot"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n"index_patterns"是索引的匹配模式；"index.lifecycle.rollover_alias"是针对这个策略的索引的别名，这里有个问题，不同的索引的别名，需要创建不同的policy，应该是在7.9版本中的data stream中解决了这个问题。\n\n\n# 实际操作\n\n 1. 准备3节点的es集群，每个节点的标签分别是hot/warm/cold\n\n 2. 临时设置es中定时器的时长间隔\n    \n    put _cluster/settings\n    {\n      "persistent": {\n        "indices.lifecycle.poll_interval":"1s"\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. 设置一个索引7天后删除的策略和一个自定义的个性化的策略\n    \n    策略1：\n    \n    put /_ilm/policy/delete_after_7d_ilm_policy\n    {\n      "policy": {\n        "phases": {\n          "hot": {\n            "actions": {\n              \n            }\n          },\n          \n          "delete": {\n            "min_age": "7d",\n            "actions": {\n              "delete": {}\n            }\n          }\n        }\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    \n    \n    策略2：\n    \n    put /_ilm/policy/nginx_ilm_policy\n    {\n      "policy": {\n        "phases": {\n          "hot": {\n            "actions": {\n              "rollover": {\n                "max_docs": "10"\n              }\n            }\n          },\n          "warm": {\n            "min_age": "5s",\n            "actions": {\n              "allocate": {\n                "include": {\n                  "box_type": "warm"\n                }\n              }\n            }\n          },\n          "delete": {\n            "min_age": "40s",\n            "actions": {\n              "delete": {}\n            }\n          }\n        }\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    \n\n 4. 创建ilm的 policy策略\n    \n    put /_template/nginx_ilm_template\n    {\n      "index_patterns": ["nginx_logs-*"],                 \n      "settings": {\n        "number_of_shards": 1,\n        "number_of_replicas": 0,\n        "index.lifecycle.name": "nginx_ilm_policy",      \n        "index.lifecycle.rollover_alias": "nginx_logs",\n        "index.routing.allocation.include.box_type": "hot"\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n\n 5. 手动create一个索引\n    \n    delete nginx_logs*\n    get _alias/nginx_logs\n    put nginx_logs-000001\n    {\n      "aliases": {\n        "nginx_logs": {\n          "is_write_index":true\n        }\n      }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n\n 6. 往索引别名中写入超过10个文档数据\n    \n    post nginx_logs/_doc\n    {\n      "name":"abbc"\n    }\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 7. 观察索引的各个阶段的状态\n    \n    get _cat/indices/nginx_logs*?v\n    \n    \n    1\n    \n\n\n# logstash导入es\n\n\n# mysql中全量导入es\n\n 1. 工具准备\n    \n    logstash的版本必须和es的版本是一致的，另外还需要连接mysql的jar包。\n    \n    logstash的jvm配置成30g, es的jvm配置成30g。\n\n 2. 新增同步的conf文件\n    \n    input {\n            jdbc {\n                    #驱动包路径\n                    jdbc_driver_library => "/app/docker/logstash-7.8.1/logstash-core/lib/jars/mysql-connector-java-5.1.40.jar"\n                    #驱动类\n                    jdbc_driver_class => "com.mysql.jdbc.driver"\n                    #jdbc url\n                    jdbc_connection_string => "jdbc:mysql://192.168.1.100:3389/test1"\n                    jdbc_user => "xxx"\n                    jdbc_password => "xxx"\n                    #schedule => "* * * * *"\n                    statement => "select * from table1"\n                    #增量标识字段名\n                    tracking_column => "table_id"\n                    #是否使用字段值作为增量标识\n                    use_column_value => true\n                    #源表字段名导入es后是否忽略大小写\n                    lowercase_column_names=> false\n                    #分页\n                    jdbc_paging_enabled => "true"\n                    #每页数据量\n                    jdbc_page_size => "500000"\n                    #默认时区\n                    jdbc_default_timezone => "utc"\n            }\n    }\n    output{\n            elasticsearch {\n                    hosts =>["http://192.168.1.200:9200"]\n                    index => "mysql_t0_es"\n                    document_id => "%{table_id}"\n                    user => "xxxx"\n                    password => "xxxx"\n            }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    \n\n 3. 启动logstash并查看相应的同步日志\n    \n    ./logstash -f ../config/oracle2es.conf >> log   -w 3 &\n    \n    \n    1\n    \n\n\n# oracle中全量导入es\n\n 1. 工具准备\n    \n    logstash的版本必须和es的版本是一致的，另外还需要连接mysql的jar包。logstash的jvm配置成30g, es的jvm配置成30g。\n\n 2. 新增同步的conf文件\n    \n    input {\n            jdbc {\n                    #驱动包路径\n                    jdbc_driver_library => "/app/ctgcloud/logstash-7.2.1/logstash-core/lib/jars/ojdbc6-11.2.0.3.jar"\n                    #驱动类\n                    jdbc_driver_class => "java::oracle.jdbc.driver.oracledriver"\n                    #jdbc url\n                    jdbc_connection_string => "jdbc:oracle:thin:@192.168.2.100:1521/testdb"\n                    jdbc_user => "xxxx"\n                    jdbc_password => "xxxx"\n                    #schedule => "* * * * *"\n                    statement => "select * from table2"\n                    #增量标识字段名\n                    tracking_column => "table2_id"\n                    #是否使用字段值作为增量标识\n                    use_column_value => true\n                    #源表字段名导入es后是否忽略大小写\n                    lowercase_column_names=> false\n                    #分页\n                    jdbc_paging_enabled => "true"\n                    #每页数据量\n                    jdbc_page_size => "500000"\n                    #默认时区\n                    jdbc_default_timezone => "utc"\n            }\n    }\n    output{\n            elasticsearch {\n                    hosts =>["http://192.168.1.200:9200"]\n                    index => "table2_es_oracle2"\n                    document_id => "%{table2_id}"\n                    user => "xxxx"\n                    password => "xxx"\n            }\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    23\n    24\n    25\n    26\n    27\n    28\n    29\n    30\n    31\n    32\n    33\n    34\n    35\n    \n\n 3. 启动logstash并查看相应的同步日志\n    \n    ./logstash -f ../config/oracle2es.conf >> log   -w 3 &\n    \n    \n    1\n    \n\n\n# es环境的优化\n\n修改"refresh_interval"，暂时设置副本分片为0，修改translog的配置，修改es的线程池和队列。\n\nput table2_es_oracle2/_settings\n{\n "refresh_interval": "-1" ,\n "number_of_replicas":0,\n "translog": {\n        "flush_threshold_size": "51200mb",         \n        "durability": "async"\n      }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n参考url：\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html\n\nhttps://cloud.tencent.com/developer/article/1696747?fromsource=gwzcw.1293314.1293314.1293314&cps_key=ad1dd5b36e1c498308f7302ab4cdabb7\n\nhttps://t.zsxq.com/bqrvfea\n\n单次bulk限制在10m左右，es目标索引的mapping规则，最好根据实际业务定义模板，es自定义的有一半机率影响性能。\n\n> ● 计算资源评估：2c8g 的配置大概能支持 5k doc/s 的写入，32c64g 的配置大概能支撑 5w doc/s的写入能力；\n\n\n# es中多表关联如何解决\n\n整个文档，摘录来自"铭毅天下"的博客。原始的地址如下：\n\n> elasticsearh多表关联设计\n\n\n# 引入问题\n\n多表关联通常是指：1对多，或者多对多关系在es中的呈现。\n\n比如：博客和评论的关系，用户和爱好的关系，主表和子表的关系。\n\n\n\n\n# 基础认知\n\n# 关系型数据库\n\n关系型数据库是专门为关系设计的，有如下特点：\n\n * 可以通过主键唯一地标识每个实体(如mysql中的行)。\n * 实体规范化。唯一实体的数据只存储一次，而相关实体只存储它的主键。只能在一个具体位置修改这个实体的数据。\n * 实体可以进行关联查询，可以跨实体搜索。\n * 支持aicd特性，即：单个实体的变化是原子的、一致的、隔离的和持久的。\n * 大多数关系型数据库支持跨多个实体的acid事务。\n\n关系型数据库的缺陷：\n\n * 第一：全文检索有限的支持能力。这点，postgresql已部分支持，但相对有限。\n * 第二：多表关联查询的耗时很长，甚至不可用。之前系统开发中使用过mysql8个表做关联查询，一次查询等待十分钟以上，甚至不可用。\n\n# elasticsearch\n\nes和大多数nosql数据库类似，是扁平化的。索引是独立文档的集合体。文档是否匹配搜索请求取决于它是否包含所有的所需信息和关联程度。\n\nes中单个文档的数据变更是满足acid的，但是如果涉及多个文档时的删除，修改时，则不支持事务。当一个事务中的部分文档更新失败的时候，是无法将所有涉及到的事务内的文档更新操作都回滚到之前状态的。\n\n扁平化有如下的优势：\n\n * 索引过程是快速和无锁的。\n * 搜索过程是快速和无锁的。\n * 因为每个文档相互都是独立的，大规模数据可以在多个节点上进行分布。\n\n# mysql vs elasticsearch\n\nmysql擅长关系管理，而es擅长的是检索。\n\nmedcl也曾强调："如果可能，尽量在设计时使用扁平的文档模型。"es的关联存储、检索、聚合操作势必会有非常大的性能开销。\n\n\n\n\n# es场景中如何解决这种关联关系\n\n关联关系仍然非常重要。某些时候，我们需要缩小扁平化和现实世界关系模型的差异。有如下的四种常用的方法，用来在es中进行关联数据的管理。\n\n# 应用端关联\n\n这是普遍使用的技术，即在应用接口层面来处理关联关系。\n\n针对上面的问题，来如下实践：\n\n1. 存储层面：独立两个索引存储\n2. 实际业务层面分两次请求：\n第一次查询返回：top5中文姓名和成绩；\n根据第一次查询的结果中，取出中文姓名。\n到第二个表中进行查询，返回对应的top5中文姓名和英文姓名。\n最后将第一次查询结果和第二次查询结果组合后，返回给用户。\n即：实际业务层面是进行两次查询，统一返回给用户。用户是无感知的。\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n适用场景：数据量少的业务场景。\n\n优点：数据量少时，用户体验好。\n\n缺点：数据量大，两次查询耗时肯定会比较长，影响用户体验。\n\n引申场景：关系型数据库和es结合，各取所长。将关系型数据库全量同步到es存储，不做冗余处理。\n\n如前所述：es擅长的是检索，而mysql才擅长关系管理。所以可以考虑二者结合，使用es多索引建立相同的别名，针对别名检索到对应id后再回mysql查询，业务层面通过关联id join 出需要的数据。\n\n# 宽表冗余存储\n\n对每个文档保持一定数量的冗余数据可以在需要访问时避免进行关联。\n\n这点通过logstash同步关联数据到es时，通常会建议：先通过视图对mysql数据做好多表关联，然后同步视图数据到es。此处的视图就是宽表。\n\n针对最开始提出的问题：姓名、英文名、成绩两张表合为一张表存储。\n\n适用场景：一对多或者多对多关联。\n\n优点：速度快。因为每个文档都包含了所需的所有信息，当这些信息需要在查询进行匹配时，并不需要进行昂贵的关联操作。\n\n缺点：索引更新或删除数据，应用程序不得不处理宽表的冗余数据；由于冗余数据，导致某些搜索和聚合操作可能无法按照预期工作。\n\n# 嵌套文档nested存储\n\nnested类型是es mapping定义的集合类型之一，它解决了原有object类型扁平化的字段属性，导致查询错误的问题，是支持独立检索的类型。\n\n举例：有一个文档描述了一个帖子和一个包含帖子上所有评论的内部对象评论。可以借助于nested实现。\n\n实践注意1：当使用嵌套文档时，使用通用的查询方式是无法访问到的，必须使用合适的查询方式(nested query、nested filter、nested facet等)，很多场景下，使用嵌套文档的复杂度在于索引阶段对关联关系的组织拼装。\n\n实践注意2：\n\nindex.mapping.nested_fields.limit 缺省值为50\n即：一个索引中最大允许拥有50个nested类型的数据。\nindex.mapping.mested_object.limit 缺省值是10000。\n即：1个文档中所有nested类型json对象数据的总量是10000。\n\n\n1\n2\n3\n4\n\n\n适用场景：对少量，子文档偶尔更新、查询频繁的场景。\n\n如果需要索引对象数组并保持数组中每个对象的独立性，则应使用嵌套nested数据类型而不是对象object数据类型。\n\n优点：nested文档可以将父子关系的两部分数据(举例：博客+评论)关联起来，可以基于nested类型做任何的查询。\n\n缺点：查询相对慢，更新子文档需要更新整篇文档。\n\n# 父子文档存储\n\n注意：6.x之前的版本的父子文档存储在相同索引的不同type中。而6.x之上的版本，单索引下已不存在多type的概念。父子文档join的都是基于相同索引相同type实现的。\n\njoin类型是es mapping定义的类型之一，用于在同一索引的文档中创建父/子关系。关系部分定义文档中的一组可能关系，每个关系是父名称和子名称。\n\n适用场景：子文档数据量要明显多于父文档的数据量，存在1对多的关系；子文档更新频繁的场景。\n\n举例：1个产品和供应商之间是1对n的关联关系。\n\n当使用父子文档时，使用has_child或has_parent做父子关联查询。\n\n优点：父子文档可以独立更新。\n\n缺点：维护join关系需要占据部分内存，查询较nested更耗资源。\n\n\n# 小结\n\nnested object和父子关系的这两种的区别如下：\n\n对比   nested object      parent/child\n优点   文档存储在一起，因此读取性能高    父子文档可以独立更新，互不影响\n缺点   更新父或子文档时需要更新整个文档   为了维护join的关系，需要占用部分内存读取性能较差\n场景   子文档偶尔更新，查询频繁       子文档更新频繁\n\n注意1：在es开发实战中对于多表关联的设计要突破关系型数据库设计的思维定式。\n\n注意2：不建议在es做join操作，父子能实现部分功能，但是它的开销比较大，如果有可能，尽量在设计时使用扁平的文档模型。\n\n注意3：尽量将业务转化为没有关联关系的文档形式，在文档建模处多下功夫，以提升检索效率。\n\n注意4：nested&join父子类型，在选项时必须考虑性能问题。nested类型检索使得检索效率慢几倍，父子join类型检索会使得检索效率慢几百倍。\n\n\n# es有哪些应用场景\n\nes的主要应用分为两大类：\n\n * 搜索类(带上聚合)，考虑事务性，频繁更新，与现有数据库进行同步，通过es进行查询聚合。\n * 日志类，包括日志收集，指标性收集，通过beats等工具收集到kafka等q中，通过logstash进行转换，输送到es中，然后通过kibana进行展示。\n\n\n# es与关系型数据库的抽象类比\n\nrdbms                          elasticsearch\nrow                            document\ntable                          index\ncolumn                         filed\nschema                         mapping\n分布式mysql设置的分片数量                setting\nsql                            dsl\n分片路由信息保存在zk中                   master node(表现为es的一个java进程)\n有点类似dbproxy                    coordinating node\nmysql的节点实例                     data node(表现为es的一个java进程)\nset集群中的master节点上的主的schema的分片   主分片(一个lucene实例)\nset集群中的slave节点上的从的schema的分片    副本分片\n\n\n# 正排索引和倒排索引\n\n这里我们主要是研究三个问题：\n\n * 正排索引和倒排索引的关系是什么？\n * 倒排索引的数据结构究竟长得什么样子？\n * 倒排索引的物理存储究竟是什么样子？\n\n\n# 倒排索引的数据结构\n\n> 原文的地址为：https://www.cnblogs.com/zlslch/p/6440114.html和https://www.jianshu.com/p/104f322de27c\n\n见其名知其意，有倒排索引，对应肯定有正向索引。\n\n正向索引(forward index)，反向索引(inverted index)，更熟悉的名字是倒排索引。\n\n\n# 为什么需要倒排索引呢?\n\n在搜索引擎中每个文件都对应一个文件id，文件内容被表示为一系列关键词的集合(实际上在搜索引擎库中，关键词也已经转换为关键词id)。例如"文档1"经过分词，提取了20个关键词，每个关键词都会记录它在文档中的出现次数和出现位置。\n\n得到的正向索引的结构如下：\n\n"文档1"的id > 单词1：出现次数，出现位置列表；单词2：出现次数，出现位置列表；......\n\n"文档2"的id > 此文档初选的关联词列表。\n\n如下图所示，我们一般也是通过key，去找vaule。\n\n\n\n当用户在主页上搜索关键词"华为手机"的时候，假设只存在正向索引(forward index)，那么就需要扫描索引库中的所有文档，找出所有包含关键词"华为手机"的文档，再根据打分模型进行打分，排出名次后呈现给用户。因为互联网上收录在搜索引擎中的文档的数目是个天文数字，这样的索引结构根本无法满足实时返回排名结果的要求。\n\n所以，搜索引擎会将正向索引重新构建为倒排索引，即把文件id对应到关键词的映射转换为关键词到文件id的映射(1对n转换为n对m)，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词。\n\n得到倒排索引的结构如下：\n\n"关键词1"："文档1"的id，"文档2"的id，.......\n\n"关键词2"：带有此关键词的文档id列表。\n\n如下图所示，从词的关键字去找文档。\n\n\n\n\n# 什么是倒排索引呢？\n\n# 单词-文档矩阵\n\n单词-文档矩阵是表达两者之间所具有的一种包含关系的概念模型，下图展示了其含义。每列代表一个文档，每行代表一个单词，打对勾的位置代表包含关系。\n\n\n\n从纵向即文档这个维度来看，每列代表文档包含了哪些单词，比如文档1包含了词汇1和词汇4，而不包含其他单词。从横向即单词这个维度来看，每行代表了哪些文档包含了某个单词。比如对于词汇1来说，文档1和文档4中出现过单词1，而其他文档不包含词汇1。矩阵中其他的行列也可以作此种解读。\n\n搜索引擎的索引其实就是实现的"单词-文档矩阵"的具体数据结构。可以有不同的方式来实现上述概念模型，比如"倒排索引"、"签名文件"、"后缀树"等方式。但是各项实验数据表名，"倒排索引"是实现单词到文档映射关系的最佳实现方式，所以本文档主要介绍"倒排索引"的技术细节。\n\n# 倒排索引基本概念\n\n文档(document)：一般搜索引擎的处理对象是互联网网页，而文档这个概念要更宽泛一些，代表以文本形式存在的存储对象，相比网页来说，涵盖更多种形式，比如word，pdf，html，xml等不同格式的文件都可以称之为文档。再比如一封邮件，一条短信，一条微博也可以称之为文档。在本书后续内容，很多情况下会使用文档来表征文本信息。\n\n文档集合(document collection)：由若干文档构成额集合称之为文档集合。比如海量的互联网网页或大量的电子邮件都是文档集合的具体例子。\n\n文档编号(document id)：在搜索引擎内部，会将文档集合内每个文档赋予一个唯一的内部编号，以此编号来作为这个文档的唯一标识，这样方便内部处理，每个文档的内部编号即称之为"文档编号"，后文有时会用docid来便捷地代表文档编号。\n\n单词编号(word id)：与文档编号类似，搜索引擎内部以唯一的编号来表征某个单词，单词编号可以作为某个单词的唯一表征。\n\n倒排索引(inverted index)：倒排索引是实现"单词-文档矩阵"的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成："单词词典"和"倒排文件"。\n\n单词词典(lexicon)：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向"倒排列表"的指针。\n\n倒排列表(postinglist)：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(posting)。根据倒排列表，即可获知哪些文档包含某个单词。\n\n倒排文件(inverted file)：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称为倒排文件，倒排文件是存储倒排索引的物理文件。\n\n上述的所有的这些概念之间的关系，可以通过下面的图示比较清晰的看出来。\n\n\n\n# 倒排索引简单实例\n\n倒排索引从逻辑结果和基本思路上来讲非常简单。下面我们通过具体的实例来进行说明，使得读者能够对倒排索引有一个宏观而直接的感受。\n\n假设文档集合包含五个文档，每个文档内容如下图所示，在图中左端一栏是每个文档对应的文档编号。我们的任务就是对这个文档集合建立倒排索引。\n\n\n\n中文和英文等语言不同，单词之间没有明确分隔符号，所以首先要用分词系统关键文档自动切分为单词序列。这样每个文档就转换为由单词序列构成的数据流，为了系统后续处理方便，需要对每个不同的单词赋予唯一的单词编号，同时记录下哪些文档包含这个单词，在如此处理结束后，我们可以得到最简单的倒排索引。在下面的图示中，"单词id"一栏记录了每个单词的单词编号，第二栏是对应的单词，第三栏即每个单词对应的倒排列表。比如单词"谷歌"，其单词编号为1，倒排列表为{1,2,3,4,5}，说明文档集合中每个文档都包含了这个单词。\n\n\n\n上面的图所示的倒排索引是最简单的，是因为这个索引系统只记载了哪些文档包含某个单词，而事实上，索引提供还可以记录除此之外的更多信息。下面的图示是一个相对复杂些的倒排索引，与上面的图的基本索引系统相比，在单词对应的倒排列表中不仅记录了文档编号，还记载了单词频率信息(tf)，即这个单词在某个文档中的出现的次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。在下面的图示的例子中，单词"创始人"的单词编号是7，对应的倒排列表内容为：(3:1)，其中的3代表文档编号为3的文档包含这个单词，数字1代表词频信息，即这个单词在3号文档中只出现过1此，其他单词对应的倒排列表所代表含义与此相同。\n\n\n\n实用的倒排索引还可以记载更多的信息，下图所示的索引系统除了记录文档编号和单词频率信息外，额外记载了两类信息，即每个单词对应的"文档频率信息"(对应下图中的第三栏)，以及在倒排列表中记录单词在某个文档出现的位置信息。\n\n\n\n"文档频率信息"代表了文档集合中有多少个文档包含某个单词，之所以要记录这个信息，其原因与单词频率信息一样，这个信息在搜索结果排序计算中是非常重要的一个因子。而单词在某个文档中出现的位置信息并非索引系统一定要记录的，在死机的索引系统里可以包含，也可以选择不包含这个信息，之所以如此，因为这个信息对于搜索系统来说并非必须的，位置信息只有在支持"短语查询"的时候才能排上用场。\n\n以单词"拉斯"为例，其单词编号为8，文档频率为2，代表整个文档集合中有两个文档包含这个单词，对应的倒排列表为：{(3;1;<4>)},(5;1;<4>)}，其函数以为在文档3和文档5出现过这个单词，单词频率都为1，单词"拉斯"在两个文档中的出现未知都是4，即文档中第四个单词是"拉斯"。\n\n上图所示的倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此，区别无非是采取哪些具体的数据结构来实现上述逻辑结构。\n\n有了这个索引系统，搜索引擎可以很方便地响应用户的请求，比如用户输入查询此"facebook"，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息既可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程。\n\n# 单词词典\n\n单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里查询，就能够获得相应的倒排列表，并以此作为后续排序的基础。\n\n对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。\n\n# 哈希加链表\n\n下图就是这种词典结构的示意图。这种词典结构主要由两个部分构成：\n\n主体部分是哈希表，每个哈希表项保存一个指针，指针指向冲突链表，在冲突链表里，相同哈希值的单词形成链表结构。之所以会有冲突链表，是因为两个单词获得相同的哈希值，如果是这样，在哈希方法里被称作是一次冲突，可以将相同哈希值的单词存储在链表里，以供后续查找。\n\n\n\n在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档的时候，对于某个在文档中出现的单词t，首先利用哈希函数获得其哈希值，之后根据哈希值对应的哈希表项读取其中保存的指针，就找到了对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解析的文档里已经出现过。如果在冲突链表里没有发现这个单词，说明单词是首次碰到，则将其加入冲突链表里。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就建立起来了。\n\n在响应用户查询请求时，其过程与建立词典类似，不同点在于即使词典里没出现过某个单词，也不会添加到词典内。以上图为例，假设用户输入的查询请求为单词3，对这个单词进行哈希，定位到哈希表内的2号槽，从其保留的指针可以获得冲突链表，依次将单词3和冲突链表内的单词比较，发现单词3在冲突链表内，于是找到这个单词，之后可以读出这个单词对应的倒排列表来进行后续的工作，如果没有找到这个单词，说明文档集合内没有任何文档包含单词，则搜索结果为空。\n\n# 树形结构\n\nb树(或者b+树)是另外一种高效查找结构，下图是一个b树结构示意图。b树与哈希方式查找不同，需要字典项能够按照大小排序(数字或字符序)，而哈希方式则无需数据满足此项要求。\n\nb树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到根据词典项比较大小进行导航的作用，最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。\n\n\n\n\n# 倒排索引的总结\n\n第一个图示就是5篇文档。\n\n\n\n第二个图示就是类似的posting list\n\n\n\n单词id：记录每个单词的单词编号。\n\n单词：对应的单词。\n\n文档频率：代表文档集合中有多少个文档包含某个单词。\n\n倒排列表：包含单词id及其他必要信息。\n\ndocid：单词出现的文档id。\n\ntf：单词在某个文档中出现的次数。\n\npos：单词在文档中出现的位置。\n\n以单词"加盟"为例，其单词编号为6，文档频率为3，代表整个文档集合中有三个文档包含这个单词，对应的倒排列表{(2;1;<4>),(3;1;<7>),(5;1;<5>)}，含义是在文档2,3,5出现过这个单词，在每个文档的出现过1次，单词"加盟"在第一个文档的pos为4，即文档的第四个单词是"加盟"，其他类似。\n\n\n# es top10监控指标\n\n\n# 监控es集群的重要性\n\nes具有通用性，可扩展性和实用性的特点，集群的基础架构必须满足如上特性。合理的集群架构能支撑其数据存储及并发响应需求。相反，不合理的集群基础架构和错误配置可能导致集群性能下降、集群无法响应甚至集群崩溃。\n\n适当地监视集群可以帮助你实时监控集群规模，并且可以有效地处理所有数据请求。\n\n本文将从五个不同的维度来看待集群，并从这些维度中提炼出监控的关键指标，并探讨通过观察这些指标可以避免哪些潜在问题。\n\n\n\n\n# 集群健康维度：分片和节点\n\n集群、索引、分片、副本的定义不再阐述。分片数的多少对集群性能的影响至关重要。分片数量设置过多或过低都会引发一些问题。\n\n> 分片数量过多，则批量写入/查询请求被分隔为过多的子写入/查询，导致该索引的写入、查询拒绝率上升。\n> \n> 对于数据量较大的索引，当分片数量过小的时候，无法充分利用节点资源，造成机器资源利用率不高或不均衡，影响写入/查询的效率。\n\n通过get _cluster/health监视群集时，可以查询集群的状态、节点数和活动分片计数的信息。还可以查看重新定位分片，初始化分片和未分配分片的计数。\n\nget _cluster/health\n{\n  "cluster_name" : "elasticsearch",\n  "status" : "yellow",\n  "timed_out" : false,\n  "number_of_nodes" : 1,\n  "number_of_data_nodes" : 1,\n  "active_primary_shards" : 127,\n  "active_shards" : 127,\n  "relocating_shards" : 0,\n  "initializing_shards" : 0,\n  "unassigned_shards" : 120,\n  "delayed_unassigned_shards" : 0,\n  "number_of_pending_tasks" : 0,\n  "number_of_in_flight_fetch" : 0,\n  "task_max_waiting_in_queue_millis" : 0,\n  "active_shards_percent_as_number" : 51.417004048582996\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n集群运行的重要指标：\n\n * status: 集群的状态。红色：部分主分片未分配。黄色：部分副本分片未分配。绿色：所有分片分配ok。\n * nodes: 节点。包括集群中的节点总数，并包括成功和失败节点的计数。ount of activate\n * shards: 活动分片计数。集群中活动分片的数量。\n * relocating shards: 重定位分片。由于节点丢失而移动的分片计数。\n * initializing shards: 初始化分片。由于添加索引而初始化的分片计数。\n * unassgned shards: 未分配的分片。尚未创建或分配副本的分片计数。\n\n\n# 搜索性能维度：请求率和延迟\n\n我们可以通过测量系统处理请求的速率和每个请求的使用时间来衡量集群的有效性。\n\n当集群收到请求时，可能需要跨多个节点访问多个分片中的数据。系统处理和返回请求的速率，当前正在进行的请求数，以及请求的持续时间等核心指标是衡量集群健康重要因素。\n\n请求过程本身分为两个阶段：\n\n * 第一个是查询阶段(query phrase)，集群将请求分发到索引中的每个分片(主分片或副本分片)。\n * 第二个是获取阶段(fetch phrase)，查询结果被收集，处理并返回给用户。\n\n通过get index_a/_stats查看对应目标索引状态。大致截取了下面的信息：\n\n      "search" : {\n        "open_contexts" : 0,\n        "query_total" : 10,\n        "query_time_in_millis" : 0,\n        "query_current" : 0,\n        "fetch_total" : 1,\n        "fetch_time_in_millis" : 0,\n        "fetch_current" : 0,\n        "scroll_total" : 5,\n        "scroll_time_in_millis" : 15850,\n        "scroll_current" : 0,\n        "suggest_total" : 0,\n        "suggest_time_in_millis" : 0,\n        "suggest_current" : 0\n      },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n请求检索性能相关的重要指标如下：\n\n * query_current: 当前正在进行的查询数。集群当前正在处理的查询计数。\n * fetch_current: 当前正在进行的fetch次数。集群中正在进行的fetch计数。\n * query_total: 查询总数。集群处理的所有查询的聚合数。\n * fetch_total: 提取总数。集群处理的所有fetch的聚合数。\n * fetch_time_in_millis: fetch 所花费的总时间。所有fetch消耗的总时间(以毫秒为单位)\n\n\n# 索引性能维度：刷新(refresh)和合并(merge)时间\n\n文档的增、删、改操作，集群需要不断更新其索引，然后在所有节点上刷新它们。所有这些都由集群负责，作为用户，除了配置refresh interval之外，我们对此过程的控制有限。\n\n增、删、改批处理操作，会形成新段(segment)并刷新到磁盘，并且由于每个段消耗资源，因此将较小的段合并为更大的段对于性能非常重要。同上类似，这由集群本身管理。\n\n监视文档的**索引速度(indexing rate)和合并时间(merge time)**有助于在开始影响集群性能之前提前识别异常和相关问题。将这些指标与每个节点的运行状况并行考虑，这些指标为系统内的潜在问题提供了重要线索，为性能优化提供重要参考。\n\n可以通过get /_nodes/stats获取索引性能指标，并可以在节点，索引或分配级别进行汇总。\n\n "merges" : {\n          "current" : 0,\n          "current_docs" : 0,\n          "current_size_in_bytes" : 0,\n          "total" : 245,\n          "total_time_in_millis" : 58332,\n          "total_docs" : 1351279,\n          "total_size_in_bytes" : 640703378,\n          "total_stopped_time_in_millis" : 0,\n          "total_throttled_time_in_millis" : 0,\n          "total_auto_throttle_in_bytes" : 2663383040\n        },\n        "refresh" : {\n          "total" : 2955,\n          "total_time_in_millis" : 244217,\n          "listeners" : 0\n        },\n        "flush" : {\n          "total" : 127,\n          "periodic" : 0,\n          "total_time_in_millis" : 13137\n        },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n索引性能维度相关重要指标：\n\n * refresh.total: 总刷新计数。刷新总数的计数。\n * refresh.total_time_in_millis: 刷新总时间。汇总所有花在刷新的时间(以毫秒为单位进行测量)\n * merges.current_docs: 目前的合并。合并目前正在处理中。\n * merges.total_docs: 合并总数。合并总数的计数。\n * merges.total_stopped_time_in_millis: 合并花费的总时间。合并段的所有时间的聚合。\n\n\n# 节点运行状况维度：内存，磁盘和cpu指标\n\n每个节点都运行物理硬件上，需要访问系统内存，磁盘存储和cpu周期，以便管理其控制下的数据并响应对集群的请求。\n\nes是一个严重依赖内存以实现性能的系统，因此密切关注内存使用情况与每个节点的运行状况和性能相关。改进指标的相关配置更改也可能会对内存分配和使用产生负面影响，因此记住从整体上查看系统运行状况非常重要。\n\n监视节点的cpu使用情况并查找峰值有助于识别节点中的低效进程或潜在问题。cpu性能与java虚拟机(jvm)的垃圾收集过程密切相关。\n\n磁盘高读写可能导致系统性能问题。由于访问磁盘在时间上是一个"昂贵"的过程，因此应尽可能减少磁盘io。\n\n通过如下命令行可以实现节点级别度量指标，并反映运行它的实例或计算机的性能。\n\nget /_cat/nodes?v&h=id,disk.total,disk.used,disk.avail,disk.used_percent,ram.current,ram.percent,ram.max,cpu\n\n\n1\n\n\n节点运行的重要指标：\n\n * disk.total: 总磁盘容量。节点主机上的总磁盘容量。\n * disk.used: 总磁盘使用量。节点主机上的磁盘使用总量。\n * avail disk: 可用磁盘空间总量。\n * disk.avail disk.used_percent: 使用的磁盘百分比。已使用的磁盘百分比。\n * ram：当前的ram使用情况。当前内存使用量(测量单位)。\n * percent ram: ram百分比。正在使用的内存百分比。\n * max: 最大ram。节点主机上的内存总量。\n * cpu：中央处理器。正在使用的cpu百分比。\n\n实际业务场景中推荐使用：elastic-hq，cerebro监控。\n\n\n# jvm运行状况维度：堆，gc和池大小(pool size)\n\n作为基于java的应用程序，es在java虚拟机(jvm)中运行。jvm在其"堆"分配中管理其内存，并通过garbage collection进行垃圾回收处理。\n\n如果应用程序的需求超过堆的容量，则应用程序开始强制使用连接的存储介质上的交换空间。虽然这可以防止系统崩溃，但它可能会对集群的性能造成严重破坏。监视可用堆空间以确保系统具有足够的容量对于集群的健康至关重要。\n\njvm内存分配给不同的内存池。我们需要密切注意这些池中每个池，以确保它们得到充分利用并且没有被超限利用的风险。\n\n垃圾收集器(gc)很像物理垃圾收集服务。我们希望让它定期运行，并确保系统不会让它过载。理想情况下，gc性能视图应类似均衡波浪线大小的常规执行。尖峰和异常可以成为更深层次问题的指标。\n\n可以通过get /_nodes/stats命令检索jvm度量标准。\n\n  "jvm" : {\n        "timestamp" : 1557582707194,\n        "uptime_in_millis" : 22970151,\n        "mem" : {\n          "heap_used_in_bytes" : 843509048,\n          "heap_used_percent" : 40,\n          "heap_committed_in_bytes" : 2077753344,\n          "heap_max_in_bytes" : 2077753344,\n          "non_heap_used_in_bytes" : 156752056,\n          "non_heap_committed_in_bytes" : 167890944,\n          "pools" : {\n            "young" : {\n              "used_in_bytes" : 415298464,\n              "max_in_bytes" : 558432256,\n              "peak_used_in_bytes" : 558432256,\n              "peak_max_in_bytes" : 558432256\n            },\n            "survivor" : {\n              "used_in_bytes" : 12172632,\n              "max_in_bytes" : 69730304,\n              "peak_used_in_bytes" : 69730304,\n              "peak_max_in_bytes" : 69730304\n            },\n            "old" : {\n              "used_in_bytes" : 416031952,\n              "max_in_bytes" : 1449590784,\n              "peak_used_in_bytes" : 416031952,\n              "peak_max_in_bytes" : 1449590784\n            }\n          }\n        },\n        "threads" : {\n          "count" : 116,\n          "peak_count" : 119\n        },\n        "gc" : {\n          "collectors" : {\n            "young" : {\n              "collection_count" : 260,\n              "collection_time_in_millis" : 3463\n            },\n            "old" : {\n              "collection_count" : 2,\n              "collection_time_in_millis" : 125\n            }\n          }\n        },\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\njvm运行的重要指标如下：\n\n * mem: 内存使用情况。堆和非堆进程和池的使用情况统计信息。\n * threads: 当前使用的线程和最大数量。\n * gc：垃圾收集。计算垃圾收集所花费的总时间。\n\n\n# es top10监控指标\n\n经过上面的分析，top10监控指标如下。\n\n 1.  cluster health - nodes and shards\n 2.  search performance - request latency and\n 3.  search performance - request rate\n 4.  indexing performance - merge times\n 5.  indexing performance - refresh times\n 6.  node health - memory usage\n 7.  node health - disk i/o\n 8.  node heatlh - cpu\n 9.  jvm health - heap usage and garbage collection\n 10. jvm health - jvm pool size\n\n对于将es作为解决方案的任何公司而言，投资全面的监控策略至关重要。有效的监控可以节省公司因非响应或无法修复的集群问题而导致的停机时间成本和经济成本。\n\n\n# seed_host/initial master区别联系\n\n\n# 腾讯大佬的灵魂9问\n\n还是没太搞懂seed_hosts和cluster.inital_master_nodes的区别。\n\n 1. seed_hosts里面一定是配置master eligible节点吗？\n 2. 还是说data节点也可以。\n 3. es是如何发现潜在机器的？\n 4. initial_master一定是master eligible节点吗？\n 5. 集群初始启动时，这几个节点一定都要在吗？\n 6. 初始化的时候是不是可以配置一个，然后集群初始化后，再加master eligible节点也可以的呢？\n 7. 多加几个以后，把initial_master里的几个去掉是不是也可以了？\n 8. 如果一个集群当前master是7，那它的quorum是4.es是支持慢慢去掉节点，quorum慢慢降低的吗？\n 9. 假如慢慢去掉了3个节点，原集群正常工作，那这三个节点重启后网络分区在一起了，那会不会自己形成集群呢？\n\n\n# 解答如下\n\n1-3问：seed_host是7.x之前的参数，initial master是之后的。seed推荐是master-eligible，不是一定的。\n\n4-6问：initial node需要是master eligible。第一次，新集群，需要配置。以后不需要，也不需要再改动。\n\n9问：7之前的版本有个最小master eligible控制，防止脑裂。7以后参数被废除了，看官方文档是说现在很智能选举很快，不用担心这种情况。\n\n\n# network.host/discovery.seed_hosts等区别和联系\n\n> https://wx.zsxq.com/dweb2/index/group/225224548581?from=mweb&type=detail\n\n\n# 提问：\n\n如何理解es7中下面几个配置使得属性解释。\n\nnetwork.bind_host\n\nnetwork.publish_host\n\nnetwork.host\n\ndiscovery.seed_hosts\n\ncluster.initial_master_nodes\n\n还有，network.host配置成ip和0.0.0.0有什么区别？\n\n\n# 解答：\n\n# 这几个参数是什么意思呢？\n\n先将network.host、network.publish_host、network.bind_host，这几个属性解答下。\n\nnetwork.host兼具有publish_host和bind_host两者的功能。\n\n注意：0.0.0.0的意思是：0.0.0.0是可以接受的ip地址，将绑定到所有网络接口。这里的ip建议设置为内网地址，不要设置0.0.0.0，0.0.0.0会暴露你的地址至公网，非常不安全，如果非要使用：建议加一层代理。\n\npublish_host means:"call me on this number",是es与其他集群机器通信的地址。\n\nbind_host means:"i\'ll answer on this number", 这是设置控制es侦听的网络地址的接口。\n\n实际业务场景中，我们只使用了network.host，其他没有配置。可以将publish_host和bind_host设置为不同的值，并且在某些情况下非常有用。\n\n# 有啥实际应用的场景吗？\n\n具体场景：我在数据中心有一个本地网络，我运行由不同节点组成的es集群。每个机器都有两个ip地址，一个用于从外部计算机到达，另一个用于本地连接到同一网络中的其他计算机。\n\n内部ip(eth1)用于让不同的es节点相互通信，发现等。\n\n外部ip(eth0)是我的web应用程序(在另外一个网络中)发出请求的地址。\n\n所以，在这种场景下，我们的web应用程序位于另外一个网络中，可以从bind_host地址访问es群集，而es使用publish_host与集群其他节点通信。\n\n# discovery.seed_hosts参数理解\n\ndiscovery.seed_hosts，在6.x/5.x中对应的名字是，discovery.zen.ping.unicast.hosts。对比：除了名称不同，解释部分一模一样。\n\nin order to join a cluster, a node needs to konw the hostname or ip \naddress of at least some of the other nodes in the cluster\n\n\n1\n2\n\n\n如果多节点集群，discovery.seed_hosts应该配置为候选主节点。\n\n# cluster.initial_master_nodes\n\n这也是7.x的特性，区别于之前设置min_master_count候选主节点的个数。\n\n白话文：设置候选主机节点的主机名称列表。\n\n当第一次启动全新的es 集群时，会出现一个集群引导步骤，该步骤确定在第一次选举中计票的主要合格节点集。\n\n你在生产模式下启动全新集群时，必须明确列出符合条件的节点的名称或ip地址，这些节点的投票应在第一次选举中计算。使用这个参数来设置此列表。\n\n\n# 常见大佬博客整理\n\n 1. wood大叔(elasticsearch 中文社区)，文档地址：\n    \n    > es中文社区\n\n 2. 彬哥(elastic认证中国第一人，普翔科技cto)\n    \n    > [rockybean]\n\n 3. 张超（奇安信搜索架构师，《elasticsearch 源码解析与实战》作者）\n    \n    > easyice\n\n 4. 死磕elasticsearch\n    \n    > 铭毅天下\n\n 5. elastic官方技术博客\n    \n    > elastic官方技术博客\n\n\n# 三节点(含master)集群高可用测试\n\n现有192.168.3.26/192.168.3.27/192.168.3.28，三台主机。这三台主机上都部署了es和kibana。\n\n这三台机器会承担es的所有角色，包含master节点角色。\n\n 1. 先停26上master节点\n    \n    目前master节点是26，通过26的kibana可以正常查询节点状态和查看写入一个索引文档。\n    \n    get _cat/nodes?v\n    get _cat/indices?v\n    post tes/_doc\n    {\n      "id": 111,\n      "name": "lili"\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n    \n    接下来kill掉26上的es 进程。\n    \n    从27的kibana上来看，现在集群为2个节点，27和28两个节点。\n    \n    再次执行上面的查看集群状态，写入索引数据，依旧正常。\n\n 2. 停掉27节点\n    \n    现在master节点在28上，我们先手动停止27上的es进程。\n    \n    登录28上的kibana来查看上面的集群状态，写入索引数据测试。\n    \n    执行如下命令报错：\n    \n    get _cat/nodes\n    get _cat/health\n    get _cluster/health\n    post tes/_doc\n    {\n      "id": 111,\n      "name": "lili"\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    报错信息：\n    \n    报错：\n    {\n      "statuscode": 504,\n      "error": "gateway time-out",\n      "message": "client request timeout"\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n    \n    查看28上面的es日志报错信息如下：\n    \n     [cluster_block_exception] blocked by: [service_unavailable/2/no master]\n    \n    \n    1\n    \n    \n    但是执行查询索引信息，虽然慢性，还是有查询返回的。\n    \n    get kibana_sample_data_flights/_search\n    get tes/_search\n    \n    \n    1\n    2\n    \n\n先将27节点上面的es 进行恢复。现在整个集群都恢复正常了，现在master节点还是在28上面。接下来停28，这个原本是master节点的上面的es实例，再次执行上面的查看集群的状态和执行写入文档信息，查看索引信息。结果和上面是一样的，可以查一些文档索引信息，但是查集群的信息报错。\n\n\n# elasticsearch不适合做什么\n\n 1. 不支持事务。对事务要求高的金融、银行业务选项慎重。\n 2. 近实时非准实时。写入到可被检索最快1s。对实时要求高选项要注意。\n 3. 聚合是非精确的。每个分片取topn导致结果不准确。对精确值要求高的选项需要注意。\n 4. es数据预处理功能相对受限(尽管可以结合：ingest和logstash filter)，但实时流队列第三方平台的要么定义开发，要么借助第三方：如kafka stream实现。选型需要注意。\n 5. 多表关联不适合，选项需要注意。传统数据的多表关联操作，在es中处理非常麻烦(尽管有宽表，nested，join等方案)。原因在于：传统数据库设计的初衷在于特定字段的关键词匹配查询；而es倒排索引的设计更擅长全文检索。\n\n\n# elasticsearch 25个默认值\n\nes中存在很多的默认值，这些默认值对于架构选型、开发实战、运维排查性能问题等都有很好的借鉴价值。\n\n需要分析常用的默认值的适用场景、参数、默认值大小、静态/动态参数类型、实战建议等知识点。\n\n\n# 参数类型以及静态和动态参数的区别\n\n# 参数类型\n\n参数类型分为：集群级别参数、索引级别、mapping级别参数等。\n\n# 集群级别参数\n\n举例1：cluster.max_shards_per_node\n\n前缀是：cluster.*，修改针对集群生效。\n\n举例2：indices.query.bool.max_clause_count\n\n需要在：elasticesarch.yml配置文件中设置，重启es生效\n\n# 索引级别参数\n\n举例：index.number_of_shards\n\n前缀是：index.*，修改针对索引生效\n\n# 区分静态参数和动态参数\n\nelasticsearch主分片数在索引创建之后，不可以修改(除非reindex)，index.number_of_shards是静态参数。\n\n但副本分片数，可以动态的借助：update-index-settings api任意调整。index.number_of_replicas是动态参数。\n\n以下内容分别从：集群层面、索引层面、映射层面、其他常用逐步展开讲解。\n\n\n# es集群bool类型默认支持最大子句个数\n\n * 适用场景：n多子句的bool组合查询，实现类型规则过滤的功能。\n * 参数：indices.query.bool.max_clause_count。\n * 参数类型：静态参数(需要在elasticsearch.yml中设置)\n * 默认最大值：1024.\n * 限制原因：为了防止搜索子句过多而占用过多的cpu和内存，导致集群性能下降。\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/search-settings.html\n\n\n# es集群数据节点支持默认分片数个数\n\n * 适用场景：大数据量的集群分片选项。\n * 参数：cluster.max_shards_per_node\n * 默认最大值：1000 (7.x版本后)\n * 扩展知识：超大规模集群会遇到这个问题\n\n每个节点可以存储的分片数和可用的堆内存大小成正比关系。\n\nelastic官方博客文章建议：堆内存和分片的配置比例为1:20，举例：30gb堆内存，最多可有600个分片。\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.0/misc-cluster.html#cluster-shard-limit\n\nhttps://github.com/elastic/kibana/issues/35529\n\n不合理分片可能问题：\n\n分片数量过多，写入放大，导致bulk queue打满，拒绝率上升；\n\n一定数据量后，分片数量过少，无法充分利用多节点资源，机器资源不均衡。\n\n\n# es集群index_buffer默认比例是多少\n\n * 适用场景：堆内存中索引缓冲区用于存储新索引的文档。填满后，缓冲区中的文档将写入磁盘上的某个段。它在节点上的所有分片之间划分。\n\n * 参数：indices.memory.index_buffer_size/indices.memory.min_index_buffer_size/indices.memory.max_index_buffer_size\n\n * 参数类型：静态参数(需要在elasticsearch.yml中设置)\n\n * 默认值：\n   \n   indices.memory.index_buffer_size: 10%\n   \n   indices.memory.min_buffer_size: 48mb\n\n * 使用建议：\n   \n   必须在集群中的每个数据节点上进行配置。\n   \n   写入优化中首选的优化参数之一，有助于提高写入性能和稳定性。\n   \n   https://www.elastic.co/guide/en/elasticsearch/reference/current/indexing-buffer.html\n\n\n# es默认磁盘使用率85%不再支持写入数据吗？\n\n * 使用场景：基于磁盘分配分配的参数之一，控制磁盘的使用率低警戒水位线值。\n\n * 参数: cluster.routing.allocation.disk.watermark.low/high/flood_stage\n\n * 默认值：\n   \n   cluster.routing.allocation.disk.watermark.low: 85%\n   \n   cluster.routing.allocation.disk.watermark.high: 90%\n   \n   cluster.routing.allocation.disk.watermark.flood_stage: 95%\n\n * 参数类型：集群动态参数\n\n * 使用建议\n   \n   85%：禁止写入；90%：索引分片迁移到其他可用节点；95%：索引只读。\n   \n   磁盘使用率也是监控的一个核心指标之一。\n\n\n# es集群默认的gc方式\n\n * 适用场景：写入到可搜索的最小时间间隔(单位s)\n\n * 默认参数：\n   \n   -xx:+userconcmarksweepgc\n   -xx:cmsinitiatingoccupancyfraction=75\n   -xx:+usercmsinitiatingocupancyonly\n   \n   \n   1\n   2\n   3\n   \n\n * 使用建议：\n   \n   官方建议：\n   \n   目前，我们仍然认为cms垃圾收集器是大多数部署的最佳选择，但是自es6.5.0(如果在jdk 11或更高版本上运行)以来，我们现在也支持g1gc。\n   \n   https://github.com/elastic/elasticsearch/issues/44321\n   \n   配置位置：jvm.options,优化参考wood大叔建议：更改为\n   \n   -xx:+useg1gc\n   -xx:maxgcpausemillis=50\n   \n   \n   1\n   2\n   \n   \n   其中-xx:maxgcpausemillis是控制预期的最高gc时长，默认值为200ms，如果线上业务特性对于gc停顿非常敏感，可以适当设置低一些。但是这个值如果设置过小，可能会带来比较高的cpu消耗。\n   \n   g1对于集群正常运作的情况下减轻g1停顿对服务时延的影响还是很有效的，但是如果是gc导致集群卡死，那么很有可能换g1也无法根本上解决问题。通常都是集群的数据模型或者query需要优化。\n   \n   https://elasticsearch.cn/question/4589\n\n\n# es索引默认主分片分配大小\n\n * 适用场景：数据存储\n\n * 参数：index.number_of_shards\n\n * 参数类型：静态参数\n\n * 默认值：1 (7.x版本，早期版本是5)；单索引最大支持分片数：1024.\n\n * 使用建议：\n   \n   只能在创建索引时设置此值。\n   \n   单索引1024个最大分片数的限制是一项安全限制，可防止因资源分配问题导致集群不稳定。\n   \n   可通过在每个节点上指定export es_java_opts = "-des.index.max_number_of_shards = 128"系统属性来修改此限制。\n\n\n# es索引默认压缩算法是什么\n\n * 适用场景：写入数据压缩\n\n * 参数：index.codec\n\n * 参数类型：静态参数\n\n * 默认值：lz4\n\n * 使用建议：\n   \n   可以将其设置为best_compression，它使用deflate以获得更高的压缩率，但代价是存储字段的性能较慢。\n   \n   不追求压缩效率，追求磁盘占用比低的用户推荐best_compression压缩\n\n\n# es索引默认副本分片数\n\n * 适用场景：确保业务数据的高可用性\n\n * 参数：index.number_of_replicas\n\n * 参数类型：动态参数\n\n * 默认值：1\n\n * 使用建议：\n   \n   根据业务需要合理设置副本，基于数据安全性考虑，建议副本至少设置1.\n\n\n# es索引默认的刷新频率\n\n * 适用场景：确保业务数据的高可用性\n * 参数：index.refresh_interval\n * 参数类型：动态参数\n * 默认最小值：1s\n * 使用建议：对于实时性要求不高且想优化写入的业务场景，建议根据业务实际调大刷新频率\n\n\n# es索引terms默认最大支持的长度是什么?\n\n * 适用场景：terms query\n * 参数：index.max_terms_count\n * 参数类型：动态参数\n * 默认最大值：65536\n * 使用建议：一般不会超过此最大值\n\n\n# es索引默认分页返回最大条数?\n\n * 适用场景：搜索的深度翻页\n\n * 参数：index.max_result_window\n\n * 参数类型：动态参数\n\n * 默认最大值：10000\n\n * 使用建议：\n   \n   深度翻页的机制，决定了越往后越慢。除非特殊业务需求，不建议修改默认值，可以参考百度和google的实现。\n   \n   全部数据遍历推荐scroll api. 仅支持向后翻页推荐：search after api\n\n\n# es索引默认管道有必要设置吗？\n\n * 适用场景：索引默认写入数据环节加上etl操作。\n\n * 参数：index.default_pipeline\n\n * 参数类型：动态参数\n\n * 默认值：自定义关东\n\n * 使用建议：\n   \n   结合实际业务需要，一些基础需要etl的功能建议加上。\n   \n   如果不加index.default_pipeline也可以，update_by_query + 自定义pipeline结合也能实现。不过第一种方法更周全、简练。\n\n\n# es索引mapping默认支持最大字段数?\n\n * 使用场景：防止索引mapping横向无限增大，导致内存泄漏等异常。\n * 参数：index.mapping.total_fields.limit\n * 参数类型：动态参数\n * 默认最大值：1000\n * 使用建议：不建议修改\n\n\n# es索引mapping字段默认的最大深度？\n\n * 使用场景：防止索引mapping纵向无限增大，导致异常。\n * 参数：index.mapping.depth.limit\n * 参数类型：动态参数\n * 默认最大值：20\n * 使用建议：不建议修改\n * 计算依据：例如，如果所有字段都在根对象级别定义，则深度为1。如果有一个对象映射，则深度为2，依次类推。默认值为20。\n\n\n# es索引mapping nested默认支持大小？\n\n * 适用场景：nested类型选型。\n\n * 参数：\n   \n   index.mapping.nested_fields.limit\n   \n   一个索引最大支持的nested类型个数\n   \n   index.mapping.nested_objects.limit\n   \n   一个nested类型支持的最大对象数\n\n * 参数类型：动态参数\n\n * 默认值：\n   \n   index.mapping.nested_fields.limit: 50\n   \n   index.mapping.nested_objects.limit: 10000\n\n * 使用建议：\n   \n   nested的可能的性能问题不容小觑。nested本质：每个嵌套对象都被索引为一个单独的lucene文档。如果我们为包含100个用户对象的的那个文档建立索引，则将创建101个lucene文档。\n   \n   nested较父子文档不同之处：\n   \n   如果子文档频繁更新，建议使用父子文档。\n   \n   如果子文档不频繁更新，查询频繁建议nested类型。\n\n\n# es索引动态mapping条件下，匹配的字符串默认匹配的是？\n\n * 适用场景：不提前设置mapping精准字段的场景。\n\n * 默认类型：text + keyword类型。\n\n * 实战举例如下：\n   \n   {\n     "my_index_0001" : {\n       "mappings" : {\n         "properties" : {\n           "cont" : {\n             "type" : "text",\n             "fields" : {\n               "keyword" : {\n                 "type" : "keyword",\n                 "ignore_above" : 256\n               }\n             }\n           }\n         }\n       }\n     }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   13\n   14\n   15\n   16\n   17\n   \n\n * 实际建议：建议结合业务需要，提前精准设置mapping，并优化数据建模。\n\n\n# es默认的评分机制是什么？\n\n * 默认值：bm 25\n\n * 除非业务需要，否则不建议修改。\n   \n   https://www.elastic.co/guide/en/elasticsearch/reference/current/similarity.html\n\n\n# es keyword类型默认支持的字符数是多少?\n\n * es 5.x版本以后，keyword支持的最大长度为32766个utf-8字符，text丢字符长度没有限制。\n * 设置ignore_above后，超过给定长度后的数据将不被索引，无法通过term精确匹配检索返回结果。\n\nhttps://blog.csdn.net/laoyang360/article/details/78207980\n\n\n# 为什么说，es默认不适用别名，不算入门es\n\n一句话概括：别名可以零停机改造(经典技巧，无缝切换)\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/6.8/indices-aliases.html\n\n\n# es集群节点默认属性值\n\n * 默认：候选主节点、数据节点、ingest节点、协调节点、机器学习节点(如果付费)的角色\n * 建议：集群规模到达一定量级后，一定要独立设置专有的主节点、协调节点、数据节点。角色划分清楚。\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html\n\n\n# es客户端请求的节点默认是？\n\n * 如果不明确指定协调节点，默认请求的节点充当协调节点的角色。\n * 每个节点都隐式地是一个协调节点。协调节点：需要具有足够的内存和cpu才能处理收集阶段。\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html\n\n\n# es默认分词器？\n\n * 适用场景：不明确指定分词器的场景。\n\n * 默认类型：analyzer分词器。\n\n * 实战举例如下：\n   \n   post /_analyze\n   {\n      "text": "fdfdfdf",\n      "analyzer": "standard"\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n * 实战建议：_analyze api在聚集分词问题中的作用巨大。\n\n\n# es聚合默认utc时间，可以修改吗？\n\n * 可以聚合时候修改，设置时区time_zone即可解决。\n\n * "+08:00"：代表东8区。\n   \n   get my_index/_search?size=0\n   {\n     "aggs": {\n       "by_day": {\n         "date_histogram": {\n           "field":     "date",\n           "calendar_interval":  "day",\n           "time_zone": "+08:00"\n         }\n       }\n     }\n   }\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   12\n   \n\n\n# es默认堆内存大小？\n\n * 默认值：2gb，建议一定结合实际机器环境修改。\n * es建议独立机器环境部署，不和其他进程：如logstash，hadoop，redis等共享机器资源。\n * jvm设置建议：min(31gb，机器内存的一半)\n\n\n# es jdk 什么版本开始默认自带的？\n\n7.0版本。7.0版本之后开始默认捆绑了jdk(安装包里自带jdk)，因此我们可以不单独安装jdk。\n\n\n# 索引(动态/静态)设置参考：\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html\n\n\n# es集群状态变成非绿怎么办？\n\nes集群处于不同的状态下，会以不同的颜色来呈现。\n\n\n# 集群状态的含义是什么\n\n红色：至少一个主分片未分配成功；\n\n黄色：至少一个副本分片未分配成功；\n\n绿色：全部主&副本都分配成功。\n\n\n# 排查实战思路\n\n# 查看集群状态\n\nget _cluster/health\n\n# 到底哪个节点出现了红色或黄色问题呢？\n\nget _cluster/health?level=indices\n\n如下的方式，更明快直接，来找到对应的索引。\n\nget /_cat/indices?v&health=yellow\n\nget /_cat/indices?v&health=red\n\n\n# 到底索引的哪个分片出现了红色或黄色问题？\n\nget _cluster/health?level=shards\n\n\n# 到底什么原因导致了集群变成红色或黄色呢？\n\nget _cluster/allocation/explain\n\n返回核心信息解读举例：\n\n"current_state" : "unassigned",——未分配\n  "unassigned_info" : {\n    "reason" : "index_created",——原因，索引创建阶段\n    "at" : "2020-01-29t07:32:39.041z",\n    "last_allocation_status" : "no"\n  },\n\n  "explanation" : """node does not match index setting [index.routing.allocation.require] filters [box_type:"hot"]"""\n        }——根本原因，shard分片过滤类型不一致\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 扩展思考：类似"current_state":"unassigned"， 未分配还有哪些?\n\nget _cat/shards?h=index,shard,prirep,state,unassigned.reason\n\n> 官方文档\n\n未分配状态及原因解读：\n\n 1.  index_created : unassigned as a result of an api creation of an index.\n 2.  cluster_recovered：unassigned as a result of a full cluster recover.\n 3.  index_reopened: unassigned as a result of opening a closed index.\n 4.  dangling_index_imported: unassigned as a result of importing a dangling index.\n 5.  new_index_restored: unassigned as a result of restoring into a new index.\n 6.  existing_index_restored: unassigned as a result of restoring into a closed index\n 7.  replica_added: unassigned as a result of explicit addition of a replica.\n 8.  allocation_failed: unassigned as a result of a failed allocation of the shard.\n 9.  node_left: unassigned as a result of the node hosting it leaving the cluster.\n 10. reroute_cancelled: unassigned as a result of explicit cannel reroute command.\n 11. reinitialized: when a shard moves from started back to initializing, for example, with shadow replicas.\n 12. reallocated_replica: a better replica location is identified and casuses the existing replica allocation to be cancelled.\n\n\n# 文档打标签\n\n作业： 写入如下数据：\n\n1、“庆俞年”更新！李国庆用章了，俞渝被“任命”新职位 \n2、当当大戏“庆俞年”上演第二集\n3、当当网创始人李国庆和俞渝之间的权力之争，在昨日如宫斗剧一般再度上演。\n4.这时候距离李国庆和妻子俞渝分居，已经过去了8个月。\n\n\n1\n2\n3\n4\n\n\n注意：写入时做匹配。\n\n包含： 庆俞年的打上“庆俞年”的tag, 包含李国庆的打上"李国庆"的tag, 包含当当的打上"当当"的tag 两个或者更多包含的tag组成数组。\n\n\n1\n\n\n举例如下:\n\n举例：1、“庆俞年”更新！李国庆用章了，俞渝被“任命”新职位 的tags为：［"庆俞年" "李国庆］ 请思考并回答您的实现？？\n\n\n1\n\n\n答案如下：\n\n可以使用ingest中的append的proessor来实现，if去判断string中是否存在该字段名，如果存在，则添加tags字段，里面是一个数组。\n\n但是，这种方式，只是适合于文档不多，不大的情况下的使用。如果文档很多的话，文档很长的话，估计会存在效率上的问题。最好的方式是，使用合适的中文分词器，结合自定义分词来实现。\n\npost _ingest/pipeline/_simulate\n{\n  "pipeline": {\n    "processors": [\n      {\n        "append": {\n          "field": "tags",\n          "value": "庆俞年",\n          "if": "ctx.text.contains(\'庆俞年\')"\n        }\n      },\n      {\n        "append": {\n          "field": "tags",\n          "value": "李国庆",\n          "if": "ctx.text.contains(\'李国庆\')"\n        }\n      },\n      {\n        "append": {\n          "field": "tags",\n          "value": "当当",\n          "if": "ctx.text.contains(\'当当\')"\n        }\n      },\n      {\n        "set": {\n          "field": "tags",\n          "value": "{tags.0}",\n          "if": "ctx.tags != null && ctx.tags instanceof list && ctx.tags.length == 1"\n        }\n      }\n    ]\n  },\n  "docs": [\n    {\n      "_source": {\n        "text": "当当网创始人李国庆和俞渝之间的权力之争，在昨日如宫斗剧一般再度上演."\n      }\n    }]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n那个set 的processor的目的，只是在于，将只有一个字段匹配上的时候，这个新增的字段，是一个text类型，而不是数组类型。\n\n其中ctx.text.contains()方法，是painless中可以使用的java 方法。\n\n> https://www.elastic.co/guide/en/elasticsearch/painless/7.2/painless-api-reference-shared.html\n\n\n# 索引的分片规划\n\n\n# 球友提问\n\n"在做集群规划的时候，大都会提到分片数，分片大小等参数。请问除了数据量的大小，数据条数方面有没有最佳实践？一个分片，一个索引，一个节点，可以塞下多少条数据？"\n\n"还有假如数据量已知的情况下，要分索引存储，那是索引数多一些好，还是每个索引的分片多一些好？例如，现在根据每个分片可以承载的数据大小，划分出需要1000个分片，是一个索引1000分片，还是10个索引，每个索引100个分片合适呢？"\n\n\n# 星主回复\n\n需要注意的是，单个分片支持的文档的个数是2的32次幂减1，大于42亿多。这只是理论值，实际上使用中，会远远低于这个值的。这点非常重要，这是lucene底层的索引机制所决定的。\n\n通常为了照顾到各个节点的数据分布均匀，规划的主分片的数量是数据节点的整数倍。我看有的书上提到的是1.5~3倍。实际业务场景中，基本是整数1倍。举个例子，5个数据节点，那就是5个分片。\n\n> https://blog.csdn.net/laoyang360/article/details/78080602\n\n\n# es磁盘空间满\n\n\n# 故障现象\n\n 1. kibana中查询对应的es日志，没有更新，只有早上3点有数据。\n\n 2. 查看相应的应用中，发现有索引read-only的错误。\n\n\n# 故障分析\n\n联想到当索引处于read-only状态时，肯定是es本身将索引处于保护的状态了，很有可能是这个时候磁盘的空间不足了。\n\n接下来查看es集群中，各个节点的磁盘空间大小。\n\n\n# 故障处理\n\n 1. 在kibana中删除过期的索引。\n\n 2. 将es节点所有的索引设置中的"read_only_alow_delete"设置为"false"\n    \n    > 参考如下url https://www.jianshu.com/p/90cec7c6523f\n\n\n# 总结\n\n 1. 在es集群中查看各个节点的磁盘大小，使用率的情况。\n    \n    get _cat/allocation?v&pretty\n    \n    \n    1\n    \n\n 2. 在es集群中查看节点的详细配置信息。\n    \n    使用如下的dsl，可以查看到es节点的几乎所有配置，各种参数/路径配置等信息。\n    \n    get _nodes\n\n 3. 后期考虑开启ilm，自动清理es索引。\n\n\n# reindex 高性能实践\n\n\n# 问题描述\n\nreindex索引的时候，发现非常缓慢。整体的es的集群发现并没有太多的瓶颈。\n\n在数据量几十个g或者上百个g的index中，执行reindex的时候，有啥性能提升的配置？\n\n\n# reindex简介\n\n5.x版本后新增reindex。reindex可以直接在es集群里面对数据进行重建，如果我们索引的mapping因为修改而需要重建，又或者索引setting修改，需要重建的时候，借助reindex可以很方便的异步进行重建，并且支持持跨集群间的数据迁移。\n\n比如按天创建的索引可以定期重建合并到以月为单位的索引里面去。当然索引里面要启用_source 。\n\npost _reindex\n{\n  "source": {\n    "index": "twitter"\n  },\n  "dest": {\n    "index": "new_twitter"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 原因分析\n\nreindex的核心是做跨索引、跨集群的数据迁移。\n\n慢的原因及优化思路无非包括：\n\n * 批量大小值可能太小。例如需要结合堆内存、线程池调整大小。\n * reindex的底层是scroll实现，借助scroll并行优化方式，提升效率；\n * 跨索引、跨集群的核心是写入数据，考虑写入优化角度提升效率。\n\n\n# reindex提升迁移效率的方案\n\n# 提升批量写入大小值\n\n默认情况下，_reindex使用1000进行批量操作，我们可以在source中调整batch_size。\n\npost _reindex\n{\n  "source": {\n    "index": "source",\n    "size": 5000\n  },\n  "dest": {\n    "index": "dest",\n    "routing": "=cat"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n批量大小设置的依据：\n\n 1. 使用批量索引请求以获得最佳性能\n    \n    批量大小取决于数据、分析和集群配置，但一个好的起点是每批处理5-15mb。\n    \n    注意，这是物理大小。文档数量不是度量批量大小的好指标。例如，如果每批索引1000个文档。\n    \n    * 每个1kb的1000个文档是1mb.\n    * 每个100kb的1000个文档是100mb.\n    \n    这些是完全不同的体积大小。\n\n 2. 逐渐递增文档容量大小的方式调试\n    \n    * 从大约5-15mb的大容量开始，慢慢增加，直到看不到性能的提升。然后开始增加批量写入的并发性(多线程等)。\n    * 使用kibana、cerebro或iostat、top和ps等工具监视节点，以查看资源何时开始出现瓶颈。如果我们开始接收到esrejectedexecutionexception，那么这个时候我们的集群就不能再跟上了，或者说至少有一个资源达到了瓶颈。要么减少并发性，或者提供更多有限的资源(例如从机械硬盘切换到ssd固态硬盘)，要么添加更多节点。\n\n\n# 借助scroll的sliced提升写入效率\n\nreindex支持sliced scroll以并行化重建索引过程。这种并行化可以提高效率，并提供一种方便的方法将请求分解为更小的部分。\n\n# sliced原理\n\n 1. 用过scroll接口吧，很慢？如果你数据量很大，用scroll遍历数据那确实是接受不了，现在scroll接口可以并发来进行数据遍历了。\n 2. 每个scroll请求，可以分成多个slice请求，可以理解为切片，各slice独立并行，利用scroll重建或者遍历要快很多倍。\n\n# slicing使用举例\n\nslicing的设定分为两种方式：手动设置分片、自动设置分片。\n\npost _reindex?slices=5&refresh\n{\n  "source": {\n    "index": "twitter"\n  },\n  "dest": {\n    "index": "new_twitter"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nslices大小设置注意事项：\n\n 1. slices大小的设置可以手动指定，或者设置slices为auto，auto的含义是：针对单索引，slices大小=分片数；针对多索引，slices = 分片的最小值。\n 2. 当slices的数量等于索引中的分片数量时，查询性能最高效。slices大小大于分片数，非但不会提升效率，反而会增加开销。\n 3. 如果这个slices数字很大(例如500)，建议选择一个较低的数字，因为过大的slices会影响性能。\n\n\n# es副本数设置为0\n\n如果要进行大量批量导入，请考虑通过设置index.number_of_replicas来禁用副本：0。\n\n主要原因在于：复制文档时，将整个文档发送到副本节点，并逐字重复索引过程。这意味着每个副本都将执行分析，索引和潜在合并过程。\n\n相反，如果你使用零副本进行索引，然后在提取完成时启用副本，则恢复过程本质上是逐字节的网络传输。这比复制索引过程更有效。\n\nput /my_logs/_settings\n{\n    "number_of_replicas": 1\n}\n\n\n1\n2\n3\n4\n\n\n\n# 增加refresh间隔\n\n如果我们的搜索结果不需要接近实时的准确性，可以考虑先不急于索引刷新refresh。可以将每个索引的refresh_interval到30s。\n\nput /my_logs/_settings\n{ "refresh_interval": -1 }\n\n\n1\n2\n\n\n\n# 设置task\n\n可以设置task，后台去执行。\n\npost _reindex?slices=auto&wait_for_completion=false\n{\n  "source":{\n    "index":"pn_num_use_rec_es_hn",\n    "type":"_doc",\n    "size": 10000\n  },\n  "dest":{\n    "index":"sr_pn_app_11_mkt_res_num_use_rec",\n    "type":"mkt_res_num_use_rec"\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n查看task的任务执行情况：\n\nget _tasks?detailed=true&actions=*reindex\n\n\n1\n\n\n停止cancel掉task:\n\npost _tasks/mwknls9ltn-2wht0oy1ewq:12512020/_cancel\n\n\n1\n\n\n\n# 小结\n\n这样调整以后，会比默认设置reindex速度提升10倍+。\n\n类似这种问题，多从官网、原理甚至源码角度思考。\n\n参考：\n\n> https://www.elastic.co/guide/en/elasticsearch/guide/master/indexing-performance.html\n> \n> https://www.elastic.co/guide/en/elasticsearch/reference/7.2/docs-reindex.html\n> \n> https://www.elastic.co/guide/en/elasticsearch/reference/7.2/tasks.html\n> \n> https://blog.csdn.net/laoyang360/article/details/81589459\n\n\n# 修改未分片，重新分片\n\nhttps://blog.csdn.net/qq_21383435/article/details/109136021\n\n\n# 删除索引数据，不删除模型\n\nhttps://cloud.tencent.com/developer/article/1737025\n\n\n# fra\n\n\n# filebeat和logstash有什么区别联系\n\n\n\n\n1\n\n\n\n# es的堆内存该设置多大，为什么?\n\n宿主机内存的一半和31gb，两个值中，取最小值。\n\n\n\n\n# 什么是堆内存\n\njava中的堆是jvm所管理的最大的一块内存空间，主要用于存放各种类的实例对象。在java中，堆被划分成两个不同的区域：新生代(young)、老年代(old)。\n\n新生代(young)又被划分为三个区域：eden、from survivor、to survivor。\n\n这样划分的目的是为了使jvm能够更好的管理堆内存中的对象，包括内存的分片以及回收。\n\n\n# 堆内存的作用是什么？\n\n在虚拟机启动时创建。\n\n堆内存的唯一目的就是创建对象实例，所有的对象实例和数组都要在堆上分配。\n\n堆是由垃圾回收来负责的，因此也叫做"gc堆"，垃圾回收采用分代算法，堆由此分为新生代和老生代。\n\n堆的优势是可以动态地分配内存大小，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，java的垃圾收集器会自动收走这些不再使用的数据。\n\n但缺点是，由于要在运行时动态分配内存，存取速度较慢。当堆内存因为满了无法扩展的时候就会抛出java.lang.outofmemoryerror:java heap space异常。出现这种情况的解决办法具体参见java调优。\n\n\n# 堆内存如何配置？\n\n默认情况下，es jvm使用堆内存最小和最大大小为2gb(5.x版本以上)。\n\n早期版本默认1gb，官网指出：这明显不够。\n\n在转移到生产环境时，配置足够容量的堆大小以确保es功能和性能是必要的。\n\nes将通过xms(最小堆大小)和xmx(最大堆大小)设置来分配jvm.options中指定的整个堆。\n\n\n# 堆内存配置建议\n\n * 将最小堆大小(xms)和最大堆大小(xmx)设置为彼此相等。\n * es可用的堆越多，可用于缓存的内存就越多。但请注意，太多的堆内存可能会使得我们长时间垃圾收集暂停。\n * 将xmx设置为不超过物理内存的50%，以确保有足够的物理内存留给内核文件系统缓存。\n * 不要将xmx设置为jvm超过32gb\n * 一般取宿主机内存大小的一半和31gb中的，两个值的最小值。\n\n\n# 堆内存为什么不能超过物理机内存的一半？\n\n堆内存对于es绝对重要。它被许多内存数据结构用来提供快速操作。但还有另外一个非常重要的内存使用者：lucene.\n\nlucene旨在利用底层操作系统来缓存内存中的数据结构。lucene段(segment)存储在单个文件中。因为段是一成不变的，所以这些文件永远不会改变。这使得它们非常容易缓存，并且底层操作系统将愉快地将热段(hot segments)保留在内存中以便更快地访问。这些段包括倒排索引(用于全文搜索)和文档值(用于聚合)。\n\nlucene的性能依赖于与操作系统的这种交互。但是如果我们把所有可用的内存都给了es的堆内存，那么lucene就不会有任何剩余的内存。这会严重影响性能。\n\n标准建议是将可用内存的50%提供给es堆，而将其他50%空闲。它不会被闲置，lucene会高兴地吞噬掉剩下的东西。\n\n如果我们不需要在字段上做聚合操作(例如，我们不需要fielddata)，则可以考虑进一步降低堆。堆越小，我们可以从es(更快的gc)和lucene(更多内存缓存)中获得更好的性能。\n\n\n# 堆内存为什么不能超过32gb?\n\n在java中，所有对象都分配在堆上并由指针引用。普通的对象指针(oop)指向这些对象，传统上它们是cpu本地字的大小：32位或64位，取决于处理器。\n\n对于32位系统，这意味着最大堆大小为4gb。对于64位系统，堆大小可能会变得更大，但是64位指针的开销意味着仅仅因为指针较大而存在更多的浪费空间。并且比浪费的空间更糟糕的是，当在主存储器和各种缓存(llc,l1等等)之间移动值时，较大的指针消耗更多的带宽。\n\njava使用称为压缩oops的技巧来解决这个问题。而不是指向内存中的确切字节位置，而是表示指针引用对象偏移量。这意味着一个32位指针可以引用40亿个对象，而不是40亿个字节。最终，这意味着堆可以增长到约32gb的物理尺寸，同时仍然使用32位指针。\n\n一旦穿越了这个神奇的32gb的边界，指针就会切换回普通的对象指针。每个指针的大小增加，使用更多的cpu内存带宽，并且实际上会丢失内存。实际上，在使用压缩oops获得32gb以下堆的相同有效内存之前，需要大约40-50gb的分配堆。\n\n以上小结为：即使我们有足够的内存空间，尽量避免跨越32gb的堆边界。\n\n否则会导致浪费了内存，降低了cpu的性能，并使gc在大堆中挣扎。\n\n\n# 我是内存土豪怎么办？\n\n首先，我们建议避免使用这种大型机器。\n\n但是，如果已经有了这些机器，我们有三种实用的选择：\n\n 1. 我们是否主要进行全文检索？\n    \n    考虑给es提供4-32gb，并让lucene通过操作系统文件系统缓存使用剩余的内存。所有内存都会缓存段，并导致快速全文检索。\n\n 2. 我们是否做很多排序/聚合？\n    \n    大部分聚合数字、日期、地理位置和not_analyzed字符串？那么这个时候，我们的聚合将在内存缓存的文档值上完成。\n    \n    分配4-32gb的内存给es堆内存，剩下的让操作系统在内存中缓存doc值。\n\n 3. 我们是否对分析过的字符串进行很多排序/聚合？(例如对于字标记或sigterms等)？\n    \n    * 这种情况下，我们需要使用fielddata，这意味着我们需要堆空间。\n    * 考虑在一台机器上运行两个或多个节点，而不是只有一个节点，配置数量巨大的ram。\n    * 尽管如此，仍然坚持50%的规则。\n    \n    小节：\n    \n    * 如果我们的机器具有128gb的ram，请运行两个节点，每个节点的容量低于32gb。这意味着小于64gb将用于堆，而lucene将剩余64gb以上。\n    * 如果我们选择此选项，需要在我们的一台机器上的几个es实例中设置 cluster.routing.allocation.same_shard.host: true。这将阻止主副本分片共享同一台物理机(因为这会消除副本高可用的好处)\n\n\n# 堆内存优化建议\n\n方式一：最好的办法是在系统上完全禁用交换分区。\n\n下面可以暂时关闭swap:\n\nsudo swapoff -a\n\n\n1\n\n\n要永久禁用它，需要编辑我们的/etc/fstab\n\n方式二：控制操作系统尝试交换内存的积极性。\n\n如果完全禁用交换不是一种选择，我们可以尝试降低swappiness。该值控制操作系统尝试交换内存的积极性。这可以防止在正常情况下交换，但仍然允许操作系统在紧急内存情况下进行交换。\n\n对于大多数linux系统，这是使用sysctl值配置的：\n\nvm.swappiness = 1\n\n\n1\n\n\n1的swappiness优于0，因为在某些内核版本上，swappiness为0可以调用oom杀手。\n\n方式三：mlockall允许jvm锁定其内存并防止其被操作系统交换\n\n最后，如果两种方法都不可行，则应启用mlockall文件。这允许jvm锁定其内存并防止其被操作系统交换。在我们的elasticsearch.yml中，设置如下：\n\nbootstrap.mlockall: true\n\n\n1\n\n\n\n# 最新认知\n\nwood大叔：事实上，给es分配的内存有一个魔法上限值26gb。这样可以确保启用zero based compressed oops，这样性能才是最佳的。\n\n\n# 参考url\n\n> 铭毅天下\n> \n> jvm之压缩指针——compressed oops\n> \n> elastic中文社区',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"ES架构规划",frontmatter:{title:"ES架构规划",date:"2022-01-20T11:26:28.000Z",permalink:"/pages/127f0c/",categories:["数据库","ELK"],tags:[null]},regularPath:"/01.%E6%95%B0%E6%8D%AE%E5%BA%93/05.ELK/02.ES%E6%9E%B6%E6%9E%84%E8%A7%84%E5%88%92.html",relativePath:"01.数据库/05.ELK/02.ES架构规划.md",key:"v-5babeb51",path:"/pages/127f0c/",headers:[{level:2,title:"为什么要进行节点规划",slug:"为什么要进行节点规划",normalizedTitle:"为什么要进行节点规划",charIndex:80},{level:2,title:"节点和机器是什么关系",slug:"节点和机器是什么关系",normalizedTitle:"节点和机器是什么关系",charIndex:169},{level:2,title:"节点类型有哪些",slug:"节点类型有哪些",normalizedTitle:"节点类型有哪些",charIndex:362},{level:2,title:"节点角色如何定义",slug:"节点角色如何定义",normalizedTitle:"节点角色如何定义",charIndex:1292},{level:3,title:"配置参数介绍",slug:"配置参数介绍",normalizedTitle:"配置参数介绍",charIndex:1362},{level:3,title:"单一角色节点配置",slug:"单一角色节点配置",normalizedTitle:"单一角色节点配置",charIndex:1637},{level:3,title:"单一角色好处",slug:"单一角色好处",normalizedTitle:"单一角色好处",charIndex:1965},{level:2,title:"数据节点水平扩展",slug:"数据节点水平扩展",normalizedTitle:"数据节点水平扩展",charIndex:2609},{level:2,title:"Coordinating node水平扩展",slug:"coordinating-node水平扩展",normalizedTitle:"coordinating node水平扩展",charIndex:2717},{level:2,title:"Ingest节点读写分离",slug:"ingest节点读写分离",normalizedTitle:"ingest节点读写分离",charIndex:2793},{level:2,title:"部署kibana",slug:"部署kibana",normalizedTitle:"部署kibana",charIndex:2895},{level:2,title:"Hot & Warm节点规划",slug:"hot-warm节点规划",normalizedTitle:"hot &amp; warm节点规划",charIndex:null},{level:2,title:"为什么要进行索引分片的规划",slug:"为什么要进行索引分片的规划",normalizedTitle:"为什么要进行索引分片的规划",charIndex:3413},{level:2,title:"什么是索引分片的规划",slug:"什么是索引分片的规划",normalizedTitle:"什么是索引分片的规划",charIndex:3473},{level:3,title:"多分片的好处",slug:"多分片的好处",normalizedTitle:"多分片的好处",charIndex:3646},{level:3,title:"分片过多的副作用",slug:"分片过多的副作用",normalizedTitle:"分片过多的副作用",charIndex:3800},{level:2,title:"如何做好索引分片的规划",slug:"如何做好索引分片的规划",normalizedTitle:"如何做好索引分片的规划",charIndex:3983},{level:3,title:"注意事项",slug:"注意事项",normalizedTitle:"注意事项",charIndex:3999},{level:3,title:"如何确定主分片数",slug:"如何确定主分片数",normalizedTitle:"如何确定主分片数",charIndex:4138},{level:3,title:"如何确定副本分片数",slug:"如何确定副本分片数",normalizedTitle:"如何确定副本分片数",charIndex:4303},{level:3,title:"优化节点中分片数量",slug:"优化节点中分片数量",normalizedTitle:"优化节点中分片数量",charIndex:4488},{level:2,title:"什么是容量规划",slug:"什么是容量规划",normalizedTitle:"什么是容量规划",charIndex:4944},{level:2,title:"评估业务的性能需求",slug:"评估业务的性能需求",normalizedTitle:"评估业务的性能需求",charIndex:5162},{level:2,title:"两个场景分类",slug:"两个场景分类",normalizedTitle:"两个场景分类",charIndex:5325},{level:2,title:"硬件配置的规划",slug:"硬件配置的规划",normalizedTitle:"硬件配置的规划",charIndex:5462},{level:2,title:"容量规划案例1：固定大小的数据集",slug:"容量规划案例1-固定大小的数据集",normalizedTitle:"容量规划案例1：固定大小的数据集",charIndex:5685},{level:2,title:"容量规划案例2：基于时间序列的数据",slug:"容量规划案例2-基于时间序列的数据",normalizedTitle:"容量规划案例2：基于时间序列的数据",charIndex:5881},{level:2,title:"拆分索引",slug:"拆分索引",normalizedTitle:"拆分索引",charIndex:6057},{level:2,title:"创建基于时间序列的索引",slug:"创建基于时间序列的索引",normalizedTitle:"创建基于时间序列的索引",charIndex:6371},{level:2,title:"索引的Setting设置",slug:"索引的setting设置",normalizedTitle:"索引的setting设置",charIndex:6587},{level:2,title:"索引的mapping设置",slug:"索引的mapping设置",normalizedTitle:"索引的mapping设置",charIndex:6604}],headersStr:"为什么要进行节点规划 节点和机器是什么关系 节点类型有哪些 节点角色如何定义 配置参数介绍 单一角色节点配置 单一角色好处 数据节点水平扩展 Coordinating node水平扩展 Ingest节点读写分离 部署kibana Hot & Warm节点规划 为什么要进行索引分片的规划 什么是索引分片的规划 多分片的好处 分片过多的副作用 如何做好索引分片的规划 注意事项 如何确定主分片数 如何确定副本分片数 优化节点中分片数量 什么是容量规划 评估业务的性能需求 两个场景分类 硬件配置的规划 容量规划案例1：固定大小的数据集 容量规划案例2：基于时间序列的数据 拆分索引 创建基于时间序列的索引 索引的Setting设置 索引的mapping设置",content:"ELK中架构知识点整理\n\n\n# ES集群节点部署方式\n\n这里说的集群部署方式的整理，主要针对的是对于node(节点)的规划，在ES中有很多种节点角色。\n\n\n# 为什么要进行节点规划\n\nES中有很多种节点的角色，承担不同的角色任务。节点角色规划的好坏，涉及到如何ES中master节点的高可用，data节点的水平扩展，读写分离等。\n\n\n# 节点和机器是什么关系\n\n节点是一个ES的实例。\n\n * 本质上就是一个JAVA进程\n\n * 一台机器上可以运行多个ES进程，但是生产环境一般建议一台机器上只运行一个es的实例。\n\n * 单节点ES的JVM内存尽量不超过32G\n\n * 单节点存储数据量尽量不要超过5TB\n\n * 高并发搜索场景推荐单节点配置16C/64G/2TB SSD\n\n * ES集群节点数不要超过300.\n\n\n# 节点类型有哪些\n\n需要注意的是节点和角色是两个概念，一个节点可以承担多种角色，也可以一个节点单一角色。\n\nES中有多种节点角色，有Master eligible、data、Ingest、Corrdinating、Machine learning等。\n\nmaster node：处理创建，删除索引等请求 / 决定分片被分配到哪个节点 / 负责索引的创建与删除；集群状态，维护了一个集群中必要的信息（所有的节点信息、所有的索引和其相关的Mapping与Setting信息、分片的路由信息）\n\nmaster eligible：一个集群，支持配置多个Master Eligible节点。这些节点可以在必要的时候(如Master节点出现故障，网络故障时)参与选主流程，成为Master节点。\n\ndata node：保存包含索引文档的分片数据，执行CRUD、搜索、聚合相关的操作。属于：内存、CPU、IO密集型，对硬件资源要求高。\n\nCoordinating Node：搜索请求在两个阶段中执行（query 和 fetch），这两个阶段由接收客户端请求的节点 - 协调节点协调。在请求阶段，协调节点将请求转发到保存数据的数据节点。 每个数据节点在本地执行请求并将其结果返回给协调节点。在收集fetch阶段，协调节点将每个数据节点的结果汇集为单个全局结果集。\n\ningress node：ingest 节点可以看作是数据前置处理转换的节点，支持 pipeline管道 设置，可以使用 ingest 对数据进行过滤、转换等操作，类似于 logstash 中 filter 的作用，功能相当强大。可以把Ingest节点的功能抽象为：大数据处理环节的“ETL”——抽取、转换、加载。\n\nhot&warm node：不同硬件配置的Data Node,用来实现Hot&Warm架构，降低集群部署的成本。\n\nMachine Learing Node：负责跑机器学习的Job，用来做异常检测。\n\n> 在开发测试环境中，一个节点可以承担多种角色；\n> \n> 在生产环境中，根据数据量、写入和查询的吞吐量，来选择合适的部署方式。建议设置单一角色的节点(dedicated node)\n\n\n# 节点角色如何定义\n\n一个节点在默认情况下会同时扮演：master eligible，data node和ingress node。\n\n\n# 配置参数介绍\n\n节点类型                配置参数          默认值\nmaster eligible     node.master   true\ndata                node.data     true\ningest              node.ingest   true\ncoordinating only   无             设置上面三个参数全部是false\nmachine learning    node.xml      true (需要enable x-pack)\n\n\n# 单一角色节点配置\n\n单一的master节点：\n\nnode.master: true\nnode.ingest: false\nnode.data: false\n\n\n1\n2\n3\n\n\n单一的data节点：\n\nnode.master: false\nnode.ingest: false\nndoe.data: true\n\n\n1\n2\n3\n\n\n单一的ingest节点：\n\nnode.master: false\nnode.ingest: true\nnode.data: false\n\n\n1\n2\n3\n\n\n单一的coordinate节点:\n\nnode.master: false\nnode.ingest: false\nnode.data: false\n\n\n1\n2\n3\n\n\n\n# 单一角色好处\n\nDedicated master eligible nodes(或者dedicate master node)：\n\n * 负责集群状态(cluter state)的管理\n * 使用低配置的CPU，RAM和磁盘\n * 在生产环境中配置3台；一个集群中只有1台活跃主节点(用于分片管理，索引创建，集群管理等操作)。高可用，避免脑裂。\n * 生产上和数据节点合用不合适(数据节点占用内存较高)。\n * 生产上和coordinate节点合用不合适(coordinate节点有时候可能会有开销很高的查询，导致OOM)\n\nDedicated data nodes:\n\n * 负责数据存储及处理客户端请求\n * 使用高配置的CPU，RAM和磁盘\n\nDedicate ingest nodes：\n\n * 负责数据处理\n * 使用高配置CPU，中等配置的RAM；低配置的磁盘。\n\nDedicate Coordinating Only Node(client node)：\n\n * 中高等配置CPU；中高等RAM；低配置的磁盘。\n * 生产环境中，建议为一些大的集群配置coordinating only nodes。\n * 可以扮演load balancers的角色。降低master和data nodes的负载。\n * 负载搜索结果的gather / reduce\n * 有时候无法预知客户端会发送怎么样的请求，例如有可能会有大量占用内存的聚合操作，一个深度聚合可能引发OOM。\n\n\n# 数据节点水平扩展\n\n当数据节点的磁盘容量无法满足的时候，可以增加数据节点；磁盘读写压力大的时候，也可以增加数据节点。\n\n如果索引设置了多分片，发现在增加了数据节点后，会自动把分片均匀分配到多个节点上了。\n\n\n\n\n# Coordinating node水平扩展\n\n当系统中有大量的复合查询及聚合的时候，可以增加coordinating节点，来增加查询的性能\n\n\n\n\n# Ingest节点读写分离\n\n通过定义多个ingest节点，专门用于处理ES的写请求，事先对数据进行ETL的处理。\n\n而平时的读请求，则通过定义多个coordinating node来实现的。\n\n\n\n\n# 部署kibana\n\n我们可以通过把kibana的节点部署在coordinating节点的机器上，因为有些时候直接通过kibana的页面进行日志的查询和统计分析也较为方便。而kibana的访问逻辑和通过coordinating节点的逻辑时同样的。\n\n\n\n\n# Hot & Warm节点规划\n\n适用场景：\n\n * 如果数据通常不会有update操作；\n * 适用于time based索引数据(生命周期管理)，同时数据量比较大的场景。(日志查询场景)\n * 引入warm节点，低配置大容量的机器存放老数据，以降低部署成本。\n\n不同配置：\n\n * Hot节点，通常使用SSD;索引有不断新文档写入。\n * Warm节点，通常适用HDD；索引不存在新数据的写入；同时也不存在大量的数据查询。\n\n标记节点：\n\n * 在ES启动的时候通过node.attr.my_node_type为hot或者warm标签\n * 在写入ES的创建索引的时候，通过setting配置，要求写入host节点。\n * 后期通过index.routing.allocation的一个索引级别的dynamic setting来设置warm。\n\n\n# 分片设定和管理\n\n\n# 为什么要进行索引分片的规划\n\n对于某个索引进行分片的规划和管理是非常重要的工作，是整体ES规划的逻辑部分内容。\n\n\n# 什么是索引分片的规划\n\n主要涉及到如何规划一个索引的主分片数和副本分片数。\n\nES的分布式特性，主要体现在多分片的设计上。\n\n主分片数量过小，如果该索引增长很快，集群无法通过增加节点实现对这个索引的数据扩展。\n\n主分片数量过大，导致单个分片容量很小，引发一个节点上有过多分片，影响性能。\n\n副本分片数设置过多，会降低集群整体的写入性能。\n\n\n# 多分片的好处\n\n * 当分片数 > 节点数的时候，一旦集群中有新的数据节点加入的时候，分片就可以自动进行分配。\n * 分片在重新分配的时候，系统不会有downtime。\n * 一个索引如果分布在不同的节点，多个节点查询可以并行执行。\n * 一个索引如果分布在不同的节点，数据写入可以分散到多个机器。\n\n\n# 分片过多的副作用\n\n * shard分片是ES实现集群水平扩展的最小单位\n * 由于每个分片是一个lucene的索引，分片过多会使用过多的机器的资源。(例如lucene indices/file descriptiors / RAM/ CPU ,每次搜索的请求，需要从每个分片上获取数据，分片的meta信息由master节点维护。控制分片总数在10W以内)\n\n\n# 如何做好索引分片的规划\n\n\n# 注意事项\n\n * 需要注意的是从7.0开始，新创建的一个索引的时候，默认只有一个主分片。\n * 单个分片也有好处，对单个分片的查询算分，聚合不准的问题都可以得到避免。\n * 单个索引，单个分片的时候，集群无法实现水平扩展。\n * 即使增加新的节点，也无法实现水平扩展。\n\n\n# 如何确定主分片数\n\n从存储的物理角度看：\n\n * 日志类应用，单个分片不要大于50GB.\n * 搜索类应用，单个分片不要超过20GB.\n\n为什么要控制主分片存储大小：\n\n * 提供update的性能\n * Merge的时候，减少所需的资源\n * 丢失节点后，具备更快的恢复速度 / 便于分片在集群内rebalancing\n\n\n# 如何确定副本分片数\n\n副本是主分片的拷贝。\n\n设置合适的副本分片，可以提高系统可用性：相应查询请求，防止数据丢失。\n\n需要占用和主分片一样的资源。\n\n * 副本会降低数据的索引速度：有几份副本就会有几倍的CPU资源消耗在索引上。\n\n * 会减缓对主分片的查询压力，但是会消耗同样的内存资源\n   \n   如何机器资源充分，提高副本数，可以提高整体的查询QPS。\n\n\n# 优化节点中分片数量\n\n如果一个已经运行的集群中里面新增加一台新机器，由于历史数据比较多，ES的自身的均衡机制（分片策略会尽量保证节点上的分片数大致相同）需要很长的时间。而我们的ES集群需要一直对外提供服务的，那么新创建的索引shards基本上都分片到了新机器上了。这样会存在热点数据过于集中，有性能问题。\n\n下面的参数的设置可以解决这些问题，但是需要注意的是，这些设置是强制执行的硬限制，可能会导致某些分片未分片。\n\n# index.routing.allocation.total_shards_per_node:\n\n动态设置允许你指定每个节点允许的单个索引中分片总数的硬限制.\n\n将分配给单个节点的最大分片数（副本和主分片）。默认为无边界。\n\n# cluster.routing.allocation.total_shards_per_node:\n\n可以限制一个节点可以拥有的分片数量，而不考虑索引。\n\n将全局分配给单个节点的最大分片数（副本和主分片）。默认为无边界（-1）。\n\n\n# 集群容量规划\n\n\n# 什么是容量规划\n\n整理的规划中需要多少个节点，每个索引多少个分片。\n\n这里的规划，更考虑的是磁盘容量的规划。\n\n * 规划上需要保持一定的余量，当负载出现波动，节点出现丢失时，还能正常工作。\n * 考虑机器的软硬件配置\n * 单条文档的尺寸、文档的总数据量、索引的总数据量(Time base数据保留的时间)、副本分片数\n * 文档是如何写入的(bulk的尺寸)\n * 文档的复杂度，文档是如何进行读取的(怎样进行查询和聚合)\n\n\n# 评估业务的性能需求\n\n首先需要评估下业务的性能需求是什么。\n\n * 数据吞吐及性能需求\n   * 数据写入的吞吐量，每秒要求写入多少数据？\n   * 查询的吞吐量？\n   * 单条查询可以接受的最大返回时间？\n * 了解我们的数据\n   * 数据的格式和数据的mapping\n   * 实际的查询和聚合长的是什么样子\n\n\n# 两个场景分类\n\n整体ES的使用场景规划，分为两个场景：\n\n搜索类：固定大小的数据集\n\n * 搜索的数据集增长相对比较缓慢\n\n日志：基于时间序列的数据\n\n * 使用ES存放日志与性能指标。数据每天不断写入，增长数据较快\n * 结合warm node做数据的老化处理。\n\n\n# 硬件配置的规划\n\n * 选择合理的硬件，数据节点尽可能使用SSD\n * 搜索等性能要求高的场景，建议SSD\n   * 按照1:10的比例配置内存和硬盘(也有认定为1:16)\n * 日志类和查询并发低的场景，可以考虑使用机械硬盘存储\n   * 按照1:50的比例配置内存和硬盘(也有认定为1:48到1:96之间)\n * 单节点数据建议控制在2TB以内，最大不建议超过5TB\n * JVM配置机器内存的一半，JVM内存配置不建议超过32G。\n\n\n# 容量规划案例1：固定大小的数据集\n\n例如存储的一些数据，如唱片信息库，产品信息。\n\n一些特性：\n\n * 被搜索的数据集很大，但是增长相对比较慢(不会有大量的写入)。更关心搜索和聚合的读取性能。\n * 数据的重要性与时间范围无关。关注的是搜索的相关度。\n\n估算索引的数据量，然后确定分片的大小：\n\n * 单个分片的数据不要超过20GB。\n * 可以通过增加副本分片，提高查询的吞吐量。\n\n\n# 容量规划案例2：基于时间序列的数据\n\n * 相关的用例\n   \n   * 日志、指标、安全相关的events\n   * 舆情分析\n\n * 一些特性\n   \n   * 每条数据都有时间戳；文档基本不会被更新(日志和指标数据)\n   * 用户更多的会查询近期的数据；对旧的数据查询相对较少\n   * 对数据的写入性能要求比较高。\n\n\n# 索引设计\n\n\n# 拆分索引\n\n * 如果业务上有大量的查询是基于一个字段进行filter，该字段又是一个数量有限的枚举值。例如订单所在的地区。那么我们可以考虑根据地区来切分索引，这样数据会分散到更多的索引和分片上，有利于提升查询性能。\n * 如果在单个索引上有大量的数据，可以考虑将索引拆分成多个索引。\n   * 查询性能可以得到提高\n   * 如果要对多个索引进行查询，还可以在查询中指定多个索引得以实现\n * 如果业务上有大量的查询是基于一个字段进行filter，该字段数值并不固定的情况下。\n   * 可以启用routing功能，按照filter字段的值分布到集群中不同的shard中，降低查询时相关的shard，提高CPU利用率\n\n\n# 创建基于时间序列的索引\n\n * 创建 time-based索引\n   * 在索引的名字中增加时间信息\n   * 按照每天、每周、每月的方式进行划分\n * 带来的好处\n   * 更加合理的组织索引，例如随着时间推移，便于对索引做老化处理\n     * 利用hot & warm architecture\n     * 备份和删除 （delete by query执行速度慢，底层不会立刻释放空间，而merge时又很消耗资源）\n\n\n# 索引的Setting设置\n\n\n# 索引的mapping设置\n\n\n# 设计案例1\n\n现有三台物理机，2路12核，256G，1T的SSD磁盘。\n\n三台机器上分别部署三个master，三个datanode，内存配置31G。\n\n如果是日志类型的应用，单个索引数据是500G。\n\n日志类型的分片单个分片不超过50G，那么就需要10个分片左右。考虑每个索引是10个主分片，另外每个都是1个副本分片。\n\n从使用来看，500G*2分布在三个节点上，每个节点估计使用300多G，估计也只能存储三天的所有数据。要指定相应的索引生命周期策略，定期删除索引。",normalizedContent:"elk中架构知识点整理\n\n\n# es集群节点部署方式\n\n这里说的集群部署方式的整理，主要针对的是对于node(节点)的规划，在es中有很多种节点角色。\n\n\n# 为什么要进行节点规划\n\nes中有很多种节点的角色，承担不同的角色任务。节点角色规划的好坏，涉及到如何es中master节点的高可用，data节点的水平扩展，读写分离等。\n\n\n# 节点和机器是什么关系\n\n节点是一个es的实例。\n\n * 本质上就是一个java进程\n\n * 一台机器上可以运行多个es进程，但是生产环境一般建议一台机器上只运行一个es的实例。\n\n * 单节点es的jvm内存尽量不超过32g\n\n * 单节点存储数据量尽量不要超过5tb\n\n * 高并发搜索场景推荐单节点配置16c/64g/2tb ssd\n\n * es集群节点数不要超过300.\n\n\n# 节点类型有哪些\n\n需要注意的是节点和角色是两个概念，一个节点可以承担多种角色，也可以一个节点单一角色。\n\nes中有多种节点角色，有master eligible、data、ingest、corrdinating、machine learning等。\n\nmaster node：处理创建，删除索引等请求 / 决定分片被分配到哪个节点 / 负责索引的创建与删除；集群状态，维护了一个集群中必要的信息（所有的节点信息、所有的索引和其相关的mapping与setting信息、分片的路由信息）\n\nmaster eligible：一个集群，支持配置多个master eligible节点。这些节点可以在必要的时候(如master节点出现故障，网络故障时)参与选主流程，成为master节点。\n\ndata node：保存包含索引文档的分片数据，执行crud、搜索、聚合相关的操作。属于：内存、cpu、io密集型，对硬件资源要求高。\n\ncoordinating node：搜索请求在两个阶段中执行（query 和 fetch），这两个阶段由接收客户端请求的节点 - 协调节点协调。在请求阶段，协调节点将请求转发到保存数据的数据节点。 每个数据节点在本地执行请求并将其结果返回给协调节点。在收集fetch阶段，协调节点将每个数据节点的结果汇集为单个全局结果集。\n\ningress node：ingest 节点可以看作是数据前置处理转换的节点，支持 pipeline管道 设置，可以使用 ingest 对数据进行过滤、转换等操作，类似于 logstash 中 filter 的作用，功能相当强大。可以把ingest节点的功能抽象为：大数据处理环节的“etl”——抽取、转换、加载。\n\nhot&warm node：不同硬件配置的data node,用来实现hot&warm架构，降低集群部署的成本。\n\nmachine learing node：负责跑机器学习的job，用来做异常检测。\n\n> 在开发测试环境中，一个节点可以承担多种角色；\n> \n> 在生产环境中，根据数据量、写入和查询的吞吐量，来选择合适的部署方式。建议设置单一角色的节点(dedicated node)\n\n\n# 节点角色如何定义\n\n一个节点在默认情况下会同时扮演：master eligible，data node和ingress node。\n\n\n# 配置参数介绍\n\n节点类型                配置参数          默认值\nmaster eligible     node.master   true\ndata                node.data     true\ningest              node.ingest   true\ncoordinating only   无             设置上面三个参数全部是false\nmachine learning    node.xml      true (需要enable x-pack)\n\n\n# 单一角色节点配置\n\n单一的master节点：\n\nnode.master: true\nnode.ingest: false\nnode.data: false\n\n\n1\n2\n3\n\n\n单一的data节点：\n\nnode.master: false\nnode.ingest: false\nndoe.data: true\n\n\n1\n2\n3\n\n\n单一的ingest节点：\n\nnode.master: false\nnode.ingest: true\nnode.data: false\n\n\n1\n2\n3\n\n\n单一的coordinate节点:\n\nnode.master: false\nnode.ingest: false\nnode.data: false\n\n\n1\n2\n3\n\n\n\n# 单一角色好处\n\ndedicated master eligible nodes(或者dedicate master node)：\n\n * 负责集群状态(cluter state)的管理\n * 使用低配置的cpu，ram和磁盘\n * 在生产环境中配置3台；一个集群中只有1台活跃主节点(用于分片管理，索引创建，集群管理等操作)。高可用，避免脑裂。\n * 生产上和数据节点合用不合适(数据节点占用内存较高)。\n * 生产上和coordinate节点合用不合适(coordinate节点有时候可能会有开销很高的查询，导致oom)\n\ndedicated data nodes:\n\n * 负责数据存储及处理客户端请求\n * 使用高配置的cpu，ram和磁盘\n\ndedicate ingest nodes：\n\n * 负责数据处理\n * 使用高配置cpu，中等配置的ram；低配置的磁盘。\n\ndedicate coordinating only node(client node)：\n\n * 中高等配置cpu；中高等ram；低配置的磁盘。\n * 生产环境中，建议为一些大的集群配置coordinating only nodes。\n * 可以扮演load balancers的角色。降低master和data nodes的负载。\n * 负载搜索结果的gather / reduce\n * 有时候无法预知客户端会发送怎么样的请求，例如有可能会有大量占用内存的聚合操作，一个深度聚合可能引发oom。\n\n\n# 数据节点水平扩展\n\n当数据节点的磁盘容量无法满足的时候，可以增加数据节点；磁盘读写压力大的时候，也可以增加数据节点。\n\n如果索引设置了多分片，发现在增加了数据节点后，会自动把分片均匀分配到多个节点上了。\n\n\n\n\n# coordinating node水平扩展\n\n当系统中有大量的复合查询及聚合的时候，可以增加coordinating节点，来增加查询的性能\n\n\n\n\n# ingest节点读写分离\n\n通过定义多个ingest节点，专门用于处理es的写请求，事先对数据进行etl的处理。\n\n而平时的读请求，则通过定义多个coordinating node来实现的。\n\n\n\n\n# 部署kibana\n\n我们可以通过把kibana的节点部署在coordinating节点的机器上，因为有些时候直接通过kibana的页面进行日志的查询和统计分析也较为方便。而kibana的访问逻辑和通过coordinating节点的逻辑时同样的。\n\n\n\n\n# hot & warm节点规划\n\n适用场景：\n\n * 如果数据通常不会有update操作；\n * 适用于time based索引数据(生命周期管理)，同时数据量比较大的场景。(日志查询场景)\n * 引入warm节点，低配置大容量的机器存放老数据，以降低部署成本。\n\n不同配置：\n\n * hot节点，通常使用ssd;索引有不断新文档写入。\n * warm节点，通常适用hdd；索引不存在新数据的写入；同时也不存在大量的数据查询。\n\n标记节点：\n\n * 在es启动的时候通过node.attr.my_node_type为hot或者warm标签\n * 在写入es的创建索引的时候，通过setting配置，要求写入host节点。\n * 后期通过index.routing.allocation的一个索引级别的dynamic setting来设置warm。\n\n\n# 分片设定和管理\n\n\n# 为什么要进行索引分片的规划\n\n对于某个索引进行分片的规划和管理是非常重要的工作，是整体es规划的逻辑部分内容。\n\n\n# 什么是索引分片的规划\n\n主要涉及到如何规划一个索引的主分片数和副本分片数。\n\nes的分布式特性，主要体现在多分片的设计上。\n\n主分片数量过小，如果该索引增长很快，集群无法通过增加节点实现对这个索引的数据扩展。\n\n主分片数量过大，导致单个分片容量很小，引发一个节点上有过多分片，影响性能。\n\n副本分片数设置过多，会降低集群整体的写入性能。\n\n\n# 多分片的好处\n\n * 当分片数 > 节点数的时候，一旦集群中有新的数据节点加入的时候，分片就可以自动进行分配。\n * 分片在重新分配的时候，系统不会有downtime。\n * 一个索引如果分布在不同的节点，多个节点查询可以并行执行。\n * 一个索引如果分布在不同的节点，数据写入可以分散到多个机器。\n\n\n# 分片过多的副作用\n\n * shard分片是es实现集群水平扩展的最小单位\n * 由于每个分片是一个lucene的索引，分片过多会使用过多的机器的资源。(例如lucene indices/file descriptiors / ram/ cpu ,每次搜索的请求，需要从每个分片上获取数据，分片的meta信息由master节点维护。控制分片总数在10w以内)\n\n\n# 如何做好索引分片的规划\n\n\n# 注意事项\n\n * 需要注意的是从7.0开始，新创建的一个索引的时候，默认只有一个主分片。\n * 单个分片也有好处，对单个分片的查询算分，聚合不准的问题都可以得到避免。\n * 单个索引，单个分片的时候，集群无法实现水平扩展。\n * 即使增加新的节点，也无法实现水平扩展。\n\n\n# 如何确定主分片数\n\n从存储的物理角度看：\n\n * 日志类应用，单个分片不要大于50gb.\n * 搜索类应用，单个分片不要超过20gb.\n\n为什么要控制主分片存储大小：\n\n * 提供update的性能\n * merge的时候，减少所需的资源\n * 丢失节点后，具备更快的恢复速度 / 便于分片在集群内rebalancing\n\n\n# 如何确定副本分片数\n\n副本是主分片的拷贝。\n\n设置合适的副本分片，可以提高系统可用性：相应查询请求，防止数据丢失。\n\n需要占用和主分片一样的资源。\n\n * 副本会降低数据的索引速度：有几份副本就会有几倍的cpu资源消耗在索引上。\n\n * 会减缓对主分片的查询压力，但是会消耗同样的内存资源\n   \n   如何机器资源充分，提高副本数，可以提高整体的查询qps。\n\n\n# 优化节点中分片数量\n\n如果一个已经运行的集群中里面新增加一台新机器，由于历史数据比较多，es的自身的均衡机制（分片策略会尽量保证节点上的分片数大致相同）需要很长的时间。而我们的es集群需要一直对外提供服务的，那么新创建的索引shards基本上都分片到了新机器上了。这样会存在热点数据过于集中，有性能问题。\n\n下面的参数的设置可以解决这些问题，但是需要注意的是，这些设置是强制执行的硬限制，可能会导致某些分片未分片。\n\n# index.routing.allocation.total_shards_per_node:\n\n动态设置允许你指定每个节点允许的单个索引中分片总数的硬限制.\n\n将分配给单个节点的最大分片数（副本和主分片）。默认为无边界。\n\n# cluster.routing.allocation.total_shards_per_node:\n\n可以限制一个节点可以拥有的分片数量，而不考虑索引。\n\n将全局分配给单个节点的最大分片数（副本和主分片）。默认为无边界（-1）。\n\n\n# 集群容量规划\n\n\n# 什么是容量规划\n\n整理的规划中需要多少个节点，每个索引多少个分片。\n\n这里的规划，更考虑的是磁盘容量的规划。\n\n * 规划上需要保持一定的余量，当负载出现波动，节点出现丢失时，还能正常工作。\n * 考虑机器的软硬件配置\n * 单条文档的尺寸、文档的总数据量、索引的总数据量(time base数据保留的时间)、副本分片数\n * 文档是如何写入的(bulk的尺寸)\n * 文档的复杂度，文档是如何进行读取的(怎样进行查询和聚合)\n\n\n# 评估业务的性能需求\n\n首先需要评估下业务的性能需求是什么。\n\n * 数据吞吐及性能需求\n   * 数据写入的吞吐量，每秒要求写入多少数据？\n   * 查询的吞吐量？\n   * 单条查询可以接受的最大返回时间？\n * 了解我们的数据\n   * 数据的格式和数据的mapping\n   * 实际的查询和聚合长的是什么样子\n\n\n# 两个场景分类\n\n整体es的使用场景规划，分为两个场景：\n\n搜索类：固定大小的数据集\n\n * 搜索的数据集增长相对比较缓慢\n\n日志：基于时间序列的数据\n\n * 使用es存放日志与性能指标。数据每天不断写入，增长数据较快\n * 结合warm node做数据的老化处理。\n\n\n# 硬件配置的规划\n\n * 选择合理的硬件，数据节点尽可能使用ssd\n * 搜索等性能要求高的场景，建议ssd\n   * 按照1:10的比例配置内存和硬盘(也有认定为1:16)\n * 日志类和查询并发低的场景，可以考虑使用机械硬盘存储\n   * 按照1:50的比例配置内存和硬盘(也有认定为1:48到1:96之间)\n * 单节点数据建议控制在2tb以内，最大不建议超过5tb\n * jvm配置机器内存的一半，jvm内存配置不建议超过32g。\n\n\n# 容量规划案例1：固定大小的数据集\n\n例如存储的一些数据，如唱片信息库，产品信息。\n\n一些特性：\n\n * 被搜索的数据集很大，但是增长相对比较慢(不会有大量的写入)。更关心搜索和聚合的读取性能。\n * 数据的重要性与时间范围无关。关注的是搜索的相关度。\n\n估算索引的数据量，然后确定分片的大小：\n\n * 单个分片的数据不要超过20gb。\n * 可以通过增加副本分片，提高查询的吞吐量。\n\n\n# 容量规划案例2：基于时间序列的数据\n\n * 相关的用例\n   \n   * 日志、指标、安全相关的events\n   * 舆情分析\n\n * 一些特性\n   \n   * 每条数据都有时间戳；文档基本不会被更新(日志和指标数据)\n   * 用户更多的会查询近期的数据；对旧的数据查询相对较少\n   * 对数据的写入性能要求比较高。\n\n\n# 索引设计\n\n\n# 拆分索引\n\n * 如果业务上有大量的查询是基于一个字段进行filter，该字段又是一个数量有限的枚举值。例如订单所在的地区。那么我们可以考虑根据地区来切分索引，这样数据会分散到更多的索引和分片上，有利于提升查询性能。\n * 如果在单个索引上有大量的数据，可以考虑将索引拆分成多个索引。\n   * 查询性能可以得到提高\n   * 如果要对多个索引进行查询，还可以在查询中指定多个索引得以实现\n * 如果业务上有大量的查询是基于一个字段进行filter，该字段数值并不固定的情况下。\n   * 可以启用routing功能，按照filter字段的值分布到集群中不同的shard中，降低查询时相关的shard，提高cpu利用率\n\n\n# 创建基于时间序列的索引\n\n * 创建 time-based索引\n   * 在索引的名字中增加时间信息\n   * 按照每天、每周、每月的方式进行划分\n * 带来的好处\n   * 更加合理的组织索引，例如随着时间推移，便于对索引做老化处理\n     * 利用hot & warm architecture\n     * 备份和删除 （delete by query执行速度慢，底层不会立刻释放空间，而merge时又很消耗资源）\n\n\n# 索引的setting设置\n\n\n# 索引的mapping设置\n\n\n# 设计案例1\n\n现有三台物理机，2路12核，256g，1t的ssd磁盘。\n\n三台机器上分别部署三个master，三个datanode，内存配置31g。\n\n如果是日志类型的应用，单个索引数据是500g。\n\n日志类型的分片单个分片不超过50g，那么就需要10个分片左右。考虑每个索引是10个主分片，另外每个都是1个副本分片。\n\n从使用来看，500g*2分布在三个节点上，每个节点估计使用300多g，估计也只能存储三天的所有数据。要指定相应的索引生命周期策略，定期删除索引。",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"ES_API集合",frontmatter:{title:"ES_API集合",date:"2022-01-20T11:26:59.000Z",permalink:"/pages/8d99ee/",categories:["数据库","ELK"],tags:[null]},regularPath:"/01.%E6%95%B0%E6%8D%AE%E5%BA%93/05.ELK/03.ES_API%E9%9B%86%E5%90%88.html",relativePath:"01.数据库/05.ELK/03.ES_API集合.md",key:"v-636c139f",path:"/pages/8d99ee/",headers:[{level:2,title:"查看集群的健康状况",slug:"查看集群的健康状况",normalizedTitle:"查看集群的健康状况",charIndex:13},{level:2,title:"查看所有index",slug:"查看所有index",normalizedTitle:"查看所有index",charIndex:127},{level:2,title:"查看索引的mapping和setting设置",slug:"查看索引的mapping和setting设置",normalizedTitle:"查看索引的mapping和setting设置",charIndex:476},{level:2,title:"查看索引中文档的总数",slug:"查看索引中文档的总数",normalizedTitle:"查看索引中文档的总数",charIndex:584},{level:2,title:"查看前10条文档",slug:"查看前10条文档",normalizedTitle:"查看前10条文档",charIndex:645},{level:2,title:"显式mapping设置",slug:"显式mapping设置",normalizedTitle:"显式mapping设置",charIndex:714},{level:3,title:"自定义mapping的建议",slug:"自定义mapping的建议",normalizedTitle:"自定义mapping的建议",charIndex:730},{level:3,title:"控制当前字段是否被索引",slug:"控制当前字段是否被索引",normalizedTitle:"控制当前字段是否被索引",charIndex:897},{level:3,title:"控制倒排索引记录的内容",slug:"控制倒排索引记录的内容",normalizedTitle:"控制倒排索引记录的内容",charIndex:1294},{level:3,title:"实现对NULL值进行搜索",slug:"实现对null值进行搜索",normalizedTitle:"实现对null值进行搜索",charIndex:1924},{level:3,title:"copy_to设置",slug:"copy-to设置",normalizedTitle:"copy_to设置",charIndex:2369},{level:3,title:"数组类型",slug:"数组类型",normalizedTitle:"数组类型",charIndex:2839},{level:2,title:"定义Index Alias",slug:"定义index-alias",normalizedTitle:"定义index alias",charIndex:2976},{level:2,title:"常见错误返回",slug:"常见错误返回",normalizedTitle:"常见错误返回",charIndex:3518},{level:2,title:"_analyzer API",slug:"analyzer-api",normalizedTitle:"_analyzer api",charIndex:3629},{level:2,title:"Create 一个文档",slug:"create-一个文档",normalizedTitle:"create 一个文档",charIndex:4124},{level:3,title:"Index的方法与create方法",slug:"index的方法与create方法",normalizedTitle:"index的方法与create方法",charIndex:4140},{level:2,title:"Read一个文档",slug:"read一个文档",normalizedTitle:"read一个文档",charIndex:5188},{level:2,title:"Update一个文档",slug:"update一个文档",normalizedTitle:"update一个文档",charIndex:5263},{level:2,title:"Delete一个文档",slug:"delete一个文档",normalizedTitle:"delete一个文档",charIndex:5534},{level:2,title:"Bulk API",slug:"bulk-api",normalizedTitle:"bulk api",charIndex:5603},{level:2,title:"mget批量读取",slug:"mget批量读取",normalizedTitle:"mget批量读取",charIndex:6166},{level:2,title:"msearch 批量查询",slug:"msearch-批量查询",normalizedTitle:"msearch 批量查询",charIndex:7160},{level:3,title:"mget与msearch的区别",slug:"mget与msearch的区别",normalizedTitle:"mget与msearch的区别",charIndex:7358},{level:3,title:"单次批量操作注意事项",slug:"单次批量操作注意事项",normalizedTitle:"单次批量操作注意事项",charIndex:7424},{level:2,title:"Search API",slug:"search-api",normalizedTitle:"search api",charIndex:7528},{level:2,title:"URI Search",slug:"uri-search",normalizedTitle:"uri search",charIndex:7678},{level:3,title:"URI Search基本查询",slug:"uri-search基本查询",normalizedTitle:"uri search基本查询",charIndex:7709},{level:3,title:"URI Search指定字段查询",slug:"uri-search指定字段查询",normalizedTitle:"uri search指定字段查询",charIndex:7979},{level:3,title:"URI Search泛查询",slug:"uri-search泛查询",normalizedTitle:"uri search泛查询",charIndex:8182},{level:3,title:"URI Search Term查询",slug:"uri-search-term查询",normalizedTitle:"uri search term查询",charIndex:8280},{level:3,title:"URI Search Phrase查询",slug:"uri-search-phrase查询",normalizedTitle:"uri search phrase查询",charIndex:8431},{level:2,title:"Request Body Search",slug:"request-body-search",normalizedTitle:"request body search",charIndex:10480},{level:3,title:"分页查询",slug:"分页查询",normalizedTitle:"分页查询",charIndex:10554},{level:3,title:"排序",slug:"排序",normalizedTitle:"排序",charIndex:253},{level:3,title:"source字段过滤",slug:"source字段过滤",normalizedTitle:"source字段过滤",charIndex:11170},{level:3,title:"脚本字段",slug:"脚本字段",normalizedTitle:"脚本字段",charIndex:10573},{level:3,title:"全文查询",slug:"全文查询",normalizedTitle:"全文查询",charIndex:11751},{level:3,title:"精确值查询",slug:"精确值查询",normalizedTitle:"精确值查询",charIndex:13807},{level:3,title:"Range",slug:"range",normalizedTitle:"range",charIndex:15956},{level:3,title:"exists查询",slug:"exists查询",normalizedTitle:"exists查询",charIndex:16719},{level:3,title:"bool查询",slug:"bool查询",normalizedTitle:"bool查询",charIndex:17262},{level:3,title:"Boosting Query",slug:"boosting-query",normalizedTitle:"boosting query",charIndex:19858},{level:3,title:"Disjunction Max Query",slug:"disjunction-max-query",normalizedTitle:"disjunction max query",charIndex:20908},{level:3,title:"Multi Match",slug:"multi-match",normalizedTitle:"multi match",charIndex:21820},{level:3,title:"高亮显示",slug:"高亮显示",normalizedTitle:"高亮显示",charIndex:22937},{level:3,title:"Search Template",slug:"search-template",normalizedTitle:"search template",charIndex:23412},{level:2,title:"Suggester API",slug:"suggester-api",normalizedTitle:"suggester api",charIndex:23962},{level:2,title:"Term Suggester",slug:"term-suggester",normalizedTitle:"term suggester",charIndex:24161},{level:3,title:"Missing Mode",slug:"missing-mode",normalizedTitle:"missing mode",charIndex:24390},{level:3,title:"Popular Mode",slug:"popular-mode",normalizedTitle:"popular mode",charIndex:24850},{level:3,title:"Always Mode",slug:"always-mode",normalizedTitle:"always mode",charIndex:25103},{level:2,title:"Phrase Suggester",slug:"phrase-suggester",normalizedTitle:"phrase suggester",charIndex:24106},{level:2,title:"Completion Suggester",slug:"completion-suggester",normalizedTitle:"completion suggester",charIndex:26417},{level:3,title:"使用自动补全suggester步骤",slug:"使用自动补全suggester步骤",normalizedTitle:"使用自动补全suggester步骤",charIndex:26631},{level:2,title:"Context Suggester",slug:"context-suggester",normalizedTitle:"context suggester",charIndex:24139},{level:3,title:"实现上下文suggester",slug:"实现上下文suggester",normalizedTitle:"实现上下文suggester",charIndex:27693},{level:2,title:"Metric Aggregation",slug:"metric-aggregation",normalizedTitle:"metric aggregation",charIndex:28838},{level:2,title:"Bucket Aggregation",slug:"bucket-aggregation",normalizedTitle:"bucket aggregation",charIndex:29579},{level:3,title:"Terms Aggregation",slug:"terms-aggregation",normalizedTitle:"terms aggregation",charIndex:29747},{level:3,title:"Cardinality",slug:"cardinality",normalizedTitle:"cardinality",charIndex:30724},{level:3,title:"指定SIZE分桶",slug:"指定size分桶",normalizedTitle:"指定size分桶",charIndex:30967},{level:3,title:"嵌套聚合",slug:"嵌套聚合",normalizedTitle:"嵌套聚合",charIndex:31175},{level:3,title:"优化terms聚合性能",slug:"优化terms聚合性能",normalizedTitle:"优化terms聚合性能",charIndex:31676},{level:3,title:"Range 聚合",slug:"range-聚合",normalizedTitle:"range 聚合",charIndex:31978},{level:3,title:"Histogram聚合",slug:"histogram聚合",normalizedTitle:"histogram聚合",charIndex:32465},{level:2,title:"Pipeline 聚合分析",slug:"pipeline-聚合分析",normalizedTitle:"pipeline 聚合分析",charIndex:32834},{level:3,title:"Sibling Pipeline",slug:"sibling-pipeline",normalizedTitle:"sibling pipeline",charIndex:33141},{level:3,title:"Parent Pipeline",slug:"parent-pipeline",normalizedTitle:"parent pipeline",charIndex:34108}],headersStr:"查看集群的健康状况 查看所有index 查看索引的mapping和setting设置 查看索引中文档的总数 查看前10条文档 显式mapping设置 自定义mapping的建议 控制当前字段是否被索引 控制倒排索引记录的内容 实现对NULL值进行搜索 copy_to设置 数组类型 定义Index Alias 常见错误返回 _analyzer API Create 一个文档 Index的方法与create方法 Read一个文档 Update一个文档 Delete一个文档 Bulk API mget批量读取 msearch 批量查询 mget与msearch的区别 单次批量操作注意事项 Search API URI Search URI Search基本查询 URI Search指定字段查询 URI Search泛查询 URI Search Term查询 URI Search Phrase查询 Request Body Search 分页查询 排序 source字段过滤 脚本字段 全文查询 精确值查询 Range exists查询 bool查询 Boosting Query Disjunction Max Query Multi Match 高亮显示 Search Template Suggester API Term Suggester Missing Mode Popular Mode Always Mode Phrase Suggester Completion Suggester 使用自动补全suggester步骤 Context Suggester 实现上下文suggester Metric Aggregation Bucket Aggregation Terms Aggregation Cardinality 指定SIZE分桶 嵌套聚合 优化terms聚合性能 Range 聚合 Histogram聚合 Pipeline 聚合分析 Sibling Pipeline Parent Pipeline",content:'# 基本的API\n\n\n# 查看集群的健康状况\n\nGET _cluster/health\n\n\n1\n\n\n当颜色为green，主分片与副本都正常分配；当颜色为Yellow，主分片全部正常分配，有副本分片未能正常分配；当颜色为Red，有主分片未能分配。\n\n\n# 查看所有index\n\nGET -cat/indices\n#查看indices\nGET /_cat/indices/kibana*?v&s=index\n#查看状态为绿的索引\nGET /_cat/indices?v&health=green\n#按照文档个数排序\nGET /_cat/indices?v&s=docs.count:desc\n#查看具体的字段\nGET /_cat/indices/kibana*?pri&v&h=health,index,pri,rep,docs.count,mt\n#How much memory is used per index?\nGET /_cat/indices?v&h=i,tm&s=tm:desc\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 查看索引的mapping和setting设置\n\nGET kibana_sample_data_ecommerce\n#单独看mapping\nGET mapping_test/_mapping\n\n\n1\n2\n3\n\n\n\n# 查看索引中文档的总数\n\nGET kibana_sample_data_ecommerce/_count\n\n\n1\n\n\n\n# 查看前10条文档\n\nPOST kibana_sample_data_ecommerce/_search\n{\n}\n\n\n1\n2\n3\n\n\n\n# 显式mapping设置\n\n\n# 自定义mapping的建议\n\n * 可以参考API手册，纯手写\n * 为了减少输入的工作量，减少出错概率，可以依照以下步骤\n   * 创建一个临时的index，写入一些样本数据\n   * 通过访问mapping api获得该临时文件的动态mapping定义\n   * 修改后用该配置创建我们需要的索引\n   * 删除临时索引\n\n\n# 控制当前字段是否被索引\n\nmapping中的index字段是用来控制当前字段是否被索引。默认为true，如果设置成false，该字段不可被索引。\n\nPUT users\n{\n    "mappings" : {\n      "properties" : {\n        "firstName" : {\n          "type" : "text"\n        },\n        "lastName" : {\n          "type" : "text"\n        },\n        "mobile" : {\n          "type" : "text",\n          "index": false\n        }\n      }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 控制倒排索引记录的内容\n\nmapping中的index options字段，有四种不同级别的设置，用来控制倒排索引记录的内容\n\n * docs 记录doc id\n * freqs 记录doc id和term frequencies\n * positions 记录doc id / term frequenies / term position\n * offsets 记录doc id / term frequenies / term position / character offects\n\nText类型默认记录positions，其他默认为docs\n\n记录内容越多，占用存储空间越大\n\nPUT users\n{\n    "mappings" : {\n      "properties" : {\n        "firstName" : {\n          "type" : "text"\n        },\n        "lastName" : {\n          "type" : "text"\n        },\n        "mobile" : {\n          "type" : "text",\n          "index_options": "offsets"\n        }\n      }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 实现对NULL值进行搜索\n\nmapping设置中，可以设置某个字段，"null_value":"NULL"，这样的话，就能实现对NULL值实现搜索。\n\n注意的是，只有keyword类型支持设定 null_value\n\nPUT users\n{\n    "mappings" : {\n      "properties" : {\n        "firstName" : {\n          "type" : "text"\n        },\n        "lastName" : {\n          "type" : "text"\n        },\n        "mobile" : {\n          "type" : "keyword",\n          "null_value": "NULL"\n        }\n\n      }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# copy_to设置\n\n可以满足一些场景，这个场景是输入的查询的字段的值，是已有的各个字段的结合的场景。例如输入的fullname的值，要匹配到mapping中的firstname和lastname。\n\n * _all在7中被copy_to所替代\n * 满足一些特定的搜索需求\n * copy_to将字段的数值拷贝到目标字段，实现类似_all的作用\n * copy_to的目标字段不出现在_source中\n\nPUT users\n{\n  "mappings": {\n    "properties": {\n      "firstName":{\n        "type": "text",\n        "copy_to": "fullName"\n      },\n      "lastName":{\n        "type": "text",\n        "copy_to": "fullName"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 数组类型\n\nes中不提供专门的数组类型。但是任何字段，都可以包含多个相同类型的数值。\n\nPUT users/_doc/1\n{\n  "name":"twobirds",\n  "interests":["reading","music"]\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 定义Index Alias\n\n通过设置索引的别名，是的零停机运维。\n\nPOST _aliases\n{\n  "actions": [\n    {\n      "add": {\n        "index": "movies-2019",\n        "alias": "movies-latest"\n      }\n    }\n  ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n设置alias创建不同查询的视图\n\nPOST _aliases\n{\n  "actions": [\n    {\n      "add": {\n        "index": "movies-2019",\n        "alias": "movies-lastest-highrate",\n        "filter": {\n          "range": {\n            "rating": {\n              "gte": 4\n            }\n          }\n        }\n      }\n    }\n  ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 常见错误返回\n\n问题       原因\n无法连接     网络故障或集群挂了\n连接无法关闭   网络故障或节点出错\n429      集群过于繁忙\n4XX      请求体格式有错\n500      集群内部错误\n\n\n# _analyzer API\n\nanalyzer api是ES自带的API，目的是用来分析和测试研究ES如何进行分词的。有如下三种使用的方式：\n\n第一种方式：通过GET直接指定Analyzer来进行测试\n\nGET /_analyze\n{\n  "analyzer": "standard"\n  "text": "Mastering Elasticsearch, elasticsearch is Action"\n}\n\n\n1\n2\n3\n4\n5\n\n\n第二种方式：通过POST索引名上的某个字段来进行测试\n\nPOST books/_analyze\n{\n  "field": "title",\n  "text": "Mastering Elasticsearch"\n}\n\n\n1\n2\n3\n4\n5\n\n\n第三种方式：通过POST自定义分词器来进行测试\n\nPOST /_analyze\n{\n  "tokenizer": "standard",\n  "filter": ["lowercase"],\n  "text": "Mastering Elasticsearch"\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Create 一个文档\n\n\n# Index的方法与create方法\n\n# index方法与create方法的联系\n\n都可以用于创建一个新的文档\n\n# index方法与create方法的区别\n\nput和post上面的区别\n\n * index只有put方法，并且一定要带上文档id\n * create可以使用put 或post\n\n是否需要指定文档id\n\n * 需要指定文档id，只能用PUT\n * 不指定文档id的情况下，只能用POST\n\n对待已经存在文档id的情况(指定了文档ID，只能用PUT)\n\n * index方法中，如果文档的ID不存在，那么久创建新的文档。否则会先删除现有的文档，再创建新的文档，版本会增加。\n * create方法中，如果文档的ID已经存在，会创建失败。\n\n是否想要自动生成文档id的情况\n\n * index方法无法自动生成文档id\n * create方法可以自动生成文档id\n\n# index方法的API(只有PUT)\n\nPUT my_index/_doc/1\n{\n    "user" : "Jack",\n    "post_date" : "2019-05-15T14:12:12",\n    "message" : "trying out Elasticsearch"\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n# create方法的API(PUT带上文档ID)\n\nPUT my_index/_create/1\n{\n    "user" : "Jack",\n    "post_date" : "2019-05-15T14:12:12",\n    "message" : "trying out Elasticsearch"\n}\nPUT users/_doc/1?op_type=create\n{\n    "user" : "Jack",\n    "post_date" : "2019-05-15T14:12:12",\n    "message" : "trying out Elasticsearch"\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# create方法的API(POST不带上文档ID)\n\nPOST users/_doc\n{\n\t"user" : "Mike",\n    "post_date" : "2019-04-15T14:12:12",\n    "message" : "trying out Kibana"\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Read一个文档\n\nGET my_index/_doc/1\n\n\n1\n\n\n找到文档，会返回HTTP 200；找不到文档，会返回HTTP 404\n\n\n# Update一个文档\n\n#Update 指定 ID  (先删除，在写入)\n#文档必须已经存在，更新只会对相应字段做增量修改\n#POST方法中，payload需要包含在"doc"中\nPOST users/_update/1\n{\n\t"doc":\n\t{\n\t  "user" : "user1",\n      "post_date" : "2019-04-15T14:12:12",\n      "message" : "trying out Kibanadddddd"\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# Delete一个文档\n\n### Delete by Id\n# 删除文档\nDELETE users/_doc/1\n\n\n1\n2\n3\n\n\n\n# Bulk API\n\n * 支持在一次API调用中，对不同的索引进行操作\n * 支持四种类型操作：index \\ create \\ update \\ delete\n * 可以在URL中指定index，也可以在请求的payload中进行\n * 操作中单条操作失败，并不会影响其他操作\n * 返回结果包括了每一条操作执行的结果\n\nPOST _bulk\n{ "index" : { "_index" : "test", "_id" : "1" } }\n{ "field1" : "value1" }\n{ "delete" : { "_index" : "test", "_id" : "2" } }\n{ "create" : { "_index" : "test2", "_id" : "3" } }\n{ "field1" : "value3" }\n{ "update" : {"_id" : "1", "_index" : "test"} }\n{ "doc" : {"field2" : "value2"} }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n其中"index"/"delete"/"create"/"update"是CRUD中的CUD。\n\n"filed1"和"filed2"是相应的列名，后面跟的是value值。\n\n\n# mget批量读取\n\n批量操作，可以减少网络连接所产生的开销，提高性能\n\n### mget 操作\nGET /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1"\n        },\n        {\n            "_index" : "test",\n            "_id" : "2"\n        }\n    ]\n}\n#URI中指定index\nGET /test/_mget\n{\n    "docs" : [\n        {\n\n            "_id" : "1"\n        },\n        {\n\n            "_id" : "2"\n        }\n    ]\n}\nGET /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1",\n            "_source" : false\n        },\n        {\n            "_index" : "test",\n            "_id" : "2",\n            "_source" : ["field3", "field4"]\n        },\n        {\n            "_index" : "test",\n            "_id" : "3",\n            "_source" : {\n                "include": ["user"],\n                "exclude": ["user.location"]\n            }\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n\n# msearch 批量查询\n\nPOST kibana_sample_data_ecommerce/_msearch\n{}\n{"query" : {"match_all" : {}},"size":1}\n{"index" : "kibana_sample_data_flights"}\n{"query" : {"match_all" : {}},"size":2}\n\n\n1\n2\n3\n4\n5\n\n\n\n# mget与msearch的区别\n\nmget是通过文档ID列表得到文档信息，而msearch是根据查询条件，搜索到相应文档。\n\n\n# 单次批量操作注意事项\n\n单次批量操作，数据量不宜过大，以免引发性能问题。\n\n一般建议是1000-5000个文档，如果文档很大，可以适当减少队列，大小建议为5-15MB，默认不能超过100M，会报错。\n\n\n# Search API\n\n返回体中的字段意思\n\nTOOK      花费的时间\ntotal     符合条件的总文档数\nhints     结果集，默认是前10个文档\n_index    索引名\n_id       文档的ID\n_score    相关度评分\n_source   文档原始信息\n\n\n# URI Search\n\n也就是在URL中使用查询参数\n\n\n# URI Search基本查询\n\n#基本查询\nGET /movies/_search?q=2012&df=title&sort=year:desc&from=0&size=10&timeout=1s\n{\n  "profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n * q指定查询语句，使用Query String Syntax\n * df默认字段，是指查询的字段filed，不指定时就是对所有字段\n * Sort排序\n * from 和size用于分页\n * Profile可以查看查询是如何被执行的（查看使用了什么查询）\n\n\n# URI Search指定字段查询\n\nGET /movies/_search?q=2012&df=title\n{\n\t"profile":"true"\n}\n#或者没有df字段，使用title:2012的方式\nGET /movies/_search?q=title:2012\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n就是查询title字段是2012的文档\n\n\n# URI Search泛查询\n\n#泛查询，正对_all,所有字段\nGET /movies/_search?q=2012\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# URI Search Term查询\n\n简而言之就是指定了term单词的查询，和上面指定了title是2012就是term query。和上面的指定字段查询类似\n\nGET /movies/_search?q=title:2012\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n\n# URI Search Phrase查询\n\n也就是带多个单词，词组的查询。这里要考虑要引号和括号(分组)的不同含义。\n\n# 不加引号和括号\n\n例如查找美丽心灵的影片，使用q=title:Beautiful Mind，通过profile的查询过程可以看出使用的是BooleanQuery，等效于tile字段中包含Beatiful，其他字段(也包含title字段)。\n\nGET /movies/_search?q=title:Beautiful Mind\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 加上引号\n\n"Beautiful Mind",等效于Beautiful AND Mind.这是PhraseQuery，在查询的时候要求这两个单词要同时出现，并且还要求前后的顺序保持一致。\n\n#使用引号，Phrase查询\nGET /movies/_search?q=title:"Beautiful Mind"\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\n# 加上括号\n\n和前面不加上括号的相比，这是布尔查询，意思是在title字段中，或者出现Beautiful或者出现Mind，或者都可以出现。\n\n#分组，Bool查询\nGET /movies/_search?q=title:(Beautiful Mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\n# 布尔操作\n\n在单词之间加入AND/OR/NOT或者&& / || !\n\n必须要大写\n\nAND：意思是必须包含Beautiful和Mind，但是对之间的顺序没有要求\n\n# 查找美丽心灵\nGET /movies/_search?q=title:(Beautiful AND Mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\nOR：意思是或者包含Beautiful，或者包含Mind，或者包含这两个，但是对这两个词的顺序没有要求\n\nGET /movies/_search?q=title:(Beautiful OR Mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\nNOT：意思是包含Beautiful，不包含Mind\n\nGET /movies/_search?q=title:(Beautiful NOT Mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 分组\n\n+：表示must，也就是一定要有，must后面跟的单词是必须要存在的。\n\n%2B：表示的是+的意思。\n\n-：表示must_not，也就是后面跟的单词一定要不存在。\n\n注意：\n\n单独使用+的时候，没有任何含义，和没有+一个意思。\n\n单独使用-的时候 ，是有意义的。\n\nGET /movies/_search?q=title:(+Beautiful -Mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 范围查询\n\n区间表示：[]闭区间，{}开区间\n\n * year:{2019 TO 2018]\n * year:[* TO 2018]\n * year:[2002 TO 2018%7D 等同于[2002 TO 2018}\n\nGET /movies/_search?q=title:beautiful AND year:[2002 TO 2018]\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 算数符号\n\n * year:>2010\n * year:(>2010 && <=2018)\n * year:(+>2010 +<=2018)\n\nGET /movies/_search?q=year:(>2010 AND  <=2018)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 通配符查询\n\n通配符查询(通配符查询效率低，占用内存大，不建议使用。特别是放在最前面)\n\n? 代表1个字符，* 代表0或多个字符\n\n * title:mi?d\n * title:be*\n\n#通配符查询\nGET /movies/_search?q=title:b*\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\n# 模糊匹配\n\ntitle:befutifl~1\n\n就是指当有单词输入错误的时候，做一个模糊的匹配\n\nGET /movies/_search?q=title:beautifl~1\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 近似匹配\n\n当输入的多个单词的时候，不要求他们一定要保证相应的顺序出现，但是两个都必须存在。\n\nGET /movies/_search?q=title:"Lord Rings"~2\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n\n# Request Body Search\n\n使用的ES内置的，也就是说在查询体中带上JSON格式，使用特定的DSL的语言方法来查询，使用最为广泛。\n\n分页查询/排序/source过滤字段/脚本字段，这些配置都是在定义query的查询体的上面来定义的。\n\n\n# 分页查询\n\n在不添加任何"from"和"size"的时候，from默认是从0开始，size是10。也就是从每个主分片上捞取from+size的数据汇聚，然后捞取前10条数据返回。\n\n"from"的意思就是定义了要查询数据的起始位置，如果from是10，那么就是说从第10条开始数据返回。\n\n"size"的意思是起始位置开始要返回的数据量的大小，如果size是10，那么就要从from的起始位置开始，返回10条数据。\n\n获取靠后的翻页成本较高。\n\nPOST /kibana_sample_data_ecommerce/_search\n{\n  "from":10,\n  "size":20,\n  "query":{\n    "match_all": {}\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 排序\n\n最好在"数字型"与"日期型"字段上排序\n\n因为对于多值类型或分析过的字段排序，系统会选一个值，无法得知该值。\n\n#对日期排序\nPOST kibana_sample_data_ecommerce/_search\n{\n  "sort":[{"order_date":"desc"}],\n  "query":{\n    "match_all": {}\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# source字段过滤\n\n就是针对查询出来的结果过滤到相应的字段的查询。\n\n如果_source没有存储，那就只返回匹配的文档的元数据\n\n_source支持使用通配符，例如_source["name*","desc*"]\n\n#source filtering\nPOST kibana_sample_data_ecommerce/_search\n{\n  "_source":["order_date"],\n  "query":{\n    "match_all": {}\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 脚本字段\n\n针对于当订单中有不同的汇率，需要结合汇率对订单价格进行排序的情况。\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "script_fields": {\n    "new_field": {\n      "script": {\n        "lang": "painless",\n        "source": "doc[\'order_date\'].value+\'hello\'"\n      }\n    }\n  },\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 全文查询\n\n# match_all的查询\n\n在query下面使用match_all来查询，默认返回10条数据。\n\n#ignore_unavailable=true，可以忽略尝试访问不存在的索引“404_idx”导致的报错\n#查询movies分页\nPOST /movies,404_idx/_search?ignore_unavailable=true\n{\n  "profile": true,\n\t"query": {\n\t\t"match_all": {}\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# match的查询\n\n查询单个单词的时候像是URI Search中的term search，如果查询多个单词的时候又有点像URI Search中的phrase search。\n\n#使用查询表达式 match，默认是OR的方式\nPOST movies/_search\n{\n  "query": {\n    "match": {\n      "title": "last christmas"\n    }\n  }\n}\n可以使用AND的方式\nPOST movies/_search\n{\n  "query": {\n    "match": {\n      "title": {\n        "query": "last christmas",\n        "operator": "and"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# match phrase查询\n\n专门用于短语的搜索，感觉功能上比match query要更灵活一些。\n\n#加入了slop的时候，就是允许在这些短语的中间加入一个其他的单词\nPOST movies/_search\n{\n  "query": {\n    "match_phrase": {\n      "title":{\n        "query": "one love",\n        "slop": 1\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# Query String Query\n\nquery string query和下面的simple query string query使用场景比较少。\n\nquery string 比simple query string使用起来更加灵活。\n\nquery string 有点类似于URI Query\n\n#里面也有和URI Search中的DF字段\nPOST users/_search\n{\n  "query": {\n    "query_string": {\n      "default_field": "name",\n      "query": "Ruan AND Yiming"\n    }\n  }\n}\n#也支持多个字段的查询，和URI Search中一样，括号代表分组\nPOST users/_search\n{\n  "query": {\n    "query_string": {\n      "fields":["name","about"],\n      "query": "(Ruan AND Yiming) OR (Java AND Elasticsearch)"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# Simple Query String Query\n\n * 类似Query String，但是会忽略错误的语法，同时只支持部分查询语法\n * 不支持AND OR NOT，会当作字符串处理\n * Term之间默认的关系是OR，可以指定operator\n * 支持部分逻辑\n   * +代替AND\n   * |代替OR\n   * -代替NOT\n\n#Simple Query 默认的operator是 Or\nPOST users/_search\n{\n  "query": {\n    "simple_query_string": {\n      "query": "Ruan AND Yiming",\n      "fields": ["name"]\n    }\n  }\n}\nPOST users/_search\n{\n  "query": {\n    "simple_query_string": {\n      "query": "Ruan Yiming",\n      "fields": ["name"],\n      "default_operator": "AND"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 精确值查询\n\n结构化数据，日期，数字，布尔，term 。。。。\n\n# Term query\n\n可以理解为是对精确值的查询。\n\n但是Term 查询的特点是不会对查询所带的字段进行分词处理，而索引文档中的text类型的字段值，在输入的时候会对text类型进行分词，小写化处理。\n\n要不查询的时候，字段值全用小写；要么使用text的keyword来进行精确匹配，也就是查询的时候输入插入的时候的值的大小写。\n\n# 改为小写查询，如果多个单词在一起的，使用起来还是要注意\nPOST /products/_search\n{\n  "query": {\n    "term": {\n      "desc": {\n        //"value": "iPhone"\n        "value":"iphone"\n      }\n    }\n  }\n}\n#使用keyword进行精确匹配\nPOST /products/_search\n{\n  //"explain": true,\n  "query": {\n    "term": {\n      "productID.keyword": {\n        "value": "XHDK-A-1293-#fJ3"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# constant score转为filter\n\n将query转为filter过滤，忽略掉了TF-IDF的算分过程，避免了相关性算分的开销。并且Filter可以有效利用缓存\n\nPOST /products/_search\n{\n  "explain": true,\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "productID.keyword": "XHDK-A-1293-#fJ3"\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 布尔值\n\n#对布尔值 match 查询，有算分\nPOST products/_search\n{\n  "profile": "true",\n  "explain": true,\n  "query": {\n    "term": {\n      "avaliable": {\n        "value": "false"\n      }\n    }\n  }\n}\n#一般对布尔值查询会结合上面的过滤到算法\n#对布尔值，通过constant score 转成 filtering，没有算分\nPOST products/_search\n{\n  "profile": "true",\n  "explain": true,\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "avaliable": {\n            "value": "false"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n# 查找多个精确值\n\n#字符类型 terms\nPOST products/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "terms": {\n          "productID.keyword": [\n            "QQPX-R-3956-#aD8",\n            "JODL-X-1937-#pV7"\n          ]\n        }\n      }\n    }\n  }\n}\n#处理多值字段，term 查询是包含，而不是等于\n#针对的是这个term字段的值是一个数值，而并非是唯一的值的情况下\n#可以通过添加一个genre count字段对每个文档的该字段的数组里面的值多个统计\n#在查询的时候，通过去定义这个字段的应该有的值的个数去精确匹配\nPOST movies/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "genre.keyword": "Comedy"\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# Range\n\n# 数字Range\n\n#数字 Range 查询\nGET products/_search\n{\n    "query" : {\n        "constant_score" : {\n            "filter" : {\n                "range" : {\n                    "price" : {\n                        "gte" : 20,\n                        "lte"  : 30\n                    }\n                }\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ngt大于\n\nlt小于\n\ngte大于等于\n\nlte小于等于\n\n# 日期Range\n\n# 日期 range\nPOST products/_search\n{\n    "query" : {\n        "constant_score" : {\n            "filter" : {\n                "range" : {\n                    "date" : {\n                      "gte" : "now-1y"\n                    }\n                }\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\ny 年、M 月、w 周、d 天、H或h 小时、m 分钟、s 秒\n\n\n# exists查询\n\n使用exist查询来处理非空NULL值\n\n#查询存在相应列的文档\n#exists查询\nPOST products/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "exists": {\n          "field": "date"\n        }\n      }\n    }\n  }\n}\n#查询不存在相应列的文档\nPOST products/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "bool": {\n          "must_not": {\n            "exists": {\n              "field": "date"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# bool查询\n\nbool查询是复合查询。must / should是query context贡献算分；must_not / filter是filter context不贡献算分。\n\n子查询可以任意顺序出现；\n\n可以嵌套多个查询；\n\n在bool查询中，没有must条件的时候，should中必须至少满足一条查询。\n\n#基本语法\nPOST /products/_search\n{\n  "query": {\n    "bool" : {\n      "must" : {\n        "term" : { "price" : "30" }\n      },\n      "filter": {\n        "term" : { "avaliable" : "true" }\n      },\n      "must_not" : {\n        "range" : {\n          "price" : { "lte" : 10 }\n        }\n      },\n      "should" : [\n        { "term" : { "productID.keyword" : "JODL-X-1937-#pV7" } },\n        { "term" : { "productID.keyword" : "XHDK-A-1293-#fJ3" } }\n      ],\n      "minimum_should_match" :1\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 解决term query包含而不是相等问题\n\n针对term查询的字段，如果是一个数组类型的时候，term查询是包含而不是相等的问题，通过bool查询来解决，需要新增一个数组里面值的个数的字段。\n\n#must，有算分\nPOST /newmovies/_search\n{\n  "query": {\n    "bool": {\n      "must": [\n        {"term": {"genre.keyword": {"value": "Comedy"}}},\n        {"term": {"genre_count": {"value": 1}}}\n\n      ]\n    }\n  }\n}\n#Filter。不参与算分，结果的score是0\nPOST /newmovies/_search\n{\n  "query": {\n    "bool": {\n      "filter": [\n        {"term": {"genre.keyword": {"value": "Comedy"}}},\n        {"term": {"genre_count": {"value": 1}}}\n        ]\n\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# bool嵌套查询\n\n#嵌套，实现了 should not 逻辑\nPOST /products/_search\n{\n  "query": {\n    "bool": {\n      "must": {\n        "term": {\n          "price": "30"\n        }\n      },\n      "should": [\n        {\n          "bool": {\n            "must_not": {\n              "term": {\n                "avaliable": "false"\n              }\n            }\n          }\n        }\n      ],\n      "minimum_should_match": 1\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# 改变相关度算分\n\n通过嵌套的bool查询中的问题，来改变相关度的算分的问题。\n\n在同一层级下竞争字段，具有相同的权重；通过嵌套bool查询，可以改变对算分的影响。\n\n#同级查询\nPOST /animals/_search\n{\n  "query": {\n    "bool": {\n      "should": [\n        { "term": { "text": "brown" }},\n        { "term": { "text": "red" }},\n        { "term": { "text": "quick"   }},\n        { "term": { "text": "dog"   }}\n      ]\n    }\n  }\n}\n#嵌套多级查询\nPOST /animals/_search\n{\n  "query": {\n    "bool": {\n      "should": [\n        { "term": { "text": "quick" }},\n        { "term": { "text": "dog"   }},\n        {\n          "bool":{\n            "should":[\n               { "term": { "text": "brown" }},\n                 { "term": { "text": "brown" }},\n            ]\n          }\n\n        }\n      ]\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# Boosting Query\n\n# 控制字段的boosting\n\n在bool查询中，通过指定boost的值来实现影响算分，控制相关度。\n\n参数boost的含义：当boost>1的时候，打分的相关度相对性提升；当0<boost<1的时候，打分的权重相对性降低;当boost<0的时候，贡献负分。\n\nPOST blogs/_search\n{\n  "query": {\n    "bool": {\n      "should": [\n        {"match": {\n          "title": {\n            "query": "apple,ipad",\n            "boost": 1.1\n          }\n        }},\n        {"match": {\n          "content": {\n            "query": "apple,ipad",\n            "boost": 2\n          }\n        }}\n      ]\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# Not Quite Not\n\n意思是查询的时候，正确的值要第一个返回(例如这里我要求查询的苹果公司产品要优先出现)，其他相关的(苹果汁，苹果派的)等信息也要返回，但是是靠后面返回。\n\n提高了查准率和查全率。\n\n使用了boosting query的方式来实现。positive的值是对查询算法加分的，negative的值是对查询算分减分的，negative_boost是指减多少分。\n\nPOST news/_search\n{\n  "query": {\n    "boosting": {\n      "positive": {\n        "match": {\n          "content": "apple"\n        }\n      },\n      "negative": {\n        "match": {\n          "content": "pie"\n        }\n      },\n      "negative_boost": 0.5\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# Disjunction Max Query\n\nDisjunction Max Query应用于单字符串多字段查询的场景，很多搜索引擎都是这样的。允许你只是输入一个字符串，要多所有的字段进行查询。\n\nPOST blogs/_search\n{\n    "query": {\n        "dis_max": {\n            "queries": [\n                { "match": { "title": "Quick pets" }},\n                { "match": { "body":  "Quick pets" }}\n            ]\n        }\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 使用tie breaker参数\n\n有一些情况下，同时匹配title和body字段的文档比只与一个字段匹配的文档的相关度更高。而disjunction max query 查询只会简单地使用单个最佳匹配语句的评分_score作为整体评分。这个时候，可以使用tie_breaker参数来调整。\n\n * 获取最佳匹配语句的评分_score;\n * 将其他匹配语句的评分与tie_breaker相乘；\n * 对以上评分求和并规范化。\n * Tier Breaker是一个介于0-1之间的浮点数。0代表使用最佳匹配；1代表所有语句同等重要。\n\nPOST blogs/_search\n{\n    "query": {\n        "dis_max": {\n            "queries": [\n                { "match": { "title": "Quick pets" }},\n                { "match": { "body":  "Quick pets" }}\n            ],\n            "tie_breaker": 0.2\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# Multi Match\n\n也是运用于单字符串多字段的场景下查询。\n\nbest_fields的场景，最佳字段，单字段之间相互竞争，又相互关联的时候。\n\nPOST blogs/_search\n{\n  "query": {\n    "multi_match": {\n      "type": "best_fields",\n      "query": "Quick pets",\n      "fields": ["title","body"],\n      "tie_breaker": 0.2,\n      "minimum_should_match": "20%"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nmost fields:多数字段的场景\n\nPUT /titles\n{\n  "mappings": {\n    "properties": {\n      "title": {\n        "type": "text",\n        "analyzer": "english",\n        "fields": {"std": {"type": "text","analyzer": "standard"}}\n      }\n    }\n  }\n}\nGET /titles/_search\n{\n   "query": {\n        "multi_match": {\n            "query":  "barking dogs",\n            "type":   "most_fields",\n            "fields": [ "title", "title.std" ]\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\ncross_fields:用于跨字段的查询\n\nGET address/_search\n{\n   "query": {\n        "multi_match": {\n            "query":  "Poland Street W1V",\n            "type":   "cross_fields",\n            "operator": "and",\n            "fields": [ "street", "city","country" ]\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 高亮显示\n\nPOST tmdb/_search\n{\n  "_source": ["title","overview"],\n  "query": {\n    "multi_match": {\n      "query": "basketball with cartoon aliens",\n      "fields": ["title","overview"]\n    }\n  },\n  "highlight" : {\n        "fields" : {\n            "overview" : { "pre_tags" : ["\\\\033[0;32;40m"], "post_tags" : ["\\\\033[0m"] },\n            "title" : { "pre_tags" : ["\\\\033[0;32;40m"], "post_tags" : ["\\\\033[0m"] }\n\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# Search Template\n\nES中的search template是用来解耦程序的开发和ES搜索DSL的\n\nPOST _scripts/tmdb\n{\n  "script": {\n    "lang": "mustache",\n    "source": {\n      "_source": [\n        "title","overview"\n      ],\n      "size": 20,\n      "query": {\n        "multi_match": {\n          "query": "{{q}}",\n          "fields": ["title","overview"]\n        }\n      }\n    }\n  }\n}\n#调用template\nPOST tmdb/_search/template\n{\n    "id":"tmdb",\n    "params": {\n        "q": "basketball with cartoon aliens"\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# Suggester API\n\n * 搜索引擎中类似的功能，在ES中是通过Suggester API实现的。\n * 原理：将输入的文本分解为Token，然后在搜索的字典里查找相似的Term并返回。\n * 根据不同的使用场景，ES设计了4种类别的Suggesters\n   * Term & Phrase Suggester\n   * Complete & Context Suggester\n\n\n# Term Suggester\n\n有三种suggestion Mode:\n\n * Missing：如索引中已经存在，就不提供建议\n * Popular：推荐出现频率更加高的词\n * Always：无论是否存在，都提供建议。\n\n每一个建议都包含一个算分，相似性是通过levenshtein edit distance的算法实现的。核心思想就是一个词改动多少字符就可以和另外一个词一致。提供了很多可选参数来控制相似性的模糊程度。例如"max_edits"\n\n\n# Missing Mode\n\nSuggester就是一个特殊类型的搜索。"text"里是调用时候提供的文本，通常来自于用户界面上用户输入的内容。\n\n用户输入的"lucen"是一个错误的拼写。\n\n回到指定的字段"body"上搜索，当无法搜索到结果时(missing)，返回建议的词。\n\nPOST /articles/_search\n{\n  "size": 1,\n  "query": {\n    "match": {\n      "body": "lucen rock"\n    }\n  },\n  "suggest": {\n    "term-suggestion": {\n      "text": "lucen rock",\n      "term": {\n        "suggest_mode": "missing",\n        "field": "body"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# Popular Mode\n\n推荐出现频率更多高的词\n\nPOST /articles/_search\n{\n\n  "suggest": {\n    "term-suggestion": {\n      "text": "lucen rock",\n      "term": {\n        "suggest_mode": "popular",\n        "field": "body"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# Always Mode\n\n无论是否存在，都提供建议\n\nPOST /articles/_search\n{\n\n  "suggest": {\n    "term-suggestion": {\n      "text": "lucen rock",\n      "term": {\n        "suggest_mode": "always",\n        "field": "body",\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nsorting by frequentcy和prefix length\n\n默认按照score排序，也可以按照"frequency"\n\n默认首字母不一致就不会匹配推荐，但是如果将prefix_length设置为0，就会为hock建议rock\n\nPOST /articles/_search\n{\n\n  "suggest": {\n    "term-suggestion": {\n      "text": "lucen hocks",\n      "term": {\n        "suggest_mode": "always",\n        "field": "body",\n        "prefix_length":0,\n        "sort": "frequency"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# Phrase Suggester\n\nPhrase Suggester在term suggester上增加了一些额外的逻辑。\n\n例如下面的参数：\n\n * Suggest Mode: missing，popular，always\n * Max Errors：最多可以拼错的Terms数\n * Confidence：限制返回结果数，默认是1\n\nPOST /articles/_search\n{\n  "suggest": {\n    "my-suggestion": {\n      "text": "lucne and elasticsear rock hello world ",\n      "phrase": {\n        "field": "body",\n        "max_errors":2,\n        "confidence":0,\n        "direct_generator":[{\n          "field":"body",\n          "suggest_mode":"always"\n        }],\n        "highlight": {\n          "pre_tag": "<em>",\n          "post_tag": "</em>"\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# Completion Suggester\n\n * completion Suggester提供了"自动完成"(auto complete)的功能。用户每输入一个字符，就需要即时发送一个查询请求到后端查找匹配项。\n\n * 对性能要求比较苛刻。ES采用了不同的数据结构，并非通过倒排索引来完成的。而是将Analyze的数据编码成FST和索引一起存放。FST会被ES整个加载进内存，速度很快。\n\n * FST只能用于前缀查找\n\n\n# 使用自动补全suggester步骤\n\n定义mapping，使用"completion" type\n\n索引数据\n\n运行"suggest"查询，得到搜索建议\n\n#定义mapping\nPUT articles\n{\n  "mappings": {\n    "properties": {\n      "title_completion":{\n        "type": "completion"\n      }\n    }\n  }\n}\n#索引数据\nPOST articles/_bulk\n{ "index" : { } }\n{ "title_completion": "lucene is very cool"}\n{ "index" : { } }\n{ "title_completion": "Elasticsearch builds on top of lucene"}\n{ "index" : { } }\n{ "title_completion": "Elasticsearch rocks"}\n{ "index" : { } }\n{ "title_completion": "elastic is the company behind ELK stack"}\n{ "index" : { } }\n{ "title_completion": "Elk stack rocks"}\n{ "index" : {} }\n#使用自动补全\nPOST articles/_search?pretty\n{\n  "size": 0,\n  "suggest": {\n    "article-suggester": {\n      "prefix": "elk ",\n      "completion": {\n        "field": "title_completion"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n\n# Context Suggester\n\n上下文建议搜索\n\ncontext suggester是completion suggester的扩展，\n\n可以在搜索中加入更多的上下文信息，例如，输入"star"\n\n * 咖啡相关：建议"starbucks"\n * 电影相关的："star wars"\n\n\n# 实现上下文suggester\n\n * 可以定义两种类型的context\n   * category 任意的字符串\n   * geo 地理位置信息\n * 实现context suggester的具体步骤\n   * 定制一个mapping\n   * 索引数据，并且为每个文档加入context信息\n   * 结合context进行suggestion查询\n\n#定义mapping\nPUT comments/_mapping\n{\n  "properties": {\n    "comment_autocomplete":{\n      "type": "completion",\n      "contexts":[{\n        "type":"category",\n        "name":"comment_category"\n      }]\n    }\n  }\n}\n#插入数据\nPOST comments/_doc\n{\n  "comment":"I love the star war movies",\n  "comment_autocomplete":{\n    "input":["star wars"],\n    "contexts":{\n      "comment_category":"movies"\n    }\n  }\n}\nPOST comments/_doc\n{\n  "comment":"Where can I find a Starbucks",\n  "comment_autocomplete":{\n    "input":["starbucks"],\n    "contexts":{\n      "comment_category":"coffee"\n    }\n  }\n}\n#带上下文搜索\nPOST comments/_search\n{\n  "suggest": {\n    "MY_SUGGESTION": {\n      "prefix": "sta",\n      "completion":{\n        "field":"comment_autocomplete",\n        "contexts":{\n          "comment_category":"coffee"\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n# Metric Aggregation\n\n单值分析：只输出一个分析结果\n\n * min,max,avg,sum\n * cardinality (类似于distincct count)\n\n多值分析：输出多个分析结果\n\n * stats, exended stats\n * percentile, percentile rank\n * top hits (排在前面的示例)\n\n# 多个 Metric 聚合，找到最低最高和平均工资\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "max_salary": {\n      "max": {\n        "field": "salary"\n      }\n    },\n    "min_salary": {\n      "min": {\n        "field": "salary"\n      }\n    },\n    "avg_salary": {\n      "avg": {\n        "field": "salary"\n      }\n    }\n  }\n}\n# 一个聚合，输出多值\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "stats_salary": {\n      "stats": {\n        "field":"salary"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# Bucket Aggregation\n\n按照一定的规则，将文档分配到不同的桶中，从而达到分类的目的。ES提供的一些常见的bucket aggregation。\n\n * terms\n * 数字类型，有range/data range，或者histogram / data histogram\n * 支持嵌套：也就是在桶里再做分桶\n\n\n# Terms Aggregation\n\n在对term进行聚合的时候，如果是text类型。有两种方法，要么指定keyword字段，要么就需要打开fieldata，但是fieldata会查找出对应分词后的term。才可以进行terms aggregation\n\n * keyword默认支持doc_values\n * text需要咋mapping中enable，会按照分词后的结果进行分\n\n# 对 Text 字段进行 terms 聚合查询，失败\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field":"job"\n      }\n    }\n  }\n}\n# 对keword 进行聚合\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field":"job.keyword"\n      }\n    }\n  }\n}\n# 对 Text 字段打开 fielddata，支持terms aggregation\nPUT employees/_mapping\n{\n  "properties" : {\n    "job":{\n       "type":     "text",\n       "fielddata": true\n    }\n  }\n}\n# 对 Text 字段进行 terms 分词。分词后的terms,结果和上面指定keyword的方式是不一样的，这是分词后的结果\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field":"job"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# Cardinality\n\n类似于SQL中的Distinct\n\n# 对job.keyword 和 job 进行 terms 聚合，分桶的总数并不一样\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "cardinate": {\n      "cardinality": {\n        "field": "job"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 指定SIZE分桶\n\n#指定 bucket 的 size\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "ages_5": {\n      "terms": {\n        "field":"age",\n        "size":3\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 嵌套聚合\n\n# 指定size，不同工种中，年纪最大的3个员工的具体信息\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field":"job.keyword"\n      },\n      "aggs":{\n        "old_employee":{\n          "top_hits":{\n            "size":3,\n            "sort":[\n              {\n                "age":{\n                  "order":"desc"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 优化terms聚合性能\n\n通过指定mapping中，打开eager_global_ordinals为true，这会让索引数据的时候就开始对数据文档进行预先处理，这样对于要求频繁使用聚合的场景，或者说是要对聚合实时数据有更高的性能的时候来使用。\n\nput index\n{\n  "mappings": {\n    "properties": {\n      "foo": {\n        "type": "keyword",\n        "eager_global_ordinals": true\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# Range 聚合\n\n自己自定义range\n\n#Salary Ranges 分桶，可以自己定义 key\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "salary_range": {\n      "range": {\n        "field":"salary",\n        "ranges":[\n          {\n            "to":10000\n          },\n          {\n            "from":10000,\n            "to":20000\n          },\n          {\n            "key":">20000",\n            "from":20000\n          }\n        ]\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# Histogram聚合\n\n通过指定分桶的间隔，来进行分桶\n\n#Salary Histogram,工资0到10万，以 5000一个区间进行分桶\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "salary_histrogram": {\n      "histogram": {\n        "field":"salary",\n        "interval":5000,\n        "extended_bounds":{\n          "min":0,\n          "max":100000\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# Pipeline 聚合分析\n\npipeline管道的概念：支持对聚合分析的结果，再次进行聚合分析。\n\nPipeline的分析结果会输出到原结果中，根据位置的不同，分为两类：\n\n * Sibling 结果和现有分析结果同级\n   * max，min，avg & sum bucket\n   * stats , extended status buccket\n   * percentiles bucket\n * Parent 结果内嵌到现有的聚合分析结果之中\n   * derivative (求导)\n   * cumultive sum（累计求和）\n   * moving function (滑动窗口)\n\n\n# Sibling Pipeline\n\n查看到平均工资最低的工作类型\n\n# 平均工资最低的工作类型\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field": "job.keyword",\n        "size": 10\n      },\n      "aggs": {\n        "avg_salary": {\n          "avg": {\n            "field": "salary"\n          }\n        }\n      }\n    },\n    "min_salary_by_job":{\n      "min_bucket": {\n        "buckets_path": "jobs>avg_salary"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n平均工资的统计分析\n\n# 平均工资的统计分析\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field": "job.keyword",\n        "size": 10\n      },\n      "aggs": {\n        "avg_salary": {\n          "avg": {\n            "field": "salary"\n          }\n        }\n      }\n    },\n    "stats_salary_by_job":{\n      "stats_bucket": {\n        "buckets_path": "jobs>avg_salary"\n      }\n    }\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# Parent Pipeline\n\n按照年龄，对工资进行求导(看工资发展的趋势)\n\n#按照年龄对平均工资求导\nPOST employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "age": {\n      "histogram": {\n        "field": "age",\n        "min_doc_count": 1,\n        "interval": 1\n      },\n      "aggs": {\n        "avg_salary": {\n          "avg": {\n            "field": "salary"\n          }\n        },\n        "derivative_avg_salary":{\n          "derivative": {\n            "buckets_path": "avg_salary"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n',normalizedContent:'# 基本的api\n\n\n# 查看集群的健康状况\n\nget _cluster/health\n\n\n1\n\n\n当颜色为green，主分片与副本都正常分配；当颜色为yellow，主分片全部正常分配，有副本分片未能正常分配；当颜色为red，有主分片未能分配。\n\n\n# 查看所有index\n\nget -cat/indices\n#查看indices\nget /_cat/indices/kibana*?v&s=index\n#查看状态为绿的索引\nget /_cat/indices?v&health=green\n#按照文档个数排序\nget /_cat/indices?v&s=docs.count:desc\n#查看具体的字段\nget /_cat/indices/kibana*?pri&v&h=health,index,pri,rep,docs.count,mt\n#how much memory is used per index?\nget /_cat/indices?v&h=i,tm&s=tm:desc\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 查看索引的mapping和setting设置\n\nget kibana_sample_data_ecommerce\n#单独看mapping\nget mapping_test/_mapping\n\n\n1\n2\n3\n\n\n\n# 查看索引中文档的总数\n\nget kibana_sample_data_ecommerce/_count\n\n\n1\n\n\n\n# 查看前10条文档\n\npost kibana_sample_data_ecommerce/_search\n{\n}\n\n\n1\n2\n3\n\n\n\n# 显式mapping设置\n\n\n# 自定义mapping的建议\n\n * 可以参考api手册，纯手写\n * 为了减少输入的工作量，减少出错概率，可以依照以下步骤\n   * 创建一个临时的index，写入一些样本数据\n   * 通过访问mapping api获得该临时文件的动态mapping定义\n   * 修改后用该配置创建我们需要的索引\n   * 删除临时索引\n\n\n# 控制当前字段是否被索引\n\nmapping中的index字段是用来控制当前字段是否被索引。默认为true，如果设置成false，该字段不可被索引。\n\nput users\n{\n    "mappings" : {\n      "properties" : {\n        "firstname" : {\n          "type" : "text"\n        },\n        "lastname" : {\n          "type" : "text"\n        },\n        "mobile" : {\n          "type" : "text",\n          "index": false\n        }\n      }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 控制倒排索引记录的内容\n\nmapping中的index options字段，有四种不同级别的设置，用来控制倒排索引记录的内容\n\n * docs 记录doc id\n * freqs 记录doc id和term frequencies\n * positions 记录doc id / term frequenies / term position\n * offsets 记录doc id / term frequenies / term position / character offects\n\ntext类型默认记录positions，其他默认为docs\n\n记录内容越多，占用存储空间越大\n\nput users\n{\n    "mappings" : {\n      "properties" : {\n        "firstname" : {\n          "type" : "text"\n        },\n        "lastname" : {\n          "type" : "text"\n        },\n        "mobile" : {\n          "type" : "text",\n          "index_options": "offsets"\n        }\n      }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 实现对null值进行搜索\n\nmapping设置中，可以设置某个字段，"null_value":"null"，这样的话，就能实现对null值实现搜索。\n\n注意的是，只有keyword类型支持设定 null_value\n\nput users\n{\n    "mappings" : {\n      "properties" : {\n        "firstname" : {\n          "type" : "text"\n        },\n        "lastname" : {\n          "type" : "text"\n        },\n        "mobile" : {\n          "type" : "keyword",\n          "null_value": "null"\n        }\n\n      }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# copy_to设置\n\n可以满足一些场景，这个场景是输入的查询的字段的值，是已有的各个字段的结合的场景。例如输入的fullname的值，要匹配到mapping中的firstname和lastname。\n\n * _all在7中被copy_to所替代\n * 满足一些特定的搜索需求\n * copy_to将字段的数值拷贝到目标字段，实现类似_all的作用\n * copy_to的目标字段不出现在_source中\n\nput users\n{\n  "mappings": {\n    "properties": {\n      "firstname":{\n        "type": "text",\n        "copy_to": "fullname"\n      },\n      "lastname":{\n        "type": "text",\n        "copy_to": "fullname"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 数组类型\n\nes中不提供专门的数组类型。但是任何字段，都可以包含多个相同类型的数值。\n\nput users/_doc/1\n{\n  "name":"twobirds",\n  "interests":["reading","music"]\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 定义index alias\n\n通过设置索引的别名，是的零停机运维。\n\npost _aliases\n{\n  "actions": [\n    {\n      "add": {\n        "index": "movies-2019",\n        "alias": "movies-latest"\n      }\n    }\n  ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n设置alias创建不同查询的视图\n\npost _aliases\n{\n  "actions": [\n    {\n      "add": {\n        "index": "movies-2019",\n        "alias": "movies-lastest-highrate",\n        "filter": {\n          "range": {\n            "rating": {\n              "gte": 4\n            }\n          }\n        }\n      }\n    }\n  ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 常见错误返回\n\n问题       原因\n无法连接     网络故障或集群挂了\n连接无法关闭   网络故障或节点出错\n429      集群过于繁忙\n4xx      请求体格式有错\n500      集群内部错误\n\n\n# _analyzer api\n\nanalyzer api是es自带的api，目的是用来分析和测试研究es如何进行分词的。有如下三种使用的方式：\n\n第一种方式：通过get直接指定analyzer来进行测试\n\nget /_analyze\n{\n  "analyzer": "standard"\n  "text": "mastering elasticsearch, elasticsearch is action"\n}\n\n\n1\n2\n3\n4\n5\n\n\n第二种方式：通过post索引名上的某个字段来进行测试\n\npost books/_analyze\n{\n  "field": "title",\n  "text": "mastering elasticsearch"\n}\n\n\n1\n2\n3\n4\n5\n\n\n第三种方式：通过post自定义分词器来进行测试\n\npost /_analyze\n{\n  "tokenizer": "standard",\n  "filter": ["lowercase"],\n  "text": "mastering elasticsearch"\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# create 一个文档\n\n\n# index的方法与create方法\n\n# index方法与create方法的联系\n\n都可以用于创建一个新的文档\n\n# index方法与create方法的区别\n\nput和post上面的区别\n\n * index只有put方法，并且一定要带上文档id\n * create可以使用put 或post\n\n是否需要指定文档id\n\n * 需要指定文档id，只能用put\n * 不指定文档id的情况下，只能用post\n\n对待已经存在文档id的情况(指定了文档id，只能用put)\n\n * index方法中，如果文档的id不存在，那么久创建新的文档。否则会先删除现有的文档，再创建新的文档，版本会增加。\n * create方法中，如果文档的id已经存在，会创建失败。\n\n是否想要自动生成文档id的情况\n\n * index方法无法自动生成文档id\n * create方法可以自动生成文档id\n\n# index方法的api(只有put)\n\nput my_index/_doc/1\n{\n    "user" : "jack",\n    "post_date" : "2019-05-15t14:12:12",\n    "message" : "trying out elasticsearch"\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n# create方法的api(put带上文档id)\n\nput my_index/_create/1\n{\n    "user" : "jack",\n    "post_date" : "2019-05-15t14:12:12",\n    "message" : "trying out elasticsearch"\n}\nput users/_doc/1?op_type=create\n{\n    "user" : "jack",\n    "post_date" : "2019-05-15t14:12:12",\n    "message" : "trying out elasticsearch"\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# create方法的api(post不带上文档id)\n\npost users/_doc\n{\n\t"user" : "mike",\n    "post_date" : "2019-04-15t14:12:12",\n    "message" : "trying out kibana"\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# read一个文档\n\nget my_index/_doc/1\n\n\n1\n\n\n找到文档，会返回http 200；找不到文档，会返回http 404\n\n\n# update一个文档\n\n#update 指定 id  (先删除，在写入)\n#文档必须已经存在，更新只会对相应字段做增量修改\n#post方法中，payload需要包含在"doc"中\npost users/_update/1\n{\n\t"doc":\n\t{\n\t  "user" : "user1",\n      "post_date" : "2019-04-15t14:12:12",\n      "message" : "trying out kibanadddddd"\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# delete一个文档\n\n### delete by id\n# 删除文档\ndelete users/_doc/1\n\n\n1\n2\n3\n\n\n\n# bulk api\n\n * 支持在一次api调用中，对不同的索引进行操作\n * 支持四种类型操作：index \\ create \\ update \\ delete\n * 可以在url中指定index，也可以在请求的payload中进行\n * 操作中单条操作失败，并不会影响其他操作\n * 返回结果包括了每一条操作执行的结果\n\npost _bulk\n{ "index" : { "_index" : "test", "_id" : "1" } }\n{ "field1" : "value1" }\n{ "delete" : { "_index" : "test", "_id" : "2" } }\n{ "create" : { "_index" : "test2", "_id" : "3" } }\n{ "field1" : "value3" }\n{ "update" : {"_id" : "1", "_index" : "test"} }\n{ "doc" : {"field2" : "value2"} }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n其中"index"/"delete"/"create"/"update"是crud中的cud。\n\n"filed1"和"filed2"是相应的列名，后面跟的是value值。\n\n\n# mget批量读取\n\n批量操作，可以减少网络连接所产生的开销，提高性能\n\n### mget 操作\nget /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1"\n        },\n        {\n            "_index" : "test",\n            "_id" : "2"\n        }\n    ]\n}\n#uri中指定index\nget /test/_mget\n{\n    "docs" : [\n        {\n\n            "_id" : "1"\n        },\n        {\n\n            "_id" : "2"\n        }\n    ]\n}\nget /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1",\n            "_source" : false\n        },\n        {\n            "_index" : "test",\n            "_id" : "2",\n            "_source" : ["field3", "field4"]\n        },\n        {\n            "_index" : "test",\n            "_id" : "3",\n            "_source" : {\n                "include": ["user"],\n                "exclude": ["user.location"]\n            }\n        }\n    ]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n\n# msearch 批量查询\n\npost kibana_sample_data_ecommerce/_msearch\n{}\n{"query" : {"match_all" : {}},"size":1}\n{"index" : "kibana_sample_data_flights"}\n{"query" : {"match_all" : {}},"size":2}\n\n\n1\n2\n3\n4\n5\n\n\n\n# mget与msearch的区别\n\nmget是通过文档id列表得到文档信息，而msearch是根据查询条件，搜索到相应文档。\n\n\n# 单次批量操作注意事项\n\n单次批量操作，数据量不宜过大，以免引发性能问题。\n\n一般建议是1000-5000个文档，如果文档很大，可以适当减少队列，大小建议为5-15mb，默认不能超过100m，会报错。\n\n\n# search api\n\n返回体中的字段意思\n\ntook      花费的时间\ntotal     符合条件的总文档数\nhints     结果集，默认是前10个文档\n_index    索引名\n_id       文档的id\n_score    相关度评分\n_source   文档原始信息\n\n\n# uri search\n\n也就是在url中使用查询参数\n\n\n# uri search基本查询\n\n#基本查询\nget /movies/_search?q=2012&df=title&sort=year:desc&from=0&size=10&timeout=1s\n{\n  "profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n * q指定查询语句，使用query string syntax\n * df默认字段，是指查询的字段filed，不指定时就是对所有字段\n * sort排序\n * from 和size用于分页\n * profile可以查看查询是如何被执行的（查看使用了什么查询）\n\n\n# uri search指定字段查询\n\nget /movies/_search?q=2012&df=title\n{\n\t"profile":"true"\n}\n#或者没有df字段，使用title:2012的方式\nget /movies/_search?q=title:2012\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n就是查询title字段是2012的文档\n\n\n# uri search泛查询\n\n#泛查询，正对_all,所有字段\nget /movies/_search?q=2012\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# uri search term查询\n\n简而言之就是指定了term单词的查询，和上面指定了title是2012就是term query。和上面的指定字段查询类似\n\nget /movies/_search?q=title:2012\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n\n# uri search phrase查询\n\n也就是带多个单词，词组的查询。这里要考虑要引号和括号(分组)的不同含义。\n\n# 不加引号和括号\n\n例如查找美丽心灵的影片，使用q=title:beautiful mind，通过profile的查询过程可以看出使用的是booleanquery，等效于tile字段中包含beatiful，其他字段(也包含title字段)。\n\nget /movies/_search?q=title:beautiful mind\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 加上引号\n\n"beautiful mind",等效于beautiful and mind.这是phrasequery，在查询的时候要求这两个单词要同时出现，并且还要求前后的顺序保持一致。\n\n#使用引号，phrase查询\nget /movies/_search?q=title:"beautiful mind"\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\n# 加上括号\n\n和前面不加上括号的相比，这是布尔查询，意思是在title字段中，或者出现beautiful或者出现mind，或者都可以出现。\n\n#分组，bool查询\nget /movies/_search?q=title:(beautiful mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\n# 布尔操作\n\n在单词之间加入and/or/not或者&& / || !\n\n必须要大写\n\nand：意思是必须包含beautiful和mind，但是对之间的顺序没有要求\n\n# 查找美丽心灵\nget /movies/_search?q=title:(beautiful and mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\nor：意思是或者包含beautiful，或者包含mind，或者包含这两个，但是对这两个词的顺序没有要求\n\nget /movies/_search?q=title:(beautiful or mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\nnot：意思是包含beautiful，不包含mind\n\nget /movies/_search?q=title:(beautiful not mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 分组\n\n+：表示must，也就是一定要有，must后面跟的单词是必须要存在的。\n\n%2b：表示的是+的意思。\n\n-：表示must_not，也就是后面跟的单词一定要不存在。\n\n注意：\n\n单独使用+的时候，没有任何含义，和没有+一个意思。\n\n单独使用-的时候 ，是有意义的。\n\nget /movies/_search?q=title:(+beautiful -mind)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 范围查询\n\n区间表示：[]闭区间，{}开区间\n\n * year:{2019 to 2018]\n * year:[* to 2018]\n * year:[2002 to 2018%7d 等同于[2002 to 2018}\n\nget /movies/_search?q=title:beautiful and year:[2002 to 2018]\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 算数符号\n\n * year:>2010\n * year:(>2010 && <=2018)\n * year:(+>2010 +<=2018)\n\nget /movies/_search?q=year:(>2010 and  <=2018)\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 通配符查询\n\n通配符查询(通配符查询效率低，占用内存大，不建议使用。特别是放在最前面)\n\n? 代表1个字符，* 代表0或多个字符\n\n * title:mi?d\n * title:be*\n\n#通配符查询\nget /movies/_search?q=title:b*\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n5\n\n\n# 模糊匹配\n\ntitle:befutifl~1\n\n就是指当有单词输入错误的时候，做一个模糊的匹配\n\nget /movies/_search?q=title:beautifl~1\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n# 近似匹配\n\n当输入的多个单词的时候，不要求他们一定要保证相应的顺序出现，但是两个都必须存在。\n\nget /movies/_search?q=title:"lord rings"~2\n{\n\t"profile":"true"\n}\n\n\n1\n2\n3\n4\n\n\n\n# request body search\n\n使用的es内置的，也就是说在查询体中带上json格式，使用特定的dsl的语言方法来查询，使用最为广泛。\n\n分页查询/排序/source过滤字段/脚本字段，这些配置都是在定义query的查询体的上面来定义的。\n\n\n# 分页查询\n\n在不添加任何"from"和"size"的时候，from默认是从0开始，size是10。也就是从每个主分片上捞取from+size的数据汇聚，然后捞取前10条数据返回。\n\n"from"的意思就是定义了要查询数据的起始位置，如果from是10，那么就是说从第10条开始数据返回。\n\n"size"的意思是起始位置开始要返回的数据量的大小，如果size是10，那么就要从from的起始位置开始，返回10条数据。\n\n获取靠后的翻页成本较高。\n\npost /kibana_sample_data_ecommerce/_search\n{\n  "from":10,\n  "size":20,\n  "query":{\n    "match_all": {}\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 排序\n\n最好在"数字型"与"日期型"字段上排序\n\n因为对于多值类型或分析过的字段排序，系统会选一个值，无法得知该值。\n\n#对日期排序\npost kibana_sample_data_ecommerce/_search\n{\n  "sort":[{"order_date":"desc"}],\n  "query":{\n    "match_all": {}\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# source字段过滤\n\n就是针对查询出来的结果过滤到相应的字段的查询。\n\n如果_source没有存储，那就只返回匹配的文档的元数据\n\n_source支持使用通配符，例如_source["name*","desc*"]\n\n#source filtering\npost kibana_sample_data_ecommerce/_search\n{\n  "_source":["order_date"],\n  "query":{\n    "match_all": {}\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 脚本字段\n\n针对于当订单中有不同的汇率，需要结合汇率对订单价格进行排序的情况。\n\nget kibana_sample_data_ecommerce/_search\n{\n  "script_fields": {\n    "new_field": {\n      "script": {\n        "lang": "painless",\n        "source": "doc[\'order_date\'].value+\'hello\'"\n      }\n    }\n  },\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 全文查询\n\n# match_all的查询\n\n在query下面使用match_all来查询，默认返回10条数据。\n\n#ignore_unavailable=true，可以忽略尝试访问不存在的索引“404_idx”导致的报错\n#查询movies分页\npost /movies,404_idx/_search?ignore_unavailable=true\n{\n  "profile": true,\n\t"query": {\n\t\t"match_all": {}\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# match的查询\n\n查询单个单词的时候像是uri search中的term search，如果查询多个单词的时候又有点像uri search中的phrase search。\n\n#使用查询表达式 match，默认是or的方式\npost movies/_search\n{\n  "query": {\n    "match": {\n      "title": "last christmas"\n    }\n  }\n}\n可以使用and的方式\npost movies/_search\n{\n  "query": {\n    "match": {\n      "title": {\n        "query": "last christmas",\n        "operator": "and"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# match phrase查询\n\n专门用于短语的搜索，感觉功能上比match query要更灵活一些。\n\n#加入了slop的时候，就是允许在这些短语的中间加入一个其他的单词\npost movies/_search\n{\n  "query": {\n    "match_phrase": {\n      "title":{\n        "query": "one love",\n        "slop": 1\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# query string query\n\nquery string query和下面的simple query string query使用场景比较少。\n\nquery string 比simple query string使用起来更加灵活。\n\nquery string 有点类似于uri query\n\n#里面也有和uri search中的df字段\npost users/_search\n{\n  "query": {\n    "query_string": {\n      "default_field": "name",\n      "query": "ruan and yiming"\n    }\n  }\n}\n#也支持多个字段的查询，和uri search中一样，括号代表分组\npost users/_search\n{\n  "query": {\n    "query_string": {\n      "fields":["name","about"],\n      "query": "(ruan and yiming) or (java and elasticsearch)"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# simple query string query\n\n * 类似query string，但是会忽略错误的语法，同时只支持部分查询语法\n * 不支持and or not，会当作字符串处理\n * term之间默认的关系是or，可以指定operator\n * 支持部分逻辑\n   * +代替and\n   * |代替or\n   * -代替not\n\n#simple query 默认的operator是 or\npost users/_search\n{\n  "query": {\n    "simple_query_string": {\n      "query": "ruan and yiming",\n      "fields": ["name"]\n    }\n  }\n}\npost users/_search\n{\n  "query": {\n    "simple_query_string": {\n      "query": "ruan yiming",\n      "fields": ["name"],\n      "default_operator": "and"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 精确值查询\n\n结构化数据，日期，数字，布尔，term 。。。。\n\n# term query\n\n可以理解为是对精确值的查询。\n\n但是term 查询的特点是不会对查询所带的字段进行分词处理，而索引文档中的text类型的字段值，在输入的时候会对text类型进行分词，小写化处理。\n\n要不查询的时候，字段值全用小写；要么使用text的keyword来进行精确匹配，也就是查询的时候输入插入的时候的值的大小写。\n\n# 改为小写查询，如果多个单词在一起的，使用起来还是要注意\npost /products/_search\n{\n  "query": {\n    "term": {\n      "desc": {\n        //"value": "iphone"\n        "value":"iphone"\n      }\n    }\n  }\n}\n#使用keyword进行精确匹配\npost /products/_search\n{\n  //"explain": true,\n  "query": {\n    "term": {\n      "productid.keyword": {\n        "value": "xhdk-a-1293-#fj3"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# constant score转为filter\n\n将query转为filter过滤，忽略掉了tf-idf的算分过程，避免了相关性算分的开销。并且filter可以有效利用缓存\n\npost /products/_search\n{\n  "explain": true,\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "productid.keyword": "xhdk-a-1293-#fj3"\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n# 布尔值\n\n#对布尔值 match 查询，有算分\npost products/_search\n{\n  "profile": "true",\n  "explain": true,\n  "query": {\n    "term": {\n      "avaliable": {\n        "value": "false"\n      }\n    }\n  }\n}\n#一般对布尔值查询会结合上面的过滤到算法\n#对布尔值，通过constant score 转成 filtering，没有算分\npost products/_search\n{\n  "profile": "true",\n  "explain": true,\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "avaliable": {\n            "value": "false"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n# 查找多个精确值\n\n#字符类型 terms\npost products/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "terms": {\n          "productid.keyword": [\n            "qqpx-r-3956-#ad8",\n            "jodl-x-1937-#pv7"\n          ]\n        }\n      }\n    }\n  }\n}\n#处理多值字段，term 查询是包含，而不是等于\n#针对的是这个term字段的值是一个数值，而并非是唯一的值的情况下\n#可以通过添加一个genre count字段对每个文档的该字段的数组里面的值多个统计\n#在查询的时候，通过去定义这个字段的应该有的值的个数去精确匹配\npost movies/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "genre.keyword": "comedy"\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n# range\n\n# 数字range\n\n#数字 range 查询\nget products/_search\n{\n    "query" : {\n        "constant_score" : {\n            "filter" : {\n                "range" : {\n                    "price" : {\n                        "gte" : 20,\n                        "lte"  : 30\n                    }\n                }\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\ngt大于\n\nlt小于\n\ngte大于等于\n\nlte小于等于\n\n# 日期range\n\n# 日期 range\npost products/_search\n{\n    "query" : {\n        "constant_score" : {\n            "filter" : {\n                "range" : {\n                    "date" : {\n                      "gte" : "now-1y"\n                    }\n                }\n            }\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\ny 年、m 月、w 周、d 天、h或h 小时、m 分钟、s 秒\n\n\n# exists查询\n\n使用exist查询来处理非空null值\n\n#查询存在相应列的文档\n#exists查询\npost products/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "exists": {\n          "field": "date"\n        }\n      }\n    }\n  }\n}\n#查询不存在相应列的文档\npost products/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "bool": {\n          "must_not": {\n            "exists": {\n              "field": "date"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n\n# bool查询\n\nbool查询是复合查询。must / should是query context贡献算分；must_not / filter是filter context不贡献算分。\n\n子查询可以任意顺序出现；\n\n可以嵌套多个查询；\n\n在bool查询中，没有must条件的时候，should中必须至少满足一条查询。\n\n#基本语法\npost /products/_search\n{\n  "query": {\n    "bool" : {\n      "must" : {\n        "term" : { "price" : "30" }\n      },\n      "filter": {\n        "term" : { "avaliable" : "true" }\n      },\n      "must_not" : {\n        "range" : {\n          "price" : { "lte" : 10 }\n        }\n      },\n      "should" : [\n        { "term" : { "productid.keyword" : "jodl-x-1937-#pv7" } },\n        { "term" : { "productid.keyword" : "xhdk-a-1293-#fj3" } }\n      ],\n      "minimum_should_match" :1\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 解决term query包含而不是相等问题\n\n针对term查询的字段，如果是一个数组类型的时候，term查询是包含而不是相等的问题，通过bool查询来解决，需要新增一个数组里面值的个数的字段。\n\n#must，有算分\npost /newmovies/_search\n{\n  "query": {\n    "bool": {\n      "must": [\n        {"term": {"genre.keyword": {"value": "comedy"}}},\n        {"term": {"genre_count": {"value": 1}}}\n\n      ]\n    }\n  }\n}\n#filter。不参与算分，结果的score是0\npost /newmovies/_search\n{\n  "query": {\n    "bool": {\n      "filter": [\n        {"term": {"genre.keyword": {"value": "comedy"}}},\n        {"term": {"genre_count": {"value": 1}}}\n        ]\n\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n# bool嵌套查询\n\n#嵌套，实现了 should not 逻辑\npost /products/_search\n{\n  "query": {\n    "bool": {\n      "must": {\n        "term": {\n          "price": "30"\n        }\n      },\n      "should": [\n        {\n          "bool": {\n            "must_not": {\n              "term": {\n                "avaliable": "false"\n              }\n            }\n          }\n        }\n      ],\n      "minimum_should_match": 1\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# 改变相关度算分\n\n通过嵌套的bool查询中的问题，来改变相关度的算分的问题。\n\n在同一层级下竞争字段，具有相同的权重；通过嵌套bool查询，可以改变对算分的影响。\n\n#同级查询\npost /animals/_search\n{\n  "query": {\n    "bool": {\n      "should": [\n        { "term": { "text": "brown" }},\n        { "term": { "text": "red" }},\n        { "term": { "text": "quick"   }},\n        { "term": { "text": "dog"   }}\n      ]\n    }\n  }\n}\n#嵌套多级查询\npost /animals/_search\n{\n  "query": {\n    "bool": {\n      "should": [\n        { "term": { "text": "quick" }},\n        { "term": { "text": "dog"   }},\n        {\n          "bool":{\n            "should":[\n               { "term": { "text": "brown" }},\n                 { "term": { "text": "brown" }},\n            ]\n          }\n\n        }\n      ]\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n\n# boosting query\n\n# 控制字段的boosting\n\n在bool查询中，通过指定boost的值来实现影响算分，控制相关度。\n\n参数boost的含义：当boost>1的时候，打分的相关度相对性提升；当0<boost<1的时候，打分的权重相对性降低;当boost<0的时候，贡献负分。\n\npost blogs/_search\n{\n  "query": {\n    "bool": {\n      "should": [\n        {"match": {\n          "title": {\n            "query": "apple,ipad",\n            "boost": 1.1\n          }\n        }},\n        {"match": {\n          "content": {\n            "query": "apple,ipad",\n            "boost": 2\n          }\n        }}\n      ]\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# not quite not\n\n意思是查询的时候，正确的值要第一个返回(例如这里我要求查询的苹果公司产品要优先出现)，其他相关的(苹果汁，苹果派的)等信息也要返回，但是是靠后面返回。\n\n提高了查准率和查全率。\n\n使用了boosting query的方式来实现。positive的值是对查询算法加分的，negative的值是对查询算分减分的，negative_boost是指减多少分。\n\npost news/_search\n{\n  "query": {\n    "boosting": {\n      "positive": {\n        "match": {\n          "content": "apple"\n        }\n      },\n      "negative": {\n        "match": {\n          "content": "pie"\n        }\n      },\n      "negative_boost": 0.5\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# disjunction max query\n\ndisjunction max query应用于单字符串多字段查询的场景，很多搜索引擎都是这样的。允许你只是输入一个字符串，要多所有的字段进行查询。\n\npost blogs/_search\n{\n    "query": {\n        "dis_max": {\n            "queries": [\n                { "match": { "title": "quick pets" }},\n                { "match": { "body":  "quick pets" }}\n            ]\n        }\n    }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 使用tie breaker参数\n\n有一些情况下，同时匹配title和body字段的文档比只与一个字段匹配的文档的相关度更高。而disjunction max query 查询只会简单地使用单个最佳匹配语句的评分_score作为整体评分。这个时候，可以使用tie_breaker参数来调整。\n\n * 获取最佳匹配语句的评分_score;\n * 将其他匹配语句的评分与tie_breaker相乘；\n * 对以上评分求和并规范化。\n * tier breaker是一个介于0-1之间的浮点数。0代表使用最佳匹配；1代表所有语句同等重要。\n\npost blogs/_search\n{\n    "query": {\n        "dis_max": {\n            "queries": [\n                { "match": { "title": "quick pets" }},\n                { "match": { "body":  "quick pets" }}\n            ],\n            "tie_breaker": 0.2\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# multi match\n\n也是运用于单字符串多字段的场景下查询。\n\nbest_fields的场景，最佳字段，单字段之间相互竞争，又相互关联的时候。\n\npost blogs/_search\n{\n  "query": {\n    "multi_match": {\n      "type": "best_fields",\n      "query": "quick pets",\n      "fields": ["title","body"],\n      "tie_breaker": 0.2,\n      "minimum_should_match": "20%"\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nmost fields:多数字段的场景\n\nput /titles\n{\n  "mappings": {\n    "properties": {\n      "title": {\n        "type": "text",\n        "analyzer": "english",\n        "fields": {"std": {"type": "text","analyzer": "standard"}}\n      }\n    }\n  }\n}\nget /titles/_search\n{\n   "query": {\n        "multi_match": {\n            "query":  "barking dogs",\n            "type":   "most_fields",\n            "fields": [ "title", "title.std" ]\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\ncross_fields:用于跨字段的查询\n\nget address/_search\n{\n   "query": {\n        "multi_match": {\n            "query":  "poland street w1v",\n            "type":   "cross_fields",\n            "operator": "and",\n            "fields": [ "street", "city","country" ]\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 高亮显示\n\npost tmdb/_search\n{\n  "_source": ["title","overview"],\n  "query": {\n    "multi_match": {\n      "query": "basketball with cartoon aliens",\n      "fields": ["title","overview"]\n    }\n  },\n  "highlight" : {\n        "fields" : {\n            "overview" : { "pre_tags" : ["\\\\033[0;32;40m"], "post_tags" : ["\\\\033[0m"] },\n            "title" : { "pre_tags" : ["\\\\033[0;32;40m"], "post_tags" : ["\\\\033[0m"] }\n\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# search template\n\nes中的search template是用来解耦程序的开发和es搜索dsl的\n\npost _scripts/tmdb\n{\n  "script": {\n    "lang": "mustache",\n    "source": {\n      "_source": [\n        "title","overview"\n      ],\n      "size": 20,\n      "query": {\n        "multi_match": {\n          "query": "{{q}}",\n          "fields": ["title","overview"]\n        }\n      }\n    }\n  }\n}\n#调用template\npost tmdb/_search/template\n{\n    "id":"tmdb",\n    "params": {\n        "q": "basketball with cartoon aliens"\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# suggester api\n\n * 搜索引擎中类似的功能，在es中是通过suggester api实现的。\n * 原理：将输入的文本分解为token，然后在搜索的字典里查找相似的term并返回。\n * 根据不同的使用场景，es设计了4种类别的suggesters\n   * term & phrase suggester\n   * complete & context suggester\n\n\n# term suggester\n\n有三种suggestion mode:\n\n * missing：如索引中已经存在，就不提供建议\n * popular：推荐出现频率更加高的词\n * always：无论是否存在，都提供建议。\n\n每一个建议都包含一个算分，相似性是通过levenshtein edit distance的算法实现的。核心思想就是一个词改动多少字符就可以和另外一个词一致。提供了很多可选参数来控制相似性的模糊程度。例如"max_edits"\n\n\n# missing mode\n\nsuggester就是一个特殊类型的搜索。"text"里是调用时候提供的文本，通常来自于用户界面上用户输入的内容。\n\n用户输入的"lucen"是一个错误的拼写。\n\n回到指定的字段"body"上搜索，当无法搜索到结果时(missing)，返回建议的词。\n\npost /articles/_search\n{\n  "size": 1,\n  "query": {\n    "match": {\n      "body": "lucen rock"\n    }\n  },\n  "suggest": {\n    "term-suggestion": {\n      "text": "lucen rock",\n      "term": {\n        "suggest_mode": "missing",\n        "field": "body"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# popular mode\n\n推荐出现频率更多高的词\n\npost /articles/_search\n{\n\n  "suggest": {\n    "term-suggestion": {\n      "text": "lucen rock",\n      "term": {\n        "suggest_mode": "popular",\n        "field": "body"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# always mode\n\n无论是否存在，都提供建议\n\npost /articles/_search\n{\n\n  "suggest": {\n    "term-suggestion": {\n      "text": "lucen rock",\n      "term": {\n        "suggest_mode": "always",\n        "field": "body",\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nsorting by frequentcy和prefix length\n\n默认按照score排序，也可以按照"frequency"\n\n默认首字母不一致就不会匹配推荐，但是如果将prefix_length设置为0，就会为hock建议rock\n\npost /articles/_search\n{\n\n  "suggest": {\n    "term-suggestion": {\n      "text": "lucen hocks",\n      "term": {\n        "suggest_mode": "always",\n        "field": "body",\n        "prefix_length":0,\n        "sort": "frequency"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# phrase suggester\n\nphrase suggester在term suggester上增加了一些额外的逻辑。\n\n例如下面的参数：\n\n * suggest mode: missing，popular，always\n * max errors：最多可以拼错的terms数\n * confidence：限制返回结果数，默认是1\n\npost /articles/_search\n{\n  "suggest": {\n    "my-suggestion": {\n      "text": "lucne and elasticsear rock hello world ",\n      "phrase": {\n        "field": "body",\n        "max_errors":2,\n        "confidence":0,\n        "direct_generator":[{\n          "field":"body",\n          "suggest_mode":"always"\n        }],\n        "highlight": {\n          "pre_tag": "<em>",\n          "post_tag": "</em>"\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# completion suggester\n\n * completion suggester提供了"自动完成"(auto complete)的功能。用户每输入一个字符，就需要即时发送一个查询请求到后端查找匹配项。\n\n * 对性能要求比较苛刻。es采用了不同的数据结构，并非通过倒排索引来完成的。而是将analyze的数据编码成fst和索引一起存放。fst会被es整个加载进内存，速度很快。\n\n * fst只能用于前缀查找\n\n\n# 使用自动补全suggester步骤\n\n定义mapping，使用"completion" type\n\n索引数据\n\n运行"suggest"查询，得到搜索建议\n\n#定义mapping\nput articles\n{\n  "mappings": {\n    "properties": {\n      "title_completion":{\n        "type": "completion"\n      }\n    }\n  }\n}\n#索引数据\npost articles/_bulk\n{ "index" : { } }\n{ "title_completion": "lucene is very cool"}\n{ "index" : { } }\n{ "title_completion": "elasticsearch builds on top of lucene"}\n{ "index" : { } }\n{ "title_completion": "elasticsearch rocks"}\n{ "index" : { } }\n{ "title_completion": "elastic is the company behind elk stack"}\n{ "index" : { } }\n{ "title_completion": "elk stack rocks"}\n{ "index" : {} }\n#使用自动补全\npost articles/_search?pretty\n{\n  "size": 0,\n  "suggest": {\n    "article-suggester": {\n      "prefix": "elk ",\n      "completion": {\n        "field": "title_completion"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n\n# context suggester\n\n上下文建议搜索\n\ncontext suggester是completion suggester的扩展，\n\n可以在搜索中加入更多的上下文信息，例如，输入"star"\n\n * 咖啡相关：建议"starbucks"\n * 电影相关的："star wars"\n\n\n# 实现上下文suggester\n\n * 可以定义两种类型的context\n   * category 任意的字符串\n   * geo 地理位置信息\n * 实现context suggester的具体步骤\n   * 定制一个mapping\n   * 索引数据，并且为每个文档加入context信息\n   * 结合context进行suggestion查询\n\n#定义mapping\nput comments/_mapping\n{\n  "properties": {\n    "comment_autocomplete":{\n      "type": "completion",\n      "contexts":[{\n        "type":"category",\n        "name":"comment_category"\n      }]\n    }\n  }\n}\n#插入数据\npost comments/_doc\n{\n  "comment":"i love the star war movies",\n  "comment_autocomplete":{\n    "input":["star wars"],\n    "contexts":{\n      "comment_category":"movies"\n    }\n  }\n}\npost comments/_doc\n{\n  "comment":"where can i find a starbucks",\n  "comment_autocomplete":{\n    "input":["starbucks"],\n    "contexts":{\n      "comment_category":"coffee"\n    }\n  }\n}\n#带上下文搜索\npost comments/_search\n{\n  "suggest": {\n    "my_suggestion": {\n      "prefix": "sta",\n      "completion":{\n        "field":"comment_autocomplete",\n        "contexts":{\n          "comment_category":"coffee"\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n\n\n\n# metric aggregation\n\n单值分析：只输出一个分析结果\n\n * min,max,avg,sum\n * cardinality (类似于distincct count)\n\n多值分析：输出多个分析结果\n\n * stats, exended stats\n * percentile, percentile rank\n * top hits (排在前面的示例)\n\n# 多个 metric 聚合，找到最低最高和平均工资\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "max_salary": {\n      "max": {\n        "field": "salary"\n      }\n    },\n    "min_salary": {\n      "min": {\n        "field": "salary"\n      }\n    },\n    "avg_salary": {\n      "avg": {\n        "field": "salary"\n      }\n    }\n  }\n}\n# 一个聚合，输出多值\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "stats_salary": {\n      "stats": {\n        "field":"salary"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# bucket aggregation\n\n按照一定的规则，将文档分配到不同的桶中，从而达到分类的目的。es提供的一些常见的bucket aggregation。\n\n * terms\n * 数字类型，有range/data range，或者histogram / data histogram\n * 支持嵌套：也就是在桶里再做分桶\n\n\n# terms aggregation\n\n在对term进行聚合的时候，如果是text类型。有两种方法，要么指定keyword字段，要么就需要打开fieldata，但是fieldata会查找出对应分词后的term。才可以进行terms aggregation\n\n * keyword默认支持doc_values\n * text需要咋mapping中enable，会按照分词后的结果进行分\n\n# 对 text 字段进行 terms 聚合查询，失败\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field":"job"\n      }\n    }\n  }\n}\n# 对keword 进行聚合\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field":"job.keyword"\n      }\n    }\n  }\n}\n# 对 text 字段打开 fielddata，支持terms aggregation\nput employees/_mapping\n{\n  "properties" : {\n    "job":{\n       "type":     "text",\n       "fielddata": true\n    }\n  }\n}\n# 对 text 字段进行 terms 分词。分词后的terms,结果和上面指定keyword的方式是不一样的，这是分词后的结果\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field":"job"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n# cardinality\n\n类似于sql中的distinct\n\n# 对job.keyword 和 job 进行 terms 聚合，分桶的总数并不一样\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "cardinate": {\n      "cardinality": {\n        "field": "job"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 指定size分桶\n\n#指定 bucket 的 size\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "ages_5": {\n      "terms": {\n        "field":"age",\n        "size":3\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 嵌套聚合\n\n# 指定size，不同工种中，年纪最大的3个员工的具体信息\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field":"job.keyword"\n      },\n      "aggs":{\n        "old_employee":{\n          "top_hits":{\n            "size":3,\n            "sort":[\n              {\n                "age":{\n                  "order":"desc"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n# 优化terms聚合性能\n\n通过指定mapping中，打开eager_global_ordinals为true，这会让索引数据的时候就开始对数据文档进行预先处理，这样对于要求频繁使用聚合的场景，或者说是要对聚合实时数据有更高的性能的时候来使用。\n\nput index\n{\n  "mappings": {\n    "properties": {\n      "foo": {\n        "type": "keyword",\n        "eager_global_ordinals": true\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# range 聚合\n\n自己自定义range\n\n#salary ranges 分桶，可以自己定义 key\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "salary_range": {\n      "range": {\n        "field":"salary",\n        "ranges":[\n          {\n            "to":10000\n          },\n          {\n            "from":10000,\n            "to":20000\n          },\n          {\n            "key":">20000",\n            "from":20000\n          }\n        ]\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# histogram聚合\n\n通过指定分桶的间隔，来进行分桶\n\n#salary histogram,工资0到10万，以 5000一个区间进行分桶\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "salary_histrogram": {\n      "histogram": {\n        "field":"salary",\n        "interval":5000,\n        "extended_bounds":{\n          "min":0,\n          "max":100000\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# pipeline 聚合分析\n\npipeline管道的概念：支持对聚合分析的结果，再次进行聚合分析。\n\npipeline的分析结果会输出到原结果中，根据位置的不同，分为两类：\n\n * sibling 结果和现有分析结果同级\n   * max，min，avg & sum bucket\n   * stats , extended status buccket\n   * percentiles bucket\n * parent 结果内嵌到现有的聚合分析结果之中\n   * derivative (求导)\n   * cumultive sum（累计求和）\n   * moving function (滑动窗口)\n\n\n# sibling pipeline\n\n查看到平均工资最低的工作类型\n\n# 平均工资最低的工作类型\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field": "job.keyword",\n        "size": 10\n      },\n      "aggs": {\n        "avg_salary": {\n          "avg": {\n            "field": "salary"\n          }\n        }\n      }\n    },\n    "min_salary_by_job":{\n      "min_bucket": {\n        "buckets_path": "jobs>avg_salary"\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n平均工资的统计分析\n\n# 平均工资的统计分析\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "jobs": {\n      "terms": {\n        "field": "job.keyword",\n        "size": 10\n      },\n      "aggs": {\n        "avg_salary": {\n          "avg": {\n            "field": "salary"\n          }\n        }\n      }\n    },\n    "stats_salary_by_job":{\n      "stats_bucket": {\n        "buckets_path": "jobs>avg_salary"\n      }\n    }\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# parent pipeline\n\n按照年龄，对工资进行求导(看工资发展的趋势)\n\n#按照年龄对平均工资求导\npost employees/_search\n{\n  "size": 0,\n  "aggs": {\n    "age": {\n      "histogram": {\n        "field": "age",\n        "min_doc_count": 1,\n        "interval": 1\n      },\n      "aggs": {\n        "avg_salary": {\n          "avg": {\n            "field": "salary"\n          }\n        },\n        "derivative_avg_salary":{\n          "derivative": {\n            "buckets_path": "avg_salary"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"Harbor",frontmatter:{title:"Harbor",date:"2022-01-20T11:42:16.000Z",permalink:"/pages/0765a4/",categories:["中间件","K8S"],tags:[null]},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.K8S/03.Harbor%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86.html",relativePath:"02.中间件/01.K8S/03.Harbor镜像管理.md",key:"v-3f34187e",path:"/pages/0765a4/",headers:[{level:2,title:"Harbor架构解析",slug:"harbor架构解析",normalizedTitle:"harbor架构解析",charIndex:17},{level:3,title:"Harbor介绍",slug:"harbor介绍",normalizedTitle:"harbor介绍",charIndex:32},{level:3,title:"Harbor核心组件",slug:"harbor核心组件",normalizedTitle:"harbor核心组件",charIndex:217},{level:3,title:"Harbor和registry的比较",slug:"harbor和registry的比较",normalizedTitle:"harbor和registry的比较",charIndex:590},{level:3,title:"简略架构图",slug:"简略架构图",normalizedTitle:"简略架构图",charIndex:956},{level:2,title:"系统环境及部署准备",slug:"系统环境及部署准备",normalizedTitle:"系统环境及部署准备",charIndex:968},{level:3,title:"增加主机名称解析",slug:"增加主机名称解析",normalizedTitle:"增加主机名称解析",charIndex:982},{level:3,title:"主机时间同步",slug:"主机时间同步",normalizedTitle:"主机时间同步",charIndex:1047},{level:3,title:"关闭防火墙服务",slug:"关闭防火墙服务",normalizedTitle:"关闭防火墙服务",charIndex:1540},{level:3,title:"关闭并禁用SElinux",slug:"关闭并禁用selinux",normalizedTitle:"关闭并禁用selinux",charIndex:1708},{level:3,title:"系统内核参数修改",slug:"系统内核参数修改",normalizedTitle:"系统内核参数修改",charIndex:1796},{level:3,title:"部署docker环境",slug:"部署docker环境",normalizedTitle:"部署docker环境",charIndex:3132},{level:2,title:"部署操作",slug:"部署操作",normalizedTitle:"部署操作",charIndex:3168},{level:3,title:"docker-compose安装",slug:"docker-compose安装",normalizedTitle:"docker-compose安装",charIndex:3177},{level:3,title:"harbor服务搭建",slug:"harbor服务搭建",normalizedTitle:"harbor服务搭建",charIndex:3602},{level:3,title:"执行install.sh脚本",slug:"执行install-sh脚本",normalizedTitle:"执行install.sh脚本",charIndex:4045},{level:3,title:"Harbor登录及使用",slug:"harbor登录及使用",normalizedTitle:"harbor登录及使用",charIndex:4755},{level:2,title:"常见操作命令",slug:"常见操作命令",normalizedTitle:"常见操作命令",charIndex:4813},{level:3,title:"Harbor服务的启停",slug:"harbor服务的启停",normalizedTitle:"harbor服务的启停",charIndex:4824},{level:3,title:"Harbor各个服务的端口",slug:"harbor各个服务的端口",normalizedTitle:"harbor各个服务的端口",charIndex:5142},{level:3,title:"修改docker私有仓库地址配置",slug:"修改docker私有仓库地址配置",normalizedTitle:"修改docker私有仓库地址配置",charIndex:5356},{level:3,title:"上传/下载仓库镜像",slug:"上传-下载仓库镜像",normalizedTitle:"上传/下载仓库镜像",charIndex:5866},{level:3,title:"仓库进行同步",slug:"仓库进行同步",normalizedTitle:"仓库进行同步",charIndex:6242}],headersStr:"Harbor架构解析 Harbor介绍 Harbor核心组件 Harbor和registry的比较 简略架构图 系统环境及部署准备 增加主机名称解析 主机时间同步 关闭防火墙服务 关闭并禁用SElinux 系统内核参数修改 部署docker环境 部署操作 docker-compose安装 harbor服务搭建 执行install.sh脚本 Harbor登录及使用 常见操作命令 Harbor服务的启停 Harbor各个服务的端口 修改docker私有仓库地址配置 上传/下载仓库镜像 仓库进行同步",content:"# Harbor镜像管理\n\n\n# Harbor架构解析\n\n\n# Harbor介绍\n\nVmware公司开源的企业级容器registry项目harbor，是由vmware中国研发团队负责开发。Harbor可以帮助用户迅速搭建企业级的registry服务，它提供了管理图形界面，基于角色的访问控制RBAC，镜像远程复制（同步），AD/LDAP集成、以及审计日志等企业用户需求的功能，同时还原生支持中文和英文，深受国内外用户的喜爱。\n\n\n# Harbor核心组件\n\nProxy: 是一个nginx的前端代理，代理harbor的registry，UI，token等服务。\n\nDB: 负责存储用户权限、审计日志、dockerimage分组信息等数据。\n\nJobservice: jobservice是负责镜像复制工作的，和registry通信，从一个registry pull镜像然后push到另一个registry，并记录job_log。\n\nCore: harbor的核心组件，主要提供权限管理、审计、管理界面UI、token service以及可供其他系统调用的API等功能。\n\nLog: 为了帮助监控harbor运行，负责收集其他组件的log。\n\nRedis: 用于存储session。\n\n注意：新版本1.8中新增了registryctl/ harbor-portal等服务。\n\n\n# Harbor和registry的比较\n\nHarbor和registry都是docker的镜像仓库，相比较于registry，有很多的优势。\n\n 1. 提供分层传输机制，优化网络传输\n\nDocker 镜像是分层的，如果每次每次传输都使用全量文件，不经济。必须提供识别分层传输的机制，以层的UUID未标识，确定传输的对象。\n\n 2. 提供WEB界面，优化用户体验\n\n只用镜像的名字来进行上传下载很不方便，需要有一个用户界面可以支持登录、搜索功能，包括区分公有、私有镜像。\n\n 3. 支持水平扩展集群\n\n当有用户对镜像的上传下载操作集中在某服务器，需要对相应的访问压力作分解。\n\n 4. 良好的安全机制\n\nRegistry缺少认证机制，任何人基于不同角色的访问控制机制。\n\n 5. 定时清理\n\nRegistry缺乏镜像清理机制。\n\n\n# 简略架构图\n\n\n\n\n# 系统环境及部署准备\n\n\n# 增加主机名称解析\n\n192.168.3.58 server58\n192.168.3.59 server59\n\n\n1\n2\n\n\n\n# 主机时间同步\n\nNtp server配置\n# vi /etc/ntp.conf\n注释掉下面行，并添加下面两行\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\nserver 127.127.1.0\nfudge 127.127.1.0 stratum 11\nNtp Client配置\n注释掉下面行，并添加下面一行\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\nserver 192.168.3.58\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 关闭防火墙服务\n\n先事先关闭所有主机上的iptables或firewalld服务：\n#systemctl stop firewalld \n#systemctl stop iptables\n#systemctl disable firewalld\n#systemctl disable iptables\n\n\n1\n2\n3\n4\n5\n\n\n\n# 关闭并禁用SElinux\n\n#vi /etc/selinux/config\n修改SELINUX配置\nSELINUX=disabled\n重启后生效\n\n\n1\n2\n3\n4\n\n\n\n# 系统内核参数修改\n\n#vi /etc/sysctl.conf \n#add by common\nfs.aio-max-nr = 1048576\nfs.file-max = 6815744\nkernel.shmall = 2097152\nkernel.shmmax = 536870912\nkernel.shmmni = 4096\nkernel.sem = 250 32000 100 128\nnet.core.rmem_default = 262144\nnet.core.rmem_max = 4194304\nnet.core.wmem_default = 262144\nnet.core.wmem_max = 1048586\nnet.core.somaxconn = 2048\nnet.ipv4.tcp_max_syn_backlog = 2048\nnet.ipv4.tcp_timestamps = 0\nnet.ipv4.tcp_synack_retries = 2\nnet.ipv4.tcp_syn_retries = 2\nnet.ipv4.tcp_tw_recycle = 1\nnet.ipv4.tcp_tw_reuse = 1\n#add by mq\nvm.overcommit_memory=1\nvm.drop_caches=1\nvm.zone_reclaim_mode=0\nvm.max_map_count=655360\nvm.dirty_background_ratio=50\nvm.dirty_ratio=50\nvm.page-cluster=3\nvm.dirty_writeback_centisecs=360000\n#add by teledb\nvm.min_free_kbytes=65536\nvm.swappiness=10\n#sysctl –p\n#echo 'ulimit -n 655350' >> /etc/profile\n#vi /etc/security/limits.conf \n* soft nofile 655350\n* hard nofile 655350\n* hard nproc 655350\n* soft nproc 655350\n* hard stack unlimited\n* soft stact unlimited\n#vi /etc/security/limits.d/20-nproc.conf \nroot       soft    nproc     unlimited\n* soft nofile 655350\n* hard nofile 655350\n* hard nproc 655350\n* soft nproc 655350\n* hard stack unlimited\n* soft stact unlimited\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n\n# 部署docker环境\n\n部署docker环境，请参考其他文档。\n\n\n# 部署操作\n\n\n# docker-compose安装\n\n1)\t下载指定的版本，注意docker-compose file的格式和对docker版本的支持\n####查看网页https://github.com/docker/compose/releases\n执行下面命令下载：\n#curl -L https://github.com/docker/compose/releases/download/1.24.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n# chmod +x /usr/local/bin/docker-compose\n2)\t查看是否安装成功\n# docker-compose –version\n3)\t加入到/usr/bin路径下\n#ln –s xxx/docker-compose  /usr/bin/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# harbor服务搭建\n\n# 下载harbor包\n\n从下面的url中下载最新的harbor包，选择offline下载\nhttps://github.com/goharbor/harbor/tags\n$tar –xvf harbor-offline-installer-v1.8.0.tgz\n\n\n1\n2\n3\n\n\n# 修改harbor参数\n\n修改harbor.yml文件\n\n1)\t修改hostname为本机的IP地址\n2)\t修改port为本机对外访问的端口\n3)\t修改data_volume为存储所有harbor文件路径\n4)\t修改location是存储日志的路径\n\n\n1\n2\n3\n4\n\n\n# 执行prepare脚本\n\n$./prepare\n\n执行这个脚本后，会根据修改的harbor.yml文件来修改docker-compose.yml文件，并且会下载goharbor/prepare: v1.8.0这个镜像，如果是离线部署的话，需要提前下载好这个镜像放到服务器本机的镜像仓库中。\n\n\n# 执行install.sh脚本\n\n$./install.sh\n\n执行完这个脚本后，会检查本机的docker/docker-compose环境，随后会下载所有相关的harbor的images镜像，然后针对harbor解压目录中的config文件进行配置，随后启动harbor各个相关的容器。\n\n如果是离线部署，需要提前下载好所有的images，具体的images如下：\n\ngoharbor/chartmuseum-photon:v0.8.1-v1.8.0\ngoharbor/harbor-migrator:v1.8.0\ngoharbor/redis-photon:v1.8.0\ngoharbor/clair-photon:v2.0.8-v1.8.0\ngoharbor/notary-server-photon:v0.6.1-v1.8.0\ngoharbor/notary-signer-photon:v0.6.1-v1.8.0\ngoharbor/harbor-registryctl:v1.8.0\ngoharbor/registry-photon:v2.7.1-patch-2819-v1.8.0\ngoharbor/nginx-photon:v1.8.0\ngoharbor/harbor-log:v1.8.0\ngoharbor/harbor-jobservice:v1.8.0\ngoharbor/harbor-core:v1.8.0\ngoharbor/harbor-portal:v1.8.0\ngoharbor/harbor-db:v1.8.0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# Harbor登录及使用\n\nhttp://IP:port   admin  Harbor12345\n\n\n1\n\n\n\n# 常见操作命令\n\n\n# Harbor服务的启停\n\n进入到harbor的解压缩目录后，执行下面的命令，其中操作的所有容器，默认情况下docker-compose就是操作同目录下的docker-compose.yml文件，如果使用其他yml文件，可以使用-f自己指定。\n\n1)\t后台启动harbor各个服务，如果容器不存在根据镜像自动创建\n$docker-compose up –d \n2)\t停止容器并删除容器\n$docker-compose down -v\n3)\t启动容器，容器不存在就无法启动，不会自动创建镜像\n$docker-compose start\n4)\t停止容器\n$docker-compose stop\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# Harbor各个服务的端口\n\n查看docker-compose.yml配置文件可以看出，除了harbor-log监听了本地127.0.0.1上的1514端口，还有nginx监听了本地所有网卡地址的8888端口，其余的容器都是使用的docker容器的内部网络链接。\n\n通过执行$docker network ls 可以查看容器都是用了一个叫做harbor-harbor的docker网络，drive类型也是bridge。\n\n\n# 修改docker私有仓库地址配置\n\n 1. 修改docker系统服务，引入系统变量配置文件。\n    \n    # vi /usr/lib/systemd/system/docker.service\n    在[Service]目录下，ExecStart之前添加如下配置\n    EnvironmentFile=-/etc/docker/daemon.json\n    \n    \n    1\n    2\n    3\n    \n\n 2. 添加daemon.json配置文件\n    \n    #vi /etc/docker/daemon.json\n    {\n      “insecure-registries\": [“x.x.x.x:port”]\n    }\n    #systemctl daemon-reload\n    #systemctl restart docker\n    $docker info       //可以查看具体的insecure-registries配置是否生效\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n\n# 上传/下载仓库镜像\n\n在harbor仓库中分为公有仓库和私有仓库，公有仓库下载不需要用户密码鉴权，公有仓库无法上传。私有仓库的上传下载需要用户鉴权。\n\n示例上传如下：\n\n$docker tag rancher/pause:3.1  192.168.3.60:8888/test/rancher/pause:3.1  //test是创建仓库项目\n$docker login 192.168.3.60:8888\n$docker push 192.168.3.60:8888/test/rancher/pause:3.1\n\n\n1\n2\n3\n\n\n示例下载如下：\n\n$ docker login 192.168.3.60:8888\n$ docker pull 192.168.3.60:8888/test/rancher/pause:3.1\n\n\n1\n2\n\n\n\n# 仓库进行同步\n\n# Harbor仓库push镜像到registry仓库\n\n 1. “仓库管理”分别创建harbor仓库和registry仓库的两个目标，其中habor仓库目标中的目标URL类似：”http://192.168.3.60:8888/test”，其中test是新创建的仓库项目。\n 2. “同步管理”中新建规则，规则的同步模式是push-based，指明目标的registry。\n 3. 选择新建的规则，点击”同步”。\n\n# Harbor仓库从registry仓库pull镜像\n\n 1. 同上，新建两个仓库的目标。\n 2. “同步管理”中新建规则，规则的同步模式是pull-based，指明源registry地址。\n 3. 选择新建的规则，点击”同步”。",normalizedContent:"# harbor镜像管理\n\n\n# harbor架构解析\n\n\n# harbor介绍\n\nvmware公司开源的企业级容器registry项目harbor，是由vmware中国研发团队负责开发。harbor可以帮助用户迅速搭建企业级的registry服务，它提供了管理图形界面，基于角色的访问控制rbac，镜像远程复制（同步），ad/ldap集成、以及审计日志等企业用户需求的功能，同时还原生支持中文和英文，深受国内外用户的喜爱。\n\n\n# harbor核心组件\n\nproxy: 是一个nginx的前端代理，代理harbor的registry，ui，token等服务。\n\ndb: 负责存储用户权限、审计日志、dockerimage分组信息等数据。\n\njobservice: jobservice是负责镜像复制工作的，和registry通信，从一个registry pull镜像然后push到另一个registry，并记录job_log。\n\ncore: harbor的核心组件，主要提供权限管理、审计、管理界面ui、token service以及可供其他系统调用的api等功能。\n\nlog: 为了帮助监控harbor运行，负责收集其他组件的log。\n\nredis: 用于存储session。\n\n注意：新版本1.8中新增了registryctl/ harbor-portal等服务。\n\n\n# harbor和registry的比较\n\nharbor和registry都是docker的镜像仓库，相比较于registry，有很多的优势。\n\n 1. 提供分层传输机制，优化网络传输\n\ndocker 镜像是分层的，如果每次每次传输都使用全量文件，不经济。必须提供识别分层传输的机制，以层的uuid未标识，确定传输的对象。\n\n 2. 提供web界面，优化用户体验\n\n只用镜像的名字来进行上传下载很不方便，需要有一个用户界面可以支持登录、搜索功能，包括区分公有、私有镜像。\n\n 3. 支持水平扩展集群\n\n当有用户对镜像的上传下载操作集中在某服务器，需要对相应的访问压力作分解。\n\n 4. 良好的安全机制\n\nregistry缺少认证机制，任何人基于不同角色的访问控制机制。\n\n 5. 定时清理\n\nregistry缺乏镜像清理机制。\n\n\n# 简略架构图\n\n\n\n\n# 系统环境及部署准备\n\n\n# 增加主机名称解析\n\n192.168.3.58 server58\n192.168.3.59 server59\n\n\n1\n2\n\n\n\n# 主机时间同步\n\nntp server配置\n# vi /etc/ntp.conf\n注释掉下面行，并添加下面两行\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\nserver 127.127.1.0\nfudge 127.127.1.0 stratum 11\nntp client配置\n注释掉下面行，并添加下面一行\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\nserver 192.168.3.58\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 关闭防火墙服务\n\n先事先关闭所有主机上的iptables或firewalld服务：\n#systemctl stop firewalld \n#systemctl stop iptables\n#systemctl disable firewalld\n#systemctl disable iptables\n\n\n1\n2\n3\n4\n5\n\n\n\n# 关闭并禁用selinux\n\n#vi /etc/selinux/config\n修改selinux配置\nselinux=disabled\n重启后生效\n\n\n1\n2\n3\n4\n\n\n\n# 系统内核参数修改\n\n#vi /etc/sysctl.conf \n#add by common\nfs.aio-max-nr = 1048576\nfs.file-max = 6815744\nkernel.shmall = 2097152\nkernel.shmmax = 536870912\nkernel.shmmni = 4096\nkernel.sem = 250 32000 100 128\nnet.core.rmem_default = 262144\nnet.core.rmem_max = 4194304\nnet.core.wmem_default = 262144\nnet.core.wmem_max = 1048586\nnet.core.somaxconn = 2048\nnet.ipv4.tcp_max_syn_backlog = 2048\nnet.ipv4.tcp_timestamps = 0\nnet.ipv4.tcp_synack_retries = 2\nnet.ipv4.tcp_syn_retries = 2\nnet.ipv4.tcp_tw_recycle = 1\nnet.ipv4.tcp_tw_reuse = 1\n#add by mq\nvm.overcommit_memory=1\nvm.drop_caches=1\nvm.zone_reclaim_mode=0\nvm.max_map_count=655360\nvm.dirty_background_ratio=50\nvm.dirty_ratio=50\nvm.page-cluster=3\nvm.dirty_writeback_centisecs=360000\n#add by teledb\nvm.min_free_kbytes=65536\nvm.swappiness=10\n#sysctl –p\n#echo 'ulimit -n 655350' >> /etc/profile\n#vi /etc/security/limits.conf \n* soft nofile 655350\n* hard nofile 655350\n* hard nproc 655350\n* soft nproc 655350\n* hard stack unlimited\n* soft stact unlimited\n#vi /etc/security/limits.d/20-nproc.conf \nroot       soft    nproc     unlimited\n* soft nofile 655350\n* hard nofile 655350\n* hard nproc 655350\n* soft nproc 655350\n* hard stack unlimited\n* soft stact unlimited\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n\n# 部署docker环境\n\n部署docker环境，请参考其他文档。\n\n\n# 部署操作\n\n\n# docker-compose安装\n\n1)\t下载指定的版本，注意docker-compose file的格式和对docker版本的支持\n####查看网页https://github.com/docker/compose/releases\n执行下面命令下载：\n#curl -l https://github.com/docker/compose/releases/download/1.24.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n# chmod +x /usr/local/bin/docker-compose\n2)\t查看是否安装成功\n# docker-compose –version\n3)\t加入到/usr/bin路径下\n#ln –s xxx/docker-compose  /usr/bin/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# harbor服务搭建\n\n# 下载harbor包\n\n从下面的url中下载最新的harbor包，选择offline下载\nhttps://github.com/goharbor/harbor/tags\n$tar –xvf harbor-offline-installer-v1.8.0.tgz\n\n\n1\n2\n3\n\n\n# 修改harbor参数\n\n修改harbor.yml文件\n\n1)\t修改hostname为本机的ip地址\n2)\t修改port为本机对外访问的端口\n3)\t修改data_volume为存储所有harbor文件路径\n4)\t修改location是存储日志的路径\n\n\n1\n2\n3\n4\n\n\n# 执行prepare脚本\n\n$./prepare\n\n执行这个脚本后，会根据修改的harbor.yml文件来修改docker-compose.yml文件，并且会下载goharbor/prepare: v1.8.0这个镜像，如果是离线部署的话，需要提前下载好这个镜像放到服务器本机的镜像仓库中。\n\n\n# 执行install.sh脚本\n\n$./install.sh\n\n执行完这个脚本后，会检查本机的docker/docker-compose环境，随后会下载所有相关的harbor的images镜像，然后针对harbor解压目录中的config文件进行配置，随后启动harbor各个相关的容器。\n\n如果是离线部署，需要提前下载好所有的images，具体的images如下：\n\ngoharbor/chartmuseum-photon:v0.8.1-v1.8.0\ngoharbor/harbor-migrator:v1.8.0\ngoharbor/redis-photon:v1.8.0\ngoharbor/clair-photon:v2.0.8-v1.8.0\ngoharbor/notary-server-photon:v0.6.1-v1.8.0\ngoharbor/notary-signer-photon:v0.6.1-v1.8.0\ngoharbor/harbor-registryctl:v1.8.0\ngoharbor/registry-photon:v2.7.1-patch-2819-v1.8.0\ngoharbor/nginx-photon:v1.8.0\ngoharbor/harbor-log:v1.8.0\ngoharbor/harbor-jobservice:v1.8.0\ngoharbor/harbor-core:v1.8.0\ngoharbor/harbor-portal:v1.8.0\ngoharbor/harbor-db:v1.8.0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# harbor登录及使用\n\nhttp://ip:port   admin  harbor12345\n\n\n1\n\n\n\n# 常见操作命令\n\n\n# harbor服务的启停\n\n进入到harbor的解压缩目录后，执行下面的命令，其中操作的所有容器，默认情况下docker-compose就是操作同目录下的docker-compose.yml文件，如果使用其他yml文件，可以使用-f自己指定。\n\n1)\t后台启动harbor各个服务，如果容器不存在根据镜像自动创建\n$docker-compose up –d \n2)\t停止容器并删除容器\n$docker-compose down -v\n3)\t启动容器，容器不存在就无法启动，不会自动创建镜像\n$docker-compose start\n4)\t停止容器\n$docker-compose stop\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# harbor各个服务的端口\n\n查看docker-compose.yml配置文件可以看出，除了harbor-log监听了本地127.0.0.1上的1514端口，还有nginx监听了本地所有网卡地址的8888端口，其余的容器都是使用的docker容器的内部网络链接。\n\n通过执行$docker network ls 可以查看容器都是用了一个叫做harbor-harbor的docker网络，drive类型也是bridge。\n\n\n# 修改docker私有仓库地址配置\n\n 1. 修改docker系统服务，引入系统变量配置文件。\n    \n    # vi /usr/lib/systemd/system/docker.service\n    在[service]目录下，execstart之前添加如下配置\n    environmentfile=-/etc/docker/daemon.json\n    \n    \n    1\n    2\n    3\n    \n\n 2. 添加daemon.json配置文件\n    \n    #vi /etc/docker/daemon.json\n    {\n      “insecure-registries\": [“x.x.x.x:port”]\n    }\n    #systemctl daemon-reload\n    #systemctl restart docker\n    $docker info       //可以查看具体的insecure-registries配置是否生效\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n\n# 上传/下载仓库镜像\n\n在harbor仓库中分为公有仓库和私有仓库，公有仓库下载不需要用户密码鉴权，公有仓库无法上传。私有仓库的上传下载需要用户鉴权。\n\n示例上传如下：\n\n$docker tag rancher/pause:3.1  192.168.3.60:8888/test/rancher/pause:3.1  //test是创建仓库项目\n$docker login 192.168.3.60:8888\n$docker push 192.168.3.60:8888/test/rancher/pause:3.1\n\n\n1\n2\n3\n\n\n示例下载如下：\n\n$ docker login 192.168.3.60:8888\n$ docker pull 192.168.3.60:8888/test/rancher/pause:3.1\n\n\n1\n2\n\n\n\n# 仓库进行同步\n\n# harbor仓库push镜像到registry仓库\n\n 1. “仓库管理”分别创建harbor仓库和registry仓库的两个目标，其中habor仓库目标中的目标url类似：”http://192.168.3.60:8888/test”，其中test是新创建的仓库项目。\n 2. “同步管理”中新建规则，规则的同步模式是push-based，指明目标的registry。\n 3. 选择新建的规则，点击”同步”。\n\n# harbor仓库从registry仓库pull镜像\n\n 1. 同上，新建两个仓库的目标。\n 2. “同步管理”中新建规则，规则的同步模式是pull-based，指明源registry地址。\n 3. 选择新建的规则，点击”同步”。",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"K8S基础知识点整理",frontmatter:{title:"K8S基础知识点整理",date:"2022-01-20T11:40:31.000Z",permalink:"/pages/cea09f/",categories:["中间件","K8S"],tags:[null]},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.K8S/01.K8S%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86.html",relativePath:"02.中间件/01.K8S/01.K8S基础知识点整理.md",key:"v-1d67f52e",path:"/pages/cea09f/",headers:[{level:2,title:"基本概念",slug:"基本概念",normalizedTitle:"基本概念",charIndex:27},{level:2,title:"Master组件",slug:"master组件",normalizedTitle:"master组件",charIndex:1246},{level:3,title:"API Server",slug:"api-server",normalizedTitle:"api server",charIndex:1259},{level:3,title:"etcd集群",slug:"etcd集群",normalizedTitle:"etcd集群",charIndex:1390},{level:3,title:"Controller Manger",slug:"controller-manger",normalizedTitle:"controller manger",charIndex:1637},{level:3,title:"调度器Scheduler",slug:"调度器scheduler",normalizedTitle:"调度器scheduler",charIndex:1791},{level:2,title:"Node组件",slug:"node组件",normalizedTitle:"node组件",charIndex:1925},{level:3,title:"kubelet",slug:"kubelet",normalizedTitle:"kubelet",charIndex:1936},{level:3,title:"Container Runtime",slug:"container-runtime",normalizedTitle:"container runtime",charIndex:2117},{level:3,title:"kube-proxy",slug:"kube-proxy",normalizedTitle:"kube-proxy",charIndex:2294},{level:2,title:"核心附件",slug:"核心附件",normalizedTitle:"核心附件",charIndex:2419},{level:3,title:"CoreDNS",slug:"coredns",normalizedTitle:"coredns",charIndex:2428},{level:3,title:"Kubernetes Dashboard",slug:"kubernetes-dashboard",normalizedTitle:"kubernetes dashboard",charIndex:2491},{level:3,title:"Ingress Controller",slug:"ingress-controller",normalizedTitle:"ingress controller",charIndex:2555},{level:2,title:"基础架构",slug:"基础架构",normalizedTitle:"基础架构",charIndex:2759},{level:3,title:"模块架构信息",slug:"模块架构信息",normalizedTitle:"模块架构信息",charIndex:2768},{level:3,title:"高可用架构1",slug:"高可用架构1",normalizedTitle:"高可用架构1",charIndex:3207},{level:3,title:"高可用架构2",slug:"高可用架构2",normalizedTitle:"高可用架构2",charIndex:3485},{level:2,title:"工作原理",slug:"工作原理",normalizedTitle:"工作原理",charIndex:3658},{level:2,title:"API对象",slug:"api对象",normalizedTitle:"api对象",charIndex:3893},{level:3,title:"声明式和命令式",slug:"声明式和命令式",normalizedTitle:"声明式和命令式",charIndex:3903},{level:3,title:"API对象的基本构成",slug:"api对象的基本构成",normalizedTitle:"api对象的基本构成",charIndex:4097},{level:2,title:"Pod",slug:"pod",normalizedTitle:"pod",charIndex:230},{level:2,title:"多Container Pod模型",slug:"多container-pod模型",normalizedTitle:"多container pod模型",charIndex:4692},{level:3,title:"sidercar pattern(边车模式)",slug:"sidercar-pattern-边车模式",normalizedTitle:"sidercar pattern(边车模式)",charIndex:4805},{level:3,title:"Ambassador pattern(大使模式)",slug:"ambassador-pattern-大使模式",normalizedTitle:"ambassador pattern(大使模式)",charIndex:4974},{level:3,title:"Adapter pattern(适配器模型)",slug:"adapter-pattern-适配器模型",normalizedTitle:"adapter pattern(适配器模型)",charIndex:5260},{level:2,title:"容器探测",slug:"容器探测",normalizedTitle:"容器探测",charIndex:5439},{level:3,title:"存活性探测",slug:"存活性探测",normalizedTitle:"存活性探测",charIndex:6069},{level:3,title:"就绪性探测",slug:"就绪性探测",normalizedTitle:"就绪性探测",charIndex:6651},{level:3,title:"limit和request",slug:"limit和request",normalizedTitle:"limit和request",charIndex:6927},{level:2,title:"Deployment控制器",slug:"deployment控制器",normalizedTitle:"deployment控制器",charIndex:7351},{level:3,title:"滚动更新",slug:"滚动更新",normalizedTitle:"滚动更新",charIndex:7495},{level:3,title:"maxSurge和maxUnavailable",slug:"maxsurge和maxunavailable",normalizedTitle:"maxsurge和maxunavailable",charIndex:7774},{level:3,title:"扩容和缩容",slug:"扩容和缩容",normalizedTitle:"扩容和缩容",charIndex:8278},{level:2,title:"DaemonSet控制器",slug:"daemonset控制器",normalizedTitle:"daemonset控制器",charIndex:8397},{level:2,title:"StatefulSet控制器",slug:"statefulset控制器",normalizedTitle:"statefulset控制器",charIndex:8958},{level:2,title:"Scheduler",slug:"scheduler",normalizedTitle:"scheduler",charIndex:160},{level:3,title:"预选策略",slug:"预选策略",normalizedTitle:"预选策略",charIndex:9822},{level:3,title:"优选策略",slug:"优选策略",normalizedTitle:"优选策略",charIndex:9827},{level:3,title:"高级调度",slug:"高级调度",normalizedTitle:"高级调度",charIndex:12432},{level:3,title:"podAffinity",slug:"podaffinity",normalizedTitle:"podaffinity",charIndex:13861},{level:3,title:"手动调度和DaemonSet",slug:"手动调度和daemonset",normalizedTitle:"手动调度和daemonset",charIndex:15244},{level:3,title:"污点和容忍度",slug:"污点和容忍度",normalizedTitle:"污点和容忍度",charIndex:15605},{level:2,title:"Service和Ingress",slug:"service和ingress",normalizedTitle:"service和ingress",charIndex:18083},{level:3,title:"Service",slug:"service",normalizedTitle:"service",charIndex:380},{level:3,title:"kube-proxy代理模式",slug:"kube-proxy代理模式",normalizedTitle:"kube-proxy代理模式",charIndex:18781},{level:3,title:"Service类型",slug:"service类型",normalizedTitle:"service类型",charIndex:19550},{level:3,title:"Ingress",slug:"ingress",normalizedTitle:"ingress",charIndex:718},{level:2,title:"Volumes",slug:"volumes",normalizedTitle:"volumes",charIndex:20177},{level:3,title:"PV/PVC",slug:"pv-pvc",normalizedTitle:"pv/pvc",charIndex:20499},{level:2,title:"ConfigMap和Secret",slug:"configmap和secret",normalizedTitle:"configmap和secret",charIndex:20941},{level:2,title:"网络模型",slug:"网络模型",normalizedTitle:"网络模型",charIndex:21113},{level:3,title:"K8S网络模型",slug:"k8s网络模型",normalizedTitle:"k8s网络模型",charIndex:21122},{level:3,title:"CNI插件",slug:"cni插件",normalizedTitle:"cni插件",charIndex:21293},{level:3,title:"Flannel/Calico的区别",slug:"flannel-calico的区别",normalizedTitle:"flannel/calico的区别",charIndex:21956},{level:2,title:"HPA",slug:"hpa",normalizedTitle:"hpa",charIndex:22059},{level:2,title:"Helm",slug:"helm",normalizedTitle:"helm",charIndex:22294},{level:2,title:"容器和虚拟机有什么区别联系",slug:"容器和虚拟机有什么区别联系",normalizedTitle:"容器和虚拟机有什么区别联系",charIndex:22303},{level:3,title:"什么是容器",slug:"什么是容器",normalizedTitle:"什么是容器",charIndex:22335},{level:3,title:"两者区别如下",slug:"两者区别如下",normalizedTitle:"两者区别如下",charIndex:22523},{level:2,title:"Kubernetes特性有哪些",slug:"kubernetes特性有哪些",normalizedTitle:"kubernetes特性有哪些",charIndex:22659},{level:3,title:"什么是K8S",slug:"什么是k8s",normalizedTitle:"什么是k8s",charIndex:22679},{level:3,title:"如下重要特性",slug:"如下重要特性",normalizedTitle:"如下重要特性",charIndex:22911},{level:2,title:"K8S的list-watch",slug:"k8s的list-watch",normalizedTitle:"k8s的list-watch",charIndex:23805},{level:3,title:"为什么需要list-watch这种机制",slug:"为什么需要list-watch这种机制",normalizedTitle:"为什么需要list-watch这种机制",charIndex:23992},{level:3,title:"什么是List-Watch",slug:"什么是list-watch",normalizedTitle:"什么是list-watch",charIndex:24213},{level:3,title:"应用模块",slug:"应用模块",normalizedTitle:"应用模块",charIndex:24536},{level:3,title:"设计理念",slug:"设计理念",normalizedTitle:"设计理念",charIndex:24802},{level:2,title:"Kube-proxy中的ipvs模式和iptables模式",slug:"kube-proxy中的ipvs模式和iptables模式",normalizedTitle:"kube-proxy中的ipvs模式和iptables模式",charIndex:25596},{level:3,title:"iptables模式是什么",slug:"iptables模式是什么",normalizedTitle:"iptables模式是什么",charIndex:25739},{level:3,title:"ipvs模式是什么",slug:"ipvs模式是什么",normalizedTitle:"ipvs模式是什么",charIndex:25757},{level:3,title:"两者的区别和联系",slug:"两者的区别和联系",normalizedTitle:"两者的区别和联系",charIndex:25837},{level:2,title:"K8S删除node",slug:"k8s删除node",normalizedTitle:"k8s删除node",charIndex:25850},{level:2,title:"修改docker storage pool",slug:"修改docker-storage-pool",normalizedTitle:"修改docker storage pool",charIndex:26328},{level:3,title:"查看storage pool配置",slug:"查看storage-pool配置",normalizedTitle:"查看storage pool配置",charIndex:26354},{level:3,title:"清理docker 空间",slug:"清理docker-空间",normalizedTitle:"清理docker 空间",charIndex:26440},{level:3,title:"修改空间大小步骤如下",slug:"修改空间大小步骤如下",normalizedTitle:"修改空间大小步骤如下",charIndex:26488},{level:2,title:"修改最大的pod限制",slug:"修改最大的pod限制",normalizedTitle:"修改最大的pod限制",charIndex:26886}],headersStr:"基本概念 Master组件 API Server etcd集群 Controller Manger 调度器Scheduler Node组件 kubelet Container Runtime kube-proxy 核心附件 CoreDNS Kubernetes Dashboard Ingress Controller 基础架构 模块架构信息 高可用架构1 高可用架构2 工作原理 API对象 声明式和命令式 API对象的基本构成 Pod 多Container Pod模型 sidercar pattern(边车模式) Ambassador pattern(大使模式) Adapter pattern(适配器模型) 容器探测 存活性探测 就绪性探测 limit和request Deployment控制器 滚动更新 maxSurge和maxUnavailable 扩容和缩容 DaemonSet控制器 StatefulSet控制器 Scheduler 预选策略 优选策略 高级调度 podAffinity 手动调度和DaemonSet 污点和容忍度 Service和Ingress Service kube-proxy代理模式 Service类型 Ingress Volumes PV/PVC ConfigMap和Secret 网络模型 K8S网络模型 CNI插件 Flannel/Calico的区别 HPA Helm 容器和虚拟机有什么区别联系 什么是容器 两者区别如下 Kubernetes特性有哪些 什么是K8S 如下重要特性 K8S的list-watch 为什么需要list-watch这种机制 什么是List-Watch 应用模块 设计理念 Kube-proxy中的ipvs模式和iptables模式 iptables模式是什么 ipvs模式是什么 两者的区别和联系 K8S删除node 修改docker storage pool 查看storage pool配置 清理docker 空间 修改空间大小步骤如下 修改最大的pod限制",content:'Kubernetes知识点整理\n\n\n# 概述\n\n\n# 基本概念\n\n按Mater/Node来区分\n\nMaster: 是集群的网关和中枢，负责诸如为用户和客户端暴露API、跟踪其他服务器的健康状态、以最优方式调度工作负载，以及编排其他组件之间的通信等任务。通常包含Api-Server、Controller-manager、Scheduler、etcd(可外置)。\n\nNode: 是k8s集群的工作节点，负责接收来自Master的工作指令并根据指令相应地创建或销毁Pod对象，以及调整网络规则以合理地路由和转发流量等。\n\n按模块组件来区分\n\nPod：\n\n是一组功能相关的Container的封装；共享存储和Network Namespace；是K8S调度和作业运行的基本单位(Scheduler调度，Kubelet运行)；Pod容易"走失"，需要workload和Service的保障。\n\nWorkloads(Pod控制器):\n\n尽管Pod是k8s的最小调度单元，但是用户通常并不会直接部署及管理Pod对象，而是要借助于另一类抽象 ---控制器(Controller)对其进行管理。 有Deployment，Statefulset，DaemonSet，Job ....，实际上是一组功能相关的Pod的封装。使用控制器之后就不再需要手动管理Pod对象了，用户只需要声明应用的期望状态，控制器就会自动对其进行进程管理。\n\nService:\n\n主要是为了让Pod "防失联"，给一组Pod设置反向代理。建立在一组Pod对象之上的资源抽象，它通过标签选择器选定一组Pod对象，并为这组Pod对象定义一个统一的固定访问入口(通常是一个IP地址)。\n\nIngress:\n\nk8s将Pod对象和外部网络环境进行了隔离，Pod和Service等对象间的通信都使用其内部专用地址进行，如若需要开放某些Pod对象提供给外部用户访问，则需要为其请求流量打开一个通往k8s集群内部的通道。除了Service之外，Ingress也是这类通道的实现方式之一。\n\nVoumes:\n\n存储卷(Volume)是独立于容器文件系统之外的存储空间，常用于扩展容器的存储空间并为它提供持久存储能力。k8s集群上的存储卷大体上可以分为临时卷、本地卷和网络卷。\n\nName和Namespace:\n\n名称(Name)是k8s集群中资源对象的标识符，它们的作用域通常是名称空间(Namespace)，因此名称空间是名称的额外的限定机制。在同一个名称空间中，同一类型资源对象的名称必须具有唯一性。名称空间通常用于实现租户或项目的资源隔离。\n\nAnnotation:\n\nAnnotation(注解)是另一种附加在对象之上的键值类型的数据，但它拥有更大的数据容量。Annotation常用于各种非标识型元数据(metadata)附加到对象上，但它不能用于标识和选择对象，通常也不会被k8s直接使用，其主要目的是方便工具或用户的阅读及查找等。\n\n\n# Master组件\n\n\n# API Server\n\nAPI Server负责输出RESTful风格的k8s API，它是发往集群的所有REST操作命令的接入点，并负责接收、校验并响应所有的REST请求，结果状态被持久化存储于etcd中。因此，API Server是整个集群的网关。\n\n\n# etcd集群\n\nk8s集群的所有状态信息都需要持久存储于存储系统etcd中，不过，etcd是由CoreOS基于Raft协议开发的分布式键值存储，可用于服务发现、共享配置以及一致性保障(如数据库主节点选择、分布式锁等)。\n\netcd不仅能提供键值数据存储，而且还为其提供了监听(watch)机制，用于监听和推送变更。k8s集群系统中，etcd中的键值发生变化时会通知到API Server，并由其通过watch API向客户端输出。基于watch机制，k8s集群的各组件实现了高效协同。\n\n\n# Controller Manger\n\n控制器完成的功能主要包括生命周期功能和API业务逻辑 .\n\n生命周期管理：包括Namespace创建和生命周期、Event垃圾回收、Pod终止相关的垃圾回收、级联垃圾回收及Node垃圾回收等。\n\nAPI业务逻辑：例如，由ReplicaSet执行的Pod扩展等。\n\n\n# 调度器Scheduler\n\nk8s是用于部署和管理大规模容器应用的平台。API Server 确认Pod对象的创建请求之后，便需要由Scheduler根据集群内各节点的可用资源状态，以及要运行的容器的资源需求做出调度决策。另外，k8s还支持用户自定义调度器。\n\n\n# Node组件\n\n\n# kubelet\n\n是Node的核心代理程序，是运行于工作节点之上的守护进程，它从API Server接收关于Pod对象的配置信息并确保它们处于期望的状态(desired state，"目标状态")。kubelete会在API Server上注册当前工作节点，定期向Master汇报节点资源的使用情况，并通过cAdvisor监控容器和节点的资源占用情况。\n\n\n# Container Runtime\n\n每个Node都要提供一个容器运行时(Container Runtime)环境，它负责下载镜像并运行容器。kubelet并未固定链接至某容器运行时环境，而是以插件的方式载入配置的容器环境。这种方式清晰地定义了各组件的边界。目前，k8s支持的容器运行环境至少包括Docker、RKT、cri-o和Fraki等。\n\n\n# kube-proxy\n\n每个工作节点都需要运行一个kube-proxy守护进程，它能够按需为Service资源对象生成iptables或ipvs规则，从而捕获访问当前Service的ClusterIP的流量并将其转发至正确的后端Pod对象。\n\n\n# 核心附件\n\n\n# CoreDNS\n\nk8s从1.11版本开始默认使用CoreDNS项目为集群提供服务注册和服务发现的动态名称解析服务。\n\n\n# Kubernetes Dashboard\n\nK8s集群的全部功能都要基于Web的UI，俩管理集群中的应用甚至集群自身。\n\n\n# Ingress Controller\n\nService 是一种工作与传统层的负载均衡器，而Ingress是在应用层实现的HTTPS(s)负载均衡机制。\n\n不过，Ingress资源自身并不能进行"流量穿透"，它仅是一组路由规则的集合，这些规则需要通过Ingress控制器(Ingress Controller)发挥作用。目前，此类的可用项目有Nginx、Trafik、Envory及HAProxy等。\n\n\n# 基础架构\n\n\n# 模块架构信息\n\n如下是K8S的架构图，展示了K8S中的各个组件模块的信息。\n\n\n\n整体来说，K8S的架构是Master、Node的模式，Master节点上通常部署有scheduler、controller-manager、api-server，以及etcd分布式数据库。Node节点上通常运行着kubelet、kube-proxy组件，以及真正运行docker实例的Pod容器。\n\n用户通过kubectl，经过一些列的集群的安全认证后，将所需要执行的资源清单配置文件提交给api-server，api-server会将此信息写入etcd，写入完成后，etcd向api-server发起一系列的事件信息，通过list-watch的机制，先后由controller-manager执行副本配置，scheduler完成调度node，kubelet完成落实到各自node上启动相应的pod。而kube-proxy则用于相应的Server资源转发到所关联的一系列的Pod对象上。\n\n\n# 高可用架构1\n\n\n\n * etcd自身提供的分布式存储集群为K8S构建一个可靠的存储层。\n * 将无状态的apiserver运行为多副本，并在其前端使用负载均衡器调度请求；需要注意的是，负载均衡器本身也需要是高可用。\n * 多副本的控制器管理器，通过其自带的leader选举功能(--leader-election)选举出主角色，余下的副本在主角色发送故障时自动启动新一轮的选举操作。\n * 多副本的调度器，通过其自带的leader选举功能(--leader-election)选举出主角色，余下的副本在主角色发生故障时自动启动新一轮的选举操作。\n\n\n# 高可用架构2\n\n如下是结合了kubease的工具的架构图示。\n\n\n\n * Master节点只需要2台即可。\n * 从原有的服务器端负载均衡转为了node客户端负载均衡。\n * 无须申请VIP资源。\n * 免去维护外部HA LB的问题。\n\n> kubeasz 项目地址：https://github.com/easzlab/kubeasz\n\n\n# 工作原理\n\n如下的图示是完整的工作原理的解析，其中核心的重点概念就是list-watch的机制。\n\n\n\n上面所有的"0"示意图，都是表示的是从k8s环境部署完毕后，各个api-server的客户端(api-server、scheduler、kubelet)都对于各个所订阅的资源对象，进行watch。\n\nlist-watch的异步消息通信机制，有效的保证了消息的可靠性、及时性、顺序性、高性能等。\n\n具体的list-watch机制，可以参考下面章节的内容整理。\n\n\n# API对象\n\n\n# 声明式和命令式\n\nkubectl的命令由此可以分为三类：陈述式命令、陈述式对象配置和声明式对象配置。第一种方式即此前用到的run、expose、delete和get等命令，它们直接作用于k8s系统上的活动对象，简单易用。\n\n声明式的使用通常要依赖于资源配置文件，声明式对象配置并不直接指明要进行的对象管理操作，而是提供配置清单文件给k8s系统，并委托系统跟踪活动对象的状态变动。\n\n\n# API对象的基本构成\n\n\n\n整体来说，在K8S中一个完整的API对象由四大部分组成。\n\ntypeMeta: 主要由apiVersion和kind两个构成。主要描述的是该类API资源对象的类别和版本。\n\nobjectMeta: metadata类别，涵盖该类型资源对象下的基本元数据的信息，包含一些名称、label、注解等。\n\nspec: 这是关键的信息内容，涵盖定义的所有的期望状态信息。\n\nstatus: 这个不是客户端所需要配置的，反映的是当前该类资源对象的实际的状态值。\n\n\n# Pod\n\n绝大多数场景中都应该于**一个容器中仅运行一个进程。\n\n不过，分别运行于各自容器的进程之间无法实现基于IPC的通信机制，此时，容器间的隔离机制对于依赖于此类通信方式的进程来说却又成了阻碍。Pod资源抽象正是用来解决此类问题的组件。前面提到，Pod对象是一组容器的集合，这些容器共享Network、UTS及IPC名称空间，因此具有相同的域名、主机名和网络接口，并可通过IPC直接通信。\n\n为一个Pod对象中的各容器提供网络名称空间等共享机制的是底层基础容器pause。如下图所示，一个由三个容器组成的Pod资源，各容器共享Network、IPC和UTS名称空间，但分别拥有各自的MNT、USR和PID名称空间。需要特别强调的是，一个Pod对象中的多个容器必须运行于同一工作节点之上。\n\n\n\n\n# 多Container Pod模型\n\nk8s系统的Pod资源对象用于运行单个容器化应用，此应用称为Pod对象的主容器，同时Pod也能容纳多个容器，不过额外的容器一般工作为sidecar模式，用于辅助主容器完成工作职能。\n\n\n# sidercar pattern(边车模式)\n\n即为Pod的主应用容器提供协同的辅助应用容器，每个应用独立运行，最为典型的代表是将主应用容器中的日志使用agent收集至日志服务器中时，可以将agent运行为辅助应用容器，即sidecar。另一个典型的应用是为主应用容器中的database server启用本地缓存。如下图：\n\n\n\n\n# Ambassador pattern(大使模式)\n\n即为远程服务创建一个本地代理，代理应用运行于容器中，主容器中的应用通过代理容器访问远程服务。如下图所示，一个典型的使用示例是主应用容器中的进程访问"一主多从"模型的远程Redis应用时，可在当前Pod容器中为Redis服务创建一个Ambassador container，主应用容器中的进程直接通过localhost接口访问Ambassador container即可。即便是Redis主从集群架构发生变动时，也仅需要将Ambassador container加以修改即可，主应用容器无须对此作出任何反应。\n\n\n\n\n# Adapter pattern(适配器模型)\n\n此种模型一般用于将主应用容器中的内容进行标准化输出，例如，日志数据或指标数据的输出，这有助于调用者统一接收数据的接口，如下图所示。另外，某应用滚动升级后的版本不兼容旧的版本时，其报告信息的格式也存在不兼容的可能性，使用Adapter container有助于避免那些调用此报告数据的应用发生错误。\n\n\n\n\n# 容器探测\n\nK8s支持三种处理器用于Pod探测，任何一种探测方式都可能存在三种结果："Success"(成功)、"Failure"(失败)或"Unknown"(未知)，只有第一种结果表示成功通过检测。\n\n * ExecAction：在容器中执行一个命令，并根据其返回的状态码进行诊断的操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。\n * TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常，否则为不健康状态。\n * HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起HTTP GET请求进行诊断，响应码为2xx或3xx时即为成功，否则为失败。\n\nKubelet可在活动容器上执行两种类型的检测：存活性检测(livenessProbe)和就绪性检测(readinessProbe)。\n\n * 存活性检测：用于判断容器是否处于"运行"(Running)状态；一旦此类检测未通过，kubelet将杀死容器并根据其restartPolicy决定是否将其重启；未定义存活性检测的容器的默认状态为"Success"。\n * 就绪性检测：用于判断容器是否准备就绪并可对外提供服务；未通过检测的容器意味着其尚未准备就绪，端点控制器(如Service对象)会将其IP从所有匹配到此Pod对象的Service对象的端点列表中移除；检测通过之后，会再次将其IP添加至端点列表中。\n\n\n# 存活性探测\n\n有不少应用程序长时间持续运行后会逐渐转为不可用状态，探测到容器的状态为不可用时，仅能通过重启操作恢复。\n\n容器的存活性探测主要配置有如下：\n\n这些属性可以通过"spec.containers.livenessProbe"的如下属性字段来给出。\n\n * initialDelaySeconds <integer>：存活性探测延迟时长，及容器启动多久之后开始第一次探测操作，显示为delay属性；默认为0秒，及容器启动后立刻便开始进行探测。\n * timeoutSeconds <integer>：存活性探测的超时时长，显示为timeout属性，默认为1s，最小值也是1s。\n * periodSeconds <integer>：存活性探测的额度，显示为period属性，默认为10s，最小值为1s；过高的频率会对Pod对象带来较大的额外开销，而过低的频率又会使得对错误的反应不及时。\n * successThreshold <integer>：处于失败状态时，探测操作至少连续多少次的成功才被认为是通过检测，显示为#success属性，默认值为1，最小值也为1。\n * failureThreshold <integer>：处于成功状态时，探测操作至少连续多少次的失败才被视为是检测不通过，显示为#failure属性，默认值为3，最小值为1。\n\n\n# 就绪性探测\n\nPod对象启动后，容器应用通常需要一段时间才能完成其初始化过程，例如加载配置或数据，甚至有些程序需要运行某类的预热过程，应该避免于Pod对象启动后立即让其处理客户端请求，而是等待容器初始化工作执行完成并转为"就绪"状态。\n\n未定义就绪性探测的Pod对象在Pod进入"Running"状态后将立即就绪。\n\n与存活性探测触发的操作不同的是，探测失败时，就绪性探测不会杀死或重启容器以保证其健康性，而是通知其尚未就绪，并触发依赖于其就绪状态的操作(例如，从Service对象中移除此Pod对象)以确保不会有客户端请求接入此Pod对象。\n\n\n# limit和request\n\n目前来说，资源隔离尚且属于容器级别，CPU和内存资源的配置需要在Pod中的容器上进行，每种资源均可由**"request"属性定义其请求的确保可用值**，即容器运行可能用不到的这些额度的资源，但用到时必须要确保有如此多的资源可用，而**"limits"属性则用于限制资源可用的最大值，即硬限制**。\n\n只有当节点上可分配资源量>=容器资源请求数时才允许将容器调度到该节点上。\n\n但Request参数不限制容器的最大可使用资源。Limits，是容器能使用的资源的最大值，设置为0表示使用资源无上限。\n\nK8S系统中，CPU的分配支持分数计量方式，一个核心(1core)相当于1000个微核心(millicores)，因此500m相当于是0.5个核心，即二分之一个核心。内存的计量方式于日常使用方式相同，默认单位是字节，也可以使用E、P、T、G、M和K作为单位后缀，或Ei、Pi、Ti、Mi和Ki形式的单位后缀。\n\n\n# Deployment控制器\n\nDeployment(简称为deploy)是k8s控制器的又一种实现，它构建于ReplicaSet控制器之上，可为Pod和ReplicaSet资源提供声明式更新。\n\n主要是配置期望副本数、定义Pod模板、定义标签选择器、回滚、暂停和启动、滚动升级。\n\n\n# 滚动更新\n\n滚动升级是默认的更新策略，它在删除一部分旧版本Pod资源的同时，补充创建一部分新版本的Pod对象进行应用升级，其优势是升级期间，容器中应用提供的服务不会中断，但要求应用程序能够应对新旧版本同时工作的情形，例如新旧版本兼容同一个数据库方法等。\n\n滚动升级的过程中，会创建另外一个ReplicaSet控制器来完成新版本的Pod升级，现有的Pod对象会处于两个不同的Replicaset之下，旧控制器的Pod对象数量不断减少的同时，新控制器的Pod对象数量不断增加，直到旧控制器不再拥有Pod对象，而新控制器的副本数量变得完全符合期望值为止。\n\n\n# maxSurge和maxUnavailable\n\n滚动更新时，应用升级期间还要确保可用的Pod对象数量不低于某阀值以确保可用持续处理客户端的服务请求，变动的方式和Pod对象的数量范围将通过spec.strategy.rollingUpdate.maxSurge和spec.strategy.rollingUpdate.maxUnavailable两个属性协同进行定义。\n\n * maxSurge：指定升级期间存在的总Pod对象数量最多可超出期望值的个数，其值可用是0或正整数，也可以是一个期望值的百分比；例如，如果期望值为3，当前的属性值为1，则表示Pod对象的总数不能超过4个。\n * maxUnavailable：升级期间正常可用的Pod副本数(包括新旧版本)最多不能低于期望数值的个数，其值可以是0或正整数，也可以是一个期望值的百分比；默认值为1，该值意味着如果期望值是3，则升级期间至少要有两个Pod对象处于正常提供服务的状态。\n\n\n\n> maxSurge和maxUnavailable属性的值不可同时为0，否则Pod对象的副本数量在符合用户期望的数量后无法做出合理变动以进行滚动更新操作。\n\n\n# 扩容和缩容\n\n通过修改spec.replicas既可以修改Deployment控制器中的Pod控制器的副本数量，它将实时作用于控制器并直接生效。\n\n另外，"kubectl scale"是专用于扩展某些控制器类型的应用规模的命令。\n\n\n# DaemonSet控制器\n\n用于在集群中的全部节点上同时运行一份指定的Pod资源副本，后续新加入集群的工作节点也会自动创建一个相关的Pod对象，当从集群移除节点时，此类Pod对象也将被自动回收而无须重建。\n\n也可以使用节点选择器及节点标签指定仅在部分具有特定特征的节点上运行指定的Pod对象。\n\nDaemonSet是一种特殊的控制器，它有特定的应用场景，通常运行那些执行系统级操作任务的应用，其应用场景具体如下。\n\n * 运行集群存储的守护进程，如在各个节点上运行的glusterd或ceph。\n * 在各个节点上运行日志收集守护进程，如fluentd和logstash。\n * 在各个节点上运行监控系统的代理守护进程，如Prometheus Node Exporter、collectd、Datadong agent、New Relic agent或Ganglia gmond等。\n\n当然，既然是需要运行于集群内的每个节点或部分节点，于是很多场景中也可以把应用直接运行为工作节点上的系统守护进程，不过，这样一来就失去了运行k8s管理所带来的便捷性。\n\n另外，也只有必须将Pod对象运行于固定的几个节点并且需要先于其他Pod启动时，才有必要使用DeamonSet控制器，否则就应该使用Deployment控制器。\n\n\n# StatefulSet控制器\n\nReplicaSet控制器所创建的Pod从本质上都是一个模板出来的，这些Pod资源除了主机名和IP都没有本质上的区别。所管理的这些Pod资源启动也不需要考虑前后顺序，任何一个Pod资源都可以被ReplicaSet控制器重构出的新版本所替代，管理员更多关注的也是它们的群体特征，而无须过于关注任何一个个体。例如Tomcat、jetty、Nginx等。\n\n而StatefulSet这类控制器管理的Pod对象可能有如下场景：应用程序在处理客户端请求时，对当前请求的处理需要以前一次或多次的请求为基础进行，新客户端发起的请求则会被其施加专用标识，以确保其后续的请求可以被识别。例如，RDBMS系统上处于同一个事务中的多个请求不但彼此之间存在关联性，而且还要以严格的顺序执行。这类应用一般需要记录请求连接的相关信息，即"状态"，有的甚至还需要持久保存由请求生成的数据，尤其是存储服务类的应用，运行于k8s系统上时需要用到的持久存储卷。\n\nReplicaSet控制器管理的Pod无法为每个Pod单独指定不同的Volume挂载，而StatefulSet(有状态副本集)则是专门用来满足此类应用的控制器类型，由其管控的每个Pod对象都有着固定的主机名和专有存储卷，即便被重构后也能保持不变。支持每个Pod对象一个专有索引、有序部署、有序终止、固定的标识符及固定的存储卷等特性。\n\n\n# Scheduler\n\nK8S中默认的调度器的核心目标就是基于资源可用性将各Pod资源公平地分布于集群节点之上。目前默认的调度器通过三个步骤来完成调度操作：节点预选(Predicate)、节点优先级排序(Priority)以及节点择优(Select)。\n\n> 下面有些调度内容，摘录自网上https://blog.csdn.net/qq_34857250/article/details/90259693\n> \n> https://www.cnblogs.com/L-dongf/p/12327401.html\n\n调度策略，分为预选策略和优选策略。预选策略，predicate是强制性规则，会遍历所有的node节点，依据具体的预选策略筛选出符合要求的node列表，如果没有node符合predicates策略规则，那么Pod就会被挂起，直到有node能够满足。优选策略，这一步会在第一步筛选的基础上，按照优选策略为待选node打分排序，获取最优者。\n\n\n# 预选策略\n\n必须完全满足\n\n * CheckNodeConditon: 检查node是否正常。\n * GeneralPredicates: 普通判断策略\n   * HostName: 检测Pod对象是否定义了pod.spec.hostname，并且检查节点中是否有同名的pod而冲突。\n   * PodFitHostPorts: 检查pod.spec.containers.ports.hostPort属性(绑定节点上的某个端口)是否定义，并且检查节点中的节点端口是否冲突。\n   * MatchNodeSelector: pods.spec.nodeSelector，检查节点选择器。\n   * PodFitsResources: 检查Pod的资源需求request是否能被节点所满足。\n * NoDiskConflict: 检测pod依赖的存储卷是否能满足需求，默认不检查。\n * PodToleratesNodeTaints: pods.spec.tolerations可容忍的污点，检查Pod是否能容忍节点上的污点。\n * PodToleratesNodeExecuteTanits: pod.tolerations属性中是否能接纳容忍NoExecute级别的污点，默认没有启用。\n * CheckNodeLablePresence: 检测node上的标签的存在与否，默认没有启用。\n * CheckServiceAffinity: 根据Pod所属的service，将相同所属的service尽可能放在同一个节点，默认不检查。\n * CheckVolumeBinding: 检查节点上已绑定和未绑定的PVC是否能够满足Pod对象的存储卷需求。\n * NoVolumeZoneConflict: 如果给定了区域限制，检查在此节点上部署Pod对象是否存在存储卷冲突。\n * CheckNodeMemoryPressure: 检测节点内存是否存在压力，如果节点内存压力过大，则检查当前Pod是否可以调度此节点上。\n * CheckNodePIDPressure: 检查节点PID数量是否存在压力。\n * CheckNodeDiskPressure: 检查节点磁盘资源的压力情况。\n * MatchInterPodAffinity: 检查给定节点是否能够满足Pod对象的亲和性或反亲和性条件，以用于实现Pod亲和性调度或反亲和性调度。\n\n\n# 优选策略\n\n优选过程中，调度器向每个通过预选的节点传递一系列的优选函数，来计算每个节点上各个优选函数后得到的值，调度器会给每个优选函数设定一个权重，大多数优先级默认为1。将所有优选函数得分乘以权重，然后相加从而得出节点的最终优先级分值。finaSoreNode=(weight1*priorityFunc1)+(weight2*priorityFunc2)+ ...\n\n下面是各个优选函数的相关说明：\n\n * LeastRequested: 节点的资源空闲率高的优选，是节点空闲资源与节点总容量的比值计算而来的。即由CPU或内存资源的总容量减去节点上已有Pod对象需求的容量总和，再减去当前要创建的Pod对象的需求容量的结果除以总容量。计算公式是：(cpu((capacity-sum(requested))*10 / capacity)+memory((capacity-sum(requested))*10 / capacity)) / 2\n * BalancedResourceAllocation: 计算节点上面的CPU和内存资源被占用的比率相近程度，越接近，比分越高，平衡节点的资源使用情况。计算公式：cpu=cpu((capacity-sum(requested))*10 / capacity) mem=memory((capacity-sum(requested))*10 / capacity)\n * NodePreferAvoidPodsPriority: 如果node上不存在"scheduler.alpha.kubernetes.io/preferAvoidPods"这个注解，那么不管什么pod都没有影响；如果node上存在相关的注解，那么注解中关联的Pod对象名称正好是要去调度的Pod，那么此类node分值会很低，如果关联的Pod对象名称和要调度的Pod名称没有任何关系，那么和没有注解是一样的效果。需要注意的是，在这个优先级中，优先级最高，得分会非常高。\n * NodeAffinityPriority: 节点的亲和性，亲和性高，得分高。基于节点亲和性调度偏好进行的优选级评估，根据Pod资源中的nodeSelector对给定节点进行匹配度检查，成功匹配到的条目越多则节点得分越高。\n * TaintTolerationPriority: 将Pod对象的spec.tolertions与节点的taints列表项进行匹配度检测，匹配的条目越多，得分越低。\n * SelectorSpreading: 尽可能的把Pod分散开，也就是没有启动这个Pod的node，得分会越高。\n * InterPodAffinityPriority: 遍历Pod对象的亲和性条目，匹配项越多，得分就越多。\n * MostRequestedPriority: 节点中空限量越少的，得分越高，与LeastRequested不能同时使用，集中各一个机器上面跑Pod，默认没有启用。\n * NodeLabelPriority: 根据node上面是否拥有特定的标签来评估得分，有标签就有分，而无论其值为何。默认没有启用。\n * ImageLocalityPriority: 一个node的得分高低，是根据node上面是否有镜像，有镜像就有得分，反之就没有(根据node上已有满足需求的image的size大小之和来计算)，默认没有启用。\n\n\n# 高级调度\n\n# nodeSelector\n\n将Pod调度到特定的Node上\n\n\n\n首选要在node上定义相应的labels，然后在pod.spec.nodeSelector中定义需要调度到的相应标签的node。类似于RDBMS中通过select node.name from xxx where node.disktype=ssd and node-flavor=s3.large.2 ，也就是说pod中nodeSelector要完全匹配node上的标签。\n\n这个匹配调度的逻辑，是之前描述的MatchNodeSelector预选调度算法，预选调度算法是必须要满足项的node。\n\n# nodeAffinity\n\n节点亲和性，是调度程序用来确定pod对象调度到哪个node上的一组规则，和nodeselecotr一样，也是基于节点上的自定义标签和Pod对象上指定的标签选择器来进行定义的，这是nodeSelector的升级版本。\n\nnodeAffinity属于优选函数算法中一种。\n\n总体来说，节点亲和性调度可以分为硬亲和(required)和软亲和(preferred)。顾名思义，硬亲和性实现的是强制性规则，它是Pod调度时必须要满足的规则，如果不满足，则Pod对象会被置于Pending状态；而软亲和则是一种柔性调度限制，它倾向于将Pod对象运行于某类特定的节点之上，而调度器也将尽量满足此需求，但是在无法满足调度需求的时候，它将退而求其次地选择一个不匹配规则的节点。\n\n\n\n# 硬亲和\n\n硬亲和的策略在pod中位置于pod.spec.affinity.nodeAffinity.requireDuringSchedulingIgnoreDuringExecution。"IgnoreDuringExecution"表明了，该调度策略只会对当前的pod和node标签，以及相应规则，做个调度匹配。以后要是节点标签发生了变化了，那么已经调度到了该node上的Pod对象不会做出改变，只会对新建的Pod对象生效。老人老办法，新人新规定。\n\n在上述map的中可以包含多个value(nodeSelectorTerms字段)，多个nodeSelecorTerm之间是"逻辑或"的关系，意思是有节点满足其中的一个nodeSelecorTerm就算这个node满足了。\n\nnodeSelectorTerms下面需要可以定义多个matchExpression，多个规则彼此之间为"逻辑与"的关系，也就是说只有该nodeSelecorTerm下面的所有matchExpression定义的规则都满足，这个node才算是满足了。\n\n标签选择器表达式(matchExpression下定义的规则)，支持使用的操作符号有In、NotIn、Exsits、DoesNotExists、Lt和Gt等。In表示的是只要满足后面的集合中的一个就算是满足了条件。\n\n# 软亲和\n\n软亲和的策略在pod中位置于pod.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoreDuringExecution。同理是"IgnoreDuringExecution"。\n\n在上述的map中可以包含多个规则，每个规则都可以配置weight属性以便用于定义其优先级。对于相关的节点分别计算出多个规则的权重值，最后分值高的节点胜出。\n\n\n# podAffinity\n\npod亲和性，顾名思义，调度和判断的主体是之前已经存在的pod，而非上面所说的node。是根据已调度或将要调度的pod的所位于的node的情况，来决定后续的pod将要部署在哪些node上，反映的是后一种pod对已存在pod的一种亲和性的关系的，调度管理。\n\npodAffinity也分为亲和性和反亲和性，每种亲和策略下又分为硬(required)亲和、软(preferred)亲和。\n\nK8S调度器通过内建的MatchInterPodAffinity预选策略为这种调度方式完成节点预选，并基于InterPodAffinityPriority优选函数进行各节点的优选级评估。\n\n# 为什么要有pod亲和性\n\n出于高效通信的需要，偶尔需要把一些pod对象组织在相近的位置(同一节点、机架、区域或地区等)，如某业务的前端Pod和后端Pod(表现为pod亲和性)。或者说要出于安全性或分布式的原因，需要将一些Pod对象在其运行的位置上隔离开来(表现为pod反亲和性)。\n\n# 什么是位置拓扑topologykey\n\nPod亲和性调度需要各相关的Pod对象运行于"同一位置"，而topologykey就恰恰定义了这个一个什么样的类别，比如区域的类别、机架的类别、主机的类别等等。\n\n在定义Pod对象的亲和性与反亲和性时，需要借助于标签选择器来选择被依赖的Pod对象，并根据选出的Pod对象所在节点的标签来判断"同一位置"的 具体意义。\n\n# pod亲和性和node亲和性有什么区别\n\n * 在pod.sepc.affinity存在podAffinity和podAntiAffinity，这两种配置都是对称的。\n * pod亲和性调度中labelSelector的匹配对象是Pod，而node亲和性调度中匹配的是node。\n * pod亲和性调度中匹配到的是根据topologykey定义的一组node，topologykey定义了分组是什么样的一个级别，相同topologykey中的key和value的值为一组。\n * 在pod亲和性中硬亲和过滤规则中，条件间只有逻辑与运算。\n\n\n\n# pod硬亲和调度\n\npod的硬亲和调度的API定义于pod.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution下，通过labelSelector 这个map定义多个匹配表达式。条件间只有逻辑与的运算，这一点和node亲和性有所不同。\n\ntopologyKey: kubernetes.io/hostname是K8S中节点的内建标签，它的值就是当前节点的节点主机名称。同理还有region(地区)、zone(区域)、rack(机架)的拓扑位置的定义。\n\n# pod软亲和调度\n\npod的软亲和调度的API定义于pod.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution下，和node软亲和性调度类型，可以定义多个调度规则，每个规则都可以定义一个权重。\n\n# podAntiAffinity反亲和性\n\n与podAffinity匹配过程相同，只是最终的结果取反。\n\n\n\n\n# 手动调度和DaemonSet\n\n当遇到调度器不工作时，需要考虑到手动调度Pod。\n\n我们只需要在pod.spec.nodeName中直接填上需要调度到的node的名称就可以了。\n\n\n\n# DaemonSet调度\n\n老的版本中DaemonSet中的pod调度都是由controller-manager直接指定pod的运行节点，不经过调度器。直到1.11版本开始，DaemonSet的pod才由scheduler引入调度。\n\nDaemonSet实际上是要求每个节点都部署一个相同的pod，通常用于部署集群中的agent，例如网络插件等。在下图的Daemonset的配置中，可以看出类似于定义了一个Deployment的清单配置文件中，对于要求一个主机上不存在相同的pod label，也就是对于Pod的反亲和性。\n\n\n\n\n# 污点和容忍度\n\n污点(taints)是定义在节点之上的键值型属性数据，用于让节点拒绝将pod调度运行于其上，除非该pod对象具有容纳节点污点的容忍度。\n\n而容忍度(tolerations)是定义在Pod对象上的键值型属性数据，用于配置其可容忍的节点污点，而且调度器仅能将Pod对象调度至其能够容忍该节点污点的节点之上。\n\nK8S中使用PodToleratesNodeTaints预选策略和TaintTolerationPriority优选函数来完成此类高级调度机制。\n\n# 污点容忍度/节点选择器/节点亲和性区别\n\n上面描述的节点选择器(nodeSelector)和节点亲和性两种调度方式都是通过在Pod对象上添加标签选择器来完成对特定类型节点标签的匹配，实现的是由Pod选择节点的机制。\n\n而污点和容忍度则是通过向节点添加污点信息来控制Pod对象的调度结果，从而赋予了节点控制何种Pod对象能够调度与其上的主控权。\n\n节点亲和性是使得Pod对象被吸引到一类特定的节点，而污点则相反，它提供了让节点排斥特定Pod对象的能力。\n\n# 污点\n\n污点(taint)的定义在 node.spec.taints下，是键值型数据，但又额外支持一个效果(effect)标识，语法格式为"key=value:effect"，其中key和value的用法及格式与资源注解信息相似，而effect则用于定义对pod对象的排斥等级，它主要包含以下三种类型。\n\n * NoSchedule: 不能容忍此污点的新Pod对象，不可调度至当前节点，属于强制型约束关系，节点上现存的Pod对象不受影响。\n * PreferNoSchedule: NoSchedule的柔性约束版本，即不能容忍此污点的新Pod对象尽量不要调度至当前节点，不过无其他节点可供调度时也允许接受相应的Pod对象。节点上现存的Pod对象不受影响。\n * NoExecute: 不能容忍此污点的新Pod对象，不可调度至当前节点，属于强制性约束关系，而且节点上现存的Pod对象会因节点污点变动或Pod容忍度变动而不再满足匹配规则时，Pod对象将被驱逐。\n\n\n\n给节点添加污点标识：\n\n$ kubectl taint nodes xxx <key>=<value>:<effect>\n例如：\n$ kubectl taint nodes node1 node-type=production:NoSchedule\n\n\n1\n2\n3\n\n\n即便是同一个键值数据，若其效用标识不同，则也分属于不同的污点信息，也就是说会增加一条污点信息，和之前的区别只是在于效用标识不同。\n\n$  kubectl taint nodes node1 node-type=production:PreferNoSchedule\n\n\n1\n\n\n删除节点上某特定键名，特定标识的污点信息。\n\n$ kubectl taint nodes node1 node-type=NoSchedule-\n\n\n1\n\n\n删除节点上某特定键名的所有污点信息。(也就是省略效用标识)\n\n$ kubectl taint nodes node1 node-type-\n\n\n1\n\n\n删除节点上所有的全部的污点信息，可以使用kubectl patch命令将节点属性spec.taints的值直接置为空即可。\n\n$ kubectl patch nodes node1 -p \'{"spec":{"taints":{}}}\'\n\n\n1\n\n\n# 容忍度\n\nPod对象的容忍度可通过其pod.spec.tolerations字段进行添加，根据使用的操作符不同，主要有两种不同的形式：一种是与污点信息完全匹配的等值关系"Equal"；另一种是判断污点信息存在性的匹配方式"Exists"。其中tolerationSeconds用于定义延迟驱逐当前Pod对象的时长。\n\n\n\n需要注意的如下信息：\n\n * 如果节点node上有多个污点信息，那么就必须该Pod对此节点上的所有污点信息都能容忍，才能调度上去。\n * 匹配逻辑和之前的pod中的nodeselector正好相反，之前的逻辑是只要是node上的一个标签满足于pod中定义的node selector就进行匹配。\n * pod中定义的operator为"Equal"时，就是需要在pod的toerations中完整填写所有的key、value、effect。\n * pod中定义的operator为"Exists"时(key/effect/operator项是必填的，而value项留空)，可以填写value为空，表示的是匹配容忍节点node中所有关于这个key中的相应的"effect"的，所有value的污点的信息。\n * pod中定义的operator为"Exists"时(key/operator项是必填，value和effect为空)，表示的是匹配容忍节点node中所有关于这个key值与之相同的，所有value的所有effect的污点信息。\n\n# 污点和容忍度的调度逻辑\n\n一个节点可以配置使用多个污点，一个Pod对象也可以有多个容忍度，不过二者在进行逻辑检查时会遵循如下逻辑。\n\n 1. 首先处理每个有着与之匹配的容忍度的污点。\n\n 2. 不能匹配到的污点上，如果存在一个污点使用NoSchedule效用标识，则拒绝调度Pod对象至此节点上。\n\n 3. 不能匹配到的污点上，若没有任何一个使用NoSchedule效用标识，但至少有一个使用了PreferNoScheduler，则应尽量避免将Pod对象调度至此节点。\n\n 4. 如果至少有一个不匹配的污点使用了NoExecute效用标识，则节点将立即驱逐Pod对象，或者不予调度至给给定节点；另外，即便容忍度可以匹配到使用了NoExecute效用标识的污点，若在定义容忍度时还同时使用了tolerationSeconds属性定义了容忍时限，则超出时限后其也将被节点驱逐。\n\n\n# Service和Ingress\n\nService端口用于接收客户端请求并将其转发至其后端的Pod中应用的相应端口之上，因此，这种代理机制也称为**"端口代理"或四层代理**，它工作于TCP/IP协议栈的传输层。Service及Pod对象的IP地址都仅在k8s集群内可达，它们无法接入集群外部的访问流量。\n\n解决此类问题的办法中，除了在单一节点上做端口暴露(hostPort)及让Pod资源共享使用工作节点的网络名称空间(hostNetwork)之外，更推荐用户使用的是NodePort或LoadBalancer类型的Service资源，或者是有着七层负载均衡能力的Ingress资源。\n\n\n# Service\n\n一个Service对象就是工作节点上的一些iptables或ipvs规则，用于将到达Service对象IP地址的流量调度转发至相应的Endpoints对象指向的IP地址和端口之上。工作于每个工作节点的kube-proxy组件通过API Server持续监控着各Service及与其关联的Pod对象，并将其创建或变动实时反映至当前工作节点上相应的iptables或ipvs规则上。\n\n客户端、Service及其Pod对象的关系如下图所示：\n\n\n\nService IP事实上是用于生成iptables或ipvs规则时使用的IP地址，它仅用于实现k8s集群网络的内部通信，并且仅能够将规则中定义的转发服务的请求作为目标地址予以响应，这也是它被称为虚拟IP的原因之一。kube-proxy将请求代理至相应端点的方式有三种：userspace(用户空间)、iptables和ipvs。\n\n\n# kube-proxy代理模式\n\n 1. Iptables模式\n\niptables代理模型中，kube-proxy负责跟踪API Server上Service和Endpoints对象的变动(创建或移除)，并据此做出Service资源定义的变动。\n\n对于每个Endpoints对象，Service资源会为其创建iptables规则并关联至挑选的后端Pod资源，默认算法是随机调度(random)。\n\n在创建Service资源时，集群中每个节点上的kube-proxy都会收到通知并将其定义为当前节点上的iptables规则，用于转发工作接口接收到的与此Service资源的ClusterIP和端口的相关流量。客户端发来的请求被相关的iptables规则进行调度和目标地址转换(DNAT)后再转发至集群内的Pod对象之上。\n\n相对于用户空间模型来说，iptables模型无须将流量在用户空间和内核空间来回切换，因而更加高效和可靠。不过，其缺点是iptables代理模型不会在被挑中的后端Pod资源无响应时自动进行重定向，而userspace模型则可以。\n\n 2. ipvs模式\n\nkube-proxy跟踪API Server上Service和Endpoints对象的变动，据此来调用netlink接口创建ipvs规则，并确保与API Server中的变动保持同步。它与iptables规则的不同之处仅在于其请求流量的调度功能由ipvs实现，余下的其他功能仍由iptables完成。\n\n类似于iptables模型，ipvs构建于netfilter的钩子函数之上，但它使用hash表作为底层数据结构并工作于内核空间，因此具有流量转发速度快、规则同步性能好的特性。另外，ipvs支持众多调度算法，例如rr、lc、dh、sh、sed和nq等。\n\n\n# Service类型\n\nK8s的Service共有四种类型：ClusterIP、NodePort、LoadBalancer和ExternalName。\n\n\n# Ingress\n\nIngress是k8s API的标准资源类型之一，它其实就是一组基于DNS名称(host)或URL路径把请求转发至指定的Service资源的规则，用于将集群外部的请求流量转发至集群内部完成服务发布。\n\n然而，Ingress资源自身并不能进行"流量穿透"，它仅是一组路由规则的集合，这些规则要想真正发挥作用还需要其他功能的辅助，如监听某套接字，然后根据这些规则的匹配机制路由请求流量。这种能够为Ingress资源监听套接字并转发流量的组件称为Ingress控制器(Ingress Controller)。\n\n> 不同于Pod控制器Deployment，Ingress控制器并不直接运行为kube-controller-manager的一部分，它是k8s集群的一个重要附件，类似于CoreDNS，需要在集群上单独部署。\n\nIngress控制器可以由任何具有反向代理(HTTP/HTTPS)功能的服务程序实现，如Nginx、Envoy、HAProxy、Vulcand和Traefik等。Ingress控制器自身也是运行于集群中的Pod资源对象，它与被代理的运行为Pod资源的应用运行于同一网络中。\n\n如下图中，ingress-nginx与pod1、pod3等的关系所示：\n\n\n\n\n# Volumes\n\nk8s支持非常丰富的存储卷类型，包括本地存储(节点)和网络存储系统中的诸多存储机制，甚至还支持Secret和ConfigMap这样的特殊存储资源。目前，k8s支持的存储卷包含以下这些类型。\n\n\n\n上述类型中，emptyDir和hostPath属于节点级别的卷类型，emptyDir的生命周期与Pod资源相同，而使用了hostPath卷的Pod一旦被重新调度至其他节点，那么它将无法再使用此前的数据。因此，这两种类型都不具有持久性。要想使用持久类型的存储卷，就得使用网络存储系统，如NFS、Ceph、GlusterFS等，或者云端存储，如gcePersistentDisk、awsElasticBlockStore等。\n\n\n# PV/PVC\n\nPersistentVolume(PV**)是指由集群管理员配置提供的某存储系统上的一段存储空间，它是对底层共享存储的抽象**.\n\nPV是集群级别的资源，不属于任何名称空间，用户对PV资源的使用需要使用PersistentVolumeClaim(PVC)提出的使用申请(或称为声明)来完成绑定，是PV资源的消费者，它向PV申请特定大小的空间及访问模式(如rw或ro)，从而创建出PVC存储卷，而后再由Pod资源通过PersistentVolumeClaim存储卷关联使用。如下图所示：\n\n\n\nk8s自1.4版本起引入了一个新的资源对象StorageClass，可用于将存储资源定义为具有显著特征的类别(Class)而不是具体的PV，例如"fast" "slow" 或"glod" "silver" "bronze"等。用户通过PVC直接向意向的类别发出申请，匹配由管理员事先创建的PV，或者由其按需为用户动态创建PV，这样做甚至免去了需要事先创建PV的过程。\n\n\n# ConfigMap和Secret\n\nConfigMap对象用于为容器中的应用提供配置数据以定制程序的行为，不过敏感的配置信息，例如密钥、证书等通常由Secret对象来进行配置。\n\n它们将相应的配置信息保存于对象中，而后在Pod资源上以存储卷的形式将其挂载并获取相关的配置，以实现配置于镜像文件的解耦。也可以通过环境变量的方式进行挂载。\n\n\n# 网络模型\n\n\n# K8S网络模型\n\nK8s的网络模型主要可用于解决四类通信需求：同一Pod内容器间的通信(Container to Container)、Pod间的通信(Pod to Pod)、Service到Pod间的通信(Service to Pod)以及集群外部与Service之间的通信(external to Service)。\n\n目前K8S支持使用CNI插件来编排网络，以实现Pod及集群网络管理功能的自动化。每次Pod被初始化或删除时，kubelet都会调用默认的CNI插件创建一个虚拟设备接口附加到相关的底层网络，为其设置IP地址、路由信息并将其映射到Pod对象的网络名称空间。\n\n\n# CNI插件\n\nCNI本身只是规范，付诸生产还需要有特定的实现。常见的CNI网络插件包含以下这些主流的项目。\n\n * Flannel：\n   \n   一个为k8s提供叠加网络的网络插件，它基于Linux TUN/TAP，使用UDP封装IP报文来创建叠加网络，并借助etcd维护网络的分配情况。\n\n * Calico：\n   \n   一个基于BGP的三层网络插件，并且也支持网络策略来实现网络的访问控制；它在每台机器上运行一个vRouter，利用Linux内核来转发网络数据包，并借助iptables实现防火墙等功能。\n\n * Canal：\n   \n   由Flannel和Calico联合发布的一个统一网络插件，提供CNI网络插件，并支持网络策略。\n\n * Weave Net：\n   \n   Weave Net是一个多主机容器的网络方案，支持去中心化的控制平面，在各个host上的wRouter间建立Full Mesh的TCP连接，并通过Gossip来同步控制信息。数据平面上，Weave通过UDP封装实现L2 Overlay，封装支持两种模式，一种是运行在user space的sleeve mode，另一种是运行在kernel space的fastpath mode。\n\n\n# Flannel/Calico的区别\n\n * Flannel是一个大二层网络。\n * Calico是一个三层的虚拟网络方案。支持丰富的网络策略，比如namespace网络隔离，Pod出入站流量控制。\n\n\n# HPA\n\n手动调整的Pod控制器副本方式依赖于用户深度参与监控容器应用的资源压力并且需要计算出合理的值进行调整，存在一定程度的滞后性。K8S提供了多种自动弹性伸缩(Auto Scaling)工具。\n\nHorizontal Pod Autoscaler，一种支持控制器对象下Pod规模弹性伸缩的工具，目前有两个版本的实现，分别称为HPA和HPA(v2)，前一种仅支持把CPU指标数据作为评估基准，而新版本支持可从资源指标API和自定义指标API中获取的指标数据。\n\n\n# Helm\n\n\n# 容器和虚拟机有什么区别联系\n\n在解释这个问题的之前，先要了解一下什么是容器。\n\n\n# 什么是容器\n\n容器是一种轻量级、可移植、自包含的软件打包技术，它使得应用程序可以在几乎任何地方以相同的方式运行。\n\n容器由应用程序本身和它的环境依赖(库和其他应用程序)两部分组成，并在宿主机(Host)操作系统的用户空间中运行，但与操作系统的其他进程互相隔离。\n\n它们的实现机制有别于诸如Vmware、KVM和Xen等实现方案的传统虚拟化技术。\n\n\n# 两者区别如下\n\n由于同一个宿主机上的所有容器都共享其底层操作系统(内核空间)，这就使得容器体积上要比传统的虚拟机小得多。\n\n另外，启动容器无需启动整个操作系统，所以容器部署和启动的速度更快，开销更小，也更容易迁移。事实上，容器赋予了应用程序超强的可移植能力。\n\n\n\n\n# Kubernetes特性有哪些\n\n\n# 什么是K8S\n\nKubernets是一种用于在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可扩展性与高可用性的方法来完全管理容器化应用程序和服务的生命周期的平台。\n\n用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。k8s提供了接口和可组合的平台原语，使得用户能够以高度的灵活性和可靠性定义及管理应用程序。\n\n\n# 如下重要特性\n\n 1. 自动装箱\n    \n    建构与容器之上，基于资源依赖及其他约束自动完成容器部署且不影响其可用性，并通过调度机制混合关键型应用和非关键型应用的工作负载与同一节点以提升资源利用率。\n\n 2. 自我修复(自愈)\n    \n    支持容器故障后自动重启、节点故障后重新调度容器，以及其他可用节点、健康状态检查失败后关闭容器并重新创建等自我修复机制。\n\n 3. 水平扩展\n    \n    支持通过简单命令或UI手动水平扩展，以及基于CPU等资源负载率的自动水平扩展机制。\n\n 4. 服务发现和负载均衡\n    \n    K8s通过其附加组件之一的KubeDNS(或CoreDNS)为系统内置了服务发现功能，它会为每个Service配置DNS名称，并允许集群内的客户端直接使用此名称发出访问请求，而Service则通过iptables或ipvs内建了负载均衡机制。\n\n 5. 自动发布和回滚\n    \n    K8S支持**"灰度"更新应用程序或其配置信息**，它会监控更新过程中应用程序的健康状态，以确保它不会在同一时刻杀掉所有实例，而此过程中一旦有故障发生，就会立即自动执行回滚操作。\n\n 6. 密钥和配置管理\n    \n    Kubernetes的ConfigMap实现了配置数据与Docker镜像解耦，需要时，仅对配置做出变更而无须重新构建Docker镜像，这为应用开发部署带来了很大的灵活性。此外，对于应用所依赖的一些敏感数据，如用户名和密码、令牌、密钥等信息，K8s专门提供了Secret对象为其解耦，既便利了应用的快速开发和交付，又提供了一定程度上的安全保障。\n\n 7. 存储编排\n    \n    K8s支持Pod对象按需自动挂载不同类型的存储系统，这包括节点本地存储、公有云服务商的云存储，以及网络存储系统(例如，NFS、iSCSI、GlusterFS、Ceph、Cinder和Flocker等)。\n\n 8. 批量处理执行\n    \n    除了服务型应用，K8s还支持批处理作业及CI(持续集成)，如果需要，一样可以实现容器故障后恢复。\n\n\n# K8S的list-watch\n\nlist-watch 是K8S设计中的精髓所在，理解list-watch对用来整体理解K8S有着很大的帮助。\n\n下面所有做的该知识点的整理和总结，都是基于网络上如下URL做的整理。\n\n> https://istio.cn/t/topic/157\n> \n> https://www.kubernetes.org.cn/174.html\n\n\n# 为什么需要list-watch这种机制\n\nK8S各个组件之间仅采用的是HTTP协议通信，没有依赖中间件。而我们在使用K8S的过程中，必然会有对资源处理实时性，完整性，可靠性的强烈需求的。\n\n而List-Watch 就是基于HTTP协议开发的，是K8S重要的异步信息通知机制。它通过list获取全量数据，通过watch API来监听增量数据，保证了消息的可靠性、实时性，性能和顺序性。\n\n说白了就是k8s为了满足于异步消息处理的系统。\n\n\n# 什么是List-Watch\n\nlist-watch 有两部分组成，分别是 list和watch。list就是调用资源的list API罗列资源，基于HTTP短连接实现。watch则是调用资源的watch API监听资源变更事件，基于HTTP长连接实现，watch是一个典型的发布-订阅模式。\n\n以pod资源为例，它的List API如下，返回值是PodList，即一组pod。\n\nGET /api/v1/pods\n\n\n1\n\n\n它的Watch API如下，往往带上watch=true，表示采用HTTP长连接持续监听pod相关事件，每当有事件来临是，返回一个WatchEvent。\n\nGET /api/v1/watch/pods\n\n\n1\n\n\n\n# 应用模块\n\nK8S的informer模块中就封装了list-watch API，用户只需要指定资源，编写相应的事件处理函数，例如AddFunc，UpdateFunc和DeleteFunc等。\n\n在下图中，通过对apiserver的list API罗列资源和watch API监听资源的变更事件后，将反馈的一系列结果放入到了一个FIFO队列中，队列的另一头有协程从中取出事件，并调用对应的注册函数处理事件。Informer还维护了一个只读的Map Store缓存，主要是为了提升查询的效率，降低apiserver的负载。\n\n\n\n\n# 设计理念\n\nList-Watch就是一个异步消息的系统，一般我们对消息系统有至少如下四点要求：消息可靠性、消息实时性、消息顺序性、高性能。\n\nlist-watch如何实现消息的可靠性：\n\nlist API可以查询到当前的资源及其对应的状态(即期望的状态)，客户端通过拿期望的状态和实际的状态进行对比，纠正状态不一致的资源。Watch API保持和api server一个长链接，接收资源的状态变更事件并做相应处理。如果仅调用watch API，若某个时间点连接中断，就有可能导致消息丢失，所以需要通过list API解决消息丢失的问题。从另一个角度来看，list API获取全量数据，watch API获取增量数据。虽然仅仅通过轮询list API，也能达到同步资源状态的效果，但是存在开销大，实时性不足的问题。\n\nlist-watch如何实现消息的实时性：\n\n每当apiserver的资源发生了状态变更事件，都会将事件及时的推送给客户端，从而保证了消息的实时性。\n\nlist-watch如何实现消息的顺序性：\n\n在并发的场景下，客户端在短时间内可能会收到同一个资源的多个事件，对于关注最终一致性的K8S来说，它需要知道哪个是最近发生的事件，并保证资源的最终状态如同最近事件所表述的状态一样。K8S在每个资源的事件中都带一个resourceVersion的标签，这个标签是递增的数字，所以当客户端并发处理同一个资源的事件时，它就可以对比resourceVersion来保证最终的状态和最新的事件所期望的状态保持一致。\n\nlist-watch如何实现消息的高性能：\n\n仅通过周期性调用list API也能达到资源最终一致性的效果，但是周期性频繁的轮询大大的增大了开销，增加了apiserver的压力。而watch作为异步消息通知机制，复用一条长链接，保证实时性的同时也保证了性能。\n\n\n# Kube-proxy中的ipvs模式和iptables模式\n\n每个工作节点都需要运行一个kube-proxy守护进程，能够按需为Service资源对象生成iptables或ipvs规则，从而捕获访问当前Service的ClusterIP的流量并将其转发至正确的后端Pod对象。\n\n\n# iptables模式是什么\n\n\n# ipvs模式是什么\n\nLVS由两部分组成，ipvs和ipvsadm。其中ipvsadm是LVS的管理工具，管理员通过ipvsadm定义或管理集群规则。\n\n\n# 两者的区别和联系\n\n\n# K8S删除node\n\n 1. 查看现有集群中有哪些节点\n    \n    $ kubectl get nodes\n    \n    \n    1\n    \n\n 2. 删除节点前，先驱赶掉上面的pod\n    \n    $ kubectl drain node-06 --delete-local-data --force --ignore-daemonsets\n    \n    \n    1\n    \n    \n    此时节点上面的pod开始迁移。\n\n 3. 再次检查节点，被标记为不可调度节点\n    \n    $ kubectl get nodes\n    node-06   Ready,SchedulingDisabled   <none>   2d18h   v1.14.1\n    \n    \n    1\n    2\n    \n\n 4. 最后删除节点\n    \n    $ kubectl delete node node-06\n    $ kubectl get nodes\n    \n    \n    1\n    2\n    \n\n\n# 修改docker storage pool\n\n\n# 查看storage pool配置\n\ndocker info \nmore /usr/lib/systemd/system/docker.service\n\n\n1\n2\n\n\n\n# 清理docker 空间\n\ndocker system prune -a -f\n\n\n1\n\n\n\n# 修改空间大小步骤如下\n\n 1. 设置节点不可调度 kubectl cordon xxx.xxx.xxx.xxx\n 2. 驱逐节点pod kubectl drain xxx.xxx.xxx.xxx\n 3. 停止docker systemctl stop docker\n 4. 修改docker配置文件 /usr/lib/systemd/system/docker.service 在ExecStart这一行加一个参数 ExecStart=/usr/bin/dockerd --storage-opt dm.loopdatasize=500G -g /app/data/ips/docker \\\n 5. systemctl daemon-reload && systemctl start docker\n 6. 取消不可调度 kubectl uncordon xxx.xxx.xxx.xxx\n\n\n# 修改最大的pod限制\n\nhttp://blog.schoolofdevops.com/how-to-increase-the-number-of-pods-limit-per-kubernetes-node/',normalizedContent:'kubernetes知识点整理\n\n\n# 概述\n\n\n# 基本概念\n\n按mater/node来区分\n\nmaster: 是集群的网关和中枢，负责诸如为用户和客户端暴露api、跟踪其他服务器的健康状态、以最优方式调度工作负载，以及编排其他组件之间的通信等任务。通常包含api-server、controller-manager、scheduler、etcd(可外置)。\n\nnode: 是k8s集群的工作节点，负责接收来自master的工作指令并根据指令相应地创建或销毁pod对象，以及调整网络规则以合理地路由和转发流量等。\n\n按模块组件来区分\n\npod：\n\n是一组功能相关的container的封装；共享存储和network namespace；是k8s调度和作业运行的基本单位(scheduler调度，kubelet运行)；pod容易"走失"，需要workload和service的保障。\n\nworkloads(pod控制器):\n\n尽管pod是k8s的最小调度单元，但是用户通常并不会直接部署及管理pod对象，而是要借助于另一类抽象 ---控制器(controller)对其进行管理。 有deployment，statefulset，daemonset，job ....，实际上是一组功能相关的pod的封装。使用控制器之后就不再需要手动管理pod对象了，用户只需要声明应用的期望状态，控制器就会自动对其进行进程管理。\n\nservice:\n\n主要是为了让pod "防失联"，给一组pod设置反向代理。建立在一组pod对象之上的资源抽象，它通过标签选择器选定一组pod对象，并为这组pod对象定义一个统一的固定访问入口(通常是一个ip地址)。\n\ningress:\n\nk8s将pod对象和外部网络环境进行了隔离，pod和service等对象间的通信都使用其内部专用地址进行，如若需要开放某些pod对象提供给外部用户访问，则需要为其请求流量打开一个通往k8s集群内部的通道。除了service之外，ingress也是这类通道的实现方式之一。\n\nvoumes:\n\n存储卷(volume)是独立于容器文件系统之外的存储空间，常用于扩展容器的存储空间并为它提供持久存储能力。k8s集群上的存储卷大体上可以分为临时卷、本地卷和网络卷。\n\nname和namespace:\n\n名称(name)是k8s集群中资源对象的标识符，它们的作用域通常是名称空间(namespace)，因此名称空间是名称的额外的限定机制。在同一个名称空间中，同一类型资源对象的名称必须具有唯一性。名称空间通常用于实现租户或项目的资源隔离。\n\nannotation:\n\nannotation(注解)是另一种附加在对象之上的键值类型的数据，但它拥有更大的数据容量。annotation常用于各种非标识型元数据(metadata)附加到对象上，但它不能用于标识和选择对象，通常也不会被k8s直接使用，其主要目的是方便工具或用户的阅读及查找等。\n\n\n# master组件\n\n\n# api server\n\napi server负责输出restful风格的k8s api，它是发往集群的所有rest操作命令的接入点，并负责接收、校验并响应所有的rest请求，结果状态被持久化存储于etcd中。因此，api server是整个集群的网关。\n\n\n# etcd集群\n\nk8s集群的所有状态信息都需要持久存储于存储系统etcd中，不过，etcd是由coreos基于raft协议开发的分布式键值存储，可用于服务发现、共享配置以及一致性保障(如数据库主节点选择、分布式锁等)。\n\netcd不仅能提供键值数据存储，而且还为其提供了监听(watch)机制，用于监听和推送变更。k8s集群系统中，etcd中的键值发生变化时会通知到api server，并由其通过watch api向客户端输出。基于watch机制，k8s集群的各组件实现了高效协同。\n\n\n# controller manger\n\n控制器完成的功能主要包括生命周期功能和api业务逻辑 .\n\n生命周期管理：包括namespace创建和生命周期、event垃圾回收、pod终止相关的垃圾回收、级联垃圾回收及node垃圾回收等。\n\napi业务逻辑：例如，由replicaset执行的pod扩展等。\n\n\n# 调度器scheduler\n\nk8s是用于部署和管理大规模容器应用的平台。api server 确认pod对象的创建请求之后，便需要由scheduler根据集群内各节点的可用资源状态，以及要运行的容器的资源需求做出调度决策。另外，k8s还支持用户自定义调度器。\n\n\n# node组件\n\n\n# kubelet\n\n是node的核心代理程序，是运行于工作节点之上的守护进程，它从api server接收关于pod对象的配置信息并确保它们处于期望的状态(desired state，"目标状态")。kubelete会在api server上注册当前工作节点，定期向master汇报节点资源的使用情况，并通过cadvisor监控容器和节点的资源占用情况。\n\n\n# container runtime\n\n每个node都要提供一个容器运行时(container runtime)环境，它负责下载镜像并运行容器。kubelet并未固定链接至某容器运行时环境，而是以插件的方式载入配置的容器环境。这种方式清晰地定义了各组件的边界。目前，k8s支持的容器运行环境至少包括docker、rkt、cri-o和fraki等。\n\n\n# kube-proxy\n\n每个工作节点都需要运行一个kube-proxy守护进程，它能够按需为service资源对象生成iptables或ipvs规则，从而捕获访问当前service的clusterip的流量并将其转发至正确的后端pod对象。\n\n\n# 核心附件\n\n\n# coredns\n\nk8s从1.11版本开始默认使用coredns项目为集群提供服务注册和服务发现的动态名称解析服务。\n\n\n# kubernetes dashboard\n\nk8s集群的全部功能都要基于web的ui，俩管理集群中的应用甚至集群自身。\n\n\n# ingress controller\n\nservice 是一种工作与传统层的负载均衡器，而ingress是在应用层实现的https(s)负载均衡机制。\n\n不过，ingress资源自身并不能进行"流量穿透"，它仅是一组路由规则的集合，这些规则需要通过ingress控制器(ingress controller)发挥作用。目前，此类的可用项目有nginx、trafik、envory及haproxy等。\n\n\n# 基础架构\n\n\n# 模块架构信息\n\n如下是k8s的架构图，展示了k8s中的各个组件模块的信息。\n\n\n\n整体来说，k8s的架构是master、node的模式，master节点上通常部署有scheduler、controller-manager、api-server，以及etcd分布式数据库。node节点上通常运行着kubelet、kube-proxy组件，以及真正运行docker实例的pod容器。\n\n用户通过kubectl，经过一些列的集群的安全认证后，将所需要执行的资源清单配置文件提交给api-server，api-server会将此信息写入etcd，写入完成后，etcd向api-server发起一系列的事件信息，通过list-watch的机制，先后由controller-manager执行副本配置，scheduler完成调度node，kubelet完成落实到各自node上启动相应的pod。而kube-proxy则用于相应的server资源转发到所关联的一系列的pod对象上。\n\n\n# 高可用架构1\n\n\n\n * etcd自身提供的分布式存储集群为k8s构建一个可靠的存储层。\n * 将无状态的apiserver运行为多副本，并在其前端使用负载均衡器调度请求；需要注意的是，负载均衡器本身也需要是高可用。\n * 多副本的控制器管理器，通过其自带的leader选举功能(--leader-election)选举出主角色，余下的副本在主角色发送故障时自动启动新一轮的选举操作。\n * 多副本的调度器，通过其自带的leader选举功能(--leader-election)选举出主角色，余下的副本在主角色发生故障时自动启动新一轮的选举操作。\n\n\n# 高可用架构2\n\n如下是结合了kubease的工具的架构图示。\n\n\n\n * master节点只需要2台即可。\n * 从原有的服务器端负载均衡转为了node客户端负载均衡。\n * 无须申请vip资源。\n * 免去维护外部ha lb的问题。\n\n> kubeasz 项目地址：https://github.com/easzlab/kubeasz\n\n\n# 工作原理\n\n如下的图示是完整的工作原理的解析，其中核心的重点概念就是list-watch的机制。\n\n\n\n上面所有的"0"示意图，都是表示的是从k8s环境部署完毕后，各个api-server的客户端(api-server、scheduler、kubelet)都对于各个所订阅的资源对象，进行watch。\n\nlist-watch的异步消息通信机制，有效的保证了消息的可靠性、及时性、顺序性、高性能等。\n\n具体的list-watch机制，可以参考下面章节的内容整理。\n\n\n# api对象\n\n\n# 声明式和命令式\n\nkubectl的命令由此可以分为三类：陈述式命令、陈述式对象配置和声明式对象配置。第一种方式即此前用到的run、expose、delete和get等命令，它们直接作用于k8s系统上的活动对象，简单易用。\n\n声明式的使用通常要依赖于资源配置文件，声明式对象配置并不直接指明要进行的对象管理操作，而是提供配置清单文件给k8s系统，并委托系统跟踪活动对象的状态变动。\n\n\n# api对象的基本构成\n\n\n\n整体来说，在k8s中一个完整的api对象由四大部分组成。\n\ntypemeta: 主要由apiversion和kind两个构成。主要描述的是该类api资源对象的类别和版本。\n\nobjectmeta: metadata类别，涵盖该类型资源对象下的基本元数据的信息，包含一些名称、label、注解等。\n\nspec: 这是关键的信息内容，涵盖定义的所有的期望状态信息。\n\nstatus: 这个不是客户端所需要配置的，反映的是当前该类资源对象的实际的状态值。\n\n\n# pod\n\n绝大多数场景中都应该于**一个容器中仅运行一个进程。\n\n不过，分别运行于各自容器的进程之间无法实现基于ipc的通信机制，此时，容器间的隔离机制对于依赖于此类通信方式的进程来说却又成了阻碍。pod资源抽象正是用来解决此类问题的组件。前面提到，pod对象是一组容器的集合，这些容器共享network、uts及ipc名称空间，因此具有相同的域名、主机名和网络接口，并可通过ipc直接通信。\n\n为一个pod对象中的各容器提供网络名称空间等共享机制的是底层基础容器pause。如下图所示，一个由三个容器组成的pod资源，各容器共享network、ipc和uts名称空间，但分别拥有各自的mnt、usr和pid名称空间。需要特别强调的是，一个pod对象中的多个容器必须运行于同一工作节点之上。\n\n\n\n\n# 多container pod模型\n\nk8s系统的pod资源对象用于运行单个容器化应用，此应用称为pod对象的主容器，同时pod也能容纳多个容器，不过额外的容器一般工作为sidecar模式，用于辅助主容器完成工作职能。\n\n\n# sidercar pattern(边车模式)\n\n即为pod的主应用容器提供协同的辅助应用容器，每个应用独立运行，最为典型的代表是将主应用容器中的日志使用agent收集至日志服务器中时，可以将agent运行为辅助应用容器，即sidecar。另一个典型的应用是为主应用容器中的database server启用本地缓存。如下图：\n\n\n\n\n# ambassador pattern(大使模式)\n\n即为远程服务创建一个本地代理，代理应用运行于容器中，主容器中的应用通过代理容器访问远程服务。如下图所示，一个典型的使用示例是主应用容器中的进程访问"一主多从"模型的远程redis应用时，可在当前pod容器中为redis服务创建一个ambassador container，主应用容器中的进程直接通过localhost接口访问ambassador container即可。即便是redis主从集群架构发生变动时，也仅需要将ambassador container加以修改即可，主应用容器无须对此作出任何反应。\n\n\n\n\n# adapter pattern(适配器模型)\n\n此种模型一般用于将主应用容器中的内容进行标准化输出，例如，日志数据或指标数据的输出，这有助于调用者统一接收数据的接口，如下图所示。另外，某应用滚动升级后的版本不兼容旧的版本时，其报告信息的格式也存在不兼容的可能性，使用adapter container有助于避免那些调用此报告数据的应用发生错误。\n\n\n\n\n# 容器探测\n\nk8s支持三种处理器用于pod探测，任何一种探测方式都可能存在三种结果："success"(成功)、"failure"(失败)或"unknown"(未知)，只有第一种结果表示成功通过检测。\n\n * execaction：在容器中执行一个命令，并根据其返回的状态码进行诊断的操作称为exec探测，状态码为0表示成功，否则即为不健康状态。\n * tcpsocketaction：通过与容器的某tcp端口尝试建立连接进行诊断，端口能够成功打开即为正常，否则为不健康状态。\n * httpgetaction：通过向容器ip地址的某指定端口的指定path发起http get请求进行诊断，响应码为2xx或3xx时即为成功，否则为失败。\n\nkubelet可在活动容器上执行两种类型的检测：存活性检测(livenessprobe)和就绪性检测(readinessprobe)。\n\n * 存活性检测：用于判断容器是否处于"运行"(running)状态；一旦此类检测未通过，kubelet将杀死容器并根据其restartpolicy决定是否将其重启；未定义存活性检测的容器的默认状态为"success"。\n * 就绪性检测：用于判断容器是否准备就绪并可对外提供服务；未通过检测的容器意味着其尚未准备就绪，端点控制器(如service对象)会将其ip从所有匹配到此pod对象的service对象的端点列表中移除；检测通过之后，会再次将其ip添加至端点列表中。\n\n\n# 存活性探测\n\n有不少应用程序长时间持续运行后会逐渐转为不可用状态，探测到容器的状态为不可用时，仅能通过重启操作恢复。\n\n容器的存活性探测主要配置有如下：\n\n这些属性可以通过"spec.containers.livenessprobe"的如下属性字段来给出。\n\n * initialdelayseconds <integer>：存活性探测延迟时长，及容器启动多久之后开始第一次探测操作，显示为delay属性；默认为0秒，及容器启动后立刻便开始进行探测。\n * timeoutseconds <integer>：存活性探测的超时时长，显示为timeout属性，默认为1s，最小值也是1s。\n * periodseconds <integer>：存活性探测的额度，显示为period属性，默认为10s，最小值为1s；过高的频率会对pod对象带来较大的额外开销，而过低的频率又会使得对错误的反应不及时。\n * successthreshold <integer>：处于失败状态时，探测操作至少连续多少次的成功才被认为是通过检测，显示为#success属性，默认值为1，最小值也为1。\n * failurethreshold <integer>：处于成功状态时，探测操作至少连续多少次的失败才被视为是检测不通过，显示为#failure属性，默认值为3，最小值为1。\n\n\n# 就绪性探测\n\npod对象启动后，容器应用通常需要一段时间才能完成其初始化过程，例如加载配置或数据，甚至有些程序需要运行某类的预热过程，应该避免于pod对象启动后立即让其处理客户端请求，而是等待容器初始化工作执行完成并转为"就绪"状态。\n\n未定义就绪性探测的pod对象在pod进入"running"状态后将立即就绪。\n\n与存活性探测触发的操作不同的是，探测失败时，就绪性探测不会杀死或重启容器以保证其健康性，而是通知其尚未就绪，并触发依赖于其就绪状态的操作(例如，从service对象中移除此pod对象)以确保不会有客户端请求接入此pod对象。\n\n\n# limit和request\n\n目前来说，资源隔离尚且属于容器级别，cpu和内存资源的配置需要在pod中的容器上进行，每种资源均可由**"request"属性定义其请求的确保可用值**，即容器运行可能用不到的这些额度的资源，但用到时必须要确保有如此多的资源可用，而**"limits"属性则用于限制资源可用的最大值，即硬限制**。\n\n只有当节点上可分配资源量>=容器资源请求数时才允许将容器调度到该节点上。\n\n但request参数不限制容器的最大可使用资源。limits，是容器能使用的资源的最大值，设置为0表示使用资源无上限。\n\nk8s系统中，cpu的分配支持分数计量方式，一个核心(1core)相当于1000个微核心(millicores)，因此500m相当于是0.5个核心，即二分之一个核心。内存的计量方式于日常使用方式相同，默认单位是字节，也可以使用e、p、t、g、m和k作为单位后缀，或ei、pi、ti、mi和ki形式的单位后缀。\n\n\n# deployment控制器\n\ndeployment(简称为deploy)是k8s控制器的又一种实现，它构建于replicaset控制器之上，可为pod和replicaset资源提供声明式更新。\n\n主要是配置期望副本数、定义pod模板、定义标签选择器、回滚、暂停和启动、滚动升级。\n\n\n# 滚动更新\n\n滚动升级是默认的更新策略，它在删除一部分旧版本pod资源的同时，补充创建一部分新版本的pod对象进行应用升级，其优势是升级期间，容器中应用提供的服务不会中断，但要求应用程序能够应对新旧版本同时工作的情形，例如新旧版本兼容同一个数据库方法等。\n\n滚动升级的过程中，会创建另外一个replicaset控制器来完成新版本的pod升级，现有的pod对象会处于两个不同的replicaset之下，旧控制器的pod对象数量不断减少的同时，新控制器的pod对象数量不断增加，直到旧控制器不再拥有pod对象，而新控制器的副本数量变得完全符合期望值为止。\n\n\n# maxsurge和maxunavailable\n\n滚动更新时，应用升级期间还要确保可用的pod对象数量不低于某阀值以确保可用持续处理客户端的服务请求，变动的方式和pod对象的数量范围将通过spec.strategy.rollingupdate.maxsurge和spec.strategy.rollingupdate.maxunavailable两个属性协同进行定义。\n\n * maxsurge：指定升级期间存在的总pod对象数量最多可超出期望值的个数，其值可用是0或正整数，也可以是一个期望值的百分比；例如，如果期望值为3，当前的属性值为1，则表示pod对象的总数不能超过4个。\n * maxunavailable：升级期间正常可用的pod副本数(包括新旧版本)最多不能低于期望数值的个数，其值可以是0或正整数，也可以是一个期望值的百分比；默认值为1，该值意味着如果期望值是3，则升级期间至少要有两个pod对象处于正常提供服务的状态。\n\n\n\n> maxsurge和maxunavailable属性的值不可同时为0，否则pod对象的副本数量在符合用户期望的数量后无法做出合理变动以进行滚动更新操作。\n\n\n# 扩容和缩容\n\n通过修改spec.replicas既可以修改deployment控制器中的pod控制器的副本数量，它将实时作用于控制器并直接生效。\n\n另外，"kubectl scale"是专用于扩展某些控制器类型的应用规模的命令。\n\n\n# daemonset控制器\n\n用于在集群中的全部节点上同时运行一份指定的pod资源副本，后续新加入集群的工作节点也会自动创建一个相关的pod对象，当从集群移除节点时，此类pod对象也将被自动回收而无须重建。\n\n也可以使用节点选择器及节点标签指定仅在部分具有特定特征的节点上运行指定的pod对象。\n\ndaemonset是一种特殊的控制器，它有特定的应用场景，通常运行那些执行系统级操作任务的应用，其应用场景具体如下。\n\n * 运行集群存储的守护进程，如在各个节点上运行的glusterd或ceph。\n * 在各个节点上运行日志收集守护进程，如fluentd和logstash。\n * 在各个节点上运行监控系统的代理守护进程，如prometheus node exporter、collectd、datadong agent、new relic agent或ganglia gmond等。\n\n当然，既然是需要运行于集群内的每个节点或部分节点，于是很多场景中也可以把应用直接运行为工作节点上的系统守护进程，不过，这样一来就失去了运行k8s管理所带来的便捷性。\n\n另外，也只有必须将pod对象运行于固定的几个节点并且需要先于其他pod启动时，才有必要使用deamonset控制器，否则就应该使用deployment控制器。\n\n\n# statefulset控制器\n\nreplicaset控制器所创建的pod从本质上都是一个模板出来的，这些pod资源除了主机名和ip都没有本质上的区别。所管理的这些pod资源启动也不需要考虑前后顺序，任何一个pod资源都可以被replicaset控制器重构出的新版本所替代，管理员更多关注的也是它们的群体特征，而无须过于关注任何一个个体。例如tomcat、jetty、nginx等。\n\n而statefulset这类控制器管理的pod对象可能有如下场景：应用程序在处理客户端请求时，对当前请求的处理需要以前一次或多次的请求为基础进行，新客户端发起的请求则会被其施加专用标识，以确保其后续的请求可以被识别。例如，rdbms系统上处于同一个事务中的多个请求不但彼此之间存在关联性，而且还要以严格的顺序执行。这类应用一般需要记录请求连接的相关信息，即"状态"，有的甚至还需要持久保存由请求生成的数据，尤其是存储服务类的应用，运行于k8s系统上时需要用到的持久存储卷。\n\nreplicaset控制器管理的pod无法为每个pod单独指定不同的volume挂载，而statefulset(有状态副本集)则是专门用来满足此类应用的控制器类型，由其管控的每个pod对象都有着固定的主机名和专有存储卷，即便被重构后也能保持不变。支持每个pod对象一个专有索引、有序部署、有序终止、固定的标识符及固定的存储卷等特性。\n\n\n# scheduler\n\nk8s中默认的调度器的核心目标就是基于资源可用性将各pod资源公平地分布于集群节点之上。目前默认的调度器通过三个步骤来完成调度操作：节点预选(predicate)、节点优先级排序(priority)以及节点择优(select)。\n\n> 下面有些调度内容，摘录自网上https://blog.csdn.net/qq_34857250/article/details/90259693\n> \n> https://www.cnblogs.com/l-dongf/p/12327401.html\n\n调度策略，分为预选策略和优选策略。预选策略，predicate是强制性规则，会遍历所有的node节点，依据具体的预选策略筛选出符合要求的node列表，如果没有node符合predicates策略规则，那么pod就会被挂起，直到有node能够满足。优选策略，这一步会在第一步筛选的基础上，按照优选策略为待选node打分排序，获取最优者。\n\n\n# 预选策略\n\n必须完全满足\n\n * checknodeconditon: 检查node是否正常。\n * generalpredicates: 普通判断策略\n   * hostname: 检测pod对象是否定义了pod.spec.hostname，并且检查节点中是否有同名的pod而冲突。\n   * podfithostports: 检查pod.spec.containers.ports.hostport属性(绑定节点上的某个端口)是否定义，并且检查节点中的节点端口是否冲突。\n   * matchnodeselector: pods.spec.nodeselector，检查节点选择器。\n   * podfitsresources: 检查pod的资源需求request是否能被节点所满足。\n * nodiskconflict: 检测pod依赖的存储卷是否能满足需求，默认不检查。\n * podtoleratesnodetaints: pods.spec.tolerations可容忍的污点，检查pod是否能容忍节点上的污点。\n * podtoleratesnodeexecutetanits: pod.tolerations属性中是否能接纳容忍noexecute级别的污点，默认没有启用。\n * checknodelablepresence: 检测node上的标签的存在与否，默认没有启用。\n * checkserviceaffinity: 根据pod所属的service，将相同所属的service尽可能放在同一个节点，默认不检查。\n * checkvolumebinding: 检查节点上已绑定和未绑定的pvc是否能够满足pod对象的存储卷需求。\n * novolumezoneconflict: 如果给定了区域限制，检查在此节点上部署pod对象是否存在存储卷冲突。\n * checknodememorypressure: 检测节点内存是否存在压力，如果节点内存压力过大，则检查当前pod是否可以调度此节点上。\n * checknodepidpressure: 检查节点pid数量是否存在压力。\n * checknodediskpressure: 检查节点磁盘资源的压力情况。\n * matchinterpodaffinity: 检查给定节点是否能够满足pod对象的亲和性或反亲和性条件，以用于实现pod亲和性调度或反亲和性调度。\n\n\n# 优选策略\n\n优选过程中，调度器向每个通过预选的节点传递一系列的优选函数，来计算每个节点上各个优选函数后得到的值，调度器会给每个优选函数设定一个权重，大多数优先级默认为1。将所有优选函数得分乘以权重，然后相加从而得出节点的最终优先级分值。finasorenode=(weight1*priorityfunc1)+(weight2*priorityfunc2)+ ...\n\n下面是各个优选函数的相关说明：\n\n * leastrequested: 节点的资源空闲率高的优选，是节点空闲资源与节点总容量的比值计算而来的。即由cpu或内存资源的总容量减去节点上已有pod对象需求的容量总和，再减去当前要创建的pod对象的需求容量的结果除以总容量。计算公式是：(cpu((capacity-sum(requested))*10 / capacity)+memory((capacity-sum(requested))*10 / capacity)) / 2\n * balancedresourceallocation: 计算节点上面的cpu和内存资源被占用的比率相近程度，越接近，比分越高，平衡节点的资源使用情况。计算公式：cpu=cpu((capacity-sum(requested))*10 / capacity) mem=memory((capacity-sum(requested))*10 / capacity)\n * nodepreferavoidpodspriority: 如果node上不存在"scheduler.alpha.kubernetes.io/preferavoidpods"这个注解，那么不管什么pod都没有影响；如果node上存在相关的注解，那么注解中关联的pod对象名称正好是要去调度的pod，那么此类node分值会很低，如果关联的pod对象名称和要调度的pod名称没有任何关系，那么和没有注解是一样的效果。需要注意的是，在这个优先级中，优先级最高，得分会非常高。\n * nodeaffinitypriority: 节点的亲和性，亲和性高，得分高。基于节点亲和性调度偏好进行的优选级评估，根据pod资源中的nodeselector对给定节点进行匹配度检查，成功匹配到的条目越多则节点得分越高。\n * tainttolerationpriority: 将pod对象的spec.tolertions与节点的taints列表项进行匹配度检测，匹配的条目越多，得分越低。\n * selectorspreading: 尽可能的把pod分散开，也就是没有启动这个pod的node，得分会越高。\n * interpodaffinitypriority: 遍历pod对象的亲和性条目，匹配项越多，得分就越多。\n * mostrequestedpriority: 节点中空限量越少的，得分越高，与leastrequested不能同时使用，集中各一个机器上面跑pod，默认没有启用。\n * nodelabelpriority: 根据node上面是否拥有特定的标签来评估得分，有标签就有分，而无论其值为何。默认没有启用。\n * imagelocalitypriority: 一个node的得分高低，是根据node上面是否有镜像，有镜像就有得分，反之就没有(根据node上已有满足需求的image的size大小之和来计算)，默认没有启用。\n\n\n# 高级调度\n\n# nodeselector\n\n将pod调度到特定的node上\n\n\n\n首选要在node上定义相应的labels，然后在pod.spec.nodeselector中定义需要调度到的相应标签的node。类似于rdbms中通过select node.name from xxx where node.disktype=ssd and node-flavor=s3.large.2 ，也就是说pod中nodeselector要完全匹配node上的标签。\n\n这个匹配调度的逻辑，是之前描述的matchnodeselector预选调度算法，预选调度算法是必须要满足项的node。\n\n# nodeaffinity\n\n节点亲和性，是调度程序用来确定pod对象调度到哪个node上的一组规则，和nodeselecotr一样，也是基于节点上的自定义标签和pod对象上指定的标签选择器来进行定义的，这是nodeselector的升级版本。\n\nnodeaffinity属于优选函数算法中一种。\n\n总体来说，节点亲和性调度可以分为硬亲和(required)和软亲和(preferred)。顾名思义，硬亲和性实现的是强制性规则，它是pod调度时必须要满足的规则，如果不满足，则pod对象会被置于pending状态；而软亲和则是一种柔性调度限制，它倾向于将pod对象运行于某类特定的节点之上，而调度器也将尽量满足此需求，但是在无法满足调度需求的时候，它将退而求其次地选择一个不匹配规则的节点。\n\n\n\n# 硬亲和\n\n硬亲和的策略在pod中位置于pod.spec.affinity.nodeaffinity.requireduringschedulingignoreduringexecution。"ignoreduringexecution"表明了，该调度策略只会对当前的pod和node标签，以及相应规则，做个调度匹配。以后要是节点标签发生了变化了，那么已经调度到了该node上的pod对象不会做出改变，只会对新建的pod对象生效。老人老办法，新人新规定。\n\n在上述map的中可以包含多个value(nodeselectorterms字段)，多个nodeselecorterm之间是"逻辑或"的关系，意思是有节点满足其中的一个nodeselecorterm就算这个node满足了。\n\nnodeselectorterms下面需要可以定义多个matchexpression，多个规则彼此之间为"逻辑与"的关系，也就是说只有该nodeselecorterm下面的所有matchexpression定义的规则都满足，这个node才算是满足了。\n\n标签选择器表达式(matchexpression下定义的规则)，支持使用的操作符号有in、notin、exsits、doesnotexists、lt和gt等。in表示的是只要满足后面的集合中的一个就算是满足了条件。\n\n# 软亲和\n\n软亲和的策略在pod中位置于pod.spec.affinity.nodeaffinity.preferredduringschedulingignoreduringexecution。同理是"ignoreduringexecution"。\n\n在上述的map中可以包含多个规则，每个规则都可以配置weight属性以便用于定义其优先级。对于相关的节点分别计算出多个规则的权重值，最后分值高的节点胜出。\n\n\n# podaffinity\n\npod亲和性，顾名思义，调度和判断的主体是之前已经存在的pod，而非上面所说的node。是根据已调度或将要调度的pod的所位于的node的情况，来决定后续的pod将要部署在哪些node上，反映的是后一种pod对已存在pod的一种亲和性的关系的，调度管理。\n\npodaffinity也分为亲和性和反亲和性，每种亲和策略下又分为硬(required)亲和、软(preferred)亲和。\n\nk8s调度器通过内建的matchinterpodaffinity预选策略为这种调度方式完成节点预选，并基于interpodaffinitypriority优选函数进行各节点的优选级评估。\n\n# 为什么要有pod亲和性\n\n出于高效通信的需要，偶尔需要把一些pod对象组织在相近的位置(同一节点、机架、区域或地区等)，如某业务的前端pod和后端pod(表现为pod亲和性)。或者说要出于安全性或分布式的原因，需要将一些pod对象在其运行的位置上隔离开来(表现为pod反亲和性)。\n\n# 什么是位置拓扑topologykey\n\npod亲和性调度需要各相关的pod对象运行于"同一位置"，而topologykey就恰恰定义了这个一个什么样的类别，比如区域的类别、机架的类别、主机的类别等等。\n\n在定义pod对象的亲和性与反亲和性时，需要借助于标签选择器来选择被依赖的pod对象，并根据选出的pod对象所在节点的标签来判断"同一位置"的 具体意义。\n\n# pod亲和性和node亲和性有什么区别\n\n * 在pod.sepc.affinity存在podaffinity和podantiaffinity，这两种配置都是对称的。\n * pod亲和性调度中labelselector的匹配对象是pod，而node亲和性调度中匹配的是node。\n * pod亲和性调度中匹配到的是根据topologykey定义的一组node，topologykey定义了分组是什么样的一个级别，相同topologykey中的key和value的值为一组。\n * 在pod亲和性中硬亲和过滤规则中，条件间只有逻辑与运算。\n\n\n\n# pod硬亲和调度\n\npod的硬亲和调度的api定义于pod.spec.affinity.podaffinity.requiredduringschedulingignoredduringexecution下，通过labelselector 这个map定义多个匹配表达式。条件间只有逻辑与的运算，这一点和node亲和性有所不同。\n\ntopologykey: kubernetes.io/hostname是k8s中节点的内建标签，它的值就是当前节点的节点主机名称。同理还有region(地区)、zone(区域)、rack(机架)的拓扑位置的定义。\n\n# pod软亲和调度\n\npod的软亲和调度的api定义于pod.spec.affinity.podaffinity.preferredduringschedulingignoredduringexecution下，和node软亲和性调度类型，可以定义多个调度规则，每个规则都可以定义一个权重。\n\n# podantiaffinity反亲和性\n\n与podaffinity匹配过程相同，只是最终的结果取反。\n\n\n\n\n# 手动调度和daemonset\n\n当遇到调度器不工作时，需要考虑到手动调度pod。\n\n我们只需要在pod.spec.nodename中直接填上需要调度到的node的名称就可以了。\n\n\n\n# daemonset调度\n\n老的版本中daemonset中的pod调度都是由controller-manager直接指定pod的运行节点，不经过调度器。直到1.11版本开始，daemonset的pod才由scheduler引入调度。\n\ndaemonset实际上是要求每个节点都部署一个相同的pod，通常用于部署集群中的agent，例如网络插件等。在下图的daemonset的配置中，可以看出类似于定义了一个deployment的清单配置文件中，对于要求一个主机上不存在相同的pod label，也就是对于pod的反亲和性。\n\n\n\n\n# 污点和容忍度\n\n污点(taints)是定义在节点之上的键值型属性数据，用于让节点拒绝将pod调度运行于其上，除非该pod对象具有容纳节点污点的容忍度。\n\n而容忍度(tolerations)是定义在pod对象上的键值型属性数据，用于配置其可容忍的节点污点，而且调度器仅能将pod对象调度至其能够容忍该节点污点的节点之上。\n\nk8s中使用podtoleratesnodetaints预选策略和tainttolerationpriority优选函数来完成此类高级调度机制。\n\n# 污点容忍度/节点选择器/节点亲和性区别\n\n上面描述的节点选择器(nodeselector)和节点亲和性两种调度方式都是通过在pod对象上添加标签选择器来完成对特定类型节点标签的匹配，实现的是由pod选择节点的机制。\n\n而污点和容忍度则是通过向节点添加污点信息来控制pod对象的调度结果，从而赋予了节点控制何种pod对象能够调度与其上的主控权。\n\n节点亲和性是使得pod对象被吸引到一类特定的节点，而污点则相反，它提供了让节点排斥特定pod对象的能力。\n\n# 污点\n\n污点(taint)的定义在 node.spec.taints下，是键值型数据，但又额外支持一个效果(effect)标识，语法格式为"key=value:effect"，其中key和value的用法及格式与资源注解信息相似，而effect则用于定义对pod对象的排斥等级，它主要包含以下三种类型。\n\n * noschedule: 不能容忍此污点的新pod对象，不可调度至当前节点，属于强制型约束关系，节点上现存的pod对象不受影响。\n * prefernoschedule: noschedule的柔性约束版本，即不能容忍此污点的新pod对象尽量不要调度至当前节点，不过无其他节点可供调度时也允许接受相应的pod对象。节点上现存的pod对象不受影响。\n * noexecute: 不能容忍此污点的新pod对象，不可调度至当前节点，属于强制性约束关系，而且节点上现存的pod对象会因节点污点变动或pod容忍度变动而不再满足匹配规则时，pod对象将被驱逐。\n\n\n\n给节点添加污点标识：\n\n$ kubectl taint nodes xxx <key>=<value>:<effect>\n例如：\n$ kubectl taint nodes node1 node-type=production:noschedule\n\n\n1\n2\n3\n\n\n即便是同一个键值数据，若其效用标识不同，则也分属于不同的污点信息，也就是说会增加一条污点信息，和之前的区别只是在于效用标识不同。\n\n$  kubectl taint nodes node1 node-type=production:prefernoschedule\n\n\n1\n\n\n删除节点上某特定键名，特定标识的污点信息。\n\n$ kubectl taint nodes node1 node-type=noschedule-\n\n\n1\n\n\n删除节点上某特定键名的所有污点信息。(也就是省略效用标识)\n\n$ kubectl taint nodes node1 node-type-\n\n\n1\n\n\n删除节点上所有的全部的污点信息，可以使用kubectl patch命令将节点属性spec.taints的值直接置为空即可。\n\n$ kubectl patch nodes node1 -p \'{"spec":{"taints":{}}}\'\n\n\n1\n\n\n# 容忍度\n\npod对象的容忍度可通过其pod.spec.tolerations字段进行添加，根据使用的操作符不同，主要有两种不同的形式：一种是与污点信息完全匹配的等值关系"equal"；另一种是判断污点信息存在性的匹配方式"exists"。其中tolerationseconds用于定义延迟驱逐当前pod对象的时长。\n\n\n\n需要注意的如下信息：\n\n * 如果节点node上有多个污点信息，那么就必须该pod对此节点上的所有污点信息都能容忍，才能调度上去。\n * 匹配逻辑和之前的pod中的nodeselector正好相反，之前的逻辑是只要是node上的一个标签满足于pod中定义的node selector就进行匹配。\n * pod中定义的operator为"equal"时，就是需要在pod的toerations中完整填写所有的key、value、effect。\n * pod中定义的operator为"exists"时(key/effect/operator项是必填的，而value项留空)，可以填写value为空，表示的是匹配容忍节点node中所有关于这个key中的相应的"effect"的，所有value的污点的信息。\n * pod中定义的operator为"exists"时(key/operator项是必填，value和effect为空)，表示的是匹配容忍节点node中所有关于这个key值与之相同的，所有value的所有effect的污点信息。\n\n# 污点和容忍度的调度逻辑\n\n一个节点可以配置使用多个污点，一个pod对象也可以有多个容忍度，不过二者在进行逻辑检查时会遵循如下逻辑。\n\n 1. 首先处理每个有着与之匹配的容忍度的污点。\n\n 2. 不能匹配到的污点上，如果存在一个污点使用noschedule效用标识，则拒绝调度pod对象至此节点上。\n\n 3. 不能匹配到的污点上，若没有任何一个使用noschedule效用标识，但至少有一个使用了prefernoscheduler，则应尽量避免将pod对象调度至此节点。\n\n 4. 如果至少有一个不匹配的污点使用了noexecute效用标识，则节点将立即驱逐pod对象，或者不予调度至给给定节点；另外，即便容忍度可以匹配到使用了noexecute效用标识的污点，若在定义容忍度时还同时使用了tolerationseconds属性定义了容忍时限，则超出时限后其也将被节点驱逐。\n\n\n# service和ingress\n\nservice端口用于接收客户端请求并将其转发至其后端的pod中应用的相应端口之上，因此，这种代理机制也称为**"端口代理"或四层代理**，它工作于tcp/ip协议栈的传输层。service及pod对象的ip地址都仅在k8s集群内可达，它们无法接入集群外部的访问流量。\n\n解决此类问题的办法中，除了在单一节点上做端口暴露(hostport)及让pod资源共享使用工作节点的网络名称空间(hostnetwork)之外，更推荐用户使用的是nodeport或loadbalancer类型的service资源，或者是有着七层负载均衡能力的ingress资源。\n\n\n# service\n\n一个service对象就是工作节点上的一些iptables或ipvs规则，用于将到达service对象ip地址的流量调度转发至相应的endpoints对象指向的ip地址和端口之上。工作于每个工作节点的kube-proxy组件通过api server持续监控着各service及与其关联的pod对象，并将其创建或变动实时反映至当前工作节点上相应的iptables或ipvs规则上。\n\n客户端、service及其pod对象的关系如下图所示：\n\n\n\nservice ip事实上是用于生成iptables或ipvs规则时使用的ip地址，它仅用于实现k8s集群网络的内部通信，并且仅能够将规则中定义的转发服务的请求作为目标地址予以响应，这也是它被称为虚拟ip的原因之一。kube-proxy将请求代理至相应端点的方式有三种：userspace(用户空间)、iptables和ipvs。\n\n\n# kube-proxy代理模式\n\n 1. iptables模式\n\niptables代理模型中，kube-proxy负责跟踪api server上service和endpoints对象的变动(创建或移除)，并据此做出service资源定义的变动。\n\n对于每个endpoints对象，service资源会为其创建iptables规则并关联至挑选的后端pod资源，默认算法是随机调度(random)。\n\n在创建service资源时，集群中每个节点上的kube-proxy都会收到通知并将其定义为当前节点上的iptables规则，用于转发工作接口接收到的与此service资源的clusterip和端口的相关流量。客户端发来的请求被相关的iptables规则进行调度和目标地址转换(dnat)后再转发至集群内的pod对象之上。\n\n相对于用户空间模型来说，iptables模型无须将流量在用户空间和内核空间来回切换，因而更加高效和可靠。不过，其缺点是iptables代理模型不会在被挑中的后端pod资源无响应时自动进行重定向，而userspace模型则可以。\n\n 2. ipvs模式\n\nkube-proxy跟踪api server上service和endpoints对象的变动，据此来调用netlink接口创建ipvs规则，并确保与api server中的变动保持同步。它与iptables规则的不同之处仅在于其请求流量的调度功能由ipvs实现，余下的其他功能仍由iptables完成。\n\n类似于iptables模型，ipvs构建于netfilter的钩子函数之上，但它使用hash表作为底层数据结构并工作于内核空间，因此具有流量转发速度快、规则同步性能好的特性。另外，ipvs支持众多调度算法，例如rr、lc、dh、sh、sed和nq等。\n\n\n# service类型\n\nk8s的service共有四种类型：clusterip、nodeport、loadbalancer和externalname。\n\n\n# ingress\n\ningress是k8s api的标准资源类型之一，它其实就是一组基于dns名称(host)或url路径把请求转发至指定的service资源的规则，用于将集群外部的请求流量转发至集群内部完成服务发布。\n\n然而，ingress资源自身并不能进行"流量穿透"，它仅是一组路由规则的集合，这些规则要想真正发挥作用还需要其他功能的辅助，如监听某套接字，然后根据这些规则的匹配机制路由请求流量。这种能够为ingress资源监听套接字并转发流量的组件称为ingress控制器(ingress controller)。\n\n> 不同于pod控制器deployment，ingress控制器并不直接运行为kube-controller-manager的一部分，它是k8s集群的一个重要附件，类似于coredns，需要在集群上单独部署。\n\ningress控制器可以由任何具有反向代理(http/https)功能的服务程序实现，如nginx、envoy、haproxy、vulcand和traefik等。ingress控制器自身也是运行于集群中的pod资源对象，它与被代理的运行为pod资源的应用运行于同一网络中。\n\n如下图中，ingress-nginx与pod1、pod3等的关系所示：\n\n\n\n\n# volumes\n\nk8s支持非常丰富的存储卷类型，包括本地存储(节点)和网络存储系统中的诸多存储机制，甚至还支持secret和configmap这样的特殊存储资源。目前，k8s支持的存储卷包含以下这些类型。\n\n\n\n上述类型中，emptydir和hostpath属于节点级别的卷类型，emptydir的生命周期与pod资源相同，而使用了hostpath卷的pod一旦被重新调度至其他节点，那么它将无法再使用此前的数据。因此，这两种类型都不具有持久性。要想使用持久类型的存储卷，就得使用网络存储系统，如nfs、ceph、glusterfs等，或者云端存储，如gcepersistentdisk、awselasticblockstore等。\n\n\n# pv/pvc\n\npersistentvolume(pv**)是指由集群管理员配置提供的某存储系统上的一段存储空间，它是对底层共享存储的抽象**.\n\npv是集群级别的资源，不属于任何名称空间，用户对pv资源的使用需要使用persistentvolumeclaim(pvc)提出的使用申请(或称为声明)来完成绑定，是pv资源的消费者，它向pv申请特定大小的空间及访问模式(如rw或ro)，从而创建出pvc存储卷，而后再由pod资源通过persistentvolumeclaim存储卷关联使用。如下图所示：\n\n\n\nk8s自1.4版本起引入了一个新的资源对象storageclass，可用于将存储资源定义为具有显著特征的类别(class)而不是具体的pv，例如"fast" "slow" 或"glod" "silver" "bronze"等。用户通过pvc直接向意向的类别发出申请，匹配由管理员事先创建的pv，或者由其按需为用户动态创建pv，这样做甚至免去了需要事先创建pv的过程。\n\n\n# configmap和secret\n\nconfigmap对象用于为容器中的应用提供配置数据以定制程序的行为，不过敏感的配置信息，例如密钥、证书等通常由secret对象来进行配置。\n\n它们将相应的配置信息保存于对象中，而后在pod资源上以存储卷的形式将其挂载并获取相关的配置，以实现配置于镜像文件的解耦。也可以通过环境变量的方式进行挂载。\n\n\n# 网络模型\n\n\n# k8s网络模型\n\nk8s的网络模型主要可用于解决四类通信需求：同一pod内容器间的通信(container to container)、pod间的通信(pod to pod)、service到pod间的通信(service to pod)以及集群外部与service之间的通信(external to service)。\n\n目前k8s支持使用cni插件来编排网络，以实现pod及集群网络管理功能的自动化。每次pod被初始化或删除时，kubelet都会调用默认的cni插件创建一个虚拟设备接口附加到相关的底层网络，为其设置ip地址、路由信息并将其映射到pod对象的网络名称空间。\n\n\n# cni插件\n\ncni本身只是规范，付诸生产还需要有特定的实现。常见的cni网络插件包含以下这些主流的项目。\n\n * flannel：\n   \n   一个为k8s提供叠加网络的网络插件，它基于linux tun/tap，使用udp封装ip报文来创建叠加网络，并借助etcd维护网络的分配情况。\n\n * calico：\n   \n   一个基于bgp的三层网络插件，并且也支持网络策略来实现网络的访问控制；它在每台机器上运行一个vrouter，利用linux内核来转发网络数据包，并借助iptables实现防火墙等功能。\n\n * canal：\n   \n   由flannel和calico联合发布的一个统一网络插件，提供cni网络插件，并支持网络策略。\n\n * weave net：\n   \n   weave net是一个多主机容器的网络方案，支持去中心化的控制平面，在各个host上的wrouter间建立full mesh的tcp连接，并通过gossip来同步控制信息。数据平面上，weave通过udp封装实现l2 overlay，封装支持两种模式，一种是运行在user space的sleeve mode，另一种是运行在kernel space的fastpath mode。\n\n\n# flannel/calico的区别\n\n * flannel是一个大二层网络。\n * calico是一个三层的虚拟网络方案。支持丰富的网络策略，比如namespace网络隔离，pod出入站流量控制。\n\n\n# hpa\n\n手动调整的pod控制器副本方式依赖于用户深度参与监控容器应用的资源压力并且需要计算出合理的值进行调整，存在一定程度的滞后性。k8s提供了多种自动弹性伸缩(auto scaling)工具。\n\nhorizontal pod autoscaler，一种支持控制器对象下pod规模弹性伸缩的工具，目前有两个版本的实现，分别称为hpa和hpa(v2)，前一种仅支持把cpu指标数据作为评估基准，而新版本支持可从资源指标api和自定义指标api中获取的指标数据。\n\n\n# helm\n\n\n# 容器和虚拟机有什么区别联系\n\n在解释这个问题的之前，先要了解一下什么是容器。\n\n\n# 什么是容器\n\n容器是一种轻量级、可移植、自包含的软件打包技术，它使得应用程序可以在几乎任何地方以相同的方式运行。\n\n容器由应用程序本身和它的环境依赖(库和其他应用程序)两部分组成，并在宿主机(host)操作系统的用户空间中运行，但与操作系统的其他进程互相隔离。\n\n它们的实现机制有别于诸如vmware、kvm和xen等实现方案的传统虚拟化技术。\n\n\n# 两者区别如下\n\n由于同一个宿主机上的所有容器都共享其底层操作系统(内核空间)，这就使得容器体积上要比传统的虚拟机小得多。\n\n另外，启动容器无需启动整个操作系统，所以容器部署和启动的速度更快，开销更小，也更容易迁移。事实上，容器赋予了应用程序超强的可移植能力。\n\n\n\n\n# kubernetes特性有哪些\n\n\n# 什么是k8s\n\nkubernets是一种用于在一组主机上运行和协同容器化应用程序的系统，旨在提供可预测性、可扩展性与高可用性的方法来完全管理容器化应用程序和服务的生命周期的平台。\n\n用户可以定义应用程序的运行方式，以及与其他应用程序或外部世界交互的途径，并能实现服务的扩容和缩容，执行平滑滚动更新，以及在不同版本的应用程序之间调度流量以测试功能或回滚有问题的部署。k8s提供了接口和可组合的平台原语，使得用户能够以高度的灵活性和可靠性定义及管理应用程序。\n\n\n# 如下重要特性\n\n 1. 自动装箱\n    \n    建构与容器之上，基于资源依赖及其他约束自动完成容器部署且不影响其可用性，并通过调度机制混合关键型应用和非关键型应用的工作负载与同一节点以提升资源利用率。\n\n 2. 自我修复(自愈)\n    \n    支持容器故障后自动重启、节点故障后重新调度容器，以及其他可用节点、健康状态检查失败后关闭容器并重新创建等自我修复机制。\n\n 3. 水平扩展\n    \n    支持通过简单命令或ui手动水平扩展，以及基于cpu等资源负载率的自动水平扩展机制。\n\n 4. 服务发现和负载均衡\n    \n    k8s通过其附加组件之一的kubedns(或coredns)为系统内置了服务发现功能，它会为每个service配置dns名称，并允许集群内的客户端直接使用此名称发出访问请求，而service则通过iptables或ipvs内建了负载均衡机制。\n\n 5. 自动发布和回滚\n    \n    k8s支持**"灰度"更新应用程序或其配置信息**，它会监控更新过程中应用程序的健康状态，以确保它不会在同一时刻杀掉所有实例，而此过程中一旦有故障发生，就会立即自动执行回滚操作。\n\n 6. 密钥和配置管理\n    \n    kubernetes的configmap实现了配置数据与docker镜像解耦，需要时，仅对配置做出变更而无须重新构建docker镜像，这为应用开发部署带来了很大的灵活性。此外，对于应用所依赖的一些敏感数据，如用户名和密码、令牌、密钥等信息，k8s专门提供了secret对象为其解耦，既便利了应用的快速开发和交付，又提供了一定程度上的安全保障。\n\n 7. 存储编排\n    \n    k8s支持pod对象按需自动挂载不同类型的存储系统，这包括节点本地存储、公有云服务商的云存储，以及网络存储系统(例如，nfs、iscsi、glusterfs、ceph、cinder和flocker等)。\n\n 8. 批量处理执行\n    \n    除了服务型应用，k8s还支持批处理作业及ci(持续集成)，如果需要，一样可以实现容器故障后恢复。\n\n\n# k8s的list-watch\n\nlist-watch 是k8s设计中的精髓所在，理解list-watch对用来整体理解k8s有着很大的帮助。\n\n下面所有做的该知识点的整理和总结，都是基于网络上如下url做的整理。\n\n> https://istio.cn/t/topic/157\n> \n> https://www.kubernetes.org.cn/174.html\n\n\n# 为什么需要list-watch这种机制\n\nk8s各个组件之间仅采用的是http协议通信，没有依赖中间件。而我们在使用k8s的过程中，必然会有对资源处理实时性，完整性，可靠性的强烈需求的。\n\n而list-watch 就是基于http协议开发的，是k8s重要的异步信息通知机制。它通过list获取全量数据，通过watch api来监听增量数据，保证了消息的可靠性、实时性，性能和顺序性。\n\n说白了就是k8s为了满足于异步消息处理的系统。\n\n\n# 什么是list-watch\n\nlist-watch 有两部分组成，分别是 list和watch。list就是调用资源的list api罗列资源，基于http短连接实现。watch则是调用资源的watch api监听资源变更事件，基于http长连接实现，watch是一个典型的发布-订阅模式。\n\n以pod资源为例，它的list api如下，返回值是podlist，即一组pod。\n\nget /api/v1/pods\n\n\n1\n\n\n它的watch api如下，往往带上watch=true，表示采用http长连接持续监听pod相关事件，每当有事件来临是，返回一个watchevent。\n\nget /api/v1/watch/pods\n\n\n1\n\n\n\n# 应用模块\n\nk8s的informer模块中就封装了list-watch api，用户只需要指定资源，编写相应的事件处理函数，例如addfunc，updatefunc和deletefunc等。\n\n在下图中，通过对apiserver的list api罗列资源和watch api监听资源的变更事件后，将反馈的一系列结果放入到了一个fifo队列中，队列的另一头有协程从中取出事件，并调用对应的注册函数处理事件。informer还维护了一个只读的map store缓存，主要是为了提升查询的效率，降低apiserver的负载。\n\n\n\n\n# 设计理念\n\nlist-watch就是一个异步消息的系统，一般我们对消息系统有至少如下四点要求：消息可靠性、消息实时性、消息顺序性、高性能。\n\nlist-watch如何实现消息的可靠性：\n\nlist api可以查询到当前的资源及其对应的状态(即期望的状态)，客户端通过拿期望的状态和实际的状态进行对比，纠正状态不一致的资源。watch api保持和api server一个长链接，接收资源的状态变更事件并做相应处理。如果仅调用watch api，若某个时间点连接中断，就有可能导致消息丢失，所以需要通过list api解决消息丢失的问题。从另一个角度来看，list api获取全量数据，watch api获取增量数据。虽然仅仅通过轮询list api，也能达到同步资源状态的效果，但是存在开销大，实时性不足的问题。\n\nlist-watch如何实现消息的实时性：\n\n每当apiserver的资源发生了状态变更事件，都会将事件及时的推送给客户端，从而保证了消息的实时性。\n\nlist-watch如何实现消息的顺序性：\n\n在并发的场景下，客户端在短时间内可能会收到同一个资源的多个事件，对于关注最终一致性的k8s来说，它需要知道哪个是最近发生的事件，并保证资源的最终状态如同最近事件所表述的状态一样。k8s在每个资源的事件中都带一个resourceversion的标签，这个标签是递增的数字，所以当客户端并发处理同一个资源的事件时，它就可以对比resourceversion来保证最终的状态和最新的事件所期望的状态保持一致。\n\nlist-watch如何实现消息的高性能：\n\n仅通过周期性调用list api也能达到资源最终一致性的效果，但是周期性频繁的轮询大大的增大了开销，增加了apiserver的压力。而watch作为异步消息通知机制，复用一条长链接，保证实时性的同时也保证了性能。\n\n\n# kube-proxy中的ipvs模式和iptables模式\n\n每个工作节点都需要运行一个kube-proxy守护进程，能够按需为service资源对象生成iptables或ipvs规则，从而捕获访问当前service的clusterip的流量并将其转发至正确的后端pod对象。\n\n\n# iptables模式是什么\n\n\n# ipvs模式是什么\n\nlvs由两部分组成，ipvs和ipvsadm。其中ipvsadm是lvs的管理工具，管理员通过ipvsadm定义或管理集群规则。\n\n\n# 两者的区别和联系\n\n\n# k8s删除node\n\n 1. 查看现有集群中有哪些节点\n    \n    $ kubectl get nodes\n    \n    \n    1\n    \n\n 2. 删除节点前，先驱赶掉上面的pod\n    \n    $ kubectl drain node-06 --delete-local-data --force --ignore-daemonsets\n    \n    \n    1\n    \n    \n    此时节点上面的pod开始迁移。\n\n 3. 再次检查节点，被标记为不可调度节点\n    \n    $ kubectl get nodes\n    node-06   ready,schedulingdisabled   <none>   2d18h   v1.14.1\n    \n    \n    1\n    2\n    \n\n 4. 最后删除节点\n    \n    $ kubectl delete node node-06\n    $ kubectl get nodes\n    \n    \n    1\n    2\n    \n\n\n# 修改docker storage pool\n\n\n# 查看storage pool配置\n\ndocker info \nmore /usr/lib/systemd/system/docker.service\n\n\n1\n2\n\n\n\n# 清理docker 空间\n\ndocker system prune -a -f\n\n\n1\n\n\n\n# 修改空间大小步骤如下\n\n 1. 设置节点不可调度 kubectl cordon xxx.xxx.xxx.xxx\n 2. 驱逐节点pod kubectl drain xxx.xxx.xxx.xxx\n 3. 停止docker systemctl stop docker\n 4. 修改docker配置文件 /usr/lib/systemd/system/docker.service 在execstart这一行加一个参数 execstart=/usr/bin/dockerd --storage-opt dm.loopdatasize=500g -g /app/data/ips/docker \\\n 5. systemctl daemon-reload && systemctl start docker\n 6. 取消不可调度 kubectl uncordon xxx.xxx.xxx.xxx\n\n\n# 修改最大的pod限制\n\nhttp://blog.schoolofdevops.com/how-to-increase-the-number-of-pods-limit-per-kubernetes-node/',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"Registry",frontmatter:{title:"Registry",date:"2022-01-20T11:41:48.000Z",permalink:"/pages/97264c/",categories:["中间件","K8S"],tags:[null]},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.K8S/02.Registry%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86.html",relativePath:"02.中间件/01.K8S/02.Registry镜像管理.md",key:"v-6d870201",path:"/pages/97264c/",headers:[{level:2,title:"安装docker环境",slug:"安装docker环境",normalizedTitle:"安装docker环境",charIndex:19},{level:2,title:"下载registry镜像",slug:"下载registry镜像",normalizedTitle:"下载registry镜像",charIndex:43},{level:2,title:"启动容器",slug:"启动容器",normalizedTitle:"启动容器",charIndex:89},{level:2,title:"修改docker环境变量",slug:"修改docker环境变量",normalizedTitle:"修改docker环境变量",charIndex:873},{level:2,title:"Registry常见操作命令",slug:"registry常见操作命令",normalizedTitle:"registry常见操作命令",charIndex:1076},{level:3,title:"上传镜像",slug:"上传镜像",normalizedTitle:"上传镜像",charIndex:1095},{level:3,title:"下载镜像",slug:"下载镜像",normalizedTitle:"下载镜像",charIndex:1204},{level:3,title:"列出所有的镜像仓库",slug:"列出所有的镜像仓库",normalizedTitle:"列出所有的镜像仓库",charIndex:1255},{level:3,title:"列出指定镜像的所有标签",slug:"列出指定镜像的所有标签",normalizedTitle:"列出指定镜像的所有标签",charIndex:1341},{level:3,title:"查看指定镜像/指定tag的digest",slug:"查看指定镜像-指定tag的digest",normalizedTitle:"查看指定镜像/指定tag的digest",charIndex:1445},{level:3,title:"删除指定的tag",slug:"删除指定的tag",normalizedTitle:"删除指定的tag",charIndex:1650},{level:3,title:"删除镜像项目名称",slug:"删除镜像项目名称",normalizedTitle:"删除镜像项目名称",charIndex:1834},{level:3,title:"垃圾回收",slug:"垃圾回收",normalizedTitle:"垃圾回收",charIndex:2041}],headersStr:"安装docker环境 下载registry镜像 启动容器 修改docker环境变量 Registry常见操作命令 上传镜像 下载镜像 列出所有的镜像仓库 列出指定镜像的所有标签 查看指定镜像/指定tag的digest 删除指定的tag 删除镜像项目名称 垃圾回收",content:'# Registry镜像管理\n\n\n# 安装docker环境\n\n请参照其他文档\n\n\n# 下载registry镜像\n\n$docker pull registry:2.7.1\n\n\n# 启动容器\n\n$ docker run –d –p 5000:5000  -v /app/docker/registry_data:/var/lib/registry –restart=always \n--name registry  -e "REGISTRY_STORAGE_DELETE_ENABLED=true" registry:2.7.1\n\n\n1\n2\n\n\n其中REGISTRY_STORAGE_DELETE_ENABLED这个环境变量是用于打开可以删除镜像的开关。 也可以在容器内设置/etc/docker/registry/config.yml文件，加入storage.delete.enabled设置为true。修改完容器后，重启容器。参考如下配置：\n\nversion: 0.1\nlog:\n  fields:\n    service: registry\nstorage:\n  cache:\n    blobdescriptor: inmemorys\n  filesystem:\n    rootdirectory: /var/lib/registry\n  delete:\n    enable: true\nhttp:        \n  addr: :5000                        \n  headers:                           \n    X-Content-Type-Options: [nosniff]\nhealth:          \n  storagedriver: \n    enabled: true\n    interval: 10s\n    threshold: 3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 修改docker环境变量\n\n添加docker环境insecure-registries参数，参考下面修改\n\n#vi /etc/docker/daemon.json\n{\n  "insecure-registries": [“192.168.3.58:5000”]\n}\n#systemctl daemon-reload\n#systemctl restart docker\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Registry常见操作命令\n\n\n# 上传镜像\n\n$docker tag xxxx:yyyy  192.168.3.60:5000/xxxx:yyyy\n$docker push 192.168.3.60:5000/xxxx:yyyy\n\n\n1\n2\n\n\n\n# 下载镜像\n\n$docker pull 192.168.3.60:5000/xxxx:yyyy\n\n\n# 列出所有的镜像仓库\n\n其中IP和端口替换为指定的源\n\n$curl -XGET http://10.128.91.224:5000/v2/_catalog\n\n\n1\n\n\n\n# 列出指定镜像的所有标签\n\n查看在该仓库下面mysql的所有tags表前列\n\n$curl -XGET http://10.128.91.224:5000/v2/mysql/tags/list\n\n\n1\n\n\n\n# 查看指定镜像/指定tag的digest\n\n查看该仓库下面mysql 的tag是5.7.25的digest号\n\n$curl  --header "Accept: application/vnd.docker.distribution.manifest.v2+json" -I -X GET http://10.128.91.224:5000/v2/mysql/manifests/5.7.25\n\n\n1\n\n\n\n# 删除指定的tag\n\n其中sha256后面的就是上面查找出来的digest号\n\n$curl -I -X DELETE http://10.128.91.224:5000/v2/mysql/manifests/sha256:fcaff905397ba63fd376d0c3019f1f1cb6e7506131389edbcb3d22719f1ae54d\n\n\n1\n\n\n\n# 删除镜像项目名称\n\n如果某个镜像名称下面的所有tag都已经删除了，而且需要删除这个镜像的目录，那么就可以考虑删除掉这个目录，避免在查询的时候还能看到这个镜像名称，而tag确实null了。\n\n$cd /app/registry_data/docker/registry/v2/repositories\n目录是具体的是创建registry时映射的路径\n然后删除这个路径下面的对应的项目名称。\n\n\n1\n2\n3\n\n\n\n# 垃圾回收\n\n其中registry是创建的容器的名称，实际上是在容器内执行垃圾回收操作。\n\n$docker exec -it registry /bin/registry garbage-collect /etc/docker/registry/config.yml',normalizedContent:'# registry镜像管理\n\n\n# 安装docker环境\n\n请参照其他文档\n\n\n# 下载registry镜像\n\n$docker pull registry:2.7.1\n\n\n# 启动容器\n\n$ docker run –d –p 5000:5000  -v /app/docker/registry_data:/var/lib/registry –restart=always \n--name registry  -e "registry_storage_delete_enabled=true" registry:2.7.1\n\n\n1\n2\n\n\n其中registry_storage_delete_enabled这个环境变量是用于打开可以删除镜像的开关。 也可以在容器内设置/etc/docker/registry/config.yml文件，加入storage.delete.enabled设置为true。修改完容器后，重启容器。参考如下配置：\n\nversion: 0.1\nlog:\n  fields:\n    service: registry\nstorage:\n  cache:\n    blobdescriptor: inmemorys\n  filesystem:\n    rootdirectory: /var/lib/registry\n  delete:\n    enable: true\nhttp:        \n  addr: :5000                        \n  headers:                           \n    x-content-type-options: [nosniff]\nhealth:          \n  storagedriver: \n    enabled: true\n    interval: 10s\n    threshold: 3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 修改docker环境变量\n\n添加docker环境insecure-registries参数，参考下面修改\n\n#vi /etc/docker/daemon.json\n{\n  "insecure-registries": [“192.168.3.58:5000”]\n}\n#systemctl daemon-reload\n#systemctl restart docker\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# registry常见操作命令\n\n\n# 上传镜像\n\n$docker tag xxxx:yyyy  192.168.3.60:5000/xxxx:yyyy\n$docker push 192.168.3.60:5000/xxxx:yyyy\n\n\n1\n2\n\n\n\n# 下载镜像\n\n$docker pull 192.168.3.60:5000/xxxx:yyyy\n\n\n# 列出所有的镜像仓库\n\n其中ip和端口替换为指定的源\n\n$curl -xget http://10.128.91.224:5000/v2/_catalog\n\n\n1\n\n\n\n# 列出指定镜像的所有标签\n\n查看在该仓库下面mysql的所有tags表前列\n\n$curl -xget http://10.128.91.224:5000/v2/mysql/tags/list\n\n\n1\n\n\n\n# 查看指定镜像/指定tag的digest\n\n查看该仓库下面mysql 的tag是5.7.25的digest号\n\n$curl  --header "accept: application/vnd.docker.distribution.manifest.v2+json" -i -x get http://10.128.91.224:5000/v2/mysql/manifests/5.7.25\n\n\n1\n\n\n\n# 删除指定的tag\n\n其中sha256后面的就是上面查找出来的digest号\n\n$curl -i -x delete http://10.128.91.224:5000/v2/mysql/manifests/sha256:fcaff905397ba63fd376d0c3019f1f1cb6e7506131389edbcb3d22719f1ae54d\n\n\n1\n\n\n\n# 删除镜像项目名称\n\n如果某个镜像名称下面的所有tag都已经删除了，而且需要删除这个镜像的目录，那么就可以考虑删除掉这个目录，避免在查询的时候还能看到这个镜像名称，而tag确实null了。\n\n$cd /app/registry_data/docker/registry/v2/repositories\n目录是具体的是创建registry时映射的路径\n然后删除这个路径下面的对应的项目名称。\n\n\n1\n2\n3\n\n\n\n# 垃圾回收\n\n其中registry是创建的容器的名称，实际上是在容器内执行垃圾回收操作。\n\n$docker exec -it registry /bin/registry garbage-collect /etc/docker/registry/config.yml',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"ECE认证考试经历总结",frontmatter:{layout:"post",title:"ECE认证考试经历总结",subtitle:"ECE认证考试",description:null,excerpt:null,date:"2020-08-04T12:00:00.000Z",author:"chuck",image:null,tags:["Elastic"],URL:"/2020/08/04/ece/",categories:["Tech"],permalink:"/pages/42d30d/"},regularPath:"/01.%E6%95%B0%E6%8D%AE%E5%BA%93/05.ELK/85.ECE%E8%80%83%E8%AF%95%E6%80%BB%E7%BB%93.html",relativePath:"01.数据库/05.ELK/85.ECE考试总结.md",key:"v-1b38a950",path:"/pages/42d30d/",headers:[{level:2,title:"从培养习惯开始",slug:"从培养习惯开始",normalizedTitle:"从培养习惯开始",charIndex:14},{level:2,title:"避免网络大翻车",slug:"避免网络大翻车",normalizedTitle:"避免网络大翻车",charIndex:1253},{level:2,title:"考前模拟练习很重要",slug:"考前模拟练习很重要",normalizedTitle:"考前模拟练习很重要",charIndex:1801}],headersStr:"从培养习惯开始 避免网络大翻车 考前模拟练习很重要",content:'# ECE考试总结\n\n\n# 从培养习惯开始\n\n之前我几乎没接触过ES，只是知道可以把日志放到ES中，便于快速查询。去年我们一个同事将数据库中的一个复杂的模糊查询的场景，搬迁到了ES中来实现，性能提升很多倍。这引起了我很大的兴趣，下定决心好好学习ES。\n\n我是19年年底的时候，在极客时间买了阮老师的ES的视频课程，开始系统的学习ES。阮老师的课程设计的很棒，每个视频短小精悍。但是，我听完第一遍后，一直是懵逼的状态，感觉糊里糊涂，抓不住学习的重点。粗略的看了一遍视频，就不得不先放弃下。那个时候，就感觉到ES的学习曲线很陡峭，如何高效的学习，就摆在我面前。\n\n今年2月份的时候，了解到群主开始组织ECE考试认证，感觉这是一个不错的学习方法。从5月1号开始，我就系统的开始准备ECE的认证考试，反复观看阮老师的视频，不懂的自己先去官方文档找找答案，最后还是不清楚的，就直接向球友提问。社群是个很好的学习环境，大家在一起积极学习，相互讨论，收获很多。\n\n与此同时，群主的死磕到底的学习认知的方法论，让我对ES的学习认知打开一扇窗。\n\n> 定下通过ECE的考试目标，是ES学习的最快路径。\n\n我们每个人不管是从事什么工作或游戏，内心地就是想赢。当我们定下这个目标时，我们会自然而然的集中自己的注意力和空闲时间，投入到ES的学习中去。但是通过考试，只是目标，不是我们的初衷，我们的初衷是充分完整学习ES的知识体系，夯实基础。通过重复看阮老师的视频、翻看田雪松老师编写的《Elastic Stack应用宝典》、官方文档，在学习的过程中，我主要在做的事情是罗列ES的知识点，绘制自己的知识脑图，提高自己的认知水平。\n\n> 打卡学习，坚持到底\n\n群主的打卡学习圈，是个很棒的想法。从5月1号，开始准备ECE考试后，我几乎每天都会去打卡，哪怕今天什么都没有学习，我也会去打卡。打卡学习，让我们每天对自己都有清醒的认识，今天有没有付出，看看打卡记录就知道了。这让我们在学习的过程中，遵从事实，特别在后期学习的时候，对自己是个莫大的鼓励。\n\n开始阶段，要找到自己学习的门户习惯。一开始的时候，我们不应该强求自己立马收获和改变很多，哪怕只是简单的翻开了几页书，也可以去打卡。哪怕这件事情确实很难，但是这不应该出现在学习的开始阶段，我们总得给自己一个坚持下去的理由。\n\n打卡的目的，是不断的重申自己的身份和归属。每天坚持打卡，只是想告诉自己，我们不是一个轻易放弃的人，我们每天都给自己这样的身份投票，坚持下来，不会感觉到这是很难的事情。\n\n> 聚焦过程，延迟满足\n\n正如阮老师所说的，学习的过程是一个延迟满足的过程。但是我们人类经过这么多年的进化，越来越倾向于即时满足的情绪。小伙伴们喜欢刷几个小时的手机，不喜欢静下来看一本书，工作中一把梭的做法比比皆是。目标对于指引我们前进的方向，是有用的，但是这不应该成为时时刻刻关注的事情，正如篮球比赛中，运动员一直盯着比分，是及其荒谬的事情。忘记目标，专注体系，享受过程的美好。\n\n\n# 避免网络大翻车\n\n4月底参考CKA考试的时候，由于家里CC网络非常差，考试系统断了很多次。特别是在临近考试结束还有半个小时的时候，当我再次连接上的时候，我意外的发现，前面的几个k8s集群居然都连接不上了，这让我心情瞬间提到了嗓子眼里面，咨询监考官，但是监考官表示也无法解决。那个时候瞬间让我很崩溃，没办法，各种骚操作，无效。最后没有办法，手动断开考试系统，再次连接进入，惊喜发现，前面的集群都回来了。。。\n\n为了再次避免网络大翻车，ECE考试前果断换了电信，我的维匹嗯，是之前买的bwg上自建的V2ray，使用的是CN2-GIA网络，据说是中美之间除专线外，最好的网络了。为了避免网络抖动不可用，我还临时准备了一个 azure 节点。由于azure的网络流量是单独收费的，我用speed test测试过，不管是网络耗时，还是上传下载的带宽，azure的确是最好的选择，缺点就是非常贵。建议准备参加考试的小伙伴，考前一定要多测试几次网络，之前翻车的那次我测试的上传和下载的速度只有5M，而我换了宽度后测试的上传和下载的速度都能达到20M。\n\n我选择的考试时间是早上5点半，这个时间段，网络的稳定是有保障的。结果看来，在ECE考试中的浏览器中进入的那个虚拟机，打开文档，拖拽页面，非常顺畅。\n\n\n# 考前模拟练习很重要\n\n博士训练题，我是从头到尾做了2遍的。博士的练习题把考试的知识点进行了全覆盖，但是感觉难度要比考试要高一些。\n\n考试前，一定要自己手敲，戒掉上来就看答案的坏习惯，要进行模拟练习。有些细节上的问题，只有自己独立去做了，才会知道自己有没有掌握。\n\n我是8月5号下午购买的考试券，300美金，真香，省了800块。但是我购买完后，只收到了订单邮件，没有收到预约考试的邮件，登上PSI网站后观察，我的账号无法进行预约考试，等到6号才收到邮件，说可以预约考试。估计当我下单考试券的时候，ES官方的订单系统立即就返回了订单成功的状态，但是将这个订单信息同步到PSI考试系统的时候，出现了延迟，也可能是最近下单ES考试的太多了吧。\n\n9号周日上午参加完考试后，就一直等待考试结果的邮件，到周三了依旧没有收到邮件，尝试给邮件去咨询情况，结果他们邮件回复说score email 周一的时候已经发出了，并给了我通过的证书链接信息。哎。。。我也确信垃圾邮箱中没收到，后来咨询后了解到，不管是通过或通不过都会发邮件出来的，如果没有3天内收到邮件，大家也不用慌，发个邮件去咨询问问。不过等待的时间，确实有些长了，每天忙碌的工作，我都快忘了我上周参加过考试了。。。\n\n考试的流程，所有PSI都是一样的，确保桌面的整洁，无纸质和电子的产品。考试的时候可以申请去趟厕所，或者吃点东西，但是监考官说，只有一次机会，时间不能超过10分钟。\n\n考试的题型和FFFro的一模一样，3个集群，10道题目，连题目的顺序也是一模一样的。我用了不到2个小时就完成了答题，整个过程都很顺畅，检查了一遍，没有啥问题，就交卷了。\n\n具体的题目如下：\n\n 1.  使用索引级别的shard allocation将logs-new分片全部分配到node1上(注意设置副本分片为0)。还有我是通过设置节点的hotwarm的属性，将logs-old的所有分片分配到node2和node3上。\n 2.  自定义一个索引的分词器，将waynes和wayne\'s搜索出相同的结果。最后reindex一下，做个验证。\n 3.  设置一个pipeline，将索引中的地址字段，街道字段，4个字段，组合拼接成一个新字段。最后使用update_by_query下，对所有的索引进行更新。大家在确定要使用update_by_query这种API的场景前，如果确定要使用复杂的script，最好使用update_by_query+pipeline的模式，pipeline有simulate API，操作前可以验证。如果没有充分验证，就直接对已有索引上面干，很容易GG，现有的索引中的数据遭到了破坏。\n 4.  定义一个nested的字段的索引，然后再写一个基于nested的字段类型的查询。直接从官方文档上找到答案。\n 5.  dynamic_mapping的设置，分别是"match_mapping_type"和"match"的设置。\n 6.  创建针对某个索引的snapshot\n 7.  开启xpack，密码最好也是手敲一个个输入，确保正确。\n 8.  地震的pipeline aggs，先按月分桶，桶内max最大的震级和max最大的深度，最后外面搞个pipeline max bucket计算出年份的所有月份中，最大的震级和最大的深度。这个题目直接找到pipeline aggs的官方文档，直接改改就可以了。\n 9.  multi-match + most_filed\n 10. match_phrase + sort + hightlight\n\n参加这种国外考试，还是很有技巧的，基本上就是那几套题目轮流转，考前总结分析下出现题目的概率，并且自己给自己出一些模拟题，是很有帮助的。\n\n自己整理的博士练习题+球友分享真题的总结:\n\n> https://github.com/chuck6/ece/blob/master/ece_exercise.md\n\n自己根据真题，自己整理的最后模拟训练真题：\n\n> https://github.com/chuck6/ece/blob/master/ece_moni.md',normalizedContent:'# ece考试总结\n\n\n# 从培养习惯开始\n\n之前我几乎没接触过es，只是知道可以把日志放到es中，便于快速查询。去年我们一个同事将数据库中的一个复杂的模糊查询的场景，搬迁到了es中来实现，性能提升很多倍。这引起了我很大的兴趣，下定决心好好学习es。\n\n我是19年年底的时候，在极客时间买了阮老师的es的视频课程，开始系统的学习es。阮老师的课程设计的很棒，每个视频短小精悍。但是，我听完第一遍后，一直是懵逼的状态，感觉糊里糊涂，抓不住学习的重点。粗略的看了一遍视频，就不得不先放弃下。那个时候，就感觉到es的学习曲线很陡峭，如何高效的学习，就摆在我面前。\n\n今年2月份的时候，了解到群主开始组织ece考试认证，感觉这是一个不错的学习方法。从5月1号开始，我就系统的开始准备ece的认证考试，反复观看阮老师的视频，不懂的自己先去官方文档找找答案，最后还是不清楚的，就直接向球友提问。社群是个很好的学习环境，大家在一起积极学习，相互讨论，收获很多。\n\n与此同时，群主的死磕到底的学习认知的方法论，让我对es的学习认知打开一扇窗。\n\n> 定下通过ece的考试目标，是es学习的最快路径。\n\n我们每个人不管是从事什么工作或游戏，内心地就是想赢。当我们定下这个目标时，我们会自然而然的集中自己的注意力和空闲时间，投入到es的学习中去。但是通过考试，只是目标，不是我们的初衷，我们的初衷是充分完整学习es的知识体系，夯实基础。通过重复看阮老师的视频、翻看田雪松老师编写的《elastic stack应用宝典》、官方文档，在学习的过程中，我主要在做的事情是罗列es的知识点，绘制自己的知识脑图，提高自己的认知水平。\n\n> 打卡学习，坚持到底\n\n群主的打卡学习圈，是个很棒的想法。从5月1号，开始准备ece考试后，我几乎每天都会去打卡，哪怕今天什么都没有学习，我也会去打卡。打卡学习，让我们每天对自己都有清醒的认识，今天有没有付出，看看打卡记录就知道了。这让我们在学习的过程中，遵从事实，特别在后期学习的时候，对自己是个莫大的鼓励。\n\n开始阶段，要找到自己学习的门户习惯。一开始的时候，我们不应该强求自己立马收获和改变很多，哪怕只是简单的翻开了几页书，也可以去打卡。哪怕这件事情确实很难，但是这不应该出现在学习的开始阶段，我们总得给自己一个坚持下去的理由。\n\n打卡的目的，是不断的重申自己的身份和归属。每天坚持打卡，只是想告诉自己，我们不是一个轻易放弃的人，我们每天都给自己这样的身份投票，坚持下来，不会感觉到这是很难的事情。\n\n> 聚焦过程，延迟满足\n\n正如阮老师所说的，学习的过程是一个延迟满足的过程。但是我们人类经过这么多年的进化，越来越倾向于即时满足的情绪。小伙伴们喜欢刷几个小时的手机，不喜欢静下来看一本书，工作中一把梭的做法比比皆是。目标对于指引我们前进的方向，是有用的，但是这不应该成为时时刻刻关注的事情，正如篮球比赛中，运动员一直盯着比分，是及其荒谬的事情。忘记目标，专注体系，享受过程的美好。\n\n\n# 避免网络大翻车\n\n4月底参考cka考试的时候，由于家里cc网络非常差，考试系统断了很多次。特别是在临近考试结束还有半个小时的时候，当我再次连接上的时候，我意外的发现，前面的几个k8s集群居然都连接不上了，这让我心情瞬间提到了嗓子眼里面，咨询监考官，但是监考官表示也无法解决。那个时候瞬间让我很崩溃，没办法，各种骚操作，无效。最后没有办法，手动断开考试系统，再次连接进入，惊喜发现，前面的集群都回来了。。。\n\n为了再次避免网络大翻车，ece考试前果断换了电信，我的维匹嗯，是之前买的bwg上自建的v2ray，使用的是cn2-gia网络，据说是中美之间除专线外，最好的网络了。为了避免网络抖动不可用，我还临时准备了一个 azure 节点。由于azure的网络流量是单独收费的，我用speed test测试过，不管是网络耗时，还是上传下载的带宽，azure的确是最好的选择，缺点就是非常贵。建议准备参加考试的小伙伴，考前一定要多测试几次网络，之前翻车的那次我测试的上传和下载的速度只有5m，而我换了宽度后测试的上传和下载的速度都能达到20m。\n\n我选择的考试时间是早上5点半，这个时间段，网络的稳定是有保障的。结果看来，在ece考试中的浏览器中进入的那个虚拟机，打开文档，拖拽页面，非常顺畅。\n\n\n# 考前模拟练习很重要\n\n博士训练题，我是从头到尾做了2遍的。博士的练习题把考试的知识点进行了全覆盖，但是感觉难度要比考试要高一些。\n\n考试前，一定要自己手敲，戒掉上来就看答案的坏习惯，要进行模拟练习。有些细节上的问题，只有自己独立去做了，才会知道自己有没有掌握。\n\n我是8月5号下午购买的考试券，300美金，真香，省了800块。但是我购买完后，只收到了订单邮件，没有收到预约考试的邮件，登上psi网站后观察，我的账号无法进行预约考试，等到6号才收到邮件，说可以预约考试。估计当我下单考试券的时候，es官方的订单系统立即就返回了订单成功的状态，但是将这个订单信息同步到psi考试系统的时候，出现了延迟，也可能是最近下单es考试的太多了吧。\n\n9号周日上午参加完考试后，就一直等待考试结果的邮件，到周三了依旧没有收到邮件，尝试给邮件去咨询情况，结果他们邮件回复说score email 周一的时候已经发出了，并给了我通过的证书链接信息。哎。。。我也确信垃圾邮箱中没收到，后来咨询后了解到，不管是通过或通不过都会发邮件出来的，如果没有3天内收到邮件，大家也不用慌，发个邮件去咨询问问。不过等待的时间，确实有些长了，每天忙碌的工作，我都快忘了我上周参加过考试了。。。\n\n考试的流程，所有psi都是一样的，确保桌面的整洁，无纸质和电子的产品。考试的时候可以申请去趟厕所，或者吃点东西，但是监考官说，只有一次机会，时间不能超过10分钟。\n\n考试的题型和fffro的一模一样，3个集群，10道题目，连题目的顺序也是一模一样的。我用了不到2个小时就完成了答题，整个过程都很顺畅，检查了一遍，没有啥问题，就交卷了。\n\n具体的题目如下：\n\n 1.  使用索引级别的shard allocation将logs-new分片全部分配到node1上(注意设置副本分片为0)。还有我是通过设置节点的hotwarm的属性，将logs-old的所有分片分配到node2和node3上。\n 2.  自定义一个索引的分词器，将waynes和wayne\'s搜索出相同的结果。最后reindex一下，做个验证。\n 3.  设置一个pipeline，将索引中的地址字段，街道字段，4个字段，组合拼接成一个新字段。最后使用update_by_query下，对所有的索引进行更新。大家在确定要使用update_by_query这种api的场景前，如果确定要使用复杂的script，最好使用update_by_query+pipeline的模式，pipeline有simulate api，操作前可以验证。如果没有充分验证，就直接对已有索引上面干，很容易gg，现有的索引中的数据遭到了破坏。\n 4.  定义一个nested的字段的索引，然后再写一个基于nested的字段类型的查询。直接从官方文档上找到答案。\n 5.  dynamic_mapping的设置，分别是"match_mapping_type"和"match"的设置。\n 6.  创建针对某个索引的snapshot\n 7.  开启xpack，密码最好也是手敲一个个输入，确保正确。\n 8.  地震的pipeline aggs，先按月分桶，桶内max最大的震级和max最大的深度，最后外面搞个pipeline max bucket计算出年份的所有月份中，最大的震级和最大的深度。这个题目直接找到pipeline aggs的官方文档，直接改改就可以了。\n 9.  multi-match + most_filed\n 10. match_phrase + sort + hightlight\n\n参加这种国外考试，还是很有技巧的，基本上就是那几套题目轮流转，考前总结分析下出现题目的概率，并且自己给自己出一些模拟题，是很有帮助的。\n\n自己整理的博士练习题+球友分享真题的总结:\n\n> https://github.com/chuck6/ece/blob/master/ece_exercise.md\n\n自己根据真题，自己整理的最后模拟训练真题：\n\n> https://github.com/chuck6/ece/blob/master/ece_moni.md',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"Python基础知识点",frontmatter:{title:"Python基础知识点",date:"2022-01-20T14:21:29.000Z",permalink:"/pages/107c9b/",categories:["编程","Python"],tags:[null]},regularPath:"/03.%E7%BC%96%E7%A8%8B/10.Python/01.Python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9.html",relativePath:"03.编程/10.Python/01.Python基础知识点.md",key:"v-00fe5646",path:"/pages/107c9b/",headers:[{level:2,title:"如何进行Pycharm安装和激活",slug:"如何进行pycharm安装和激活",normalizedTitle:"如何进行pycharm安装和激活",charIndex:35},{level:3,title:"Pycharm专业版和社区版有什么区别",slug:"pycharm专业版和社区版有什么区别",normalizedTitle:"pycharm专业版和社区版有什么区别",charIndex:94},{level:3,title:"安装的具体步骤如下",slug:"安装的具体步骤如下",normalizedTitle:"安装的具体步骤如下",charIndex:210},{level:3,title:"配置agent包",slug:"配置agent包",normalizedTitle:"配置agent包",charIndex:425},{level:3,title:"导入密钥",slug:"导入密钥",normalizedTitle:"导入密钥",charIndex:1306},{level:3,title:"参考URL信息",slug:"参考url信息",normalizedTitle:"参考url信息",charIndex:5126},{level:2,title:"如何理解多进程与多线程",slug:"如何理解多进程与多线程",normalizedTitle:"如何理解多进程与多线程",charIndex:5248},{level:3,title:"fork()系统调用",slug:"fork-系统调用",normalizedTitle:"fork()系统调用",charIndex:5264},{level:3,title:"单进程",slug:"单进程",normalizedTitle:"单进程",charIndex:5695},{level:3,title:"多进程",slug:"多进程",normalizedTitle:"多进程",charIndex:5252},{level:3,title:"多线程",slug:"多线程",normalizedTitle:"多线程",charIndex:5256},{level:2,title:"如何实现Python数据库访问",slug:"如何实现python数据库访问",normalizedTitle:"如何实现python数据库访问",charIndex:10260},{level:3,title:"数据库编程接口",slug:"数据库编程接口",normalizedTitle:"数据库编程接口",charIndex:10280},{level:3,title:"访问Mysql",slug:"访问mysql",normalizedTitle:"访问mysql",charIndex:11924},{level:2,title:"为什么pip安准包会报错",slug:"为什么pip安准包会报错",normalizedTitle:"为什么pip安准包会报错",charIndex:14724},{level:3,title:"问题描述",slug:"问题描述",normalizedTitle:"问题描述",charIndex:14741},{level:3,title:"解决办法一",slug:"解决办法一",normalizedTitle:"解决办法一",charIndex:14873},{level:3,title:"解决办法二",slug:"解决办法二",normalizedTitle:"解决办法二",charIndex:15095},{level:2,title:"如何进行pip离线安装模块",slug:"如何进行pip离线安装模块",normalizedTitle:"如何进行pip离线安装模块",charIndex:15485},{level:3,title:"下载第三方包",slug:"下载第三方包",normalizedTitle:"下载第三方包",charIndex:15503},{level:3,title:"离线安装",slug:"离线安装",normalizedTitle:"离线安装",charIndex:15492},{level:2,title:"如何实现Python中Request模块中post请求",slug:"如何实现python中request模块中post请求",normalizedTitle:"如何实现python中request模块中post请求",charIndex:16313},{level:3,title:"form形式发送post请求",slug:"form形式发送post请求",normalizedTitle:"form形式发送post请求",charIndex:17106},{level:3,title:"json形式发送post请求",slug:"json形式发送post请求",normalizedTitle:"json形式发送post请求",charIndex:17495},{level:3,title:"multipart形式发送post请求",slug:"multipart形式发送post请求",normalizedTitle:"multipart形式发送post请求",charIndex:17753},{level:2,title:"如何理解字典和列表的嵌套",slug:"如何理解字典和列表的嵌套",normalizedTitle:"如何理解字典和列表的嵌套",charIndex:17980},{level:3,title:"什么是序列",slug:"什么是序列",normalizedTitle:"什么是序列",charIndex:17997},{level:3,title:"什么是元组",slug:"什么是元组",normalizedTitle:"什么是元组",charIndex:18086},{level:3,title:"什么是列表",slug:"什么是列表",normalizedTitle:"什么是列表",charIndex:18465},{level:3,title:"元组和列表有什么区别和联系",slug:"元组和列表有什么区别和联系",normalizedTitle:"元组和列表有什么区别和联系",charIndex:18549},{level:3,title:"什么是字典",slug:"什么是字典",normalizedTitle:"什么是字典",charIndex:18621},{level:3,title:"字典嵌套字典",slug:"字典嵌套字典",normalizedTitle:"字典嵌套字典",charIndex:18856},{level:3,title:"字典嵌套列表",slug:"字典嵌套列表",normalizedTitle:"字典嵌套列表",charIndex:19334},{level:3,title:"列表嵌套字典",slug:"列表嵌套字典",normalizedTitle:"列表嵌套字典",charIndex:19923},{level:3,title:"列表嵌套列表",slug:"列表嵌套列表",normalizedTitle:"列表嵌套列表",charIndex:20412},{level:2,title:"什么时候使用()号，什么时候使用[]号，什么时候使用{}号",slug:"什么时候使用-号-什么时候使用-号-什么时候使用-号",normalizedTitle:"什么时候使用()号，什么时候使用[]号，什么时候使用{}号",charIndex:20935},{level:3,title:"在Python中使用()小括号有如下两种场景：",slug:"在python中使用-小括号有如下两种场景",normalizedTitle:"在python中使用()小括号有如下两种场景：",charIndex:20969},{level:3,title:"在Python中使用[]中括号有如下两种情形：",slug:"在python中使用-中括号有如下两种情形",normalizedTitle:"在python中使用[]中括号有如下两种情形：",charIndex:21477},{level:3,title:"在Python中使用{}号有如下 一种场景",slug:"在python中使用-号有如下-一种场景",normalizedTitle:"在python中使用{}号有如下 一种场景",charIndex:22733},{level:2,title:"如何使用Anaconda",slug:"如何使用anaconda",normalizedTitle:"如何使用anaconda",charIndex:22869},{level:3,title:"windows安装Anaconda",slug:"windows安装anaconda",normalizedTitle:"windows安装anaconda",charIndex:22886},{level:3,title:"Centos7安装Anaconda",slug:"centos7安装anaconda",normalizedTitle:"centos7安装anaconda",charIndex:23319},{level:3,title:"常见命令",slug:"常见命令",normalizedTitle:"常见命令",charIndex:24171},{level:2,title:"如何在vscode中conda虚拟环境配置",slug:"如何在vscode中conda虚拟环境配置",normalizedTitle:"如何在vscode中conda虚拟环境配置",charIndex:25231},{level:2,title:"为什么Python运行慢",slug:"为什么python运行慢",normalizedTitle:"为什么python运行慢",charIndex:25751},{level:2,title:"如何理解Python中for循环range()函数",slug:"如何理解python中for循环range-函数",normalizedTitle:"如何理解python中for循环range()函数",charIndex:26207},{level:2,title:"如何理解for循环和while循环的区别",slug:"如何理解for循环和while循环的区别",normalizedTitle:"如何理解for循环和while循环的区别",charIndex:26521},{level:2,title:"如何理解Python中的do...while",slug:"如何理解python中的do-while",normalizedTitle:"如何理解python中的do...while",charIndex:26874},{level:2,title:"为什么pip install报SSL ERROR",slug:"为什么pip-install报ssl-error",normalizedTitle:"为什么pip install报ssl error",charIndex:27598},{level:2,title:"如何理解Python中的函数和方法的区别",slug:"如何理解python中的函数和方法的区别",normalizedTitle:"如何理解python中的函数和方法的区别",charIndex:27765},{level:2,title:"如何实现print函数格式化输出",slug:"如何实现print函数格式化输出",normalizedTitle:"如何实现print函数格式化输出",charIndex:28441},{level:3,title:"占位符方式",slug:"占位符方式",normalizedTitle:"占位符方式",charIndex:28525},{level:3,title:"format方式格式化输出",slug:"format方式格式化输出",normalizedTitle:"format方式格式化输出",charIndex:29097},{level:3,title:"f-string 格式化字符串常量",slug:"f-string-格式化字符串常量",normalizedTitle:"f-string 格式化字符串常量",charIndex:30156},{level:2,title:'为什么单个元素的元组需要有逗号","',slug:"为什么单个元素的元组需要有逗号",normalizedTitle:"为什么单个元素的元组需要有逗号&quot;,&quot;",charIndex:null},{level:2,title:"如何理解print函数中end和flush",slug:"如何理解print函数中end和flush",normalizedTitle:"如何理解print函数中end和flush",charIndex:31057},{level:3,title:"换行输出和不换行输出",slug:"换行输出和不换行输出",normalizedTitle:"换行输出和不换行输出",charIndex:31176},{level:3,title:"flush参数",slug:"flush参数",normalizedTitle:"flush参数",charIndex:31566},{level:2,title:"Python中self的作用是什么",slug:"python中self的作用是什么",normalizedTitle:"python中self的作用是什么",charIndex:32179},{level:3,title:"self出现在类的自定义方法的第一个参数上",slug:"self出现在类的自定义方法的第一个参数上",normalizedTitle:"self出现在类的自定义方法的第一个参数上",charIndex:32201},{level:3,title:"self出现在类的自定义方法体内",slug:"self出现在类的自定义方法体内",normalizedTitle:"self出现在类的自定义方法体内",charIndex:32461},{level:2,title:"_init_()方法有什么作用",slug:"init-方法有什么作用",normalizedTitle:"<em>init</em>()方法有什么作用",charIndex:null},{level:2,title:"super()有什么作用",slug:"super-有什么作用",normalizedTitle:"super()有什么作用",charIndex:33944},{level:2,title:"virtualenv/pip/conda区别和联系",slug:"virtualenv-pip-conda区别和联系",normalizedTitle:"virtualenv/pip/conda区别和联系",charIndex:35210},{level:3,title:"什么是virtualenv",slug:"什么是virtualenv",normalizedTitle:"什么是virtualenv",charIndex:35240},{level:3,title:"什么是pip",slug:"什么是pip",normalizedTitle:"什么是pip",charIndex:35468},{level:3,title:"什么是conda",slug:"什么是conda",normalizedTitle:"什么是conda",charIndex:35638},{level:2,title:"回车与换行",slug:"回车与换行",normalizedTitle:"回车与换行",charIndex:36139},{level:2,title:"如何理解字符串前缀u/b/r",slug:"如何理解字符串前缀u-b-r",normalizedTitle:"如何理解字符串前缀u/b/r",charIndex:36554},{level:3,title:"无前缀 和 u前缀",slug:"无前缀-和-u前缀",normalizedTitle:"无前缀 和 u前缀",charIndex:36573},{level:3,title:"b前缀",slug:"b前缀",normalizedTitle:"b前缀",charIndex:36735},{level:3,title:"r前缀",slug:"r前缀",normalizedTitle:"r前缀",charIndex:36912},{level:3,title:"三个前缀总结",slug:"三个前缀总结",normalizedTitle:"三个前缀总结",charIndex:37190},{level:2,title:"如何理解Python3中的bytes和str类型",slug:"如何理解python3中的bytes和str类型",normalizedTitle:"如何理解python3中的bytes和str类型",charIndex:37387},{level:2,title:"web开发理解(摘自廖雪峰)",slug:"web开发理解-摘自廖雪峰",normalizedTitle:"web开发理解(摘自廖雪峰)",charIndex:37473},{level:3,title:"HTTP协议简介",slug:"http协议简介",normalizedTitle:"http协议简介",charIndex:38157},{level:3,title:"总结一下",slug:"总结一下",normalizedTitle:"总结一下",charIndex:40961},{level:3,title:"HTML简介",slug:"html简介",normalizedTitle:"html简介",charIndex:42450},{level:3,title:"WSGI接口",slug:"wsgi接口",normalizedTitle:"wsgi接口",charIndex:43860},{level:2,title:"使用Web框架",slug:"使用web框架",normalizedTitle:"使用web框架",charIndex:47279},{level:3,title:"为什么要有Web框架",slug:"为什么要有web框架",normalizedTitle:"为什么要有web框架",charIndex:47330},{level:3,title:"什么是Web框架",slug:"什么是web框架",normalizedTitle:"什么是web框架",charIndex:48343},{level:3,title:"如何使用Web框架",slug:"如何使用web框架",normalizedTitle:"如何使用web框架",charIndex:48424},{level:2,title:"使用模板",slug:"使用模板",normalizedTitle:"使用模板",charIndex:49758},{level:3,title:"为什么需要模板",slug:"为什么需要模板",normalizedTitle:"为什么需要模板",charIndex:49796},{level:3,title:"什么是模板技术",slug:"什么是模板技术",normalizedTitle:"什么是模板技术",charIndex:50172},{level:3,title:"如何使用模板",slug:"如何使用模板",normalizedTitle:"如何使用模板",charIndex:50805},{level:3,title:"Jinja2模板",slug:"jinja2模板",normalizedTitle:"jinja2模板",charIndex:51834},{level:2,title:"如何理解Nginx/WSGI/Flash之间的关系",slug:"如何理解nginx-wsgi-flash之间的关系",normalizedTitle:"如何理解nginx/wsgi/flash之间的关系",charIndex:52201},{level:3,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:52231},{level:3,title:"Web服务器层",slug:"web服务器层",normalizedTitle:"web服务器层",charIndex:52299},{level:3,title:"Web框架层",slug:"web框架层",normalizedTitle:"web框架层",charIndex:52631},{level:3,title:"WSGI层",slug:"wsgi层",normalizedTitle:"wsgi层",charIndex:47446},{level:3,title:"相关名词解释",slug:"相关名词解释",normalizedTitle:"相关名词解释",charIndex:54191},{level:3,title:"参考信息",slug:"参考信息",normalizedTitle:"参考信息",charIndex:54909},{level:2,title:"如何理解import和from import的区别",slug:"如何理解import和from-import的区别",normalizedTitle:"如何理解import和from import的区别",charIndex:54988},{level:2,title:"定义函数一定要有形参，一定要有返回值吗？",slug:"定义函数一定要有形参-一定要有返回值吗",normalizedTitle:"定义函数一定要有形参，一定要有返回值吗？",charIndex:55249},{level:2,title:"在Python中点号.有哪些用法",slug:"在python中点号-有哪些用法",normalizedTitle:"在python中点号.有哪些用法",charIndex:55324},{level:2,title:"如何写Python函数注释",slug:"如何写python函数注释",normalizedTitle:"如何写python函数注释",charIndex:55368},{level:3,title:'方法一：使用"""xxxx"""的格式',slug:"方法一-使用-xxxx-的格式",normalizedTitle:"方法一：使用&quot;&quot;&quot;xxxx&quot;&quot;&quot;的格式",charIndex:null},{level:3,title:"谷歌风格的注释",slug:"谷歌风格的注释",normalizedTitle:"谷歌风格的注释",charIndex:55752},{level:3,title:"Rest风格",slug:"rest风格",normalizedTitle:"rest风格",charIndex:56104},{level:3,title:"方法二：定义参数的时候加入注释",slug:"方法二-定义参数的时候加入注释",normalizedTitle:"方法二：定义参数的时候加入注释",charIndex:56395},{level:2,title:"如何写Python类的注释",slug:"如何写python类的注释",normalizedTitle:"如何写python类的注释",charIndex:56834},{level:2,title:"如何利用Pyecharts库来画图表",slug:"如何利用pyecharts库来画图表",normalizedTitle:"如何利用pyecharts库来画图表",charIndex:56852},{level:3,title:"安装pyecharts包",slug:"安装pyecharts包",normalizedTitle:"安装pyecharts包",charIndex:56875},{level:3,title:"折线图的示例",slug:"折线图的示例",normalizedTitle:"折线图的示例",charIndex:56922},{level:3,title:"柱形图示例",slug:"柱形图示例",normalizedTitle:"柱形图示例",charIndex:57664},{level:2,title:"conda channel的镜像设置",slug:"conda-channel的镜像设置",normalizedTitle:"conda channel的镜像设置",charIndex:58444},{level:3,title:"显示所有channel",slug:"显示所有channel",normalizedTitle:"显示所有channel",charIndex:58467},{level:3,title:"移除镜像源",slug:"移除镜像源",normalizedTitle:"移除镜像源",charIndex:58572},{level:3,title:"添加可用的清华镜像源",slug:"添加可用的清华镜像源",normalizedTitle:"添加可用的清华镜像源",charIndex:58703},{level:3,title:"关闭安装过程中默认yes",slug:"关闭安装过程中默认yes",normalizedTitle:"关闭安装过程中默认yes",charIndex:59029},{level:3,title:"一些其他的conda指令",slug:"一些其他的conda指令",normalizedTitle:"一些其他的conda指令",charIndex:59161},{level:2,title:"vscode配置 python",slug:"vscode配置-python",normalizedTitle:"vscode配置 python",charIndex:59252},{level:2,title:"python安装psycopg2包",slug:"python安装psycopg2包",normalizedTitle:"python安装psycopg2包",charIndex:59477},{level:3,title:"环境准备",slug:"环境准备",normalizedTitle:"环境准备",charIndex:59499},{level:3,title:"源码安装psycopg2",slug:"源码安装psycopg2",normalizedTitle:"源码安装psycopg2",charIndex:60347}],headersStr:'如何进行Pycharm安装和激活 Pycharm专业版和社区版有什么区别 安装的具体步骤如下 配置agent包 导入密钥 参考URL信息 如何理解多进程与多线程 fork()系统调用 单进程 多进程 多线程 如何实现Python数据库访问 数据库编程接口 访问Mysql 为什么pip安准包会报错 问题描述 解决办法一 解决办法二 如何进行pip离线安装模块 下载第三方包 离线安装 如何实现Python中Request模块中post请求 form形式发送post请求 json形式发送post请求 multipart形式发送post请求 如何理解字典和列表的嵌套 什么是序列 什么是元组 什么是列表 元组和列表有什么区别和联系 什么是字典 字典嵌套字典 字典嵌套列表 列表嵌套字典 列表嵌套列表 什么时候使用()号，什么时候使用[]号，什么时候使用{}号 在Python中使用()小括号有如下两种场景： 在Python中使用[]中括号有如下两种情形： 在Python中使用{}号有如下 一种场景 如何使用Anaconda windows安装Anaconda Centos7安装Anaconda 常见命令 如何在vscode中conda虚拟环境配置 为什么Python运行慢 如何理解Python中for循环range()函数 如何理解for循环和while循环的区别 如何理解Python中的do...while 为什么pip install报SSL ERROR 如何理解Python中的函数和方法的区别 如何实现print函数格式化输出 占位符方式 format方式格式化输出 f-string 格式化字符串常量 为什么单个元素的元组需要有逗号"," 如何理解print函数中end和flush 换行输出和不换行输出 flush参数 Python中self的作用是什么 self出现在类的自定义方法的第一个参数上 self出现在类的自定义方法体内 _init_()方法有什么作用 super()有什么作用 virtualenv/pip/conda区别和联系 什么是virtualenv 什么是pip 什么是conda 回车与换行 如何理解字符串前缀u/b/r 无前缀 和 u前缀 b前缀 r前缀 三个前缀总结 如何理解Python3中的bytes和str类型 web开发理解(摘自廖雪峰) HTTP协议简介 总结一下 HTML简介 WSGI接口 使用Web框架 为什么要有Web框架 什么是Web框架 如何使用Web框架 使用模板 为什么需要模板 什么是模板技术 如何使用模板 Jinja2模板 如何理解Nginx/WSGI/Flash之间的关系 概述 Web服务器层 Web框架层 WSGI层 相关名词解释 参考信息 如何理解import和from import的区别 定义函数一定要有形参，一定要有返回值吗？ 在Python中点号.有哪些用法 如何写Python函数注释 方法一：使用"""xxxx"""的格式 谷歌风格的注释 Rest风格 方法二：定义参数的时候加入注释 如何写Python类的注释 如何利用Pyecharts库来画图表 安装pyecharts包 折线图的示例 柱形图示例 conda channel的镜像设置 显示所有channel 移除镜像源 添加可用的清华镜像源 关闭安装过程中默认yes 一些其他的conda指令 vscode配置 python python安装psycopg2包 环境准备 源码安装psycopg2',content:"Python知识点FRA整理\n\n\n# Python基础知识点\n\n\n# 如何进行Pycharm安装和激活\n\n这里演示的是windows如何安装专业版的pycharm，并且进行激活。\n\n\n# Pycharm专业版和社区版有什么区别\n\n专业版比社区版多了例如：Web开发，Python Web框架，Python的探测，远程开发能力，数据库和SQL的支持。其他的功能两者都是一样的。\n\n具体的功能区别看如下的图示：\n\n\n\n\n# 安装的具体步骤如下\n\n 1. 下载pycharm的版本软件\n    \n    http://xxxx:7000/file/tools/pycharm-professional-2019.1.2.exe\n\n 2. 选择相应的版本\n    \n    \n\n 3. 选择运行pycharm\n    \n    \n\n 4. 选择先不设置\n    \n    一直选择点击下一步，下一步。\n    \n    \n    \n    \n\n\n\n\n\n\n# 配置agent包\n\n 1. 下载agent包\n    \n    下载agent的jar包到相应的目录下。\n    \n    http://xxxx:7000/file/tools/jetbrains-agent.jar\n\n 2. 打开pycharm配置vm options\n    \n    打开pycharm后，选择\"configure\"或\"Help\" --\x3e Edit Custom VM options\n    \n    # custom PyCharm VM options\n    \n    -Xms1024m\n    -Xmx1024m\n    -XX:ReservedCodeCacheSize=240m\n    -XX:+UseConcMarkSweepGC\n    -XX:SoftRefLRUPolicyMSPerMB=50\n    -ea\n    -Dsun.io.useCanonCaches=false\n    -Djava.net.preferIPv4Stack=true\n    -Djdk.http.auth.tunneling.disabledSchemes=\"\"\n    -XX:+HeapDumpOnOutOfMemoryError\n    -XX:-OmitStackTraceInFastThrow\n    -javaagent:C:\\pycharm\\jetbrains-agent.jar\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    \n\n 3. 重启pycharm\n    \n    注意如果启动不来，要注意最后添加的-javaagent信息是否正确，相应的需要打开如下的目录文件进行调整。\n    \n    C:\\Users\\Administrator\\.PyCharm2019.1\\config\\pycharm64.exe.vmoptions\n\n\n# 导入密钥\n\n打开pycharm软件，然后选择\"Configure\"--\x3e \"Manage License\"\n\n选择\"Activation code\"\n\n3AGXEJXFK9-eyJsaWNlbnNlSWQiOiIzQUdYRUpYRks5IiwibGljZW5zZWVOYW1lIjoiaHR0cHM6Ly96aGlsZS5pbyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiIiLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkFDIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRQTiIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJQUyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJHTyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJETSIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJDTCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSUzAiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUkMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUkQiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUEMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUk0iLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiV1MiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREIiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlNVIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9XSwiaGFzaCI6IjEyNzk2ODc3LzAiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-WGTHs6XpDhr+uumvbwQPOdlxWnQwgnGaL4eRnlpGKApEEkJyYvNEuPWBSrQkPmVpim/8Sab6HV04Dw3IzkJT0yTc29sPEXBf69+7y6Jv718FaJu4MWfsAk/ZGtNIUOczUQ0iGKKnSSsfQ/3UoMv0q/yJcfvj+me5Zd/gfaisCCMUaGjB/lWIPpEPzblDtVJbRexB1MALrLCEoDv3ujcPAZ7xWb54DiZwjYhQvQ+CvpNNF2jeTku7lbm5v+BoDsdeRq7YBt9ANLUKPr2DahcaZ4gctpHZXhG96IyKx232jYq9jQrFDbQMtVr3E+GsCekMEWSD//dLT+HuZdc1sAIYrw==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g==\n\n\n1\n\n\n如果激活窗口一直弹出(error 1653219),需要在hosts文件里面移除jetbrains相关的信息。\n\n\n# 参考URL信息\n\n具体参考如下的URL信息\n\nhttps://www.52pojie.cn/thread-961836-1-1.html\n\nhttps://www.sdbeta.com/wg/2019/0509/229677.html\n\n\n# 如何理解多进程与多线程\n\n\n# fork()系统调用\n\nfork系统调用用于从已存在进程中创建一个新进程，新进程称为子进程，而原进程称为父进程。fork调用一次，返回两次，这两个返回分别带回它们各自的返回值，其中在父进程中的返回值是子进程的进程号，而子进程中的返回值则返回0。因此，可以通过返回值来判定该进程是父进程还是子进程。\n\n使用fork函数得到的子进程是父进程的一个复制品，它从父进程处继承了整个进程的地址空间，包括进程上下文、进程堆栈、内存信息、打开的文件描述符、信号控制设定、进程优先级、进程组号、当前工作目录、根目录、资源限制、控制终端等，而子进程所独有的只有它的进程号、计时器等。因此可以看出，使用fork系统调用的代价是很大的，它复制了父进程中的数据段和堆栈段里的绝大部分内容，使得fork系统调用的执行速度并不是很快。\n\n> 参考如下URL:\n> \n> https://blog.csdn.net/guoping16/article/details/6580006\n\n\n# 单进程\n\n在下面的例子中，我们看到的是程序中的代码是按顺序一点点的往下执行，即使是两个不相关的下载任务，也需要先等待一个文件下载完成后才能开始下一个下载任务。\n\nfrom random import randint\nfrom time import time,sleep\n\ndef download_task(filename):\n    print('开始下载%...' % filename)\n    time_to_download = randint(5, 10)\n    sleep(time_to_download)\n    print('%下载完成！耗费%d秒' % (filename, time_to_download))\n    \ndef main():\n    start = time()\n    download_task('Python从入门到住院.pdf')\n    download_task('Peking Hot.avi')\n    end = time()\n    print('总共耗费%.2f秒.' % (end - start))\n\nif __name__ == '__main__':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 多进程\n\n# 概念\n\nUnix和Linux操作系统上提供了fork()系统调用来创建进程。Python的os模块提供了fork()函数。由于windows系统没有fork()调用，因此要实现跨平台的多进程编程，可以使用multiprocessing模块的Porcess类来创建子进程，而且该模块还提供了更高级的封装，例如批量启动进程的进程池(Pool)、用于进程间通信的队列(Queue)和管道(pipe)等。\n\n# 多进程示例\n\n在下面的代码中，我们通过Process类创建了进程对象，通过target参数我们传入一个函数来表示进程启动后要执行的代码，后面的args是一个元组，它代表了传递给函数的参数。'Python从入门到住院.pdf',代表这个元组中只有一个元素。\n\nfrom multiprocessing import Process\nfrom os import getpid\nfrom random import randint\nfrom time import time, sleep\n\ndef download_task(filename):\n    print('启动下载进程,进程号[%d].' % getpid())\n    print('开始下载%s...' % filename)\n    time_to_download =  randint(5, 10)\n    sleep(time_to_download)\n    print('%s下载完成!耗时了%d秒' % (filename, time_to_download))\n\ndef main():\n    start = time()\n    p1 = Process(target=download_task, args=('Python从入门到住院.pdf',))\n    p1.start()\n    p2 = Process(target=download_task, args=('Peking Hot.avi',))\n    p2.start()\n    p1.join()\n    p2.join()\n    end = time()\n    print('总共耗费了%.2f秒.' % (end - start))\n\nif __name__ == '__main__':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# join()方法有什么作用\n\n该方法的意思是指，阻塞当前进程，直到调用join方法的那个进程执行完，再继续执行当前进程。\n\n默认的格式可以为:join([timeout])，该方法内中括号的timeout为一个时间单位为秒的值，中括号为可选参数。如果指定时间到点，就释放阻塞。\n\n如果我们在上面注释掉了p1.join()和p2.join()，那么可以看到程序的运行会不等待两个进程执行完毕就执行后面的end = time()和print。这个不符合我们的预期的。\n\n# run()方法有什么作用\n\nrun()方法是指，如果我们在创建Process对象的时候不指定target，那么就会默认执行Process的run方法。如果我们不改写run方法，那么执行的时候是没有任何效果的，因为默认的run方法是判断如果不指定target，那就什么都不做。例如下面的示例：\n\nfrom multiprocessing import Process\nimport os, time, random\n\ndef r():\n    print 'run method'\n    \nif __name__ == \"__main__\":\n        print \"main process run...\"\n        #没有指定Process的targt\n        p1 = Process()\n        p2 = Process()\n        #如果在创建Process时不指定target，那么执行时没有任何效果。因为默认的run方法是判断如果不指定target，那就什么都不做\n        #所以这里手动改变了run方法\n        p1.run = r\n        p2.run = r\n        \n        p1.start()\n        p2.start()\n        p1.join()\n        p2.join()\n        print \"main process runned all lines...\"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 为什么要依次调用start再调用join\n\n在多进程中，为什么要先依次调用start再调用join，而不是start完了就调用join呢？\n\n例如下面的例子中，如果是p1.start()完后，立即执行p1.join()，随后才是p2.start()呢？结果我们会发现，p2的进程会一直等待p1的进程完成后再去执行。join是用来阻塞当前线程的。\n\np1.start()\np2.start()\np1.join()\np2.join()\n#改为\np1.start()\np1.join()\np2.start()\np2.join()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 参考如下URL：\n> \n> https://www.cnblogs.com/lipijin/p/3709903.html\n\n\n# 多线程\n\n目前的多线程开发，我们使用threading模块，该模块对多线程编程提供了更好的面向对象的封装。\n\n# 使用Thread类创建线程\n\n我们可以直接使用threading模块中的Thread类来创建线程对象，整体的流程和多进程很类似。\n\nfrom random import randint\nfrom threading import Thread\nfrom time import time, sleep\n\ndef download(filename):\n    print(\"开始下载%s...\" % filename)\n    time_to_download = randint(5, 10)\n    sleep(time_to_download)\n    print(\"%s下载完成! 耗费了%d秒\" % (filename, time_to_download))\n\ndef main():\n    start = time()\n    t1 = Thread(target=download, args=(\"Python从入门到住院.pdf\",))\n    t1.start()\n    t2 = Thread(target=download, args=(\"Peking Hot.avi\",))\n    t2.start()\n    t1.join()\n    t2.join()\n    end = time()\n    print(\"总共耗费了%.3f秒\" % (end - start))\n\nif __name__ == '__main__':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n# 继承Thread类自定义线程类\n\n我们通过继承Thread类的方式来创建自定义的线程类，然后再创建线程对象并启动线程。这个时候同样，需要自己定义一个run方法。\n\nfrom random import randint\nfrom threading import Thread\nfrom time import time, sleep\n\nclass DownloadTask(Thread):\n    def __init__(self, filename):\n        super().__init__()\n        self._filename = filename\n    \n    def run(self):\n        print('开始下载%s...' % self._filename)\n        time_to_download = randint(5, 10)\n        sleep(time_to_download)\n        print('%s下载完成！耗费了%d秒' % (self._filename, time_to_download))\n\ndef main():\n    start = time()\n    t1 = DownloadTask('Python从入门到住院.pdf')\n    t1.start()\n    t2 = DownloadTask('Peking Hot.avi')\n    t2.start()\n    t1.join()\n    t2.join()\n    end = time()\n    print('总共耗费了%.2f秒.' % (end - start))\n\nif __name__ == '__main__':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 如何实现Python数据库访问\n\n\n# 数据库编程接口\n\n为了对数据库进行统一的操作，大多数的语言都提供了简单的、标准化的数据库接口(API)。在Python Database API 2.0规范中，定义了Python数据库API接口的各个部分，如模块接口、连接对象、游标对象、类型对象和构造器、DB API的可选扩展以及可选的错误处理机制。\n\n# 连接对象\n\n# 什么是数据库连接对象\n\n数据库的连接对象主要提供获取数据库游标对象和提交、回滚事务的方法，以及关闭数据库连接。\n\n# 为什么需要数据库连接对象\n\n我的理解是创建了数据库的连接对象，实际上是已经创建了一个数据库的连接，我们可以基于这个连接，进行数据库的各种操作。\n\n# 如何获取连接对象\n\n如果是对mysql进行操作的话，我们可以使用pymysql模块中的connect()函数，来获取连接对象。该函数有多个参数，分别可以用来设置主机、IP、端口、用户、密码、数据库名称等信息。\n\nimport pymysql\nconn = pymysql.connect(host='localhost', user='user', password='passwd', db='test', charset='utf8', cursorclass='pymysql.cursors.DictCursor')\n\n\n1\n2\n\n\n# 连接对象的方法\n\nconnect()函数返回连接对象，这个对象表示目前和数据库的会话，连接对象支持的方法如下：\n\n方法名          说明\nclose()      关闭数据库连接\ncommit()     提交事务\nrollback()   回滚事务\ncursor()     获取游标对象，操作数据库，如执行DML操作，调用存储过程等\n\n# 游标对象\n\n是什么？\n\n游标对象代表了数据库中的游标，用于指示抓取数据操作的上下文，主要提供执行SQL语句、调用存储过程、获取查询结果等方法。\n\n如何获取游标对象？\n\n通过使用连接对象的cursor()方法，可以获取到游标对象。游标对象的属性有两个：\n\n * description： 数据库列类型和值的描述信息\n * rowcount：回返结果的行数统计信息，如select、update、callproc等。\n\n游标对象的方法如下表：\n\n方法名                                     说明\ncallproc(procname, [,parameters])       调用存储过程，需要数据库支持\nclose()                                 关闭当前游标\nexecute(operation[,parameters])         执行数据库操作，SQL语句或数据库命令\nexecutemany(operation, seq_of_params)   用于批量操作，如批量更新\nfetchone()                              获取查询结果集中的下一条记录\nfetchmany(size)                         获取指定数量的记录\nfetchall()                              获取结果集的所有记录\nnextset()                               跳至下一个可用的结果集\narraysize                               指定使用fetchmany()获取的行数，默认为1\nsetinputsizes(sizes)                    设置在调用execute*()方法时分配的内存区域大小\nsetoutputsize(sizes)                    设置列缓冲区大小，对大数据列(如longs和blobs)尤其有用\n\n\n# 访问Mysql\n\n# 安装pymysql模块\n\npip install PyMySQL\n\n\n1\n\n\n# 执行创建表\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法创建一个游标对象cursor\ncursor = db.cursor()\n# 使用execute()方法执行SQL，如果表存在则删除\ncursor.execute(\"DROP TABLE IF EXISTS books\")\n# 使用预处理语句创建表\nsql = \"\"\"\nCREATE TABLE books (\n  id int(8) NOT NULL AUTO_INCREMENT,\n  name varchar(50) NOT NULL,\n  category varchar(50) NOT NULL,\n  price decimal(10,2) DEFAULT NULL,\n  publish_time date DEFAULT NULL,\n  PRIMARY KEY(id)\n) ENGINE=MyISAM AUTO_INCREMENT=1 DEFAULT CHARSET=uft8;\n\"\"\"\n# 执行SQL语句\ncursor.execute(sql)\n# 关闭数据库连接\ndb.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 查询语句\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\n# 使用execute()方法执行SQL查询\ncursor.execute(\"select * from books\")\n\n# 使用fetchone()方法获取单条数据\n# 返回的是一个元组\ndata1 = cursor.fetchone()\nprint(data1)\n\n# 使用fetchmany(x)方法查询多条数据\n# fetchmany()方法传递一个参数，其值为2，默认为1\n# 返回的是一个列表，列表中包含2个元组\ndata2 = cursor.fetchmany(2)\nprint(data2)\n\n# 使用fetchall()方法查询所有数据\n# 返回的也是一个列表\ndata3 = cursor.fetchall()\nprint(data3)\n\n# 带条件的查询，使用问好作为占位符号\n# 然后使用一个元组来替换问号\n# 注意，不要忽略元组中最后的逗号\ncursor.execute('select * from user where id >?',(1,))\ndata4 = cursor.fetchall()\nprint(data4)\n\n# 关闭游标\ncursor.close()\n# 关闭connection\nconn.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n# 执行插入\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\n# 数据列表\ndata = [(\"学python\", 'python', '79', '2018-5-20'),\n        (\"学java\", 'java', '100', '2018-6-18'),\n        (\"学php\", 'php', '80', '2019-3-11'),\n        (\"学docker\", 'docker', '200', '2018-12-03')]\ntry:\n    # 执行SQL语句，插入多条数据\n    cursor.executemany(\"insert into books(name, category, price, publish_time) values (%s, %s, %s, %s)\", data)\n    # 提交数据\n    db.commit()\nexcept:\n    # 发生错误时回滚\n    db.rollback()\n# 关闭数据库连接\ndb.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 修改表数据\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\n# 执行update\ncursor.execute('Update user set name = ? where id = ?',('MR',1))\ncursor.execute('select * from user')\ndata = cursor.fetchall()\nprint(data)\n# 关闭游标\ncursor.close()\n# 关闭Connection\nconn.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 删除表数据\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\ncursor.execute('delete from user where id = ?',(1,))\ncursor.execute('select * from user')\ndata = cursor.fetchall()\nprint(data)\n# 关闭游标\ncursor.close()\n# 提交事务\nconn.commit()\n# 关闭connection\nconn.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 为什么pip安准包会报错\n\n\n# 问题描述\n\n在Windows中使用pip install xxxx的方式来安装第三方插件的时候，出现了如下的报错：\n\n\n\n碰到这个问题，主要的原因是网站pypi.python.org在国内是被墙了的，在安装程序的时候，无法从python官网下载而导致的。\n\n\n# 解决办法一\n\n在C盘，进入%user%/pip目录，就是在当前的用户下找到pip文件夹，没有就自己创建一个。在pip文件夹下创建pip.ini文件，修改pip.ini内容如下：\n\n[global] \nindex-url=http://pypi.douban.com/simple/\n[install]\ntrusted-host=pypi.douban.com\n\n\n1\n2\n3\n4\n\n\n随后，重新使用pip install xxx即可。\n\n\n# 解决办法二\n\n可以临时解决这个报错的问题，使用如下的命令进行pip安装\n\npip install 插件名 -i 国内镜像地址 http://pypi.douban.com/simple --trusted-host pypi.douban.com\n\n\n1\n\n\n可以把豆瓣镜像地址换成如下的镜像地址\n\nhttp://mirrors.aliyun.com/pypi/simple/ 阿里云\nhttps://pypi.mirrors.ustc.edu.cn/simple/  中国科技大学\nhttp://pypi.douban.com/simple/  豆瓣\nhttps://pypi.tuna.tsinghua.edu.cn/simple/ 清华大学\nhttp://pypi.mirrors.ustc.edu.cn/simple/ 中国科学技术大学\n\n\n1\n2\n3\n4\n5\n\n\n\n# 如何进行pip离线安装模块\n\n\n# 下载第三方包\n\n下载第三方包，有两种方式\n\n方法一：下载单个包\n\n$ pip download -d /usr/local/download/pip/ jieba\n\n\n1\n\n\n方法二：下载多个包，并指定版本\n\n编写requirement.txt，可参照如下格式：\n\nalembic==1.0.0            # via flask-migrate\namqp==2.3.2               # via kombu\nasn1crypto==0.24.0        # via cryptography\nbabel==2.6.0              # via flask-babel\nbilliard==3.5.0.4         # via celery\nbleach==3.0.2\ncelery==4.2.0\ncertifi==2018.8.24        # via requests\ncffi==1.11.5              # via cryptography\nchardet==3.0.4            # via requests\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n执行下面的命令进行下载:\n\npip download -d /usr/local/download/pip/ -r requirement.txt\n\n\n1\n\n\n\n# 离线安装\n\n方法一：单独安装一个模块\n\npip install --no-index --find-links=/usr/local/download/pip/ jieba \n\n\n1\n\n\n方法二：批量安装定义需要安装的模块\n\npip install --no-index --find-links=/usr/local/download/pip/ -r requirement.txt\n\n\n1\n\n\n\n# 如何实现Python中Request模块中post请求\n\n利用requeset模块中post方法，来进行post请求的发送。\n\n下面的案例是发起post请求，带参数，带请求头。\n\n#! /usr/bin/env python\n# -*- coding: utf-8 -*-\nimport requests\nimport json\nurl = 'http://official-account/app/messages/group'\nbody = {\"type\": \"text\", \"content\": \"测试文本\", \"tag_id\": \"20717\"}\nheaders = {'content-type': \"application/json\", 'Authorization': 'APP appid = 4abf1a,token = 9480295ab2e2eddb8'}\n\nprint type(body)\nprint type(json.dumps(body))\n# 这里有个细节，如果body需要json形式的话，需要做处理\n# 可以是data = json.dumps(body)\nresponse = requests.post(url, data = json.dumps(body), headers = headers)\n# 也可以直接将data字段换成json字段，2.4.3版本之后支持\n# response  = requests.post(url, json = body, headers = headers)\n\n# 返回信息\nprint response.text\n# 返回响应头\nprint response.status_code\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# form形式发送post请求\n\nrequest支持以form表单形式发送post请求，只需要将请求的参数构造成一个字典，然后传给requests.post()的data参数即可。\n\nimport requests\n\nurl = 'http://httpbin.org/post'\nd = {'key1': 'value1', 'key2': 'value2'}\nr = requests.post(url, data=d)\nprint r.text\n\n\n1\n2\n3\n4\n5\n6\n\n\n我们可以看到，请求头中的content-type字段已经设置为application/x-www-form-urlencoded，且d = {'key1': 'value1', 'key2': 'value2'}以form表单的形式提交到服务端，服务端返回的form字段即是提交的数据。\n\n\n# json形式发送post请求\n\n也就是将一个json串传给requests.post()的data参数。\n\nurl = 'http://httpbin.org/post'\ns = json.dumps({'key1': 'value1', 'key2': 'value2'})\nr = requests.post(url, data=s)\nprint r.text\n\n\n1\n2\n3\n4\n\n\n可以看到，请求的content-type设置为application/json，并将这个json串提交到了服务端中。\n\n\n# multipart形式发送post请求\n\nrequests也支持以multipart形式发送post请求，只需将一文件传给requests.post()的file参数即可。\n\nurl = 'http://httpbin.org/post'\nfiles = {'file': open('report.txt', 'rb')}\nr = requests.post(url, files=files)\nprint r.text\n\n\n1\n2\n3\n4\n\n\n\n# 如何理解字典和列表的嵌套\n\n\n# 什么是序列\n\n序列是有顺序的数据集合，就好像一列排好对的士兵。\n\n序列包含的一个数据被称为序列的一个元素。\n\n序列可以分为两种类型，元组(tuple)和列表(list)\n\n\n# 什么是元组\n\n定义：是一组不可变更的元素，是不可变的序列，也是一种可以存储各种数据类型的集合，用小括号()表示元组的开始和结束，元素之间用逗号,分隔。\n\n这里的不可变，包括不能对元组对象进行增加元素、变换元素位置、修改元素、删除元素操作。元组中的每个元素提供对应的一个下标，下标从0开始，0、1、2...按顺序标注。\n\n值得注意的是：多个元素可以是不同的类型，可以是整型，可以是浮点型，也可以是字符序，布尔型。用小括号来定义。\n\n元组的定义如下：\n\nexample_tuple = (2, \"love\", 1.3 , False)\n\n\n1\n\n\n元组的引用如下：引用的格式是，引用名+中括号，中括号里面是从0开始的数字，代表第一个元素。\n\nexample_tuple[0]\nexample_tuple[0]  =  \"12121212\"\n\n\n1\n2\n\n\n\n# 什么是列表\n\n列表是可变个序列，也是一种可以存储各种数据类型的集合，用中括号[]表示列表的开始和结束，元素之间用逗号,分隔。列表中每个元素提供一个对应的下标。\n\n\n# 元组和列表有什么区别和联系\n\n元组和列表都是用来存储多个元素的容器。\n\n不同的地方在于，元组的各个元素不可再变更，而列表元素可以变更。\n\n\n# 什么是字典\n\n词典也是一个可以容纳多个元素的容器。\n\n词典包含多个元素，每个元素以逗号分隔。\n\n词典的元素包含两个部分，键(key)和值(value)。键是数据的索引，值是数据本身。\n\n词典的元素可以通过键来引用。\n\n字典的定义：\n\nexample_dict = {\"tom\":11, \"sam\":57, \"lily\":100}\n\n\n1\n\n\n字典的引用：\n\nexample_dict[\"tom\"]\nexample_dict[\"tom\"] = 30\n\n\n1\n2\n\n\n\n# 字典嵌套字典\n\n是指在一个字典的内部再嵌套一个字典。{}表示是一个字典，jiangsu和anhui是里面的两个字典，jiangsu里面有嵌套了多个字典。\n\n不太清楚什么时候用{}还是[]的时候，用type来判断确认。\n\n用{}还是[]关键在于，里面的内容是啥，是key/value的形式值，还是单纯的是一个列表。\n\ndict5 = {\n    'jiangsu': {\n        'nanjing': '025',\n        'wuxi': '0510',\n        'xuzhou': '0516',\n        'changzhou': '0519'\n    },\n    'anhui': {\n        'hefei': '0551',\n        'wuhu': '0553',\n        'buben': '0552'\n    }\n}\nprint(type(dict5))\nprint(dict5['jiangsu'])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 字典嵌套列表\n\n是指在字典中嵌套了一个列表。在下面的例子中，jiangsu和anhui是两个字典，jiangsu里面又嵌套了多个字典，其中nanjing这个字典里面嵌套了一个列表。\n\n不太清楚什么时候使用{}和[]的时候，用type来判断。\n\n用{}还是[]关键在于，里面的内容是啥，是key/value的形式值，还是单纯的是一个列表。\n\ndict6 = {\n    'jiangsu': {\n        'nanjing': ['025','111111','222222'],\n        'wuxi': '0510',\n        'xuzhou': '0516',\n        'changzhou': '0519'\n    },\n    'anhui': {\n        'hefei': '0551',\n        'wuhu': '0553',\n        'buben': '0552'\n    }\n}\nprint(type(dict6))\nprint(dict6['jiangsu'])\nprint(dict6['jiangsu']['nanjing'])\nprint(dict6['jiangsu']['nanjing'][1])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 列表嵌套字典\n\n下面的例子中是，在dict7是一个列表，这个列表中有两个元素，jiangsu那个元素是一个大的字典，在这个字典里面，nanjing里面又嵌套了多个列表的元素。\n\ndict7 = [\n    {\n    'jiangsu': {\n        'nanjing': ['025','111111','222222'],\n        'wuxi': '0510',\n        'xuzhou': '0516',\n        'changzhou': '0519'\n    }},\n    {'anhui': {\n        'hefei': '0551',\n        'wuhu': '0553',\n        'buben': '0552'\n    }}\n]\nprint(type(dict7))\nprint(dict7[0])\nprint(dict7[1])\nprint(dict7[1]['anhui']['hefei'])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 列表嵌套列表\n\n在下面的例子中，dict8是一个列表。这个列表里面有两个元素，jiangsu这个元素是一个字典，后面的一个元素是一个列表。[{'hefei': '0551'}],[{'wuhu': '0553'}],[{'buben': '0552'}] 里面的每一个元素是一个key/value的字典。\n\ndict8 = [\n    {\n    'jiangsu': {\n        'nanjing': ['025','111111','222222'],\n        'wuxi': '0510',\n        'xuzhou': '0516',\n        'changzhou': '0519'\n    }},\n    [[{'hefei': '0551'}],[{'wuhu': '0553'}],[{'buben': '0552'}]]\n]\nprint(type(dict8))\nprint((dict8)[1])\nprint(type((dict8)[1][0]))\nprint((dict8)[1][0][0]['hefei'])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 什么时候使用()号，什么时候使用[]号，什么时候使用{}号\n\n\n# 在Python中使用()小括号有如下两种场景：\n\n 1. 定义一个元组tuple的时候\n    \n    #一个元组，括号\n    #注意元组的各个元素定义完后不可再更改了\n    #也就是说后面不可能再出现对元组某个元素赋值的操作了\n    example_tuple = (2, 1.3, \"love\", 5.6, False)\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 函数定义中表示参数/调用函数时参数传递(和类中方法一个意思)\n    \n    # 定义函数\n    def square_sum(a,b):\n        a = a**2\n        b = b**2\n        c = a + b\n        return c\n    # 调用函数\n    a = 5\n    b = 6\n    x = square_sum(a, b)\n    print(x)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n\n\n# 在Python中使用[]中括号有如下两种情形：\n\n 1. 定义一个列表(list)\n    \n    在序列中，我们可以通过下面的方式来定义一个列表，主要注意的是定义的时候，变量名和后面[]中间有个=号来连接的。\n    \n    example_list = [True, 5, \"smile\"]             \n    #一个列表，中括号\n    \n    \n    1\n    2\n    \n\n 2. 表示对某个或某些元素的引用\n    \n    需要注意的是，引用元素的时候，中间是没有=号的。但是如何对字典的某个元素进行赋值的时候，是可以有=号的。\n    \n    * 元组中对元素的引用\n      \n      #显示序列example_tuple元组中第一个元素\n      example_tuple[0]\n      \n      \n      1\n      2\n      \n    \n    * 列表中对元素的引用\n      \n      #显示的是第二个元素中，\n      #第二个元素是一个嵌套的列表，该嵌套列表中的第三个元素\n      nest_list[1][2]\n      \n      \n      1\n      2\n      3\n      \n    \n    * 元组或列表中多个元素的引用\n      \n      序列中可以通过范围引用，来找到多个元素。范围引用的基本样式是：序列名[下限:上限:步长]\n      \n      下限表示起始下标，上限表示结尾下标。在起始下标和结尾下标之间，按照步长的间隔来找到元素。默认的步长是1。\n      \n      # 下标为0，2，4的元组元素\n      example_tuple[0:5:2]\n      # 倒数第一(最后一个)的元素\n      # -1代表最后一个\n      example_tuple[-1]\n      # 倒数第三个元素\n      # -3代表倒数第三\n      example_tuple[-3]\n      #序列的第二个到倒数第二个元素\n      #这里是下限是-1，下限是最后一个元素\n      example_tuple[1:-1] \n      \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      \n    \n    * 对字典中的key进行赋值\n      \n      example_dict = {\"tom\":11, \"sam\":57, \"lily\":100}\n      example_dict[\"tom\"]\n      example_dict[\"tom\"] = 30\n      \n      \n      1\n      2\n      3\n      \n\n\n# 在Python中使用{}号有如下 一种场景\n\n在Python中定义字典的时候，使用{}，但是引用的时候使用[]\n\nexample_dict = {\"tom\":11, \"sam\":57, \"lily\":100}\nexample_dict[\"tom\"]\n\n\n1\n2\n\n\n\n# 如何使用Anaconda\n\n\n# windows安装Anaconda\n\nAnaconda指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。 因为包含了大量的科学包，Anaconda 的下载文件比较大（约 531 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用Miniconda这个较小的发行版（仅包含conda和 Python）。 Conda是一个开源的包、环境管理器，可以用于在同一个机器上安装不同版本的软件包及其依赖，并能够在不同的环境之间切换。\n\nAnaconda包括Conda、Python以及一大堆安装好的工具包，比如：numpy、pandas等。\n\nMiniconda包括Conda、Python。\n\n分为下载，安装，配置环境变量。\n\n设置如下环境变量：\n\nC:\\Anaconda3\\Scripts\nC:\\Anaconda3\\Library\\bin\n而C:\\Anaconda3可以先不设置看看\n\n\n1\n2\n3\n\n\n\n# Centos7安装Anaconda\n\n# 下载Anaconda\n\n可以在清华镜像的网站下载最新的匹配的Anaconda版本，地址是 清华大学镜像\n\n\n\n# 安装Anaconda\n\n$ bash Anaconda3-2019.10-Linux-x86_64.sh \n\n\n1\n\n\n许可证声明，回车\n\nIn order to continue the installation process, please review the license\nagreement.\nPlease, press ENTER to continue\n>>> \n\n\n1\n2\n3\n4\n\n\n选择yes\n\nDo you accept the license terms? [yes|no]\n[no] >>> \n\n\n1\n2\n\n\n选择安装目录，这里我们就直接回车，不修改目录了。\n\nAnaconda3 will now be installed into this location:\n/app/python/anaconda3\n\n  - Press ENTER to confirm the location\n  - Press CTRL-C to abort the installation\n  - Or specify a different location below\n\n[/app/python/anaconda3] >>> \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n是否选择利用conda初始化，我们这里选择yes。\n\n这样自动添加环境变量，会使得开机自动启动base环境\n\nDo you wish the installer to initialize Anaconda3\nby running conda init? [yes|no]\n[no] >>> \n\n\n1\n2\n3\n\n\n退出，重新进入python用户，进行验证。\n\n(base) [python@lteindicator091224 ~]$ \n\n\n1\n\n\n\n# 常见命令\n\n# 查看conda版本\n\n$ conda --version\n\n\n1\n\n\n# 升级conda版本\n\n$ conda upgrade --all\n\n\n1\n\n\n# 虚拟环境是什么\n\n默认会有一个base环境的python解释器，我们可以将原有环境变量中的python去除掉，这个时候使用的python不是原来环境变量中设置的，而是base环境下的python。而命令行前面也会多一个(base)说明当前我们处于的是base环境下。\n\n\n\n# 新建虚拟环境\n\n下面创建一个名为test1的虚拟环境，并指定python版本为3.7(这里conda会自动找3.7中最新的版本下载)\n\n$ conda create -n test1 python=3.7\n\n\n1\n\n\n# 激活环境\n\nwindows机器\n$ activate test1\nLinux和mac机器\n$ source activate test1\n\n\n1\n2\n3\n4\n\n\n# 取消激活的环境\n\nwindows机器\n$ deactivate test1\nlinux和mac机器\n$ source deactivate test1\n\n\n1\n2\n3\n4\n\n\n# 切换环境\n\n$ activate test1\n\n\n1\n\n\n# 查看所有的环境\n\n$ conda env list\n\n\n1\n\n\n# 卸载环境\n\n$ conda remove --name test1 --all\n\n\n1\n\n\n# 卸载第三方包\n\n利用conda卸载\n$ conda remove requests\n利用pip卸载\n$ pip uninstall requests\n\n\n1\n2\n3\n4\n\n\n# 安装第三方包\n\n利用conda安装\n下面的name是packages包的名字\n$ conda install name\n# 使用conda install package=version 就能安装指定版本的package\n利用pip安装\n$ pip install name\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 查看环境中包信息\n\n$ conda list\n\n\n1\n\n\n# 导入导出环境\n\n如果想要导出当前环境的包信息可以用，下面的命令导出yaml文件中\n\n$ conda env export > environment.yaml\n\n\n1\n\n\n重新创建一个相同的虚拟环境(导入)可以用，下面的命令\n\n$ conda env create -f environment.yaml\n\n\n1\n\n\n\n# 如何在vscode中conda虚拟环境配置\n\n 1. 安装Anaconda\n\n 2. 安装vscode\n\n 3. 配置window的环境变量\n    \n    /c/Anaconda3/Library/bin\n    /c/Anaconda3/Scripts\n    /c/Anaconda3\n    \n    \n    1\n    2\n    3\n    \n\n 4. 设置vscode中变量\n    \n    文件--首选项--设置--扩展--Python\n    \n    在用户配置或工作区配置，选择python.pythonPath为anaconda中的python路径，这里是\"C:\\Anaconda3\"\n    \n    \n\n 5. 新建虚拟环境\n    \n    参照前面的Anaconda3中conda的设置\n\n 6. 切换python虚拟环境\n    \n    左下角，选择相应的解释器。\n    \n    \n    \n    选择需要相应虚拟环境的，相应的工作目录。这些目录需要在工作区内。\n    \n    \n    \n    随后打开上面选择的目录下的python文件，发现会已经切换了\n    \n    \n\n\n# 为什么Python运行慢\n\n 1. Python是动态性语言不是静态性语言\n    \n    Python程序执行的时候，编译器不知道变量的类型。Python只知道是一个对象，动态类型意味着任何操作都需要更多的步骤。这是Python在数值数据操作方面比C慢的主要原因。\n\n 2. Python是解释性语言而不是编译性语言\n    \n    解释型语言与编译型语言它们本身的区别也会造成程序在执行的时候的速度差异。一个智能化的编译器可以预测并针对重复和不需要的操作进行优化。这也是提升程序执行的速度。\n\n 3. Python的对象模型会导致访问内存效率低下\n    \n    相对于C语言，例如Python中对整数进行操作会有一个额外的类型信息层。当对数据进行操作时(例如排序、计算、查找等)，无论是存续成本还是访问成本，都很高。\n\n> 参考如下文档，进行的整理\n> \n> https://www.jianshu.com/p/3fa56d9f58cb\n\n\n# 如何理解Python中单引号/双引号/多引号\n\n\n# 如何理解Python中for循环range()函数\n\n在Java中的for循环的格式是如下的：\n\nfor (i=0, i<10, i++){\n    system.out.print(i);\n}\n\n\n1\n2\n3\n\n\n在python中要改写成这样：\n\nfor i in range(0, 10, 1):\n    print(i)\n\n\n1\n2\n\n\n其中range()函数中的第一个参数是开始(包含本身)，第二个参数是结束(不包含本身)，第三个参数是步长。\n\n> 参考的如下的网上文档：\n> \n> https://jingyan.baidu.com/article/fec7a1e5d6f00a1190b4e7a5.html\n\n\n# 如何理解for循环和while循环的区别\n\nfor循环，就是遍历某一对象，也就是说根据循环次数限制做多少次重复操作。\n\n例如如下的for循环操作：for i in range(3)意思就是i循环4次，i的取值为0、1、12。\n\nwhile循环，就是当满足什么条件的时候，才做某种操作。\n\n例如如下的while操作：while count < 3意思是当count小于3时，才做下面的操作。\n\n案例分析：在做一个登陆的小程序的时候，最多输入用户名和密码3次，这时就应该用while循环，而不是for循环，因为循环次数不一定(我们的循环体内写的是重复登陆要输入的用户名和密码)。\n\n> 参考如下的网页：\n> \n> https://www.cnblogs.com/klmm/p/8620331.html\n\n\n# 如何理解Python中的do...while\n\nJava中有相应的do while的语法，与while的差别在于，do while是先执行循环体语句，然后进行判断语句，也就是说无论判断是否为true都会至少执行一次。\n\npublic class whiletest {\n    public static void main(string[] args) {\n        int i = 1;\n        do {\n            i++;\n            system.out.println(i);\n        } while (i<1) \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n在python中是没有do while这种语法格式的，但是可以使用while(无限循环)和break组合起来实现do .... while 。这里的while(无限循环)，相当于是do 里面的内容；而带上if 条件后的break，就是原有的while 中的内容。\n\nn = 0 \nwhile True:     # 无限循环....\n    print n\n    n += 1\n    if n == 10:\n        break\n\n\n1\n2\n3\n4\n5\n6\n\n\n> 参照如下网页：\n> \n> https://blog.csdn.net/qq_37131111/article/details/54580587\n> \n> https://www.jianshu.com/p/028cd9460ccb\n> \n> https://segmentfault.com/q/1010000002428584\n\n\n# 为什么pip install报SSL ERROR\n\n正常执行pip install某个模块的时候，就会报SSL的错误。\n\n改为如下命令执行：\n\npip install requests -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com\n\n\n1\n\n\n\n# 如何理解Python中的函数和方法的区别\n\n函数的定义：在函数定义的过程中，用的是def关键字，有函数的形参，有相应的return 返回值。\n\ndef square_sum(a,b):\n    a = a**2\n    b = b**2\n    c = a + b\n    return c\n\n\n1\n2\n3\n4\n5\n\n\n函数的调用：函数的调用，和函数的定义很相似。只不过在调用函数的时候，我们把真实的数据填入到括号中，作为参数传递给函数。\n\na = 5\nb = 6\nx = square_sum(a, b)\nprint(x)\n\n\n1\n2\n3\n4\n\n\n方法的定义：方法属于定义类中包含的内容，方法是用来定义类的一些\"行为\"属性，也就是说在类的内部定义函数的方式来说明方法。但是和定义函数的方式还是有不同。也是使用def关键字，方法的内部的第一个参数是self，是为了在方法内部引用对象自身。无论该参数是否用到，方法的第一个参数必须是用于指代对象自身的self。\n\nclass Bird(object):\n    feather = True\n    reproduction = \"egg\"\n    def chirp(self, sound):\n        print(sound)\n\n\n1\n2\n3\n4\n5\n\n\n方法的调用：我们是通过对象来调用方法，先要实例化一个对象，然后像调用函数那样，通过传参来实现调用。\n\nsummer = Bird()\nsummer.chirp(\"jijijij\")    #打印'jijijij'\n\n\n1\n2\n\n\n\n# 如何实现print函数格式化输出\n\n在python中，我们有时候需要对输出进行格式化输出，有两种方式：第一种方法是占位符的方式；第二种方法是格式化输出的方式。\n\n\n# 占位符方式\n\n例如我们需要输出\"你好XX，你的额度是XX\"，其中XX的变量，我们是无法预知的。这个时候就需要格式化输出，和C语言一样，我们可以使用占位符%?，其中?代表不同的字符，例如%s代表字符串，%d代表十进制整数，%f代表浮点数。\n\n%前面是占位符号，后面是真实的值，name将替换%s的位置，b将替换%d的位置，字符串后的%用来说明是哪些变量要替换前面的占位符，当只有一个变量的时候，可以省略括号。\n\nname = \"potato\"\nb = 100\nprint(\"你好%s，你的额度是%d\" % (name,b))\n\n\n1\n2\n3\n\n\n占位符还可以控制输出的格式，例如保留几位小数。如果不知道数据类型的话，用%s即转为字符串进行输出会比较稳妥。\n\n# %.2f代表保留两位小数\nprint(\"小数: %.2f\" % 3.14159)\n# %.2f保留两位小数，不够的位用0补充\nprint(\"小数: %.2f\" % 4.5)\n# %3d代表这个数的宽度为3，不够的话用空格在前面补，如果数的宽度大于3，则正常输出\nprint(\"占位: %3d\" % 5)\n# %05d代表这个数的宽度为5，不够的话用0在前面补，如果数的宽度大于5，则正常输出\nprint(\"前导0: %05d\" % 2)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# format方式格式化输出\n\n可以利用string对象的format对象，进行格式化。\n\n#{0}代表占位符和format里的参数对应，{1:.2f}，\n#冒号后是格式控制，代表保留两位小数，\n#下面的结果是\"你好potato,你余额是3.10\"\nprint(\"你好{0}，你的余额是{1:.2f}\".format(\"Potato\",3.1))\n#或者可以使用py内置的format函数\nprint(format(3.1415,\".2f\"))\n#结果是3.14\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 参考的URL如下：\n> \n> https://segmentfault.com/a/1190000018081959\n\n从Python2.6开始，新增了一种格式化字符串的函数str.format()，它增强了字符串格式化的功能。基本的语法是通过{}和:来代替以前的%.\n\nformat函数可以接受不限参数个数，位置可以不按顺序。\n\n字符串里面的{}，想怎么写就怎么写，不需要考虑在{}上面加上''或\"\"，在format函数后面的括号里面的各个值，要注意写的方式，如果是字符串就加上''或\"\"，如果是数字类型，那么就什么也不要加。\n\nstr.format()里面只是描述了，在字符串的某个位置，应该填入哪些变量，至于这个变量合不合理，类型是不是有误，这就不是str.format()函数关心的事情了，例如如果写入sql中的字段不对，那么数据库自己根据事务性的原则自己去回滚。\n\n# 不指定位置\n\nstr里面的{}和format()函数里面的各个值，不去指定对应的关系，那么就按照两者出现的先后顺序去匹配上。\n\nprint('i love nanjing: {} {}'.format(\"sha\",\"lili\"))\n\nprint('i love nanjing: ({},{})'.format(100,\"lili\"))\n\n\n1\n2\n3\n\n\n# 指定位置\n\n通过对str里面的{0}、{1} .... 等方式，来和后面的format()函数中的各个值匹配上，{0}代表匹配的是format()函数中的第一个值，{1}代表匹配的是format()函数中的第二个值，以此类推。\n\nprint('i love nanjing: {1} {0}'.format(\"sha\",\"lili\"))\n\nprint('i love nanjing: ({1},{0})'.format(100,\"lili\"))\n\n\n1\n2\n3\n\n\n\n# f-string 格式化字符串常量\n\n摘录于如下的URL地址：\n\n> Python格式化字符串f-string概览\n\nf-string，被称为格式化字符串常量，是Python3.6新引入的一种字符串格式化方法，主要目的是事格式化字符串的操作更加简便。f-string在形式上是以f或F修饰符引领的字符串(f'xxx'或F'xxx')，以大括号{} 标明被替换的字段；f-string在本质上并不是字符串常量，而是一个在运行时运算求值的表达式。\n\nf-string在功能方面不逊于传统的%-formatting语句和str.format()函数，同时性能又优于两者，且使用起来也更加简明，因此对于Python3.6及以后的版本，推荐使用f-string进行字符串格式化。\n\ns_num = 1005\nprint(f\"学号为{s_num}的详细信息为：\" f\"number={num},name={name},class_name={cs_name}\")\n\n\n1\n2\n\n\n\n# 为什么单个元素的元组需要有逗号\",\"\n\n在进行多进程的案例中，出现了在初始化Process类为对象的时候，需要传参一个元组，但是有时候这个元组只有一个元素，很奇怪的发现了是如下的写法args=('Peking Hot.avi', )\n\n其实这是在Python中的规范，如果一个元组中只有一个元素，那么必须需要在这个元素后面加上','来表示这是个元组，否则的话Python解释器会把这个解释为其他的类型。例如下面的案例：\n\na = (1, 2)\nb = (3)\nc = a + b\n\n\n1\n2\n3\n\n\n执行上面的代码会发现报错TypeError: can only concatenate tuple (not \"int\") to tuple,原来python解释器把(3)当作一个算数表达式来处理的，它的结果就是一个int型对象。为了和只有单个元素的元组区分，python规定要在元素后面带上一个逗号，例如d=(3,)。也就是如下的形式：\n\na = (1, 2)\nb = (3,)\nc = a + b\n\n\n1\n2\n3\n\n\n\n# 如何理解print函数中end和flush\n\n代码中有时候会出现如下的print的格式：\n\nfor i in range(5):\n    print(\"hello\", end = '', flush = True)\n\n\n1\n2\n\n\n\n# 换行输出和不换行输出\n\n**end = \"\"**的意思是，双引号之间的内容就是结束的内容，可以是空格，也可以是其他字符，如果不去定义end，那么默认就是换行的。\n\n换行输出：\n\nfor i in range(5):\n    print(\"hello\", end = '\\n', flush = True)\nfor i in range(5):\n    print(\"hello\")\n\n\n1\n2\n3\n4\n\n\n不换行输出，每次输出以一个空格结尾：\n\nfor i in range(5):\n    print(\"hello\", end = \" \", flush = True)\n\n\n1\n2\n\n\n不换行输出，每次输出都紧接着上一次的输出：\n\nfor i in range(5):\n    print(\"hello\", end = \"\", flush = True)\n\n\n1\n2\n\n\n\n# flush参数\n\n简单来说就是将缓存里面的内容立即输出到标准输出流(这里是sys.stout，也就是默认的显示器)。\n\n这个功能在客户端脚本几乎用不上，大多用于服务器端。\n\n比如反向Ajax里面就要用到flush,举个例子: 在线web聊天页面会实时显示聊天的内容, 其实后台是一直在向服务器请求数据的,正常情况下是请求完毕之后才会输出相应内容, 但是是即时聊天, 需要一有相应就得立即返回,flush也就起作用了。\n\n下面的例子中，我们用print写内容到一个文件中，先运行第一段代码，我们发现123.txt文件，并没有\"123456789\"被写入，文件内容为空。只有f.close()后才将内容写进文件中。如果加入flush = True，那么可以不用f.close()即可将内容写进文件中。\n\nflush参数主要是刷新，默认flush=False，不刷新。也就是print到f中的内容先存到内存中，当文件对象关闭时才把内容输出到123.txt中；而当flush=True时它会立即把内容刷新存到123.txt中。\n\n# 示例1\nf = open(\"123.txt\", \"w\")\nprint(\"123456789\", file = f)\n# 示例2\nf = open(\"123.txt\", \"w\")\nprint(\"123456789\",file = f, flush = True)\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# Python中self的作用是什么\n\n\n# self出现在类的自定义方法的第一个参数上\n\nself英文是自己的意思，在python中定义类的方法的时候，我们一般习惯于，将self作为该类中的定义的某个方法的一个参数。而这个self的意思在于，在方法内部引用了对象自身(self代表类的实例，而非类)。\n\nclass Bird(object):\n    feather = True\n    reproduction = \"egg\"\n    def chirp(self, sound):\n        print(sound)\n\n\n1\n2\n3\n4\n5\n\n\n\n# self出现在类的自定义方法体内\n\n出现在类的自定义方法体内的话，self代表的是对于对象(instance)自身的引用。\n\n或者可以说是通过在方法体中的self来配置该实例的个性化对象属性。\n\n这就涉及到一个作用域的问题，下面案例中的self.inputname和input1这两个变量，都赋值为name，那么这两个值有什么区别呢？\n\nclass Person:\n    def setName(self, name):\n        self.name = name\n        self.inputname = name\n        input1 = name\n    def getName(self):\n        return self.name\n    def greet(self):\n        print \"Hello world! I'm %s . \" % self.name\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nself.name = name的意思是，setName方法中的传参name赋值给，初始化Person这个class后的某个具体对象的name属性。而input1 = name则是正常的类中的某个方法中的属性赋值。\n\n总结：不加self的变量是局部变量，作用域是当前函数方法。(该类中其他方法无法使用这个变量)；而加了self的变量的是实例变量(某个具体对象的变量)，作用域是当前实例，可以在该类的其他方法中对该变量的使用。\n\n> 参考如下的URL：\n> \n> https://www.jianshu.com/p/bdbd577314f9\n> \n> https://blog.csdn.net/bing900713/article/details/60884931\n> \n> https://www.cnblogs.com/jessonluo/p/4717140.html\n\n\n# _init_()方法有什么作用\n\n_init_()函数本身就是python中的，针对类的初始化的构造方法。用来初始化新创建的对象的各种属性，在一个对象被创建以后就会被立即调用。\n\n * 带有两个下划线开头的函数是声明该属性为私有，不能再类的外部被使用或直接访问。\n * init函数（方法）支持带参数的类的初始化 ，也可为声明该类的属性\n * init函数（方法）的第一个参数必须是 self（self为习惯用法，也可以用别的名字），后续参数则可 以自由指定，和定义函数没有任何区别。\n\n在类中没有_init_()函数方法的时候，我们如果想在创建这个类的对象后，就想让这个对象拥有一些个性化(父类没有)的属性的时候，我们就需要手动写个方法，然后在创建完对象后，手动调用这个方法来实现，(但是想在各个方法中共用这个初始化的属性，还想做更多)，实现起来非常的麻烦。\n\nclass Bird(object):\n    def __init__(self, sound):\n        self.sound = sound\n        print(\"my sound is:\", sound)\n    def chirp(self):\n        print(self.sound)\n\nsummer = Bird(\"ji\")\nsummer.chirp()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 参考如下的URL：\n> \n> https://www.javazhiyin.com/40640.html\n\n\n# super()有什么作用\n\n我们在子类中使用super关键字，这样就可以在子类中调用父类中被覆盖的方法。\n\nsuper是用来解决多重继承问题的，直接用类名调用父类方法在使用单继承的时候没有问题，但是如果使用多继承，会涉及到查找顺序(MRO)、重复调用(钻石继承)等种种问题。\n\nPython3中可以直接使用super().xxx代替super(Class, self).xxx\n\n示例如下：\n\n# coding=utf-8\n\n# 胖子老板的父类\nclass FatFather(object):\n    def __init__(self,name):\n        print('FatFather的init开始被调用')\n        self.name = name\n        print('调用FatFather类的name是%s' % self.name)\n        print('FatFather的init调用结束')\n\n# 胖子老板类 继承 FatFather 类\nclass FatBoss(FatFather):\n    def __init__(self,name,hobby):\n        print('胖子老板的类被调用啦！')\n        self.hobby = hobby\n        #FatFather.__init__(self,name)   # 直接调用父类的构造方法\n        super().__init__(name)\n        print(\"%s 的爱好是 %s\" % (name,self.hobby))\n\n\ndef main():\n   #ff = FatFather(\"胖子老板的父亲\")   \n   fatboss = FatBoss(\"胖子老板\",\"打斗地主\")\n\nif __name__ == \"__main__\":\n   main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n注意事项：\n\n * super().__init__相对于类名.__init__，在单继承上用法基本无差\n * 但在多继承上有区别，super方法能保证每个父类的方法只会执行一次，而使用类名的方法会导致方法被执行多次\n * 多继承时，使用super方法，对父类的传参数，应该是由于python中super的算法导致的原因，必须把参数全部传递，否则会报错\n * 单继承时，使用super方法，则不能全部传递，只能传父类方法所需的参数，否则会报错\n * 多继承时，相对于使用类名.__init__方法，要把每个父类全部写一遍, 而使用super方法，只需写一句话便执行了全部父类的方法，这也是为何多继承需要全部传参的一个原因\n\n> 参考如下的URL：\n> \n> https://www.jianshu.com/p/3b7ebe0389e4\n\n\n# virtualenv/pip/conda区别和联系\n\n\n# 什么是virtualenv\n\n就是可以利用virtualenv工具来创建任意多个虚拟环境，每个虚拟环境可以用于不同的项目。\n\n这样当我们运行环境中运行多种不同类型的python程序的时候，不要担心不同环境中不同的python版本，模块版本，导致python的运行受到影响。\n\n * 分门别类的创建不同的虚拟环境，互不污染。(如机器学习和爬虫互不影响)\n * 其次，一旦我们不使用了，可以直接删除虚拟环境，而不用管各种文件残留，可能带来的关联问题。\n\n\n# 什么是pip\n\npip 是Python最常用的包管理器，该工具提供了对python包的查找、下载、安装、卸载的功能。能够自动处理依赖。类似于linux的yum工具。\n\n主要的安装包的使用方法为pip install packagename, conda也具有包管理器的功能，命令为conda install packagename\n\n\n# 什么是conda\n\nconda也可以用来创建虚拟环境，并且安装python 包。\n\n * venv是虚拟环境管理器，pip是包管理器，那么conda则是两者的结合。\n * conda的包管理器一般会安装过多的依赖，大多数情况下还是使用pip安装包。\n * pip只能安装python包，conda可以安装一些工具软件，即使这些软件不是基于Python开发的。\n * 在虚拟环境管理上，venv会在该项目下创建虚拟环境；然而conda的每个虚拟环境不会占用项目文件夹的空间，它创建在用户设定的一个位置，这使得多个项目共享一个虚拟环境更加方便。\n * conda虚拟环境是独立于操作系统解释器环境，即无论操作系统解释器是什么版本，我们也可以指定虚拟环境python版本为3.7，而venv是依赖主环境的。\n * conda是环境自动集成了numpy这样的主流科学计算包，venv每个包都要自行下载。\n * conda有图形化环境管理器，venv没有。\n\n参考如下URL：\n\n> https://blog.csdn.net/zhouchen1998/article/details/84671528\n\n\n# 回车与换行\n\n\\n和\\r的区别的理解:\n\n在python中，\"\\n\"和\"\\r\"都是转义字符。也就是说后面的n和r，不是单纯的字母n和字母r的意思了。\"\\n\"代表着回车，光标在下一行；\"\\r\"代表着换行，光标在上一行。\n\n需要注意的是，\\n和\\r同样都有移动光标位置的功能，但是不同的就是在于光标的具体位置不一样。\\n后的光标在下一行开头，\\r后的光标在本行的开头。\n\nprint('你好吗？\\n朋友') \nprint (\"——分隔线——\")\nprint (\"你好吗？\\r朋友\")\n# 返回的结果是\n你好吗？\n朋友\n——分隔线——\n朋友吗？\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n我们可以看到在加了\\n的符号后，显示的是\"你好吗？\"的\"朋友\"进行了换行，光标在另外一行的开头了。而加了\\r的符号后，显示的是\"朋友吗？\"，这是由于将光标移动到了这一行的行首了，那就将\"朋友\"替换了原来的\"你好\"，最后显示的就是\"朋友吗？\"\n\n\n# 如何理解字符串前缀u/b/r\n\n\n# 无前缀 和 u前缀\n\n字符串默认创建即以Unicode编码存储，可以存储中文。\n\nstring = 'a' 等效于 string = u'a'\n\n这里是string 类\n\nUnicode中通常每个字符由2个字节表示\n\n * u'a'即 u'\\u0061' 实际内存中为[0000 0000] [0110 0001]\n\n\n# b前缀\n\nb前缀的意思是，将字符串存储ASCII码，但是ASCII码无法存储中文。\n\n在ASCII码中，每个字符由1个字节表示(8位)。\n\n * b'a'即b'\\x61' 实际内存中为[0110 0001]\n\n如下面的例子中，如果有b前缀，并且后面的字符串带有中文，那么会报错。\n\nresult = b'你好61'\n会报语法错误！\n\n\n1\n2\n\n\n\n# r前缀\n\nr前缀主要解决的是转义字符，特殊字符的问题。里面的转义字符变得无效了。\n\n如果以r开头的，那么说明后面的字符，都是普通的字符了，即如果是\"\\n\"那么表示一个反斜杠字符和一个字母n。而不是表示换行了。\n\n通常以r或R开头的字符，常用于正则表达式，对应着re模块。\n\n正如下面的例子中，带上r或R，能够方便处理过滤反斜杠的问题。\n\nf = open(r'D:\\工作内容\\最近工作\\集团省份上线文档\\山西\\1keepalived.conf','r')\nfor i in f:\n    print(i)\nf.close()\n\n\n1\n2\n3\n4\n\n\n\n# 三个前缀总结\n\n参照下面的例子来看，我们分别以r为前缀，b为前缀，u为前缀。\n\nprint(r'A\\tA')   # 结果为A\\tA    原来是什么就是什么，转义字符无效。\nprint(b'A\\tA')   # 结果为b'A\\tA' 转为ASCII码，字节类型   \nprint(u'A\\tA')   # 结果为A        A，这里\\t转义字符生效为8个空格\n\n\n1\n2\n3\n\n\n\n# 如何理解Python3中的bytes和str类型\n\nstr类型也就是字符串类型，bytes类型也就是字节流类型。\n\npython3最重要的新特性之一就是对字符串和\n\n\n# web开发理解(摘自廖雪峰)\n\n在BS架构下，客户端只需要浏览器，应用程序的逻辑和数据都存储在服务器端。浏览器只需要请求服务器，获取Web页面，并把Web页面展示给用户即可。\n\nWeb页面具有极强的交互性。由于Web页面是用HTML编写的，而HTML具备超强的表现力，并且，服务器端升级后，客户端无需任何部署就可以使用到新的版本。\n\nWeb开发经历了如下几个阶段：\n\n * 静态web页面：由文本编辑器直接编辑并生成静态的HTML页面，如果要修改Web页面的内容，就需要再次编辑HTML源文件，早期的互联网Web页面就是静态的。\n * CGI：由于静态Web页面无法与用户交互，比如用户填写了一个注册表单，静态Web页面就无法处理。要处理用户发来的动态数据，出现了Common Gateway Interface，简称为CGI，用C/C++编写。\n * ASP/JSP/PHP：由于Web应用特点是修改频繁，用C/C++这样的低级语言非常不适合Web开发，而脚本语言由于开发效率高，与HTML结合紧密，因此，迅速取代了CGI模式。ASP是微软退出的用VBScript脚本语言编程的Web开发技术，而JSP用Java来编写脚本，PHP本身则是开源的脚本语言。\n * MVC：为了解决直接用脚本语言嵌入HTML导致的可维护性差的问题，Web应用也引入Model-View-Controller的模式，来简化Web开发。ASP发展为ASP.Net，JSP和PHP也有一大堆MVC框架。\n\n目前，Web开发技术仍在快速发展中，异步开发、新的MVVM前端技术层出不穷。\n\n\n# HTTP协议简介\n\n在Web应用中，服务器把网页传给浏览器，实际上就是把网页的HTML代码发送给浏览器，让浏览器展示出来。\n\n而浏览器和服务器之间的传输协议是HTTP。(讲明一个通信协议的关键是谁与谁之间的通信协议)\n\n# 什么是HTTP协议\n\n引入HTML，HTML是一种用于定义网页的文本，会HTML，就可以编写网页。\n\nHTTP是在网络上传输HTML的协议，用于浏览器和服务器的通信。\n\n# 查看HTTP交互过程\n\n我们这里使用的是谷歌浏览器的开发者工具，去访问wwww.sina.com.cn的网址，去理解我们自己的客户端和服务器端的HTTP协议的交互过程。\n\n# 打开谷歌浏览器的开发者工具\n\nChrome提供了一套完整地调试工具，非常适合Web开发。\n\n选择浏览器右上角的三个点 -> \"更多工具\" -> \"开发者工具\"\n\n或者按F12，进入\"开发者工具\"\n\n\n\n * Elements显示的是网页的结构\n * Network显示的是浏览器和服务器的通信，我们点击\"Network\"的时候，确保第一个小红点亮着，代表chrome浏览器会记录所有浏览器和服务器之间的通信。\n\n# 查看HTTP的请求信息\n\n基于上面已经打开\"开发者模式\"的情况下，我们切换到\"Network\"子菜单中，在浏览器地址栏中输入\"www.sina.com.cn\"。\n\n这个时候，我们通过Network就可以查看到所有的，我们的浏览器作为客户端通过HTTP协议去请求www.sina.com.cn资源的时候，和sina的服务器之间的通信过程。\n\n我们在Network中，定位到第一条记录，\"Headers\" -> \"Request Headers\" -> \"view source\"，意思就是显示服务器返回的原始响应数据。\n\n\n\n重要解析如下：\n\n第一行：GET / HTTP/1.1\n\nGET表示一个读取请求，将从服务器获取网页数据， /表示的是URL的路径，URL总是以/开头，/就表示首页。 HTTP/1.1表示的是采用HTTP协议版本是1.1。HTTP1.1版本和1.0版本的主要区别在于1.1版本允许多个HTTP请求复用一个TCP连接，以加快传输速度。\n\n从第二行，开始都是类似于xxx:yyyy的形式\n\n例如Host: www.sina.com.cn表示的是请求的域名是www.sina.com.cn\n\n# 查看HTTP的返回信息\n\nHTTP响应分为Header和Body两部分(Body是可选项)。\n\n在上面的同样的页面中，选择\"Response Headers\"，点击\"view source\"，同样来显示服务器返回的原始响应数据。\n\n\n\n重要解析如下：\n\n第一行中的200 OK\n\n200表示一个成功的响应码，失败的响应码有404 Not Found:网页不存在，500 Internal Server Error：服务器内部出错，等等。\n\nContent-Type行的内容：\n\nContent-Type表示的是响应的内容，这里是text/html表示HTML页面。需要注意的是浏览器是根据Content-Type来判读响应的内容是网页还是图片，是视频还是音乐。反过来说，浏览器不是靠URL的连接来判断响应的内容，例如URL是http://example.com/abc.jpg，它也不一定是图片。\n\n例如Content-Type: text/html;charset=utf-8表示响应类型是HTML文本，并且编码是UTF-8,Content-Type: image/jpeg表示响应类型是JPEG格式的图片。\n\n如何查看HTTP响应的Body内容：\n\n我们可以在上面的页面上，从\"Headers\"切换到\"Response\"来查看返回的body内容。或者直接在网页上右键\"查看网页源码\"\n\n\n\n# Content-Length的理解\n\n我们在上面的\"Response Headers\"中可以发现有一个\"Content-Length\"的header头信息。这个信息很重要，下面主要从\"Content-Length\"的是什么？为什么(有什么作用)的角度来理解，这个header头参数。\n\nContent-Length的header头告诉了浏览器报文中body主体的大小。单位是字节数。这个大小是包含了内容编码的，比如对文件进行了gzip压缩，Content-Length就是压缩后的大小(这点对我们编写服务器非常重要)。除非使用了分块编码，否则Content-Length首部就是带有实体主体的报文必须使用的。\n\n使用Content-Length首部是为了能够检测出服务器崩溃而导致的报文截尾，并对共享持久连接的多个报文进行正确分段。\n\n 1. 检测截尾\n\nHTTP的早期版本采用了关闭连接的办法来划定报文的结束。但是，没有Content-Length的话，客户端无法区分到底是报文结束时正常的关闭连接还是报文传输中由于服务器崩溃而导致的连接关闭。客户端要通过Content-Length来检测报文截尾。\n\n报文截尾的问题对缓存代理服务来说尤为重要。如果缓存服务器收到截尾的报文却没有识别出截尾的话，它可能会存储不完整的内容并多次使用他来提供服务。缓存代理服务器通常不会为没有显式Content-Length首部的HTTP主体做缓存，以此来减少缓存已截尾报文的风险。\n\n 2. Content-Length与持久连接\n\nContent-Length首部对于持久链接是必不可少的。如果响应通过持久连接传送，就可能有另一条HTTP响应紧随其后。客户端通过Content-Length首部就可以知道报文在何处结束，下一条报文从何处开始。因为连接是持久的，客户端无法依赖连接关闭来判断报文的结束。\n\n 3. 分块编码的介绍\n\n有一种情况下，使用持久连接可以没有Content-Length首部，即采用分块编码(chunked encoding)时。在分块编码的情况下，数据是分为一系列的块来发送的，每一块都有大小说明。哪怕服务器在生成首部的时候不知道整个实体的大小(通常是因为实体是动态生成的)，仍然可以使用分块编码传输若干已知大小的块。\n\n参考如下URL：\n\n> https://my.oschina.net/xishuixixia/blog/93185\n\n# 浏览器读取到HTML源码做了什么\n\n当浏览器读取到新浪首页的HTML源码后，它会解析HTML，显示页面，然后，根据HTML里面的各种链接，再发送HTTP请求给新浪服务器，拿到相应的图片、视频、Flash、JavaScript脚本、CSS等各种资源，最终显示出一个完整的页面。\n\n所以，我们会在\"Network\"下面看到很多额外的HTTP请求。\n\n\n# 总结一下\n\n# HTTP请求流程\n\n 1. 步骤1，浏览器首先向服务器发送HTTP请求，请求包括：\n\n方法：GET还是POST，GET仅请求资源，而POST会附带用户数据；\n\n路径：/full/url/path\n\n域名：由Host头指定，Host: www.sina.com.cn\n\n以及其他相关的Header；\n\n如果是POST，那么请求还应该包括一个Body，包含用户数据。\n\n 2. 步骤2，服务器向浏览器返回HTTP响应，响应包括：\n\n响应代码：200表示成功，3XX表示重定向，4XX表示客户端发送的请求有错误，5XX表示服务器端处理时发生了错误；\n\n响应类型：由Content-Type指定，例如Content-Type: text/html表示响应类型是HTML文本，Content-Type: image/jpeg表示响应类型是JPEG格式的图片；\n\n以及其他相关的Header；\n\n通常服务器的HTTP响应会携带内容，也就是有一个Body，包含响应的内容，网页的HTML源码就在Body中。\n\n 3. 步骤3，如果浏览器还需要继续向服务器请求其他资源，比如图片，就再次发送HTTP请求，重复步骤1和步骤2。\n\n# HTTP协议对服务器开发的理解\n\nWeb采用的HTTP协议中采用了非常简单的请求-响应模式，从而大大简化了开发。当我们编写一个页面的时候，我们只需要在HTTP响应中把HTML发送出去，不需要考虑如何附带图片、视频等，浏览器如果需要请求图片和视频，它会发送另外一个HTTP请求，因此，一个HTTP请求只处理一个资源。\n\nHTTP协议同时具备极强的扩展性，例如虽然浏览器请求的是http://www.sina.com.cn/的首页，但是新浪在HTML中可以链入其他服务器的资源，比如<imgsrc=\"http://i1.sinaimg.cn/xxxxxxxx.png\">，从而将请求压力分散到各个服务器上。并且，一个站点可以链接到其他站点，无数个站点互相链接起来。\n\n# HTTP格式\n\n每个HTTP请求和响应都遵循相同的格式，一个HTTP包含Header和Body两部分，其中Body是可选的。HTTP协议是一种文本协议，所以，它的格式也非常简单。\n\nHTTP GET请求的格式，每个Header一行一个，换行符是\\r\\n。\n\nGET /path HTTP/1.1\nHeader1: Value1\nHeader2: Value2\nHeader4: Value3\n\n\n1\n2\n3\n4\n\n\nHTTP POST请求的格式，每个Header一行一个，换行符是\\r\\n；当遇到两个\\r\\n的时候，Header部分结束，后面的数据全部是Body。\n\nPOST /path HTTP/1.1\nHeader1: Value1\nHeader2: Value2\nHeader4: Value3\n\nbody data goes here ....\n\n\n1\n2\n3\n4\n5\n6\n\n\nHTTP响应的格式，如果响应中包含了body，也是通过\\r\\n\\r\\n来分隔的。需要再次注意的是，body的数据类型由Content-Type头来确定，如果是网页，Body就是文本，如果是图片，Body就是图片的二进制数据。\n\n当存在Content-Encoding时，Body数据是被压缩的，最常见的压缩方式是gzip，所以，看到Content-Encoding: gzip的时候，需要将Body数据先解压缩，才能得到真正的数据。压缩的目的在于减少Body的大小，加快网络传输。\n\n\n# HTML简介\n\n在这个章节，根据廖雪峰老师的文档，总结了HTML的基本格式，大致了解了HTML是定义了页面的内容；CSS是用来控制页面元素的样式；而JavaScript负责页面的交互逻辑。\n\n# HTML的理解\n\n可以理解为网页就是HTML。在网页中不但包含了文字，还有图片、视频、Flash小游戏，有复杂的排版、动画效果，所以，HTML定义了一套语法规则，来告诉浏览器如何把一个丰富多彩的页面展示出来。\n\n一个简单的HTML的格式内容大致如下：\n\n<html>\n<head>\n  <title>Hello</title>\n</head>\n<body>\n  <h1>Hello, world!</h1>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n可以看出，HTML文档就是一系列的Tag组成的，最外层的Tag是<html>，还有<head>...</heand>,<body>...</body>(这里不要和HTTP的Header、Body搞混了)，由于HTML是富文档模型，所以，还有一系列的Tag用来表示链接、图片、表格、表单等等。\n\n# CSS简介\n\nCSS是Cascading Style Sheets(层叠样式表)的简称，CSS用来控制HTML里的所有元素如何展现，比如，给标题元素加上一个样式，变成48号字体，灰色，带阴影：\n\n<html>\n<head>\n  <title>Hello</title>\n  <style>\n    h1 {\n      color: #333333;\n      font-size: 48px;\n      text-shadow: 3px 3px 3px #666666;\n    }\n  </style>\n</head>\n<body>\n  <h1>Hello, world!</h1>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# JavaScript简介\n\nJavaScript，它和Java一点关系没有。JavaScript是为了让HTML具有交互性而作为脚本语言添加的，JavaScript既可以内嵌到HTML中，也可以从外部链接到HTML中。例如，我们希望当用户点击标题的时候把标题变成红色，就必须通过JavaScript来实现：\n\n<html>\n<head>\n  <title>Hello</title>\n  <style>\n    h1 {\n      color: #333333;\n      font-size: 48px;\n      text-shadow: 3px 3px 3px #666666;\n    }\n  </style>\n  <script>\n    function change() {\n      document.getElementsByTagName('h1')[0].style.color = '#ff0000';\n    }\n  <\/script>\n</head>\n<body>\n  <h1 onclick=\"change()\">Hello, world!</h1>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# WSGI接口\n\n# 为什么需要WSGI\n\n在描述为什么的时候，又有几个概念需要讲清楚：web访问基本流程/静态动态页面/WSGI能做些什么\n\n在理解为什么需要WSGI的时候，先要有个web应用访问的基本流程：\n\n 1. 浏览器发送一个HTTP请求;\n\n 2. 服务器收到请求，生产一个HTML文档;\n\n 3. 服务器把HTML文档作为HTTP响应的Body发送给浏览器;\n\n 4. 浏览器收到HTTP响应，从HTTP Body取出HTML文档并显示。\n\n静态页面和动态页面的区别(有两种分类，主要分类是第一种)：\n\n第一种:\n\n静态页面：根据页面语言脚本来区分，把纯HTML+JS脚本成为静态页面，这种页面内容基本固定；\n\n动态页面：把ASP/PHP/JSP/Python 等程序语言写的页面称为动态页面，这种页面基本都会调用数据库，或者通过和用户交互产生变化。\n\n第二种：\n\n静态页面：根据页面的直观表示。把只有文本和静态图片的HTML页面称为静态页面。\n\n动态页面：把包含GIF动画图片和Flash动画以及一些视频音乐等富媒体的HTML页面称为动态页面。\n\n如果只是展示静态的HTML，确实不需要WSGI：\n\n最简单的Web应用就是先把HTML用文件保存好，用一个现成的HTTP服务器软件(Apache/Nginx/Lighttpd)等这些常见的静态服务器，来接受用户请求，从文件中读取HTML。\n\n如果要展示动态的HTML，确实需要WSGI的存在：\n\n要动态生成HTML，那么缺少不了程序语言的存在，而单独靠类似Nginx这种Web服务器无法完成从接受请求到动态生成HTML，返回请求内容的需求。那么这个时候就需要Nginx和Python Web框架之间有个共同的通信协议，这个协议就是WSGI。\n\nWSGI就是为了方便我们为了访问动态HTML的时候，做了接受客户端的HTTP请求，解析HTTP请求，然后发送返回HTTP响应，做了这些工作。而Python web框架只是关注于如何响应生产动态HTML文档就行了。有了这个，我们就可以专心用Python编写Web业务。\n\n# 什么是WSGI\n\nWSGI，也就是Web Server Gateway Interface。是定义在web服务器和web应用的一种接口规范，只适用于Python语言来使用。\n\n# 基于WSGI来定义web app\n\n在WSGI的接口规范中，它只要求Web开发者实现一个函数，就可以响应HTTP请求。例如下面的例子中：\n\ndef application(environ, start_reponse):\n    start_response('200 OK', [('Content-Type', 'text/html')])\n    return [b'<h1>Hello, web!</h1>']\n\n\n1\n2\n3\n\n\n上面的application()函数就是符合WSGI标准的一个HTTP处理函数，它接收两个参数：\n\n * environ：一个包含所有HTTP请求信息的dict对象；\n * start_response：一个发送HTTP响应的函数。(主要是响应码，和响应的header头信息，由web server自己实现的)\n\n在application()函数中，调用了start_response方法，发送了HTTP响应的Header，注意的是Header只能发送一次，也就是只能调用一次start_response()函数。start_response()函数接收两个参数，一个是HTTP响应码，一个是一组list表示的HTTP Header，每个Header用一个包含两个str的tuple表示。\n\n函数的返回值b'<h1>Hello, web!</h1>'，将作为HTTP响应的Body发送给浏览器。\n\n# 思考WSGI标准下的web app开发\n\n有了WSGI，我们关心的就是如何从environ这个dict对象拿到HTTP请求信息，然后构造HTML，通过start_response()发送Header，最后返回Body。\n\n整个application()函数本身没有涉及到任何解析HTTP的部分，也就是说，底层代码不需要我们自己编写，我们只负责在更高层次上考虑如何响应请求就可以了。\n\napplication()函数是由符合WSGI标准的web服务器来调用的。\n\n# 运行WSGI服务\n\n这里使用python内置的一个wsgiref模块，来演示一下，如何整体运行一个wsgi的Web服务。\n\n这里的代码分为hello.py和server.py两个部分，hello.py是我们根据WSGI的规范来书写的app应用函数，上面介绍过，这个app应用程序主要是根据请求的environ字典信息，返回header 头信息和body信息。\n\n而server.py则是负责启动指定端口的wsgi服务器，在这个程序中会导入上面我们书写的app应用函数，作为HTTP的返回header和返回body。\n\n示例1：\n\nhello.py\n\ndef application(environ, start_response):\n    start_response('200 OK', [('Content-Type', 'text/html')])\n    return [b'<h1>Hello, web!</h1>']\n\n\n1\n2\n3\n\n\nserver.py\n\n# server.py\n# 从wsgiref模块导入\nfrom wsgiref.simple_server import make_server\n# 导入我们自己编写的application函数\nfrom hello import application\n\n# 创建一个服务器，IP地址为空，端口是8000，处理函数是application:\nhttpd = make_server('', 8000, application)\nprint('Serving HTTP on port 8000 ...')\n# 开始监听HTTP请求：\nhttpd.serve_forever()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n示例2：\n\nhello.py\n\ndef application(environ, start_response):\n    start_response('200 OK', [('Content-Type', 'text/heml')])\n    body = '<h1>Hello, %s!</h1>' % (environ['PATH_INFO'][1:] or 'web')\n    return [body.encode('utf-8')]\n# 这里environ['PATH_INFO'][1:]\n# 是对environ字典集合中'PATH_INFO'这个key的引用\n# 而[1:]是指'PATH_INFO'这个key对应的value还是一个序列\n# 这个序列是从下标1往后到最后一个元素的所有元素\n# 'PATH_INFO'是访问URL中端口后面的上下文信息\n# 例如http://127.0.0.1:8000/text/tt 中\n# /text/tt是'PATH_INFO'中[0:]\n# [1:0]是text/tt\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nserver.py\n\n# server.py\n# 从wsgiref模块导入\nfrom wsgiref.simple_server import make_server\n# 导入我们自己编写的application函数\nfrom hello import application\n\n# 创建一个服务器，IP地址为空，端口是8000，处理函数是application:\nhttpd = make_server('', 8000, application)\nprint('Serving HTTP on port 8000 ...')\n# 开始监听HTTP请求：\nhttpd.serve_forever()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 使用Web框架\n\n这块的知识点不太好整理，还是从遵循为什么，是什么，怎么样的逻辑的来整理下。\n\n\n# 为什么要有Web框架\n\n一个Web框架的出现，肯定是为了解决我们现实的Web开发中遇到的痛点而存在的。那么我们根据上面学习的WSGT接口的app应用开发来看，遇到的痛点是什么呢？\n\n静态网页(只有web服务器和静态HTML) -> WSGI层(动态网页，处理重复的接受请求，解析请求，回复请求等工作) -> Web框架(基于WSGI进一步抽象，专注于一个函数处理一个URL，而URL到函数映射交给Web框架来做)\n\nWSGI接口的app开发痛点：\n\n上面我们了解到了WSGI的框架的出现，是为了解决动态HTML的生产而出现的，换句话说，是为了解决开发动态网页，解决让WSGI层去处理重复的接收请求，解析请求，回复请求的繁琐的工作，而让app的应用专注于自己业务逻辑的开发而出现的。\n\n如果HTTP请求，需要处理很多的不同的URL(带着不同的上下文)，每个URL可能还是不同的方法(GET/POST/PUT/DELETE)，如果还是用WSGI接口来处理的话，这种不同的URL和不同方法的代码，维护起来非常麻烦，这个时候我们基于WSGI层做一个进一步的抽象，用Web框架来处理这些复杂的映射。\n\n查看下面的例子中(WSGI处理不同的URL，不同的方法)：\n\ndef application(environ, start_response):\n    method = environ['REQUEST_METHOD']\n    path = environ['PATH_INFO']\n    if method=='GET' and path=='/':\n        return handle_home(environ, start_response)\n    if method=='POST' and path=='/signin':\n        return handle_signin(environ, start_response)\n    ...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n这样写下去的代码是无法维护的。原因在于WSGI提供的接口虽然比HTTP接口高级了不少，但是和Web APP的处理函数比(处理函数由于上下文和方法的不同，比较复杂)，还是比较低级，我们需要在WSGI接口上能够进一步的抽象，让我们专注于用一个函数处理一个URL，至于URL到函数的映射，就交给了Web框架来处理了。\n\n\n# 什么是Web框架\n\n简单来说就是对WSGI接口上的进一步抽象，维护代码层面上URL到函数的映射，这样我们写代码的时候就能专注于用一个函数处理一个URL。\n\n\n# 如何使用Web框架\n\n这里以Flask这个Web框架来介绍下如何开发一个web应用程序。\n\n# 准备Flask环境\n\n可以使用pip来安装flask包\n\n$ pip install flask\n\n\n1\n\n\n也可以通过conda或者pycharm来创建一个flask的虚拟环境。\n\n# 规划一个web应用\n\n需要编写一个app.py，处理3个URL，分别是：\n\nGET /：首页，返回HOME;\n\nGET /signin：登录页，显示登录表单；\n\nPOST /signin：处理登录表单，显示登录结果。\n\n同一个URL/signin分别有GET和POST两种请求，映射到两个处理函数中。\n\nFlask是通过Python的装饰器在内部自动地把URL和函数关联起来，所以，书写的代码如下：\n\nfrom flask import Flask\nfrom flask import request\n\napp = Flask(__name__)\n\n\n@app.route('/', methods=['GET', 'POST'])\ndef hello_world():\n    return 'Hello World!'\n\n@app.route('/signin', methods=['GET'])\ndef signin_form():\n    return ''' <form action=\"/signin\" method=\"post\">\n               <p><input name=\"username\"></p>\n               <p><input name=\"password\" type=\"password\"></p>\n               <p><button type=\"submit\">Sign In</button></p>\n               </form>'''\n\n@app.route('/signin', methods=['POST'])\ndef singin():\n    # 需要从request对象的form方法来读取表单内容：\n    if request.form['username']=='admin' and request.form['password']=='password':\n        return '<h3>Hello, admin!</h3>'\n    return '<h3>Bad username or password.</h3>'\n\n\nif __name__ == '__main__':\n    app.run()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n运行这个app.py，Flask自带的Server在端口5000上监听：\n\n当我们在浏览器中输入http://127.0.0.1:5000/signin，会显示登录表单：\n\n输入用户名admin和密码password，登录成功；\n\n输入其他错误的用户名和口令，则登录失败；\n\n\n# 使用模板\n\n按照之前的逻辑，还是先阐述下为什么，是什么，怎么样的。\n\n\n# 为什么需要模板\n\n根据我们的经验，模板的出现，也是为了解决Web开发中出现的一些痛点而存在的。\n\n我们从WSGI层中应用函数的编写，到现在使用Web框架，解决了不同的URL，不同的方法的映射的问题。\n\n但是我们在实际的Web app的开发中，不仅仅是单纯的处理逻辑，还需要展示给用户页面。我们在函数中返回一个包含HTML的字符串，简单的页面还是可以的，但是如果对于大量的HTML文档信息，我们是无法单纯的依靠Python的字符串中正确的书写出来的。\n\nWeb APP最复杂的部分就在HTML页面。HTML不仅要正确，还要通过CSS美化，再加上复杂的JavaScript脚本来实现各种交互和动画效果。所以手动书写生成HTML页面的难度很大。\n\n为了更快，更方便的生成HTML页面，而且还为了更大限度的重用HTML页面，这个时候模板的技术出现了。\n\n\n# 什么是模板技术\n\n使用模板，我们需要预先准备一个HTML文档，这个HTML文档不是普通的HTML，而是嵌入了一些变量和指令，然后，根据我们传入的数据，替换后，得到最终的HTML，发送给用户。\n\n这也就是我们所说的MVC：Model - View - Controller，\"模型-视图-控制器\"。通过MVC，我们在Python代码中处理M：Model和C：Controller，而V：View是通过模板来处理的，这样，我们就可以把Python代码和HTML代码最大限度地分离了。\n\n使用模板的另外一大好处在于，模板改起来很方便，而且，改完保存后，刷新浏览器就能看到最新的效果，这对于调试HTML、CSS和JavaScript的前端工程是太重要了。\n\n控制器：\n\n在Python中处理URL的函数就是C，Controller，Controller负责业务逻辑，比如检查用户名是否存在，取出用户信息等等；\n\n视图：\n\n也就是包含了变量的模板HTML文件，就是View，视图。View负责显示逻辑，通过简单地替换一些变量，View最终输出的就是用户看到的HTML。\n\n模型：\n\n在MVC中的Mode，模型。是用来传给View值的，用来替换View中变量。也就是说从Model中取出相应的数据，去替换View中的变量。一般情况下，是我们请求URL中一个上下文信息，或者是相应的head信息，或body信息中的内容。是存在一个dict字典中的。\n\n处理逻辑如下：\n\n\n\n\n# 如何使用模板\n\n# 安装jinja2\n\nFlask默认支持的模板是jinja2，需要先安装jinja2\n\n$ pip install jinja2\n\n\n1\n\n\n# 准备模板的HTML\n\n所有的模板的HTML文件都需要放在正确的templates目录下，templates和app.py在同级目录下：\n\nhome.html用来显示首页的模板：\n\n<html>\n<head>\n  <title>Home</title>\n</head>\n<body>\n  <h1 style=\"font-style:italic\">Home</h1>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nform.html用来显示登录表单的模板：\n\n<html>\n<head>\n  <title>Please Sign In</title>\n</head>\n<body>\n  {% if message %}\n  <p style=\"color:red\">{{ message }}</p>\n  {% endif %}\n  <form action=\"/signin\" method=\"post\">\n    <legend>Please sign in:</legend>\n    <p><input name=\"username\" placeholder=\"Username\" value=\"{{ username }}\"></p>\n    <p><input name=\"password\" placeholder=\"Password\" type=\"password\"></p>\n    <p><button type=\"submit\">Sign In</button></p>\n  </form>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nsignin-ok.html用来登录成功的模板：\n\n<html>\n<head>\n  <title>Welcome, {{ username }}</title>\n</head>\n<body>\n  <p>Welcome, {{ username }}!</p>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n登录失败的模板，我们用form.html中加入一点条件判断，把这个html重用为登录失败的模板。\n\n\n# Jinja2模板\n\n在jinja2模板中，我们用类似{{name}}表示一个需要替换的变量。很多时候，还需要循环、条件判断等指令语句，在Jinja2中，用{%...%}表示指令。\n\n比如下面的循环输出页码：\n\n{% for i in page_list %}\n    <a href=\"/page/{{i}}\">{{i}}</a>\n{% endfor %}\n\n\n1\n2\n3\n\n\n如果page_list是一个list:[1,2,3,4,5],上面的模板将输出5个超链接。\n\n除了Jinja2，常见的模板还有：\n\nMako: 用<%...%>和${xxx}的一个模板；\n\nCheetah：也是用<%...%>和${xxx}的一个模板；\n\nDjango：Django是一站式框架，内置一个用<%...%>和${xxx}的一个模板。\n\n\n# 如何理解Nginx/WSGI/Flash之间的关系\n\n\n# 概述\n\n这里主要描述的是Nginx，WSGI(或者uWSGI)，Flask(或者Django)，这几者之间的关系。\n\nNginx代表的是Web服务器层，WSGI代表的是WSGI层，Flask代表的是web框架层。\n\n总体来说，客户端从发送一个HTTP请求到Flask处理请求，分别经过了web服务器层，WSGI层，web框架层，这三个层次。不同的层次其作用也不同，下面简要介绍各层的作用。\n\n\n\n\n# Web服务器层\n\n对应的是我们的Nginx，对于传统的客户端-服务器架构，其请求的处理过程是，客户端向服务器发送请求，服务器接收请求并处理请求，然后给客户端返回响应。在这个过程中，服务器的作用是：\n\n 1. 接收请求\n\n 2. 处理请求\n\n 3. 返回响应\n\nweb服务器是一类特殊的服务器，其作用主要是接收HTTP请求并返回响应。常见的web服务器有nginx，Apache，IIS等。\n\n\n# Web框架层\n\nWeb框架的作用主要是方便我们开发web应用程序，HTTP请求的动态数据就是由web框架层来提供的。常见的web框架有Flask，Django等，如下的例子中，以Flask为例，展示了Web框架的作用：\n\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/hello')\ndef hello_world():\n    return 'Hello World!'\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n以上简单的几行代码，就创建了一个web应用程序对象app。app监听机器所有ip的8080端口，接受用户的请求连接。\n\n我们知道，HTTP协议使用URL来定位资源，上面的程序会将路径/hello的请求交由hello_world()方法来处理，hello_world()方法返回'Hello World!'字符串。\n\n对于web框架的使用者来说，他们并不关心如何接收HTTP请求，也不关系如何将请求路由到具体方法处理并将响应结果返回给用户。Web框架的使用者在大部分情况下，只需要关系如何实现业务的逻辑即可。\n\n\n# WSGI层\n\nWSGI不是服务器，也不是用于与程序交互的API，更不是真实的代码，WSGI只是一种接口，它只适用于Python语言，其全称为Web Server Gateway Interface，定义了web服务器和web应用之间的接口规范。\n\n也就是说，只要web服务器和web应用都遵守WSGI协议，那么web服务器和web应用就可以随意的组合。\n\n下面的用例中展示的就是web服务器和web应用基于WSGI的协议，组合在了一起。\n\ndef application(env, start_response):\n    start_response('200 OK', [('Content-Type', 'text/html')])\n    return [b\"Hello World\"]\n\n\n1\n2\n3\n\n\n这个application函数是web应用框架中定义的，这个函数有两个入参，一个参数是env，是一个字典，包含了类似于HTTP_HOST，HOST_USER_AGENT等从web服务器(nginx)中传入的HTTP协议中的request headers；\n\n另外一个参数是start_response，这是一个由web服务器来实现的函数，从start这个字眼可以看出，是正式返回response body体之前的内容，这个方法start_reponse，接受两个参数，分别是status,response_headers，这个status，是http返回中的状态码信息，而response_headers，则是http返回中的头信息。response_headers是一组list表示的HTTP Header，所以外面用了中括号，里面的每个Header用一个包含两个str的tuple元组来表示。这两个入参，都是我们Web应用要自己实现来传入的。\n\napplication函数的作用是，设置http响应的状态码和Content-Type等头部信息，并返回响应的具体结果。\n\nPS：上述代码就是一个完整的WSGI的应用，当一个支持WSGI的web服务器接收到客户端的请求后，便会调用这个application方法。WSGI层并不关心env，start_response这两个变量是如何实现的，就像在application里面做的，直接使用者两个变量即可。\n\n\n# 相关名词解释\n\n * uwsgi\n   \n   同wsgi一样也是一种协议，uWSGI服务器就是使用了uwsgi协议\n\n * uWSGI\n   \n   实现了uwsgi和WSGI两种协议的web服务器。注意uWSGI本质上也是一种web服务器，处于上面描述的三层结构中的web服务器层。\n\n * CGI\n   \n   通用网关接口，并不限于Python语言，定义了web服务器是如何向客户端提供动态的内容。例如，规定了客户端如何将参数传递给web服务器，web服务器如何将参数传递给web应用，web应用如何将它的输出如何发送给客户端，等等。\n   \n   生产环境下的web应用都不实用CGI了，CGI进程(类似Python解释器)针对每个请求创建，用完就抛弃，效率低下。WSGI正是为了替代CGI而出现的。\n\n# 总结\n\n这里总结下WSGI在web服务器和web框架之间的作用：WSGI就像一条纽带，将web服务器与web框架连接起来。\n\n以下面的三者之间的对话来理解：\n\nNginx：Hey，WSGI，我刚收到了一个请求，我需要你作些准备，然后由Flask来处理这个请求。 WSGI：OK，Nginx。我会设置好环境变量，然后将这个请求传递给Flask处理。 Flask：Thanks WSGI！给我一些时间，我将会把请求的响应返回给你。 WSGI：Alright，那我等你。 Flask：Okay，我完成了，这里是请求的响应结果，请求把结果传递给Nginx。 WSGI：Good job！Nginx，这里是响应结果，已经按照要求给你传递回来了。 Nginx：Cool，我收到了，我把响应结果返回给客户端。大家合作愉快！\n\n\n# 参考信息\n\n参考如下的URL信息\n\n> https://blog.csdn.net/lihao21/article/details/52304119\n\n\n# 如何理解import和from import的区别\n\n在python中单单import的话是引入整个包，而from ... import ...是引入了某个包中某个函数或某个类。例子如下：\n\n# 引入了整个datetime包\nimport datetime\nprint(datetime.datetime.now())\n\n# 从datetime包中引入datetime这个类\nfrom datetime import datetime\nprint(datetime.now())\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 定义函数一定要有形参，一定要有返回值吗？\n\n细分是三个问题：函数定义一定要有形参吗；函数定义中一定要有返回值吗；函数调用的时候一定要有实参吗\n\n\n# 在Python中点号.有哪些用法\n\nimport包中\n\n调用类中某个方法的时候\n\n\n# 如何写Python函数注释\n\n需要注意的是，在书写的注释中，不管哪种形式写的，python对这些注释信息，不做检查，不做检查，不做验证！什么也不做！\n\n在python函数中，写注释大体有如下两种方法：\n\n\n# 方法一：使用\"\"\"xxxx\"\"\"的格式\n\n也就是说直接在函数内部，用三个双引号括起来的部分就是需要注释的内容。\n\n类似如下的例子，可以用help(函数名称) 的方法来查看注释信息。\n\ndef square_sum(a,b):\n    \"\"\" \n    return the square\n    sum of two arguments\n    \"\"\"\n    a = a**2\n    b = b**2\n    c = a + b\n    return c\n\nhelp(square_sum)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 谷歌风格的注释\n\ndef foo():\n    \"\"\" \n    this is a group style docs.\n    \n    Parameters:\n        param1 - this is the first param\n        param2 - this is a second param\n    \n    Returns:\n        this is a description of what is returned\n    \n    Raises:\n        KeyError - raises an exception\n    \"\"\"\n\nhelp(foo)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# Rest风格\n\ndef chuck():\n    \"\"\"\n    This is a Rest Style.\n\n    :param param1: this is the first param\n    :param param1: this is a second param\n    :returns: this is a description of what is returned\n    :raises keyError: raises an exception\n    \"\"\"\n\nhelp(chuck)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 方法二：定义参数的时候加入注释\n\n通常会在定义的函数的参数后面加入\":\"冒号，写上一个建议的参数的数据类型。如果很长可以加上单引号。\n\n这些注释信息都是函数的元信息，保存在f.___annotations__字典中。\n\n需要注意的是，python对注释信息和f.__annotations__的一致性，不做检查，不做强制，不做验证！什么都不做。\n\n定义完参数后用->一个参数类型，来表示建议返回的值的类型。\n\ndef f(text:str, max_len:'int>0'=80) ->str:\n    \"\"\"这个是函数的说明文档，help的时候会显示\"\"\"\n    return True\n\n\"\"\"\n函数声明中，text是参数名称，：冒号的str是参数的注释\n如果参数有默认值，还要加上注释，那么就在默认值的\"=\"等号前写上\n如'int>0'就是对默认值max_len=80的注释\n->str是函数返回值的注释\n\"\"\"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 如何写Python类的注释\n\n\n# 如何利用Pyecharts库来画图表\n\n\n# 安装pyecharts包\n\n$ pip install pyecharts\n\n\n1\n\n\n\n# 折线图的示例\n\n下面的例子中，有两条折线，定义了主标题，副标题，加入了工具栏，加入了x轴方向的滚动轴。\n\nimport pyecharts.options as opts\nfrom pyecharts.faker import  Faker\nfrom pyecharts.charts import Line\ndef line_base() -> Line:\n    c = (\n        Line()\n        .add_xaxis(Faker.choose())\n        .add_yaxis(\"商家A\", Faker.values())\n        .add_yaxis(\"商家B\", Faker.values())\n        .set_global_opts(\n            title_opts={\n                \"text\": \"line的测试\",\n                \"subtext\": \"副标题\",\n                },\n            toolbox_opts=opts.ToolboxOpts(),\n            # legend_opts=opts.LegendOpts(is_show=False),\n            datazoom_opts=opts.DataZoomOpts(),\n        )\n    )\n    return c\n\nline_base().render()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n最终显示的样式如下：\n\n\n\n\n# 柱形图示例\n\n下面的图示中，有两种类型的柱形图。加入了工具栏，X轴方向的滚动轴，主副标题。\n\nfrom pyecharts.faker import Faker\nfrom pyecharts import options as opts\nfrom pyecharts.charts import Bar\nfrom pyecharts.commons.utils import JsCode\ndef bar_base() -> Bar:\n    c = (\n        Bar()\n        .add_xaxis(Faker.choose())\n        .add_yaxis(\"商家A\", Faker.values())\n        .add_yaxis(\"商家B\", Faker.values())\n        .set_global_opts(\n            title_opts={\n                \"text\": \"bar的测试\",\n                \"subtext\": \"副标题\",\n                },\n            toolbox_opts=opts.ToolboxOpts(),\n            # legend_opts=opts.LegendOpts(is_show=False),\n            datazoom_opts=opts.DataZoomOpts(),\n        )\n    )\n    return c\n\nbar_base().render()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n最终显示的图示如下：\n\n\n\n\n# conda channel的镜像设置\n\n\n# 显示所有channel\n\n输入conda config --show能够显示出所有的config的信息\n\n输入conda config --show channels 显示的是所有channels信息\n\n\n# 移除镜像源\n\n这里可以移除掉上面 显示出来的镜像源的信息。\n\nconda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\n\n\n# 添加可用的清华镜像源\n\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --set show_channel_urls yes\n\n\n1\n2\n3\n\n\n其中conda config --set show_channel_urls yes 意思是从channel中安装包时显示channel的url，这样就可以知道包的安装来源了。\n\n\n# 关闭安装过程中默认yes\n\n默认使用conda install xxx的时候，是不需要输入yes or no的，conda直接跳过了，默认是yes，如果需要自己加以判断，如下设置。\n\nconda config --set always_yes false\n\n\n# 一些其他的conda指令\n\nconda install <包名> 安装指定包 conda remove <包名> 移除指定包 conda update <包名> 更新指定包\n\n\n# vscode配置 python\n\nhttps://zhuanlan.zhihu.com/p/88458083\n\nhttps://www.jianshu.com/p/ef1ae10ba950\n\nhttps://blog.csdn.net/joson1234567890/article/details/105134711\n\nhttps://blog.csdn.net/qq_38875402/article/details/107141964\n\n\n# python安装psycopg2包\n\n\n# 环境准备\n\n由于python在centos7下连接postgresql数据库报错:\n\npython SCRAM authentication requires libpq version 10 or above\n\n大概意思是libpg的版本低了，但使用 yum install postgresql-devel 只能更新到 9.2.24版本\n\n需要安装高版本的postgresql相关的rpm 包\n\n 1. 添加pg包的源\n    \n    rpm -Uvh https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm\n\n 2. 下载pg包，准备离线安装\n    \n    yumdownloader --destdir ./ --resolve postgresql10 postgresql10-dev postgresql10-libs\n\n 3. 生产环境中安装RPM包\n    \n    rpm -ivh postgresql10*.rpm\n\n 4. 查找pg_config的位置\n    \n    find / -type f -name \"pg_config\"\n\n 5. 先删除/usr/bin下的pg_config\n\n 6. 建立软链接\n    \n    ln -s /usr/pgsql-10/bin/pg_config pg_config\n\n 7. 查看pg_config版本\n    \n    #pg_config --version\n    \n    PostgreSQL 10.18\n    \n    \n    1\n    2\n    3\n    \n\n参考URL：\n\n下载rpm包的两种方法\n\npython SCRAM authentication requires libpq version 10 or above\n\n\n# 源码安装psycopg2\n\n 1. 下载psycopg2\n    \n    pip download -d /usr/local/download/pip/ psycopg2\n\n 2. 离线安装\n    \n    tar -xzvf psycopg2-2.9.3.tar.gz\n    cd psycopg2-2.9.3\n    python setup.py build\n    python setup.py install\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 3. 测试psycopg2\n    \n    python\n    > import psycopg2\n    \n    \n    1\n    2\n    \n\n离线安装psycopg2",normalizedContent:"python知识点fra整理\n\n\n# python基础知识点\n\n\n# 如何进行pycharm安装和激活\n\n这里演示的是windows如何安装专业版的pycharm，并且进行激活。\n\n\n# pycharm专业版和社区版有什么区别\n\n专业版比社区版多了例如：web开发，python web框架，python的探测，远程开发能力，数据库和sql的支持。其他的功能两者都是一样的。\n\n具体的功能区别看如下的图示：\n\n\n\n\n# 安装的具体步骤如下\n\n 1. 下载pycharm的版本软件\n    \n    http://xxxx:7000/file/tools/pycharm-professional-2019.1.2.exe\n\n 2. 选择相应的版本\n    \n    \n\n 3. 选择运行pycharm\n    \n    \n\n 4. 选择先不设置\n    \n    一直选择点击下一步，下一步。\n    \n    \n    \n    \n\n\n\n\n\n\n# 配置agent包\n\n 1. 下载agent包\n    \n    下载agent的jar包到相应的目录下。\n    \n    http://xxxx:7000/file/tools/jetbrains-agent.jar\n\n 2. 打开pycharm配置vm options\n    \n    打开pycharm后，选择\"configure\"或\"help\" --\x3e edit custom vm options\n    \n    # custom pycharm vm options\n    \n    -xms1024m\n    -xmx1024m\n    -xx:reservedcodecachesize=240m\n    -xx:+useconcmarksweepgc\n    -xx:softreflrupolicymspermb=50\n    -ea\n    -dsun.io.usecanoncaches=false\n    -djava.net.preferipv4stack=true\n    -djdk.http.auth.tunneling.disabledschemes=\"\"\n    -xx:+heapdumponoutofmemoryerror\n    -xx:-omitstacktraceinfastthrow\n    -javaagent:c:\\pycharm\\jetbrains-agent.jar\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    \n\n 3. 重启pycharm\n    \n    注意如果启动不来，要注意最后添加的-javaagent信息是否正确，相应的需要打开如下的目录文件进行调整。\n    \n    c:\\users\\administrator\\.pycharm2019.1\\config\\pycharm64.exe.vmoptions\n\n\n# 导入密钥\n\n打开pycharm软件，然后选择\"configure\"--\x3e \"manage license\"\n\n选择\"activation code\"\n\n3agxejxfk9-eyjsawnlbnnlswqioiizqudyrupyrks5iiwibgljzw5zzwvoyw1lijoiahr0chm6ly96aglszs5pbyisimfzc2lnbmvltmftzsi6iiisimfzc2lnbmvlrw1hawwioiiilcjsawnlbnnlumvzdhjpy3rpb24ioiiilcjjagvja0nvbmn1cnjlbnrvc2uiomzhbhnllcjwcm9kdwn0cyi6w3siy29kzsi6ikljiiwizmfsbgjhy2teyxrlijoimja4os0wny0wnyisinbhawrvcfrvijoimja4os0wny0wnyj9lhsiy29kzsi6ikfdiiwizmfsbgjhy2teyxrlijoimja4os0wny0wnyisinbhawrvcfrvijoimja4os0wny0wnyj9lhsiy29kzsi6ikrqtiisimzhbgxiywnrrgf0zsi6ijiwodktmdctmdcilcjwywlkvxbubyi6ijiwodktmdctmdcifsx7imnvzguioijquyisimzhbgxiywnrrgf0zsi6ijiwodktmdctmdcilcjwywlkvxbubyi6ijiwodktmdctmdcifsx7imnvzguioijhtyisimzhbgxiywnrrgf0zsi6ijiwodktmdctmdcilcjwywlkvxbubyi6ijiwodktmdctmdcifsx7imnvzguioijetsisimzhbgxiywnrrgf0zsi6ijiwodktmdctmdcilcjwywlkvxbubyi6ijiwodktmdctmdcifsx7imnvzguioijdtcisimzhbgxiywnrrgf0zsi6ijiwodktmdctmdcilcjwywlkvxbubyi6ijiwodktmdctmdcifsx7imnvzguioijsuzailcjmywxsymfja0rhdguioiiymdg5lta3lta3iiwicgfpzfvwvg8ioiiymdg5lta3lta3in0seyjjb2rlijoiukmilcjmywxsymfja0rhdguioiiymdg5lta3lta3iiwicgfpzfvwvg8ioiiymdg5lta3lta3in0seyjjb2rlijoiukqilcjmywxsymfja0rhdguioiiymdg5lta3lta3iiwicgfpzfvwvg8ioiiymdg5lta3lta3in0seyjjb2rlijoiuemilcjmywxsymfja0rhdguioiiymdg5lta3lta3iiwicgfpzfvwvg8ioiiymdg5lta3lta3in0seyjjb2rlijoiuk0ilcjmywxsymfja0rhdguioiiymdg5lta3lta3iiwicgfpzfvwvg8ioiiymdg5lta3lta3in0seyjjb2rlijoiv1milcjmywxsymfja0rhdguioiiymdg5lta3lta3iiwicgfpzfvwvg8ioiiymdg5lta3lta3in0seyjjb2rlijoireiilcjmywxsymfja0rhdguioiiymdg5lta3lta3iiwicgfpzfvwvg8ioiiymdg5lta3lta3in0seyjjb2rlijoiremilcjmywxsymfja0rhdguioiiymdg5lta3lta3iiwicgfpzfvwvg8ioiiymdg5lta3lta3in0seyjjb2rlijoiulnviiwizmfsbgjhy2teyxrlijoimja4os0wny0wnyisinbhawrvcfrvijoimja4os0wny0wnyj9xswiagfzaci6ijeynzk2odc3lzailcjncmfjzvblcmlvzerhexmiojcsimf1dg9qcm9sb25nyxrlzci6zmfsc2usimlzqxv0b1byb2xvbmdhdgvkijpmywxzzx0=-wgths6xpdhr+uumvbwqpodlxwnqwgngal4ernlpgkapeekjyyvneupwbsrqkpmvpim/8sab6hv04dw3izkjt0ytc29spexbf69+7y6jv718faju4mwfsak/zgtniuoczuq0igkknsssfq/3uomv0q/yjcfvj+me5zd/gfaisccmuagjb/lwippepzbldtvjbrexb1malrlceodv3ujcpaz7xwb54dizwjyhqvq+cvpnnf2jetku7lbm5v+bodsderq7ybt9anlukpr2dahcaz4gctphzxhg96iykx232jyq9jqrfdbqmtvr3e+gscekmewsd//dlt+huzdc1saiyrw==-miieltccan2gawibagibctanbgkqhkig9w0baqsfadaymrywfaydvqqdda1kzxrqcm9mawxlienbmb4xdte4mtewmteymjk0nloxdtiwmtewmjeymjk0nlowadelmakga1uebhmcq1oxdjambgnvbagmbu51c2xlmq8wdqydvqqhdazqcmfndwuxgtaxbgnvbaomeepldejyywlucybzlniuby4xhtabbgnvbammfhbyb2qzes1mcm9tltiwmtgxmtaxmiibijanbgkqhkig9w0baqefaaocaq8amiibcgkcaqea5ndaik1gd0nytdqkzgurqzgw+rgxcdbitpxiwpjhhad0sxga4xszbeboipdy6xv6pofujeyfi9dxsy4mmt0d+skost3rsw96xaf9fxpvojn4prmtdj3ji3cyqrgwequ2nzyqfrp1qynlabavihrkujryhi6gcvqcbje0lq8qquivma9wg/pqwscpnmtf9kp2iej+z5ouxf33zzm+vg/nyv31hlf7fjuapli/1nm+zg8k+axwgykchtknl3sw9pcqa3a3impl9gvtounxc0wcutil8mqvewcsqchyxsiauajwlpfzoo2ahk4mfybsstaqejoxrtuj17mo8q6m2shocwidaqabo4gzmigwmakga1udewqcmaawhqydvr0obbyefgepg9ozgcflmgnbky7sghimggtcmegga1udiwrbmd+afkoetkhnqhi2qb1t4lm0ofkll/gzorykgjaymrywfaydvqqdda1kzxrqcm9mawxlienbggka0myxg7kdeeewewydvr0lbawwcgyikwybbquhawewcwydvr0pbaqdagwgma0gcsqgsib3dqebcwuaa4icaqbonmu8oa3vmnaa4rqp8gpglx3sqaa3wcruaj6zrlk8aeskv1yskh5d2l+yuk6njysgzfr1bir5xf8eup5xxc4/g7ntvyrsmvrd6rfqchoyk5ufjlm+8utmymidrzozlqutst8nxfpbcvcfv5wnru4rchrcuaryvgakbmp9ymkw1pu6+hoo5i2wu3iktmrv8irjrlsstynzxpnptwt7bja19ousk56r40smlmc04gddherr0ei2ubjuua5kw71qn9g02tl9feri2ssrjqrvpbn9inwrwl5+k05mlkekbtbu2ev2wojfzk4wexad/gaadezzdumv8t2iddfl7cairjwcrbfpawpexr52oktpnxfi0l5+g9gnt/wfixcrpelx6yctr6il3gc2vr4jtz6yatt4ntz59/thot7njqhr6aylkhhjcdkze2cob/kouvp4ivv7q3fc6hx7eephaaf/dpxwgorg9smx6coxlgfp0b1ru2u/tunid04rpnxtmuettrt8wsskqvajd3rh8r7cnrj6y2hltkja82hlpdurdxdtrvv+krbwmr26sb/40bjpmurdrcekuibahc0dcou/4+ze1l94wvuhdkcfl0gpjrmscdek+xeuru18hb7wt+thxbkdl6vpfdhsrvqanhr2g4b+qzgidmuky5nuzvfeazqv/g==\n\n\n1\n\n\n如果激活窗口一直弹出(error 1653219),需要在hosts文件里面移除jetbrains相关的信息。\n\n\n# 参考url信息\n\n具体参考如下的url信息\n\nhttps://www.52pojie.cn/thread-961836-1-1.html\n\nhttps://www.sdbeta.com/wg/2019/0509/229677.html\n\n\n# 如何理解多进程与多线程\n\n\n# fork()系统调用\n\nfork系统调用用于从已存在进程中创建一个新进程，新进程称为子进程，而原进程称为父进程。fork调用一次，返回两次，这两个返回分别带回它们各自的返回值，其中在父进程中的返回值是子进程的进程号，而子进程中的返回值则返回0。因此，可以通过返回值来判定该进程是父进程还是子进程。\n\n使用fork函数得到的子进程是父进程的一个复制品，它从父进程处继承了整个进程的地址空间，包括进程上下文、进程堆栈、内存信息、打开的文件描述符、信号控制设定、进程优先级、进程组号、当前工作目录、根目录、资源限制、控制终端等，而子进程所独有的只有它的进程号、计时器等。因此可以看出，使用fork系统调用的代价是很大的，它复制了父进程中的数据段和堆栈段里的绝大部分内容，使得fork系统调用的执行速度并不是很快。\n\n> 参考如下url:\n> \n> https://blog.csdn.net/guoping16/article/details/6580006\n\n\n# 单进程\n\n在下面的例子中，我们看到的是程序中的代码是按顺序一点点的往下执行，即使是两个不相关的下载任务，也需要先等待一个文件下载完成后才能开始下一个下载任务。\n\nfrom random import randint\nfrom time import time,sleep\n\ndef download_task(filename):\n    print('开始下载%...' % filename)\n    time_to_download = randint(5, 10)\n    sleep(time_to_download)\n    print('%下载完成！耗费%d秒' % (filename, time_to_download))\n    \ndef main():\n    start = time()\n    download_task('python从入门到住院.pdf')\n    download_task('peking hot.avi')\n    end = time()\n    print('总共耗费%.2f秒.' % (end - start))\n\nif __name__ == '__main__':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 多进程\n\n# 概念\n\nunix和linux操作系统上提供了fork()系统调用来创建进程。python的os模块提供了fork()函数。由于windows系统没有fork()调用，因此要实现跨平台的多进程编程，可以使用multiprocessing模块的porcess类来创建子进程，而且该模块还提供了更高级的封装，例如批量启动进程的进程池(pool)、用于进程间通信的队列(queue)和管道(pipe)等。\n\n# 多进程示例\n\n在下面的代码中，我们通过process类创建了进程对象，通过target参数我们传入一个函数来表示进程启动后要执行的代码，后面的args是一个元组，它代表了传递给函数的参数。'python从入门到住院.pdf',代表这个元组中只有一个元素。\n\nfrom multiprocessing import process\nfrom os import getpid\nfrom random import randint\nfrom time import time, sleep\n\ndef download_task(filename):\n    print('启动下载进程,进程号[%d].' % getpid())\n    print('开始下载%s...' % filename)\n    time_to_download =  randint(5, 10)\n    sleep(time_to_download)\n    print('%s下载完成!耗时了%d秒' % (filename, time_to_download))\n\ndef main():\n    start = time()\n    p1 = process(target=download_task, args=('python从入门到住院.pdf',))\n    p1.start()\n    p2 = process(target=download_task, args=('peking hot.avi',))\n    p2.start()\n    p1.join()\n    p2.join()\n    end = time()\n    print('总共耗费了%.2f秒.' % (end - start))\n\nif __name__ == '__main__':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# join()方法有什么作用\n\n该方法的意思是指，阻塞当前进程，直到调用join方法的那个进程执行完，再继续执行当前进程。\n\n默认的格式可以为:join([timeout])，该方法内中括号的timeout为一个时间单位为秒的值，中括号为可选参数。如果指定时间到点，就释放阻塞。\n\n如果我们在上面注释掉了p1.join()和p2.join()，那么可以看到程序的运行会不等待两个进程执行完毕就执行后面的end = time()和print。这个不符合我们的预期的。\n\n# run()方法有什么作用\n\nrun()方法是指，如果我们在创建process对象的时候不指定target，那么就会默认执行process的run方法。如果我们不改写run方法，那么执行的时候是没有任何效果的，因为默认的run方法是判断如果不指定target，那就什么都不做。例如下面的示例：\n\nfrom multiprocessing import process\nimport os, time, random\n\ndef r():\n    print 'run method'\n    \nif __name__ == \"__main__\":\n        print \"main process run...\"\n        #没有指定process的targt\n        p1 = process()\n        p2 = process()\n        #如果在创建process时不指定target，那么执行时没有任何效果。因为默认的run方法是判断如果不指定target，那就什么都不做\n        #所以这里手动改变了run方法\n        p1.run = r\n        p2.run = r\n        \n        p1.start()\n        p2.start()\n        p1.join()\n        p2.join()\n        print \"main process runned all lines...\"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n# 为什么要依次调用start再调用join\n\n在多进程中，为什么要先依次调用start再调用join，而不是start完了就调用join呢？\n\n例如下面的例子中，如果是p1.start()完后，立即执行p1.join()，随后才是p2.start()呢？结果我们会发现，p2的进程会一直等待p1的进程完成后再去执行。join是用来阻塞当前线程的。\n\np1.start()\np2.start()\np1.join()\np2.join()\n#改为\np1.start()\np1.join()\np2.start()\np2.join()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 参考如下url：\n> \n> https://www.cnblogs.com/lipijin/p/3709903.html\n\n\n# 多线程\n\n目前的多线程开发，我们使用threading模块，该模块对多线程编程提供了更好的面向对象的封装。\n\n# 使用thread类创建线程\n\n我们可以直接使用threading模块中的thread类来创建线程对象，整体的流程和多进程很类似。\n\nfrom random import randint\nfrom threading import thread\nfrom time import time, sleep\n\ndef download(filename):\n    print(\"开始下载%s...\" % filename)\n    time_to_download = randint(5, 10)\n    sleep(time_to_download)\n    print(\"%s下载完成! 耗费了%d秒\" % (filename, time_to_download))\n\ndef main():\n    start = time()\n    t1 = thread(target=download, args=(\"python从入门到住院.pdf\",))\n    t1.start()\n    t2 = thread(target=download, args=(\"peking hot.avi\",))\n    t2.start()\n    t1.join()\n    t2.join()\n    end = time()\n    print(\"总共耗费了%.3f秒\" % (end - start))\n\nif __name__ == '__main__':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n# 继承thread类自定义线程类\n\n我们通过继承thread类的方式来创建自定义的线程类，然后再创建线程对象并启动线程。这个时候同样，需要自己定义一个run方法。\n\nfrom random import randint\nfrom threading import thread\nfrom time import time, sleep\n\nclass downloadtask(thread):\n    def __init__(self, filename):\n        super().__init__()\n        self._filename = filename\n    \n    def run(self):\n        print('开始下载%s...' % self._filename)\n        time_to_download = randint(5, 10)\n        sleep(time_to_download)\n        print('%s下载完成！耗费了%d秒' % (self._filename, time_to_download))\n\ndef main():\n    start = time()\n    t1 = downloadtask('python从入门到住院.pdf')\n    t1.start()\n    t2 = downloadtask('peking hot.avi')\n    t2.start()\n    t1.join()\n    t2.join()\n    end = time()\n    print('总共耗费了%.2f秒.' % (end - start))\n\nif __name__ == '__main__':\n    main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n\n# 如何实现python数据库访问\n\n\n# 数据库编程接口\n\n为了对数据库进行统一的操作，大多数的语言都提供了简单的、标准化的数据库接口(api)。在python database api 2.0规范中，定义了python数据库api接口的各个部分，如模块接口、连接对象、游标对象、类型对象和构造器、db api的可选扩展以及可选的错误处理机制。\n\n# 连接对象\n\n# 什么是数据库连接对象\n\n数据库的连接对象主要提供获取数据库游标对象和提交、回滚事务的方法，以及关闭数据库连接。\n\n# 为什么需要数据库连接对象\n\n我的理解是创建了数据库的连接对象，实际上是已经创建了一个数据库的连接，我们可以基于这个连接，进行数据库的各种操作。\n\n# 如何获取连接对象\n\n如果是对mysql进行操作的话，我们可以使用pymysql模块中的connect()函数，来获取连接对象。该函数有多个参数，分别可以用来设置主机、ip、端口、用户、密码、数据库名称等信息。\n\nimport pymysql\nconn = pymysql.connect(host='localhost', user='user', password='passwd', db='test', charset='utf8', cursorclass='pymysql.cursors.dictcursor')\n\n\n1\n2\n\n\n# 连接对象的方法\n\nconnect()函数返回连接对象，这个对象表示目前和数据库的会话，连接对象支持的方法如下：\n\n方法名          说明\nclose()      关闭数据库连接\ncommit()     提交事务\nrollback()   回滚事务\ncursor()     获取游标对象，操作数据库，如执行dml操作，调用存储过程等\n\n# 游标对象\n\n是什么？\n\n游标对象代表了数据库中的游标，用于指示抓取数据操作的上下文，主要提供执行sql语句、调用存储过程、获取查询结果等方法。\n\n如何获取游标对象？\n\n通过使用连接对象的cursor()方法，可以获取到游标对象。游标对象的属性有两个：\n\n * description： 数据库列类型和值的描述信息\n * rowcount：回返结果的行数统计信息，如select、update、callproc等。\n\n游标对象的方法如下表：\n\n方法名                                     说明\ncallproc(procname, [,parameters])       调用存储过程，需要数据库支持\nclose()                                 关闭当前游标\nexecute(operation[,parameters])         执行数据库操作，sql语句或数据库命令\nexecutemany(operation, seq_of_params)   用于批量操作，如批量更新\nfetchone()                              获取查询结果集中的下一条记录\nfetchmany(size)                         获取指定数量的记录\nfetchall()                              获取结果集的所有记录\nnextset()                               跳至下一个可用的结果集\narraysize                               指定使用fetchmany()获取的行数，默认为1\nsetinputsizes(sizes)                    设置在调用execute*()方法时分配的内存区域大小\nsetoutputsize(sizes)                    设置列缓冲区大小，对大数据列(如longs和blobs)尤其有用\n\n\n# 访问mysql\n\n# 安装pymysql模块\n\npip install pymysql\n\n\n1\n\n\n# 执行创建表\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法创建一个游标对象cursor\ncursor = db.cursor()\n# 使用execute()方法执行sql，如果表存在则删除\ncursor.execute(\"drop table if exists books\")\n# 使用预处理语句创建表\nsql = \"\"\"\ncreate table books (\n  id int(8) not null auto_increment,\n  name varchar(50) not null,\n  category varchar(50) not null,\n  price decimal(10,2) default null,\n  publish_time date default null,\n  primary key(id)\n) engine=myisam auto_increment=1 default charset=uft8;\n\"\"\"\n# 执行sql语句\ncursor.execute(sql)\n# 关闭数据库连接\ndb.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n# 查询语句\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\n# 使用execute()方法执行sql查询\ncursor.execute(\"select * from books\")\n\n# 使用fetchone()方法获取单条数据\n# 返回的是一个元组\ndata1 = cursor.fetchone()\nprint(data1)\n\n# 使用fetchmany(x)方法查询多条数据\n# fetchmany()方法传递一个参数，其值为2，默认为1\n# 返回的是一个列表，列表中包含2个元组\ndata2 = cursor.fetchmany(2)\nprint(data2)\n\n# 使用fetchall()方法查询所有数据\n# 返回的也是一个列表\ndata3 = cursor.fetchall()\nprint(data3)\n\n# 带条件的查询，使用问好作为占位符号\n# 然后使用一个元组来替换问号\n# 注意，不要忽略元组中最后的逗号\ncursor.execute('select * from user where id >?',(1,))\ndata4 = cursor.fetchall()\nprint(data4)\n\n# 关闭游标\ncursor.close()\n# 关闭connection\nconn.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n# 执行插入\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\n# 数据列表\ndata = [(\"学python\", 'python', '79', '2018-5-20'),\n        (\"学java\", 'java', '100', '2018-6-18'),\n        (\"学php\", 'php', '80', '2019-3-11'),\n        (\"学docker\", 'docker', '200', '2018-12-03')]\ntry:\n    # 执行sql语句，插入多条数据\n    cursor.executemany(\"insert into books(name, category, price, publish_time) values (%s, %s, %s, %s)\", data)\n    # 提交数据\n    db.commit()\nexcept:\n    # 发生错误时回滚\n    db.rollback()\n# 关闭数据库连接\ndb.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 修改表数据\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\n# 执行update\ncursor.execute('update user set name = ? where id = ?',('mr',1))\ncursor.execute('select * from user')\ndata = cursor.fetchall()\nprint(data)\n# 关闭游标\ncursor.close()\n# 关闭connection\nconn.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 删除表数据\n\nimport pymysql\n# 打开数据库连接\ndb = pymysql.connect(\"localhost\", \"root\", \"root\", \"mrsoft\")\n# 使用cursor()方法获取操作游标\ncursor = db.cursor()\ncursor.execute('delete from user where id = ?',(1,))\ncursor.execute('select * from user')\ndata = cursor.fetchall()\nprint(data)\n# 关闭游标\ncursor.close()\n# 提交事务\nconn.commit()\n# 关闭connection\nconn.close()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 为什么pip安准包会报错\n\n\n# 问题描述\n\n在windows中使用pip install xxxx的方式来安装第三方插件的时候，出现了如下的报错：\n\n\n\n碰到这个问题，主要的原因是网站pypi.python.org在国内是被墙了的，在安装程序的时候，无法从python官网下载而导致的。\n\n\n# 解决办法一\n\n在c盘，进入%user%/pip目录，就是在当前的用户下找到pip文件夹，没有就自己创建一个。在pip文件夹下创建pip.ini文件，修改pip.ini内容如下：\n\n[global] \nindex-url=http://pypi.douban.com/simple/\n[install]\ntrusted-host=pypi.douban.com\n\n\n1\n2\n3\n4\n\n\n随后，重新使用pip install xxx即可。\n\n\n# 解决办法二\n\n可以临时解决这个报错的问题，使用如下的命令进行pip安装\n\npip install 插件名 -i 国内镜像地址 http://pypi.douban.com/simple --trusted-host pypi.douban.com\n\n\n1\n\n\n可以把豆瓣镜像地址换成如下的镜像地址\n\nhttp://mirrors.aliyun.com/pypi/simple/ 阿里云\nhttps://pypi.mirrors.ustc.edu.cn/simple/  中国科技大学\nhttp://pypi.douban.com/simple/  豆瓣\nhttps://pypi.tuna.tsinghua.edu.cn/simple/ 清华大学\nhttp://pypi.mirrors.ustc.edu.cn/simple/ 中国科学技术大学\n\n\n1\n2\n3\n4\n5\n\n\n\n# 如何进行pip离线安装模块\n\n\n# 下载第三方包\n\n下载第三方包，有两种方式\n\n方法一：下载单个包\n\n$ pip download -d /usr/local/download/pip/ jieba\n\n\n1\n\n\n方法二：下载多个包，并指定版本\n\n编写requirement.txt，可参照如下格式：\n\nalembic==1.0.0            # via flask-migrate\namqp==2.3.2               # via kombu\nasn1crypto==0.24.0        # via cryptography\nbabel==2.6.0              # via flask-babel\nbilliard==3.5.0.4         # via celery\nbleach==3.0.2\ncelery==4.2.0\ncertifi==2018.8.24        # via requests\ncffi==1.11.5              # via cryptography\nchardet==3.0.4            # via requests\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n执行下面的命令进行下载:\n\npip download -d /usr/local/download/pip/ -r requirement.txt\n\n\n1\n\n\n\n# 离线安装\n\n方法一：单独安装一个模块\n\npip install --no-index --find-links=/usr/local/download/pip/ jieba \n\n\n1\n\n\n方法二：批量安装定义需要安装的模块\n\npip install --no-index --find-links=/usr/local/download/pip/ -r requirement.txt\n\n\n1\n\n\n\n# 如何实现python中request模块中post请求\n\n利用requeset模块中post方法，来进行post请求的发送。\n\n下面的案例是发起post请求，带参数，带请求头。\n\n#! /usr/bin/env python\n# -*- coding: utf-8 -*-\nimport requests\nimport json\nurl = 'http://official-account/app/messages/group'\nbody = {\"type\": \"text\", \"content\": \"测试文本\", \"tag_id\": \"20717\"}\nheaders = {'content-type': \"application/json\", 'authorization': 'app appid = 4abf1a,token = 9480295ab2e2eddb8'}\n\nprint type(body)\nprint type(json.dumps(body))\n# 这里有个细节，如果body需要json形式的话，需要做处理\n# 可以是data = json.dumps(body)\nresponse = requests.post(url, data = json.dumps(body), headers = headers)\n# 也可以直接将data字段换成json字段，2.4.3版本之后支持\n# response  = requests.post(url, json = body, headers = headers)\n\n# 返回信息\nprint response.text\n# 返回响应头\nprint response.status_code\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# form形式发送post请求\n\nrequest支持以form表单形式发送post请求，只需要将请求的参数构造成一个字典，然后传给requests.post()的data参数即可。\n\nimport requests\n\nurl = 'http://httpbin.org/post'\nd = {'key1': 'value1', 'key2': 'value2'}\nr = requests.post(url, data=d)\nprint r.text\n\n\n1\n2\n3\n4\n5\n6\n\n\n我们可以看到，请求头中的content-type字段已经设置为application/x-www-form-urlencoded，且d = {'key1': 'value1', 'key2': 'value2'}以form表单的形式提交到服务端，服务端返回的form字段即是提交的数据。\n\n\n# json形式发送post请求\n\n也就是将一个json串传给requests.post()的data参数。\n\nurl = 'http://httpbin.org/post'\ns = json.dumps({'key1': 'value1', 'key2': 'value2'})\nr = requests.post(url, data=s)\nprint r.text\n\n\n1\n2\n3\n4\n\n\n可以看到，请求的content-type设置为application/json，并将这个json串提交到了服务端中。\n\n\n# multipart形式发送post请求\n\nrequests也支持以multipart形式发送post请求，只需将一文件传给requests.post()的file参数即可。\n\nurl = 'http://httpbin.org/post'\nfiles = {'file': open('report.txt', 'rb')}\nr = requests.post(url, files=files)\nprint r.text\n\n\n1\n2\n3\n4\n\n\n\n# 如何理解字典和列表的嵌套\n\n\n# 什么是序列\n\n序列是有顺序的数据集合，就好像一列排好对的士兵。\n\n序列包含的一个数据被称为序列的一个元素。\n\n序列可以分为两种类型，元组(tuple)和列表(list)\n\n\n# 什么是元组\n\n定义：是一组不可变更的元素，是不可变的序列，也是一种可以存储各种数据类型的集合，用小括号()表示元组的开始和结束，元素之间用逗号,分隔。\n\n这里的不可变，包括不能对元组对象进行增加元素、变换元素位置、修改元素、删除元素操作。元组中的每个元素提供对应的一个下标，下标从0开始，0、1、2...按顺序标注。\n\n值得注意的是：多个元素可以是不同的类型，可以是整型，可以是浮点型，也可以是字符序，布尔型。用小括号来定义。\n\n元组的定义如下：\n\nexample_tuple = (2, \"love\", 1.3 , false)\n\n\n1\n\n\n元组的引用如下：引用的格式是，引用名+中括号，中括号里面是从0开始的数字，代表第一个元素。\n\nexample_tuple[0]\nexample_tuple[0]  =  \"12121212\"\n\n\n1\n2\n\n\n\n# 什么是列表\n\n列表是可变个序列，也是一种可以存储各种数据类型的集合，用中括号[]表示列表的开始和结束，元素之间用逗号,分隔。列表中每个元素提供一个对应的下标。\n\n\n# 元组和列表有什么区别和联系\n\n元组和列表都是用来存储多个元素的容器。\n\n不同的地方在于，元组的各个元素不可再变更，而列表元素可以变更。\n\n\n# 什么是字典\n\n词典也是一个可以容纳多个元素的容器。\n\n词典包含多个元素，每个元素以逗号分隔。\n\n词典的元素包含两个部分，键(key)和值(value)。键是数据的索引，值是数据本身。\n\n词典的元素可以通过键来引用。\n\n字典的定义：\n\nexample_dict = {\"tom\":11, \"sam\":57, \"lily\":100}\n\n\n1\n\n\n字典的引用：\n\nexample_dict[\"tom\"]\nexample_dict[\"tom\"] = 30\n\n\n1\n2\n\n\n\n# 字典嵌套字典\n\n是指在一个字典的内部再嵌套一个字典。{}表示是一个字典，jiangsu和anhui是里面的两个字典，jiangsu里面有嵌套了多个字典。\n\n不太清楚什么时候用{}还是[]的时候，用type来判断确认。\n\n用{}还是[]关键在于，里面的内容是啥，是key/value的形式值，还是单纯的是一个列表。\n\ndict5 = {\n    'jiangsu': {\n        'nanjing': '025',\n        'wuxi': '0510',\n        'xuzhou': '0516',\n        'changzhou': '0519'\n    },\n    'anhui': {\n        'hefei': '0551',\n        'wuhu': '0553',\n        'buben': '0552'\n    }\n}\nprint(type(dict5))\nprint(dict5['jiangsu'])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 字典嵌套列表\n\n是指在字典中嵌套了一个列表。在下面的例子中，jiangsu和anhui是两个字典，jiangsu里面又嵌套了多个字典，其中nanjing这个字典里面嵌套了一个列表。\n\n不太清楚什么时候使用{}和[]的时候，用type来判断。\n\n用{}还是[]关键在于，里面的内容是啥，是key/value的形式值，还是单纯的是一个列表。\n\ndict6 = {\n    'jiangsu': {\n        'nanjing': ['025','111111','222222'],\n        'wuxi': '0510',\n        'xuzhou': '0516',\n        'changzhou': '0519'\n    },\n    'anhui': {\n        'hefei': '0551',\n        'wuhu': '0553',\n        'buben': '0552'\n    }\n}\nprint(type(dict6))\nprint(dict6['jiangsu'])\nprint(dict6['jiangsu']['nanjing'])\nprint(dict6['jiangsu']['nanjing'][1])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 列表嵌套字典\n\n下面的例子中是，在dict7是一个列表，这个列表中有两个元素，jiangsu那个元素是一个大的字典，在这个字典里面，nanjing里面又嵌套了多个列表的元素。\n\ndict7 = [\n    {\n    'jiangsu': {\n        'nanjing': ['025','111111','222222'],\n        'wuxi': '0510',\n        'xuzhou': '0516',\n        'changzhou': '0519'\n    }},\n    {'anhui': {\n        'hefei': '0551',\n        'wuhu': '0553',\n        'buben': '0552'\n    }}\n]\nprint(type(dict7))\nprint(dict7[0])\nprint(dict7[1])\nprint(dict7[1]['anhui']['hefei'])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 列表嵌套列表\n\n在下面的例子中，dict8是一个列表。这个列表里面有两个元素，jiangsu这个元素是一个字典，后面的一个元素是一个列表。[{'hefei': '0551'}],[{'wuhu': '0553'}],[{'buben': '0552'}] 里面的每一个元素是一个key/value的字典。\n\ndict8 = [\n    {\n    'jiangsu': {\n        'nanjing': ['025','111111','222222'],\n        'wuxi': '0510',\n        'xuzhou': '0516',\n        'changzhou': '0519'\n    }},\n    [[{'hefei': '0551'}],[{'wuhu': '0553'}],[{'buben': '0552'}]]\n]\nprint(type(dict8))\nprint((dict8)[1])\nprint(type((dict8)[1][0]))\nprint((dict8)[1][0][0]['hefei'])\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 什么时候使用()号，什么时候使用[]号，什么时候使用{}号\n\n\n# 在python中使用()小括号有如下两种场景：\n\n 1. 定义一个元组tuple的时候\n    \n    #一个元组，括号\n    #注意元组的各个元素定义完后不可再更改了\n    #也就是说后面不可能再出现对元组某个元素赋值的操作了\n    example_tuple = (2, 1.3, \"love\", 5.6, false)\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 函数定义中表示参数/调用函数时参数传递(和类中方法一个意思)\n    \n    # 定义函数\n    def square_sum(a,b):\n        a = a**2\n        b = b**2\n        c = a + b\n        return c\n    # 调用函数\n    a = 5\n    b = 6\n    x = square_sum(a, b)\n    print(x)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n\n\n# 在python中使用[]中括号有如下两种情形：\n\n 1. 定义一个列表(list)\n    \n    在序列中，我们可以通过下面的方式来定义一个列表，主要注意的是定义的时候，变量名和后面[]中间有个=号来连接的。\n    \n    example_list = [true, 5, \"smile\"]             \n    #一个列表，中括号\n    \n    \n    1\n    2\n    \n\n 2. 表示对某个或某些元素的引用\n    \n    需要注意的是，引用元素的时候，中间是没有=号的。但是如何对字典的某个元素进行赋值的时候，是可以有=号的。\n    \n    * 元组中对元素的引用\n      \n      #显示序列example_tuple元组中第一个元素\n      example_tuple[0]\n      \n      \n      1\n      2\n      \n    \n    * 列表中对元素的引用\n      \n      #显示的是第二个元素中，\n      #第二个元素是一个嵌套的列表，该嵌套列表中的第三个元素\n      nest_list[1][2]\n      \n      \n      1\n      2\n      3\n      \n    \n    * 元组或列表中多个元素的引用\n      \n      序列中可以通过范围引用，来找到多个元素。范围引用的基本样式是：序列名[下限:上限:步长]\n      \n      下限表示起始下标，上限表示结尾下标。在起始下标和结尾下标之间，按照步长的间隔来找到元素。默认的步长是1。\n      \n      # 下标为0，2，4的元组元素\n      example_tuple[0:5:2]\n      # 倒数第一(最后一个)的元素\n      # -1代表最后一个\n      example_tuple[-1]\n      # 倒数第三个元素\n      # -3代表倒数第三\n      example_tuple[-3]\n      #序列的第二个到倒数第二个元素\n      #这里是下限是-1，下限是最后一个元素\n      example_tuple[1:-1] \n      \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      \n    \n    * 对字典中的key进行赋值\n      \n      example_dict = {\"tom\":11, \"sam\":57, \"lily\":100}\n      example_dict[\"tom\"]\n      example_dict[\"tom\"] = 30\n      \n      \n      1\n      2\n      3\n      \n\n\n# 在python中使用{}号有如下 一种场景\n\n在python中定义字典的时候，使用{}，但是引用的时候使用[]\n\nexample_dict = {\"tom\":11, \"sam\":57, \"lily\":100}\nexample_dict[\"tom\"]\n\n\n1\n2\n\n\n\n# 如何使用anaconda\n\n\n# windows安装anaconda\n\nanaconda指的是一个开源的python发行版本，其包含了conda、python等180多个科学包及其依赖项。 因为包含了大量的科学包，anaconda 的下载文件比较大（约 531 mb），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用miniconda这个较小的发行版（仅包含conda和 python）。 conda是一个开源的包、环境管理器，可以用于在同一个机器上安装不同版本的软件包及其依赖，并能够在不同的环境之间切换。\n\nanaconda包括conda、python以及一大堆安装好的工具包，比如：numpy、pandas等。\n\nminiconda包括conda、python。\n\n分为下载，安装，配置环境变量。\n\n设置如下环境变量：\n\nc:\\anaconda3\\scripts\nc:\\anaconda3\\library\\bin\n而c:\\anaconda3可以先不设置看看\n\n\n1\n2\n3\n\n\n\n# centos7安装anaconda\n\n# 下载anaconda\n\n可以在清华镜像的网站下载最新的匹配的anaconda版本，地址是 清华大学镜像\n\n\n\n# 安装anaconda\n\n$ bash anaconda3-2019.10-linux-x86_64.sh \n\n\n1\n\n\n许可证声明，回车\n\nin order to continue the installation process, please review the license\nagreement.\nplease, press enter to continue\n>>> \n\n\n1\n2\n3\n4\n\n\n选择yes\n\ndo you accept the license terms? [yes|no]\n[no] >>> \n\n\n1\n2\n\n\n选择安装目录，这里我们就直接回车，不修改目录了。\n\nanaconda3 will now be installed into this location:\n/app/python/anaconda3\n\n  - press enter to confirm the location\n  - press ctrl-c to abort the installation\n  - or specify a different location below\n\n[/app/python/anaconda3] >>> \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n是否选择利用conda初始化，我们这里选择yes。\n\n这样自动添加环境变量，会使得开机自动启动base环境\n\ndo you wish the installer to initialize anaconda3\nby running conda init? [yes|no]\n[no] >>> \n\n\n1\n2\n3\n\n\n退出，重新进入python用户，进行验证。\n\n(base) [python@lteindicator091224 ~]$ \n\n\n1\n\n\n\n# 常见命令\n\n# 查看conda版本\n\n$ conda --version\n\n\n1\n\n\n# 升级conda版本\n\n$ conda upgrade --all\n\n\n1\n\n\n# 虚拟环境是什么\n\n默认会有一个base环境的python解释器，我们可以将原有环境变量中的python去除掉，这个时候使用的python不是原来环境变量中设置的，而是base环境下的python。而命令行前面也会多一个(base)说明当前我们处于的是base环境下。\n\n\n\n# 新建虚拟环境\n\n下面创建一个名为test1的虚拟环境，并指定python版本为3.7(这里conda会自动找3.7中最新的版本下载)\n\n$ conda create -n test1 python=3.7\n\n\n1\n\n\n# 激活环境\n\nwindows机器\n$ activate test1\nlinux和mac机器\n$ source activate test1\n\n\n1\n2\n3\n4\n\n\n# 取消激活的环境\n\nwindows机器\n$ deactivate test1\nlinux和mac机器\n$ source deactivate test1\n\n\n1\n2\n3\n4\n\n\n# 切换环境\n\n$ activate test1\n\n\n1\n\n\n# 查看所有的环境\n\n$ conda env list\n\n\n1\n\n\n# 卸载环境\n\n$ conda remove --name test1 --all\n\n\n1\n\n\n# 卸载第三方包\n\n利用conda卸载\n$ conda remove requests\n利用pip卸载\n$ pip uninstall requests\n\n\n1\n2\n3\n4\n\n\n# 安装第三方包\n\n利用conda安装\n下面的name是packages包的名字\n$ conda install name\n# 使用conda install package=version 就能安装指定版本的package\n利用pip安装\n$ pip install name\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 查看环境中包信息\n\n$ conda list\n\n\n1\n\n\n# 导入导出环境\n\n如果想要导出当前环境的包信息可以用，下面的命令导出yaml文件中\n\n$ conda env export > environment.yaml\n\n\n1\n\n\n重新创建一个相同的虚拟环境(导入)可以用，下面的命令\n\n$ conda env create -f environment.yaml\n\n\n1\n\n\n\n# 如何在vscode中conda虚拟环境配置\n\n 1. 安装anaconda\n\n 2. 安装vscode\n\n 3. 配置window的环境变量\n    \n    /c/anaconda3/library/bin\n    /c/anaconda3/scripts\n    /c/anaconda3\n    \n    \n    1\n    2\n    3\n    \n\n 4. 设置vscode中变量\n    \n    文件--首选项--设置--扩展--python\n    \n    在用户配置或工作区配置，选择python.pythonpath为anaconda中的python路径，这里是\"c:\\anaconda3\"\n    \n    \n\n 5. 新建虚拟环境\n    \n    参照前面的anaconda3中conda的设置\n\n 6. 切换python虚拟环境\n    \n    左下角，选择相应的解释器。\n    \n    \n    \n    选择需要相应虚拟环境的，相应的工作目录。这些目录需要在工作区内。\n    \n    \n    \n    随后打开上面选择的目录下的python文件，发现会已经切换了\n    \n    \n\n\n# 为什么python运行慢\n\n 1. python是动态性语言不是静态性语言\n    \n    python程序执行的时候，编译器不知道变量的类型。python只知道是一个对象，动态类型意味着任何操作都需要更多的步骤。这是python在数值数据操作方面比c慢的主要原因。\n\n 2. python是解释性语言而不是编译性语言\n    \n    解释型语言与编译型语言它们本身的区别也会造成程序在执行的时候的速度差异。一个智能化的编译器可以预测并针对重复和不需要的操作进行优化。这也是提升程序执行的速度。\n\n 3. python的对象模型会导致访问内存效率低下\n    \n    相对于c语言，例如python中对整数进行操作会有一个额外的类型信息层。当对数据进行操作时(例如排序、计算、查找等)，无论是存续成本还是访问成本，都很高。\n\n> 参考如下文档，进行的整理\n> \n> https://www.jianshu.com/p/3fa56d9f58cb\n\n\n# 如何理解python中单引号/双引号/多引号\n\n\n# 如何理解python中for循环range()函数\n\n在java中的for循环的格式是如下的：\n\nfor (i=0, i<10, i++){\n    system.out.print(i);\n}\n\n\n1\n2\n3\n\n\n在python中要改写成这样：\n\nfor i in range(0, 10, 1):\n    print(i)\n\n\n1\n2\n\n\n其中range()函数中的第一个参数是开始(包含本身)，第二个参数是结束(不包含本身)，第三个参数是步长。\n\n> 参考的如下的网上文档：\n> \n> https://jingyan.baidu.com/article/fec7a1e5d6f00a1190b4e7a5.html\n\n\n# 如何理解for循环和while循环的区别\n\nfor循环，就是遍历某一对象，也就是说根据循环次数限制做多少次重复操作。\n\n例如如下的for循环操作：for i in range(3)意思就是i循环4次，i的取值为0、1、12。\n\nwhile循环，就是当满足什么条件的时候，才做某种操作。\n\n例如如下的while操作：while count < 3意思是当count小于3时，才做下面的操作。\n\n案例分析：在做一个登陆的小程序的时候，最多输入用户名和密码3次，这时就应该用while循环，而不是for循环，因为循环次数不一定(我们的循环体内写的是重复登陆要输入的用户名和密码)。\n\n> 参考如下的网页：\n> \n> https://www.cnblogs.com/klmm/p/8620331.html\n\n\n# 如何理解python中的do...while\n\njava中有相应的do while的语法，与while的差别在于，do while是先执行循环体语句，然后进行判断语句，也就是说无论判断是否为true都会至少执行一次。\n\npublic class whiletest {\n    public static void main(string[] args) {\n        int i = 1;\n        do {\n            i++;\n            system.out.println(i);\n        } while (i<1) \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n在python中是没有do while这种语法格式的，但是可以使用while(无限循环)和break组合起来实现do .... while 。这里的while(无限循环)，相当于是do 里面的内容；而带上if 条件后的break，就是原有的while 中的内容。\n\nn = 0 \nwhile true:     # 无限循环....\n    print n\n    n += 1\n    if n == 10:\n        break\n\n\n1\n2\n3\n4\n5\n6\n\n\n> 参照如下网页：\n> \n> https://blog.csdn.net/qq_37131111/article/details/54580587\n> \n> https://www.jianshu.com/p/028cd9460ccb\n> \n> https://segmentfault.com/q/1010000002428584\n\n\n# 为什么pip install报ssl error\n\n正常执行pip install某个模块的时候，就会报ssl的错误。\n\n改为如下命令执行：\n\npip install requests -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com\n\n\n1\n\n\n\n# 如何理解python中的函数和方法的区别\n\n函数的定义：在函数定义的过程中，用的是def关键字，有函数的形参，有相应的return 返回值。\n\ndef square_sum(a,b):\n    a = a**2\n    b = b**2\n    c = a + b\n    return c\n\n\n1\n2\n3\n4\n5\n\n\n函数的调用：函数的调用，和函数的定义很相似。只不过在调用函数的时候，我们把真实的数据填入到括号中，作为参数传递给函数。\n\na = 5\nb = 6\nx = square_sum(a, b)\nprint(x)\n\n\n1\n2\n3\n4\n\n\n方法的定义：方法属于定义类中包含的内容，方法是用来定义类的一些\"行为\"属性，也就是说在类的内部定义函数的方式来说明方法。但是和定义函数的方式还是有不同。也是使用def关键字，方法的内部的第一个参数是self，是为了在方法内部引用对象自身。无论该参数是否用到，方法的第一个参数必须是用于指代对象自身的self。\n\nclass bird(object):\n    feather = true\n    reproduction = \"egg\"\n    def chirp(self, sound):\n        print(sound)\n\n\n1\n2\n3\n4\n5\n\n\n方法的调用：我们是通过对象来调用方法，先要实例化一个对象，然后像调用函数那样，通过传参来实现调用。\n\nsummer = bird()\nsummer.chirp(\"jijijij\")    #打印'jijijij'\n\n\n1\n2\n\n\n\n# 如何实现print函数格式化输出\n\n在python中，我们有时候需要对输出进行格式化输出，有两种方式：第一种方法是占位符的方式；第二种方法是格式化输出的方式。\n\n\n# 占位符方式\n\n例如我们需要输出\"你好xx，你的额度是xx\"，其中xx的变量，我们是无法预知的。这个时候就需要格式化输出，和c语言一样，我们可以使用占位符%?，其中?代表不同的字符，例如%s代表字符串，%d代表十进制整数，%f代表浮点数。\n\n%前面是占位符号，后面是真实的值，name将替换%s的位置，b将替换%d的位置，字符串后的%用来说明是哪些变量要替换前面的占位符，当只有一个变量的时候，可以省略括号。\n\nname = \"potato\"\nb = 100\nprint(\"你好%s，你的额度是%d\" % (name,b))\n\n\n1\n2\n3\n\n\n占位符还可以控制输出的格式，例如保留几位小数。如果不知道数据类型的话，用%s即转为字符串进行输出会比较稳妥。\n\n# %.2f代表保留两位小数\nprint(\"小数: %.2f\" % 3.14159)\n# %.2f保留两位小数，不够的位用0补充\nprint(\"小数: %.2f\" % 4.5)\n# %3d代表这个数的宽度为3，不够的话用空格在前面补，如果数的宽度大于3，则正常输出\nprint(\"占位: %3d\" % 5)\n# %05d代表这个数的宽度为5，不够的话用0在前面补，如果数的宽度大于5，则正常输出\nprint(\"前导0: %05d\" % 2)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# format方式格式化输出\n\n可以利用string对象的format对象，进行格式化。\n\n#{0}代表占位符和format里的参数对应，{1:.2f}，\n#冒号后是格式控制，代表保留两位小数，\n#下面的结果是\"你好potato,你余额是3.10\"\nprint(\"你好{0}，你的余额是{1:.2f}\".format(\"potato\",3.1))\n#或者可以使用py内置的format函数\nprint(format(3.1415,\".2f\"))\n#结果是3.14\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n> 参考的url如下：\n> \n> https://segmentfault.com/a/1190000018081959\n\n从python2.6开始，新增了一种格式化字符串的函数str.format()，它增强了字符串格式化的功能。基本的语法是通过{}和:来代替以前的%.\n\nformat函数可以接受不限参数个数，位置可以不按顺序。\n\n字符串里面的{}，想怎么写就怎么写，不需要考虑在{}上面加上''或\"\"，在format函数后面的括号里面的各个值，要注意写的方式，如果是字符串就加上''或\"\"，如果是数字类型，那么就什么也不要加。\n\nstr.format()里面只是描述了，在字符串的某个位置，应该填入哪些变量，至于这个变量合不合理，类型是不是有误，这就不是str.format()函数关心的事情了，例如如果写入sql中的字段不对，那么数据库自己根据事务性的原则自己去回滚。\n\n# 不指定位置\n\nstr里面的{}和format()函数里面的各个值，不去指定对应的关系，那么就按照两者出现的先后顺序去匹配上。\n\nprint('i love nanjing: {} {}'.format(\"sha\",\"lili\"))\n\nprint('i love nanjing: ({},{})'.format(100,\"lili\"))\n\n\n1\n2\n3\n\n\n# 指定位置\n\n通过对str里面的{0}、{1} .... 等方式，来和后面的format()函数中的各个值匹配上，{0}代表匹配的是format()函数中的第一个值，{1}代表匹配的是format()函数中的第二个值，以此类推。\n\nprint('i love nanjing: {1} {0}'.format(\"sha\",\"lili\"))\n\nprint('i love nanjing: ({1},{0})'.format(100,\"lili\"))\n\n\n1\n2\n3\n\n\n\n# f-string 格式化字符串常量\n\n摘录于如下的url地址：\n\n> python格式化字符串f-string概览\n\nf-string，被称为格式化字符串常量，是python3.6新引入的一种字符串格式化方法，主要目的是事格式化字符串的操作更加简便。f-string在形式上是以f或f修饰符引领的字符串(f'xxx'或f'xxx')，以大括号{} 标明被替换的字段；f-string在本质上并不是字符串常量，而是一个在运行时运算求值的表达式。\n\nf-string在功能方面不逊于传统的%-formatting语句和str.format()函数，同时性能又优于两者，且使用起来也更加简明，因此对于python3.6及以后的版本，推荐使用f-string进行字符串格式化。\n\ns_num = 1005\nprint(f\"学号为{s_num}的详细信息为：\" f\"number={num},name={name},class_name={cs_name}\")\n\n\n1\n2\n\n\n\n# 为什么单个元素的元组需要有逗号\",\"\n\n在进行多进程的案例中，出现了在初始化process类为对象的时候，需要传参一个元组，但是有时候这个元组只有一个元素，很奇怪的发现了是如下的写法args=('peking hot.avi', )\n\n其实这是在python中的规范，如果一个元组中只有一个元素，那么必须需要在这个元素后面加上','来表示这是个元组，否则的话python解释器会把这个解释为其他的类型。例如下面的案例：\n\na = (1, 2)\nb = (3)\nc = a + b\n\n\n1\n2\n3\n\n\n执行上面的代码会发现报错typeerror: can only concatenate tuple (not \"int\") to tuple,原来python解释器把(3)当作一个算数表达式来处理的，它的结果就是一个int型对象。为了和只有单个元素的元组区分，python规定要在元素后面带上一个逗号，例如d=(3,)。也就是如下的形式：\n\na = (1, 2)\nb = (3,)\nc = a + b\n\n\n1\n2\n3\n\n\n\n# 如何理解print函数中end和flush\n\n代码中有时候会出现如下的print的格式：\n\nfor i in range(5):\n    print(\"hello\", end = '', flush = true)\n\n\n1\n2\n\n\n\n# 换行输出和不换行输出\n\n**end = \"\"**的意思是，双引号之间的内容就是结束的内容，可以是空格，也可以是其他字符，如果不去定义end，那么默认就是换行的。\n\n换行输出：\n\nfor i in range(5):\n    print(\"hello\", end = '\\n', flush = true)\nfor i in range(5):\n    print(\"hello\")\n\n\n1\n2\n3\n4\n\n\n不换行输出，每次输出以一个空格结尾：\n\nfor i in range(5):\n    print(\"hello\", end = \" \", flush = true)\n\n\n1\n2\n\n\n不换行输出，每次输出都紧接着上一次的输出：\n\nfor i in range(5):\n    print(\"hello\", end = \"\", flush = true)\n\n\n1\n2\n\n\n\n# flush参数\n\n简单来说就是将缓存里面的内容立即输出到标准输出流(这里是sys.stout，也就是默认的显示器)。\n\n这个功能在客户端脚本几乎用不上，大多用于服务器端。\n\n比如反向ajax里面就要用到flush,举个例子: 在线web聊天页面会实时显示聊天的内容, 其实后台是一直在向服务器请求数据的,正常情况下是请求完毕之后才会输出相应内容, 但是是即时聊天, 需要一有相应就得立即返回,flush也就起作用了。\n\n下面的例子中，我们用print写内容到一个文件中，先运行第一段代码，我们发现123.txt文件，并没有\"123456789\"被写入，文件内容为空。只有f.close()后才将内容写进文件中。如果加入flush = true，那么可以不用f.close()即可将内容写进文件中。\n\nflush参数主要是刷新，默认flush=false，不刷新。也就是print到f中的内容先存到内存中，当文件对象关闭时才把内容输出到123.txt中；而当flush=true时它会立即把内容刷新存到123.txt中。\n\n# 示例1\nf = open(\"123.txt\", \"w\")\nprint(\"123456789\", file = f)\n# 示例2\nf = open(\"123.txt\", \"w\")\nprint(\"123456789\",file = f, flush = true)\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# python中self的作用是什么\n\n\n# self出现在类的自定义方法的第一个参数上\n\nself英文是自己的意思，在python中定义类的方法的时候，我们一般习惯于，将self作为该类中的定义的某个方法的一个参数。而这个self的意思在于，在方法内部引用了对象自身(self代表类的实例，而非类)。\n\nclass bird(object):\n    feather = true\n    reproduction = \"egg\"\n    def chirp(self, sound):\n        print(sound)\n\n\n1\n2\n3\n4\n5\n\n\n\n# self出现在类的自定义方法体内\n\n出现在类的自定义方法体内的话，self代表的是对于对象(instance)自身的引用。\n\n或者可以说是通过在方法体中的self来配置该实例的个性化对象属性。\n\n这就涉及到一个作用域的问题，下面案例中的self.inputname和input1这两个变量，都赋值为name，那么这两个值有什么区别呢？\n\nclass person:\n    def setname(self, name):\n        self.name = name\n        self.inputname = name\n        input1 = name\n    def getname(self):\n        return self.name\n    def greet(self):\n        print \"hello world! i'm %s . \" % self.name\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nself.name = name的意思是，setname方法中的传参name赋值给，初始化person这个class后的某个具体对象的name属性。而input1 = name则是正常的类中的某个方法中的属性赋值。\n\n总结：不加self的变量是局部变量，作用域是当前函数方法。(该类中其他方法无法使用这个变量)；而加了self的变量的是实例变量(某个具体对象的变量)，作用域是当前实例，可以在该类的其他方法中对该变量的使用。\n\n> 参考如下的url：\n> \n> https://www.jianshu.com/p/bdbd577314f9\n> \n> https://blog.csdn.net/bing900713/article/details/60884931\n> \n> https://www.cnblogs.com/jessonluo/p/4717140.html\n\n\n# _init_()方法有什么作用\n\n_init_()函数本身就是python中的，针对类的初始化的构造方法。用来初始化新创建的对象的各种属性，在一个对象被创建以后就会被立即调用。\n\n * 带有两个下划线开头的函数是声明该属性为私有，不能再类的外部被使用或直接访问。\n * init函数（方法）支持带参数的类的初始化 ，也可为声明该类的属性\n * init函数（方法）的第一个参数必须是 self（self为习惯用法，也可以用别的名字），后续参数则可 以自由指定，和定义函数没有任何区别。\n\n在类中没有_init_()函数方法的时候，我们如果想在创建这个类的对象后，就想让这个对象拥有一些个性化(父类没有)的属性的时候，我们就需要手动写个方法，然后在创建完对象后，手动调用这个方法来实现，(但是想在各个方法中共用这个初始化的属性，还想做更多)，实现起来非常的麻烦。\n\nclass bird(object):\n    def __init__(self, sound):\n        self.sound = sound\n        print(\"my sound is:\", sound)\n    def chirp(self):\n        print(self.sound)\n\nsummer = bird(\"ji\")\nsummer.chirp()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n> 参考如下的url：\n> \n> https://www.javazhiyin.com/40640.html\n\n\n# super()有什么作用\n\n我们在子类中使用super关键字，这样就可以在子类中调用父类中被覆盖的方法。\n\nsuper是用来解决多重继承问题的，直接用类名调用父类方法在使用单继承的时候没有问题，但是如果使用多继承，会涉及到查找顺序(mro)、重复调用(钻石继承)等种种问题。\n\npython3中可以直接使用super().xxx代替super(class, self).xxx\n\n示例如下：\n\n# coding=utf-8\n\n# 胖子老板的父类\nclass fatfather(object):\n    def __init__(self,name):\n        print('fatfather的init开始被调用')\n        self.name = name\n        print('调用fatfather类的name是%s' % self.name)\n        print('fatfather的init调用结束')\n\n# 胖子老板类 继承 fatfather 类\nclass fatboss(fatfather):\n    def __init__(self,name,hobby):\n        print('胖子老板的类被调用啦！')\n        self.hobby = hobby\n        #fatfather.__init__(self,name)   # 直接调用父类的构造方法\n        super().__init__(name)\n        print(\"%s 的爱好是 %s\" % (name,self.hobby))\n\n\ndef main():\n   #ff = fatfather(\"胖子老板的父亲\")   \n   fatboss = fatboss(\"胖子老板\",\"打斗地主\")\n\nif __name__ == \"__main__\":\n   main()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n注意事项：\n\n * super().__init__相对于类名.__init__，在单继承上用法基本无差\n * 但在多继承上有区别，super方法能保证每个父类的方法只会执行一次，而使用类名的方法会导致方法被执行多次\n * 多继承时，使用super方法，对父类的传参数，应该是由于python中super的算法导致的原因，必须把参数全部传递，否则会报错\n * 单继承时，使用super方法，则不能全部传递，只能传父类方法所需的参数，否则会报错\n * 多继承时，相对于使用类名.__init__方法，要把每个父类全部写一遍, 而使用super方法，只需写一句话便执行了全部父类的方法，这也是为何多继承需要全部传参的一个原因\n\n> 参考如下的url：\n> \n> https://www.jianshu.com/p/3b7ebe0389e4\n\n\n# virtualenv/pip/conda区别和联系\n\n\n# 什么是virtualenv\n\n就是可以利用virtualenv工具来创建任意多个虚拟环境，每个虚拟环境可以用于不同的项目。\n\n这样当我们运行环境中运行多种不同类型的python程序的时候，不要担心不同环境中不同的python版本，模块版本，导致python的运行受到影响。\n\n * 分门别类的创建不同的虚拟环境，互不污染。(如机器学习和爬虫互不影响)\n * 其次，一旦我们不使用了，可以直接删除虚拟环境，而不用管各种文件残留，可能带来的关联问题。\n\n\n# 什么是pip\n\npip 是python最常用的包管理器，该工具提供了对python包的查找、下载、安装、卸载的功能。能够自动处理依赖。类似于linux的yum工具。\n\n主要的安装包的使用方法为pip install packagename, conda也具有包管理器的功能，命令为conda install packagename\n\n\n# 什么是conda\n\nconda也可以用来创建虚拟环境，并且安装python 包。\n\n * venv是虚拟环境管理器，pip是包管理器，那么conda则是两者的结合。\n * conda的包管理器一般会安装过多的依赖，大多数情况下还是使用pip安装包。\n * pip只能安装python包，conda可以安装一些工具软件，即使这些软件不是基于python开发的。\n * 在虚拟环境管理上，venv会在该项目下创建虚拟环境；然而conda的每个虚拟环境不会占用项目文件夹的空间，它创建在用户设定的一个位置，这使得多个项目共享一个虚拟环境更加方便。\n * conda虚拟环境是独立于操作系统解释器环境，即无论操作系统解释器是什么版本，我们也可以指定虚拟环境python版本为3.7，而venv是依赖主环境的。\n * conda是环境自动集成了numpy这样的主流科学计算包，venv每个包都要自行下载。\n * conda有图形化环境管理器，venv没有。\n\n参考如下url：\n\n> https://blog.csdn.net/zhouchen1998/article/details/84671528\n\n\n# 回车与换行\n\n\\n和\\r的区别的理解:\n\n在python中，\"\\n\"和\"\\r\"都是转义字符。也就是说后面的n和r，不是单纯的字母n和字母r的意思了。\"\\n\"代表着回车，光标在下一行；\"\\r\"代表着换行，光标在上一行。\n\n需要注意的是，\\n和\\r同样都有移动光标位置的功能，但是不同的就是在于光标的具体位置不一样。\\n后的光标在下一行开头，\\r后的光标在本行的开头。\n\nprint('你好吗？\\n朋友') \nprint (\"——分隔线——\")\nprint (\"你好吗？\\r朋友\")\n# 返回的结果是\n你好吗？\n朋友\n——分隔线——\n朋友吗？\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n我们可以看到在加了\\n的符号后，显示的是\"你好吗？\"的\"朋友\"进行了换行，光标在另外一行的开头了。而加了\\r的符号后，显示的是\"朋友吗？\"，这是由于将光标移动到了这一行的行首了，那就将\"朋友\"替换了原来的\"你好\"，最后显示的就是\"朋友吗？\"\n\n\n# 如何理解字符串前缀u/b/r\n\n\n# 无前缀 和 u前缀\n\n字符串默认创建即以unicode编码存储，可以存储中文。\n\nstring = 'a' 等效于 string = u'a'\n\n这里是string 类\n\nunicode中通常每个字符由2个字节表示\n\n * u'a'即 u'\\u0061' 实际内存中为[0000 0000] [0110 0001]\n\n\n# b前缀\n\nb前缀的意思是，将字符串存储ascii码，但是ascii码无法存储中文。\n\n在ascii码中，每个字符由1个字节表示(8位)。\n\n * b'a'即b'\\x61' 实际内存中为[0110 0001]\n\n如下面的例子中，如果有b前缀，并且后面的字符串带有中文，那么会报错。\n\nresult = b'你好61'\n会报语法错误！\n\n\n1\n2\n\n\n\n# r前缀\n\nr前缀主要解决的是转义字符，特殊字符的问题。里面的转义字符变得无效了。\n\n如果以r开头的，那么说明后面的字符，都是普通的字符了，即如果是\"\\n\"那么表示一个反斜杠字符和一个字母n。而不是表示换行了。\n\n通常以r或r开头的字符，常用于正则表达式，对应着re模块。\n\n正如下面的例子中，带上r或r，能够方便处理过滤反斜杠的问题。\n\nf = open(r'd:\\工作内容\\最近工作\\集团省份上线文档\\山西\\1keepalived.conf','r')\nfor i in f:\n    print(i)\nf.close()\n\n\n1\n2\n3\n4\n\n\n\n# 三个前缀总结\n\n参照下面的例子来看，我们分别以r为前缀，b为前缀，u为前缀。\n\nprint(r'a\\ta')   # 结果为a\\ta    原来是什么就是什么，转义字符无效。\nprint(b'a\\ta')   # 结果为b'a\\ta' 转为ascii码，字节类型   \nprint(u'a\\ta')   # 结果为a        a，这里\\t转义字符生效为8个空格\n\n\n1\n2\n3\n\n\n\n# 如何理解python3中的bytes和str类型\n\nstr类型也就是字符串类型，bytes类型也就是字节流类型。\n\npython3最重要的新特性之一就是对字符串和\n\n\n# web开发理解(摘自廖雪峰)\n\n在bs架构下，客户端只需要浏览器，应用程序的逻辑和数据都存储在服务器端。浏览器只需要请求服务器，获取web页面，并把web页面展示给用户即可。\n\nweb页面具有极强的交互性。由于web页面是用html编写的，而html具备超强的表现力，并且，服务器端升级后，客户端无需任何部署就可以使用到新的版本。\n\nweb开发经历了如下几个阶段：\n\n * 静态web页面：由文本编辑器直接编辑并生成静态的html页面，如果要修改web页面的内容，就需要再次编辑html源文件，早期的互联网web页面就是静态的。\n * cgi：由于静态web页面无法与用户交互，比如用户填写了一个注册表单，静态web页面就无法处理。要处理用户发来的动态数据，出现了common gateway interface，简称为cgi，用c/c++编写。\n * asp/jsp/php：由于web应用特点是修改频繁，用c/c++这样的低级语言非常不适合web开发，而脚本语言由于开发效率高，与html结合紧密，因此，迅速取代了cgi模式。asp是微软退出的用vbscript脚本语言编程的web开发技术，而jsp用java来编写脚本，php本身则是开源的脚本语言。\n * mvc：为了解决直接用脚本语言嵌入html导致的可维护性差的问题，web应用也引入model-view-controller的模式，来简化web开发。asp发展为asp.net，jsp和php也有一大堆mvc框架。\n\n目前，web开发技术仍在快速发展中，异步开发、新的mvvm前端技术层出不穷。\n\n\n# http协议简介\n\n在web应用中，服务器把网页传给浏览器，实际上就是把网页的html代码发送给浏览器，让浏览器展示出来。\n\n而浏览器和服务器之间的传输协议是http。(讲明一个通信协议的关键是谁与谁之间的通信协议)\n\n# 什么是http协议\n\n引入html，html是一种用于定义网页的文本，会html，就可以编写网页。\n\nhttp是在网络上传输html的协议，用于浏览器和服务器的通信。\n\n# 查看http交互过程\n\n我们这里使用的是谷歌浏览器的开发者工具，去访问wwww.sina.com.cn的网址，去理解我们自己的客户端和服务器端的http协议的交互过程。\n\n# 打开谷歌浏览器的开发者工具\n\nchrome提供了一套完整地调试工具，非常适合web开发。\n\n选择浏览器右上角的三个点 -> \"更多工具\" -> \"开发者工具\"\n\n或者按f12，进入\"开发者工具\"\n\n\n\n * elements显示的是网页的结构\n * network显示的是浏览器和服务器的通信，我们点击\"network\"的时候，确保第一个小红点亮着，代表chrome浏览器会记录所有浏览器和服务器之间的通信。\n\n# 查看http的请求信息\n\n基于上面已经打开\"开发者模式\"的情况下，我们切换到\"network\"子菜单中，在浏览器地址栏中输入\"www.sina.com.cn\"。\n\n这个时候，我们通过network就可以查看到所有的，我们的浏览器作为客户端通过http协议去请求www.sina.com.cn资源的时候，和sina的服务器之间的通信过程。\n\n我们在network中，定位到第一条记录，\"headers\" -> \"request headers\" -> \"view source\"，意思就是显示服务器返回的原始响应数据。\n\n\n\n重要解析如下：\n\n第一行：get / http/1.1\n\nget表示一个读取请求，将从服务器获取网页数据， /表示的是url的路径，url总是以/开头，/就表示首页。 http/1.1表示的是采用http协议版本是1.1。http1.1版本和1.0版本的主要区别在于1.1版本允许多个http请求复用一个tcp连接，以加快传输速度。\n\n从第二行，开始都是类似于xxx:yyyy的形式\n\n例如host: www.sina.com.cn表示的是请求的域名是www.sina.com.cn\n\n# 查看http的返回信息\n\nhttp响应分为header和body两部分(body是可选项)。\n\n在上面的同样的页面中，选择\"response headers\"，点击\"view source\"，同样来显示服务器返回的原始响应数据。\n\n\n\n重要解析如下：\n\n第一行中的200 ok\n\n200表示一个成功的响应码，失败的响应码有404 not found:网页不存在，500 internal server error：服务器内部出错，等等。\n\ncontent-type行的内容：\n\ncontent-type表示的是响应的内容，这里是text/html表示html页面。需要注意的是浏览器是根据content-type来判读响应的内容是网页还是图片，是视频还是音乐。反过来说，浏览器不是靠url的连接来判断响应的内容，例如url是http://example.com/abc.jpg，它也不一定是图片。\n\n例如content-type: text/html;charset=utf-8表示响应类型是html文本，并且编码是utf-8,content-type: image/jpeg表示响应类型是jpeg格式的图片。\n\n如何查看http响应的body内容：\n\n我们可以在上面的页面上，从\"headers\"切换到\"response\"来查看返回的body内容。或者直接在网页上右键\"查看网页源码\"\n\n\n\n# content-length的理解\n\n我们在上面的\"response headers\"中可以发现有一个\"content-length\"的header头信息。这个信息很重要，下面主要从\"content-length\"的是什么？为什么(有什么作用)的角度来理解，这个header头参数。\n\ncontent-length的header头告诉了浏览器报文中body主体的大小。单位是字节数。这个大小是包含了内容编码的，比如对文件进行了gzip压缩，content-length就是压缩后的大小(这点对我们编写服务器非常重要)。除非使用了分块编码，否则content-length首部就是带有实体主体的报文必须使用的。\n\n使用content-length首部是为了能够检测出服务器崩溃而导致的报文截尾，并对共享持久连接的多个报文进行正确分段。\n\n 1. 检测截尾\n\nhttp的早期版本采用了关闭连接的办法来划定报文的结束。但是，没有content-length的话，客户端无法区分到底是报文结束时正常的关闭连接还是报文传输中由于服务器崩溃而导致的连接关闭。客户端要通过content-length来检测报文截尾。\n\n报文截尾的问题对缓存代理服务来说尤为重要。如果缓存服务器收到截尾的报文却没有识别出截尾的话，它可能会存储不完整的内容并多次使用他来提供服务。缓存代理服务器通常不会为没有显式content-length首部的http主体做缓存，以此来减少缓存已截尾报文的风险。\n\n 2. content-length与持久连接\n\ncontent-length首部对于持久链接是必不可少的。如果响应通过持久连接传送，就可能有另一条http响应紧随其后。客户端通过content-length首部就可以知道报文在何处结束，下一条报文从何处开始。因为连接是持久的，客户端无法依赖连接关闭来判断报文的结束。\n\n 3. 分块编码的介绍\n\n有一种情况下，使用持久连接可以没有content-length首部，即采用分块编码(chunked encoding)时。在分块编码的情况下，数据是分为一系列的块来发送的，每一块都有大小说明。哪怕服务器在生成首部的时候不知道整个实体的大小(通常是因为实体是动态生成的)，仍然可以使用分块编码传输若干已知大小的块。\n\n参考如下url：\n\n> https://my.oschina.net/xishuixixia/blog/93185\n\n# 浏览器读取到html源码做了什么\n\n当浏览器读取到新浪首页的html源码后，它会解析html，显示页面，然后，根据html里面的各种链接，再发送http请求给新浪服务器，拿到相应的图片、视频、flash、javascript脚本、css等各种资源，最终显示出一个完整的页面。\n\n所以，我们会在\"network\"下面看到很多额外的http请求。\n\n\n# 总结一下\n\n# http请求流程\n\n 1. 步骤1，浏览器首先向服务器发送http请求，请求包括：\n\n方法：get还是post，get仅请求资源，而post会附带用户数据；\n\n路径：/full/url/path\n\n域名：由host头指定，host: www.sina.com.cn\n\n以及其他相关的header；\n\n如果是post，那么请求还应该包括一个body，包含用户数据。\n\n 2. 步骤2，服务器向浏览器返回http响应，响应包括：\n\n响应代码：200表示成功，3xx表示重定向，4xx表示客户端发送的请求有错误，5xx表示服务器端处理时发生了错误；\n\n响应类型：由content-type指定，例如content-type: text/html表示响应类型是html文本，content-type: image/jpeg表示响应类型是jpeg格式的图片；\n\n以及其他相关的header；\n\n通常服务器的http响应会携带内容，也就是有一个body，包含响应的内容，网页的html源码就在body中。\n\n 3. 步骤3，如果浏览器还需要继续向服务器请求其他资源，比如图片，就再次发送http请求，重复步骤1和步骤2。\n\n# http协议对服务器开发的理解\n\nweb采用的http协议中采用了非常简单的请求-响应模式，从而大大简化了开发。当我们编写一个页面的时候，我们只需要在http响应中把html发送出去，不需要考虑如何附带图片、视频等，浏览器如果需要请求图片和视频，它会发送另外一个http请求，因此，一个http请求只处理一个资源。\n\nhttp协议同时具备极强的扩展性，例如虽然浏览器请求的是http://www.sina.com.cn/的首页，但是新浪在html中可以链入其他服务器的资源，比如<imgsrc=\"http://i1.sinaimg.cn/xxxxxxxx.png\">，从而将请求压力分散到各个服务器上。并且，一个站点可以链接到其他站点，无数个站点互相链接起来。\n\n# http格式\n\n每个http请求和响应都遵循相同的格式，一个http包含header和body两部分，其中body是可选的。http协议是一种文本协议，所以，它的格式也非常简单。\n\nhttp get请求的格式，每个header一行一个，换行符是\\r\\n。\n\nget /path http/1.1\nheader1: value1\nheader2: value2\nheader4: value3\n\n\n1\n2\n3\n4\n\n\nhttp post请求的格式，每个header一行一个，换行符是\\r\\n；当遇到两个\\r\\n的时候，header部分结束，后面的数据全部是body。\n\npost /path http/1.1\nheader1: value1\nheader2: value2\nheader4: value3\n\nbody data goes here ....\n\n\n1\n2\n3\n4\n5\n6\n\n\nhttp响应的格式，如果响应中包含了body，也是通过\\r\\n\\r\\n来分隔的。需要再次注意的是，body的数据类型由content-type头来确定，如果是网页，body就是文本，如果是图片，body就是图片的二进制数据。\n\n当存在content-encoding时，body数据是被压缩的，最常见的压缩方式是gzip，所以，看到content-encoding: gzip的时候，需要将body数据先解压缩，才能得到真正的数据。压缩的目的在于减少body的大小，加快网络传输。\n\n\n# html简介\n\n在这个章节，根据廖雪峰老师的文档，总结了html的基本格式，大致了解了html是定义了页面的内容；css是用来控制页面元素的样式；而javascript负责页面的交互逻辑。\n\n# html的理解\n\n可以理解为网页就是html。在网页中不但包含了文字，还有图片、视频、flash小游戏，有复杂的排版、动画效果，所以，html定义了一套语法规则，来告诉浏览器如何把一个丰富多彩的页面展示出来。\n\n一个简单的html的格式内容大致如下：\n\n<html>\n<head>\n  <title>hello</title>\n</head>\n<body>\n  <h1>hello, world!</h1>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n可以看出，html文档就是一系列的tag组成的，最外层的tag是<html>，还有<head>...</heand>,<body>...</body>(这里不要和http的header、body搞混了)，由于html是富文档模型，所以，还有一系列的tag用来表示链接、图片、表格、表单等等。\n\n# css简介\n\ncss是cascading style sheets(层叠样式表)的简称，css用来控制html里的所有元素如何展现，比如，给标题元素加上一个样式，变成48号字体，灰色，带阴影：\n\n<html>\n<head>\n  <title>hello</title>\n  <style>\n    h1 {\n      color: #333333;\n      font-size: 48px;\n      text-shadow: 3px 3px 3px #666666;\n    }\n  </style>\n</head>\n<body>\n  <h1>hello, world!</h1>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# javascript简介\n\njavascript，它和java一点关系没有。javascript是为了让html具有交互性而作为脚本语言添加的，javascript既可以内嵌到html中，也可以从外部链接到html中。例如，我们希望当用户点击标题的时候把标题变成红色，就必须通过javascript来实现：\n\n<html>\n<head>\n  <title>hello</title>\n  <style>\n    h1 {\n      color: #333333;\n      font-size: 48px;\n      text-shadow: 3px 3px 3px #666666;\n    }\n  </style>\n  <script>\n    function change() {\n      document.getelementsbytagname('h1')[0].style.color = '#ff0000';\n    }\n  <\/script>\n</head>\n<body>\n  <h1 onclick=\"change()\">hello, world!</h1>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# wsgi接口\n\n# 为什么需要wsgi\n\n在描述为什么的时候，又有几个概念需要讲清楚：web访问基本流程/静态动态页面/wsgi能做些什么\n\n在理解为什么需要wsgi的时候，先要有个web应用访问的基本流程：\n\n 1. 浏览器发送一个http请求;\n\n 2. 服务器收到请求，生产一个html文档;\n\n 3. 服务器把html文档作为http响应的body发送给浏览器;\n\n 4. 浏览器收到http响应，从http body取出html文档并显示。\n\n静态页面和动态页面的区别(有两种分类，主要分类是第一种)：\n\n第一种:\n\n静态页面：根据页面语言脚本来区分，把纯html+js脚本成为静态页面，这种页面内容基本固定；\n\n动态页面：把asp/php/jsp/python 等程序语言写的页面称为动态页面，这种页面基本都会调用数据库，或者通过和用户交互产生变化。\n\n第二种：\n\n静态页面：根据页面的直观表示。把只有文本和静态图片的html页面称为静态页面。\n\n动态页面：把包含gif动画图片和flash动画以及一些视频音乐等富媒体的html页面称为动态页面。\n\n如果只是展示静态的html，确实不需要wsgi：\n\n最简单的web应用就是先把html用文件保存好，用一个现成的http服务器软件(apache/nginx/lighttpd)等这些常见的静态服务器，来接受用户请求，从文件中读取html。\n\n如果要展示动态的html，确实需要wsgi的存在：\n\n要动态生成html，那么缺少不了程序语言的存在，而单独靠类似nginx这种web服务器无法完成从接受请求到动态生成html，返回请求内容的需求。那么这个时候就需要nginx和python web框架之间有个共同的通信协议，这个协议就是wsgi。\n\nwsgi就是为了方便我们为了访问动态html的时候，做了接受客户端的http请求，解析http请求，然后发送返回http响应，做了这些工作。而python web框架只是关注于如何响应生产动态html文档就行了。有了这个，我们就可以专心用python编写web业务。\n\n# 什么是wsgi\n\nwsgi，也就是web server gateway interface。是定义在web服务器和web应用的一种接口规范，只适用于python语言来使用。\n\n# 基于wsgi来定义web app\n\n在wsgi的接口规范中，它只要求web开发者实现一个函数，就可以响应http请求。例如下面的例子中：\n\ndef application(environ, start_reponse):\n    start_response('200 ok', [('content-type', 'text/html')])\n    return [b'<h1>hello, web!</h1>']\n\n\n1\n2\n3\n\n\n上面的application()函数就是符合wsgi标准的一个http处理函数，它接收两个参数：\n\n * environ：一个包含所有http请求信息的dict对象；\n * start_response：一个发送http响应的函数。(主要是响应码，和响应的header头信息，由web server自己实现的)\n\n在application()函数中，调用了start_response方法，发送了http响应的header，注意的是header只能发送一次，也就是只能调用一次start_response()函数。start_response()函数接收两个参数，一个是http响应码，一个是一组list表示的http header，每个header用一个包含两个str的tuple表示。\n\n函数的返回值b'<h1>hello, web!</h1>'，将作为http响应的body发送给浏览器。\n\n# 思考wsgi标准下的web app开发\n\n有了wsgi，我们关心的就是如何从environ这个dict对象拿到http请求信息，然后构造html，通过start_response()发送header，最后返回body。\n\n整个application()函数本身没有涉及到任何解析http的部分，也就是说，底层代码不需要我们自己编写，我们只负责在更高层次上考虑如何响应请求就可以了。\n\napplication()函数是由符合wsgi标准的web服务器来调用的。\n\n# 运行wsgi服务\n\n这里使用python内置的一个wsgiref模块，来演示一下，如何整体运行一个wsgi的web服务。\n\n这里的代码分为hello.py和server.py两个部分，hello.py是我们根据wsgi的规范来书写的app应用函数，上面介绍过，这个app应用程序主要是根据请求的environ字典信息，返回header 头信息和body信息。\n\n而server.py则是负责启动指定端口的wsgi服务器，在这个程序中会导入上面我们书写的app应用函数，作为http的返回header和返回body。\n\n示例1：\n\nhello.py\n\ndef application(environ, start_response):\n    start_response('200 ok', [('content-type', 'text/html')])\n    return [b'<h1>hello, web!</h1>']\n\n\n1\n2\n3\n\n\nserver.py\n\n# server.py\n# 从wsgiref模块导入\nfrom wsgiref.simple_server import make_server\n# 导入我们自己编写的application函数\nfrom hello import application\n\n# 创建一个服务器，ip地址为空，端口是8000，处理函数是application:\nhttpd = make_server('', 8000, application)\nprint('serving http on port 8000 ...')\n# 开始监听http请求：\nhttpd.serve_forever()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n示例2：\n\nhello.py\n\ndef application(environ, start_response):\n    start_response('200 ok', [('content-type', 'text/heml')])\n    body = '<h1>hello, %s!</h1>' % (environ['path_info'][1:] or 'web')\n    return [body.encode('utf-8')]\n# 这里environ['path_info'][1:]\n# 是对environ字典集合中'path_info'这个key的引用\n# 而[1:]是指'path_info'这个key对应的value还是一个序列\n# 这个序列是从下标1往后到最后一个元素的所有元素\n# 'path_info'是访问url中端口后面的上下文信息\n# 例如http://127.0.0.1:8000/text/tt 中\n# /text/tt是'path_info'中[0:]\n# [1:0]是text/tt\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nserver.py\n\n# server.py\n# 从wsgiref模块导入\nfrom wsgiref.simple_server import make_server\n# 导入我们自己编写的application函数\nfrom hello import application\n\n# 创建一个服务器，ip地址为空，端口是8000，处理函数是application:\nhttpd = make_server('', 8000, application)\nprint('serving http on port 8000 ...')\n# 开始监听http请求：\nhttpd.serve_forever()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 使用web框架\n\n这块的知识点不太好整理，还是从遵循为什么，是什么，怎么样的逻辑的来整理下。\n\n\n# 为什么要有web框架\n\n一个web框架的出现，肯定是为了解决我们现实的web开发中遇到的痛点而存在的。那么我们根据上面学习的wsgt接口的app应用开发来看，遇到的痛点是什么呢？\n\n静态网页(只有web服务器和静态html) -> wsgi层(动态网页，处理重复的接受请求，解析请求，回复请求等工作) -> web框架(基于wsgi进一步抽象，专注于一个函数处理一个url，而url到函数映射交给web框架来做)\n\nwsgi接口的app开发痛点：\n\n上面我们了解到了wsgi的框架的出现，是为了解决动态html的生产而出现的，换句话说，是为了解决开发动态网页，解决让wsgi层去处理重复的接收请求，解析请求，回复请求的繁琐的工作，而让app的应用专注于自己业务逻辑的开发而出现的。\n\n如果http请求，需要处理很多的不同的url(带着不同的上下文)，每个url可能还是不同的方法(get/post/put/delete)，如果还是用wsgi接口来处理的话，这种不同的url和不同方法的代码，维护起来非常麻烦，这个时候我们基于wsgi层做一个进一步的抽象，用web框架来处理这些复杂的映射。\n\n查看下面的例子中(wsgi处理不同的url，不同的方法)：\n\ndef application(environ, start_response):\n    method = environ['request_method']\n    path = environ['path_info']\n    if method=='get' and path=='/':\n        return handle_home(environ, start_response)\n    if method=='post' and path=='/signin':\n        return handle_signin(environ, start_response)\n    ...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n这样写下去的代码是无法维护的。原因在于wsgi提供的接口虽然比http接口高级了不少，但是和web app的处理函数比(处理函数由于上下文和方法的不同，比较复杂)，还是比较低级，我们需要在wsgi接口上能够进一步的抽象，让我们专注于用一个函数处理一个url，至于url到函数的映射，就交给了web框架来处理了。\n\n\n# 什么是web框架\n\n简单来说就是对wsgi接口上的进一步抽象，维护代码层面上url到函数的映射，这样我们写代码的时候就能专注于用一个函数处理一个url。\n\n\n# 如何使用web框架\n\n这里以flask这个web框架来介绍下如何开发一个web应用程序。\n\n# 准备flask环境\n\n可以使用pip来安装flask包\n\n$ pip install flask\n\n\n1\n\n\n也可以通过conda或者pycharm来创建一个flask的虚拟环境。\n\n# 规划一个web应用\n\n需要编写一个app.py，处理3个url，分别是：\n\nget /：首页，返回home;\n\nget /signin：登录页，显示登录表单；\n\npost /signin：处理登录表单，显示登录结果。\n\n同一个url/signin分别有get和post两种请求，映射到两个处理函数中。\n\nflask是通过python的装饰器在内部自动地把url和函数关联起来，所以，书写的代码如下：\n\nfrom flask import flask\nfrom flask import request\n\napp = flask(__name__)\n\n\n@app.route('/', methods=['get', 'post'])\ndef hello_world():\n    return 'hello world!'\n\n@app.route('/signin', methods=['get'])\ndef signin_form():\n    return ''' <form action=\"/signin\" method=\"post\">\n               <p><input name=\"username\"></p>\n               <p><input name=\"password\" type=\"password\"></p>\n               <p><button type=\"submit\">sign in</button></p>\n               </form>'''\n\n@app.route('/signin', methods=['post'])\ndef singin():\n    # 需要从request对象的form方法来读取表单内容：\n    if request.form['username']=='admin' and request.form['password']=='password':\n        return '<h3>hello, admin!</h3>'\n    return '<h3>bad username or password.</h3>'\n\n\nif __name__ == '__main__':\n    app.run()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n运行这个app.py，flask自带的server在端口5000上监听：\n\n当我们在浏览器中输入http://127.0.0.1:5000/signin，会显示登录表单：\n\n输入用户名admin和密码password，登录成功；\n\n输入其他错误的用户名和口令，则登录失败；\n\n\n# 使用模板\n\n按照之前的逻辑，还是先阐述下为什么，是什么，怎么样的。\n\n\n# 为什么需要模板\n\n根据我们的经验，模板的出现，也是为了解决web开发中出现的一些痛点而存在的。\n\n我们从wsgi层中应用函数的编写，到现在使用web框架，解决了不同的url，不同的方法的映射的问题。\n\n但是我们在实际的web app的开发中，不仅仅是单纯的处理逻辑，还需要展示给用户页面。我们在函数中返回一个包含html的字符串，简单的页面还是可以的，但是如果对于大量的html文档信息，我们是无法单纯的依靠python的字符串中正确的书写出来的。\n\nweb app最复杂的部分就在html页面。html不仅要正确，还要通过css美化，再加上复杂的javascript脚本来实现各种交互和动画效果。所以手动书写生成html页面的难度很大。\n\n为了更快，更方便的生成html页面，而且还为了更大限度的重用html页面，这个时候模板的技术出现了。\n\n\n# 什么是模板技术\n\n使用模板，我们需要预先准备一个html文档，这个html文档不是普通的html，而是嵌入了一些变量和指令，然后，根据我们传入的数据，替换后，得到最终的html，发送给用户。\n\n这也就是我们所说的mvc：model - view - controller，\"模型-视图-控制器\"。通过mvc，我们在python代码中处理m：model和c：controller，而v：view是通过模板来处理的，这样，我们就可以把python代码和html代码最大限度地分离了。\n\n使用模板的另外一大好处在于，模板改起来很方便，而且，改完保存后，刷新浏览器就能看到最新的效果，这对于调试html、css和javascript的前端工程是太重要了。\n\n控制器：\n\n在python中处理url的函数就是c，controller，controller负责业务逻辑，比如检查用户名是否存在，取出用户信息等等；\n\n视图：\n\n也就是包含了变量的模板html文件，就是view，视图。view负责显示逻辑，通过简单地替换一些变量，view最终输出的就是用户看到的html。\n\n模型：\n\n在mvc中的mode，模型。是用来传给view值的，用来替换view中变量。也就是说从model中取出相应的数据，去替换view中的变量。一般情况下，是我们请求url中一个上下文信息，或者是相应的head信息，或body信息中的内容。是存在一个dict字典中的。\n\n处理逻辑如下：\n\n\n\n\n# 如何使用模板\n\n# 安装jinja2\n\nflask默认支持的模板是jinja2，需要先安装jinja2\n\n$ pip install jinja2\n\n\n1\n\n\n# 准备模板的html\n\n所有的模板的html文件都需要放在正确的templates目录下，templates和app.py在同级目录下：\n\nhome.html用来显示首页的模板：\n\n<html>\n<head>\n  <title>home</title>\n</head>\n<body>\n  <h1 style=\"font-style:italic\">home</h1>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nform.html用来显示登录表单的模板：\n\n<html>\n<head>\n  <title>please sign in</title>\n</head>\n<body>\n  {% if message %}\n  <p style=\"color:red\">{{ message }}</p>\n  {% endif %}\n  <form action=\"/signin\" method=\"post\">\n    <legend>please sign in:</legend>\n    <p><input name=\"username\" placeholder=\"username\" value=\"{{ username }}\"></p>\n    <p><input name=\"password\" placeholder=\"password\" type=\"password\"></p>\n    <p><button type=\"submit\">sign in</button></p>\n  </form>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nsignin-ok.html用来登录成功的模板：\n\n<html>\n<head>\n  <title>welcome, {{ username }}</title>\n</head>\n<body>\n  <p>welcome, {{ username }}!</p>\n</body>\n</html>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n登录失败的模板，我们用form.html中加入一点条件判断，把这个html重用为登录失败的模板。\n\n\n# jinja2模板\n\n在jinja2模板中，我们用类似{{name}}表示一个需要替换的变量。很多时候，还需要循环、条件判断等指令语句，在jinja2中，用{%...%}表示指令。\n\n比如下面的循环输出页码：\n\n{% for i in page_list %}\n    <a href=\"/page/{{i}}\">{{i}}</a>\n{% endfor %}\n\n\n1\n2\n3\n\n\n如果page_list是一个list:[1,2,3,4,5],上面的模板将输出5个超链接。\n\n除了jinja2，常见的模板还有：\n\nmako: 用<%...%>和${xxx}的一个模板；\n\ncheetah：也是用<%...%>和${xxx}的一个模板；\n\ndjango：django是一站式框架，内置一个用<%...%>和${xxx}的一个模板。\n\n\n# 如何理解nginx/wsgi/flash之间的关系\n\n\n# 概述\n\n这里主要描述的是nginx，wsgi(或者uwsgi)，flask(或者django)，这几者之间的关系。\n\nnginx代表的是web服务器层，wsgi代表的是wsgi层，flask代表的是web框架层。\n\n总体来说，客户端从发送一个http请求到flask处理请求，分别经过了web服务器层，wsgi层，web框架层，这三个层次。不同的层次其作用也不同，下面简要介绍各层的作用。\n\n\n\n\n# web服务器层\n\n对应的是我们的nginx，对于传统的客户端-服务器架构，其请求的处理过程是，客户端向服务器发送请求，服务器接收请求并处理请求，然后给客户端返回响应。在这个过程中，服务器的作用是：\n\n 1. 接收请求\n\n 2. 处理请求\n\n 3. 返回响应\n\nweb服务器是一类特殊的服务器，其作用主要是接收http请求并返回响应。常见的web服务器有nginx，apache，iis等。\n\n\n# web框架层\n\nweb框架的作用主要是方便我们开发web应用程序，http请求的动态数据就是由web框架层来提供的。常见的web框架有flask，django等，如下的例子中，以flask为例，展示了web框架的作用：\n\nfrom flask import flask\napp = flask(__name__)\n\n@app.route('/hello')\ndef hello_world():\n    return 'hello world!'\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n以上简单的几行代码，就创建了一个web应用程序对象app。app监听机器所有ip的8080端口，接受用户的请求连接。\n\n我们知道，http协议使用url来定位资源，上面的程序会将路径/hello的请求交由hello_world()方法来处理，hello_world()方法返回'hello world!'字符串。\n\n对于web框架的使用者来说，他们并不关心如何接收http请求，也不关系如何将请求路由到具体方法处理并将响应结果返回给用户。web框架的使用者在大部分情况下，只需要关系如何实现业务的逻辑即可。\n\n\n# wsgi层\n\nwsgi不是服务器，也不是用于与程序交互的api，更不是真实的代码，wsgi只是一种接口，它只适用于python语言，其全称为web server gateway interface，定义了web服务器和web应用之间的接口规范。\n\n也就是说，只要web服务器和web应用都遵守wsgi协议，那么web服务器和web应用就可以随意的组合。\n\n下面的用例中展示的就是web服务器和web应用基于wsgi的协议，组合在了一起。\n\ndef application(env, start_response):\n    start_response('200 ok', [('content-type', 'text/html')])\n    return [b\"hello world\"]\n\n\n1\n2\n3\n\n\n这个application函数是web应用框架中定义的，这个函数有两个入参，一个参数是env，是一个字典，包含了类似于http_host，host_user_agent等从web服务器(nginx)中传入的http协议中的request headers；\n\n另外一个参数是start_response，这是一个由web服务器来实现的函数，从start这个字眼可以看出，是正式返回response body体之前的内容，这个方法start_reponse，接受两个参数，分别是status,response_headers，这个status，是http返回中的状态码信息，而response_headers，则是http返回中的头信息。response_headers是一组list表示的http header，所以外面用了中括号，里面的每个header用一个包含两个str的tuple元组来表示。这两个入参，都是我们web应用要自己实现来传入的。\n\napplication函数的作用是，设置http响应的状态码和content-type等头部信息，并返回响应的具体结果。\n\nps：上述代码就是一个完整的wsgi的应用，当一个支持wsgi的web服务器接收到客户端的请求后，便会调用这个application方法。wsgi层并不关心env，start_response这两个变量是如何实现的，就像在application里面做的，直接使用者两个变量即可。\n\n\n# 相关名词解释\n\n * uwsgi\n   \n   同wsgi一样也是一种协议，uwsgi服务器就是使用了uwsgi协议\n\n * uwsgi\n   \n   实现了uwsgi和wsgi两种协议的web服务器。注意uwsgi本质上也是一种web服务器，处于上面描述的三层结构中的web服务器层。\n\n * cgi\n   \n   通用网关接口，并不限于python语言，定义了web服务器是如何向客户端提供动态的内容。例如，规定了客户端如何将参数传递给web服务器，web服务器如何将参数传递给web应用，web应用如何将它的输出如何发送给客户端，等等。\n   \n   生产环境下的web应用都不实用cgi了，cgi进程(类似python解释器)针对每个请求创建，用完就抛弃，效率低下。wsgi正是为了替代cgi而出现的。\n\n# 总结\n\n这里总结下wsgi在web服务器和web框架之间的作用：wsgi就像一条纽带，将web服务器与web框架连接起来。\n\n以下面的三者之间的对话来理解：\n\nnginx：hey，wsgi，我刚收到了一个请求，我需要你作些准备，然后由flask来处理这个请求。 wsgi：ok，nginx。我会设置好环境变量，然后将这个请求传递给flask处理。 flask：thanks wsgi！给我一些时间，我将会把请求的响应返回给你。 wsgi：alright，那我等你。 flask：okay，我完成了，这里是请求的响应结果，请求把结果传递给nginx。 wsgi：good job！nginx，这里是响应结果，已经按照要求给你传递回来了。 nginx：cool，我收到了，我把响应结果返回给客户端。大家合作愉快！\n\n\n# 参考信息\n\n参考如下的url信息\n\n> https://blog.csdn.net/lihao21/article/details/52304119\n\n\n# 如何理解import和from import的区别\n\n在python中单单import的话是引入整个包，而from ... import ...是引入了某个包中某个函数或某个类。例子如下：\n\n# 引入了整个datetime包\nimport datetime\nprint(datetime.datetime.now())\n\n# 从datetime包中引入datetime这个类\nfrom datetime import datetime\nprint(datetime.now())\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 定义函数一定要有形参，一定要有返回值吗？\n\n细分是三个问题：函数定义一定要有形参吗；函数定义中一定要有返回值吗；函数调用的时候一定要有实参吗\n\n\n# 在python中点号.有哪些用法\n\nimport包中\n\n调用类中某个方法的时候\n\n\n# 如何写python函数注释\n\n需要注意的是，在书写的注释中，不管哪种形式写的，python对这些注释信息，不做检查，不做检查，不做验证！什么也不做！\n\n在python函数中，写注释大体有如下两种方法：\n\n\n# 方法一：使用\"\"\"xxxx\"\"\"的格式\n\n也就是说直接在函数内部，用三个双引号括起来的部分就是需要注释的内容。\n\n类似如下的例子，可以用help(函数名称) 的方法来查看注释信息。\n\ndef square_sum(a,b):\n    \"\"\" \n    return the square\n    sum of two arguments\n    \"\"\"\n    a = a**2\n    b = b**2\n    c = a + b\n    return c\n\nhelp(square_sum)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 谷歌风格的注释\n\ndef foo():\n    \"\"\" \n    this is a group style docs.\n    \n    parameters:\n        param1 - this is the first param\n        param2 - this is a second param\n    \n    returns:\n        this is a description of what is returned\n    \n    raises:\n        keyerror - raises an exception\n    \"\"\"\n\nhelp(foo)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# rest风格\n\ndef chuck():\n    \"\"\"\n    this is a rest style.\n\n    :param param1: this is the first param\n    :param param1: this is a second param\n    :returns: this is a description of what is returned\n    :raises keyerror: raises an exception\n    \"\"\"\n\nhelp(chuck)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 方法二：定义参数的时候加入注释\n\n通常会在定义的函数的参数后面加入\":\"冒号，写上一个建议的参数的数据类型。如果很长可以加上单引号。\n\n这些注释信息都是函数的元信息，保存在f.___annotations__字典中。\n\n需要注意的是，python对注释信息和f.__annotations__的一致性，不做检查，不做强制，不做验证！什么都不做。\n\n定义完参数后用->一个参数类型，来表示建议返回的值的类型。\n\ndef f(text:str, max_len:'int>0'=80) ->str:\n    \"\"\"这个是函数的说明文档，help的时候会显示\"\"\"\n    return true\n\n\"\"\"\n函数声明中，text是参数名称，：冒号的str是参数的注释\n如果参数有默认值，还要加上注释，那么就在默认值的\"=\"等号前写上\n如'int>0'就是对默认值max_len=80的注释\n->str是函数返回值的注释\n\"\"\"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 如何写python类的注释\n\n\n# 如何利用pyecharts库来画图表\n\n\n# 安装pyecharts包\n\n$ pip install pyecharts\n\n\n1\n\n\n\n# 折线图的示例\n\n下面的例子中，有两条折线，定义了主标题，副标题，加入了工具栏，加入了x轴方向的滚动轴。\n\nimport pyecharts.options as opts\nfrom pyecharts.faker import  faker\nfrom pyecharts.charts import line\ndef line_base() -> line:\n    c = (\n        line()\n        .add_xaxis(faker.choose())\n        .add_yaxis(\"商家a\", faker.values())\n        .add_yaxis(\"商家b\", faker.values())\n        .set_global_opts(\n            title_opts={\n                \"text\": \"line的测试\",\n                \"subtext\": \"副标题\",\n                },\n            toolbox_opts=opts.toolboxopts(),\n            # legend_opts=opts.legendopts(is_show=false),\n            datazoom_opts=opts.datazoomopts(),\n        )\n    )\n    return c\n\nline_base().render()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n最终显示的样式如下：\n\n\n\n\n# 柱形图示例\n\n下面的图示中，有两种类型的柱形图。加入了工具栏，x轴方向的滚动轴，主副标题。\n\nfrom pyecharts.faker import faker\nfrom pyecharts import options as opts\nfrom pyecharts.charts import bar\nfrom pyecharts.commons.utils import jscode\ndef bar_base() -> bar:\n    c = (\n        bar()\n        .add_xaxis(faker.choose())\n        .add_yaxis(\"商家a\", faker.values())\n        .add_yaxis(\"商家b\", faker.values())\n        .set_global_opts(\n            title_opts={\n                \"text\": \"bar的测试\",\n                \"subtext\": \"副标题\",\n                },\n            toolbox_opts=opts.toolboxopts(),\n            # legend_opts=opts.legendopts(is_show=false),\n            datazoom_opts=opts.datazoomopts(),\n        )\n    )\n    return c\n\nbar_base().render()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n最终显示的图示如下：\n\n\n\n\n# conda channel的镜像设置\n\n\n# 显示所有channel\n\n输入conda config --show能够显示出所有的config的信息\n\n输入conda config --show channels 显示的是所有channels信息\n\n\n# 移除镜像源\n\n这里可以移除掉上面 显示出来的镜像源的信息。\n\nconda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\n\n\n# 添加可用的清华镜像源\n\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --set show_channel_urls yes\n\n\n1\n2\n3\n\n\n其中conda config --set show_channel_urls yes 意思是从channel中安装包时显示channel的url，这样就可以知道包的安装来源了。\n\n\n# 关闭安装过程中默认yes\n\n默认使用conda install xxx的时候，是不需要输入yes or no的，conda直接跳过了，默认是yes，如果需要自己加以判断，如下设置。\n\nconda config --set always_yes false\n\n\n# 一些其他的conda指令\n\nconda install <包名> 安装指定包 conda remove <包名> 移除指定包 conda update <包名> 更新指定包\n\n\n# vscode配置 python\n\nhttps://zhuanlan.zhihu.com/p/88458083\n\nhttps://www.jianshu.com/p/ef1ae10ba950\n\nhttps://blog.csdn.net/joson1234567890/article/details/105134711\n\nhttps://blog.csdn.net/qq_38875402/article/details/107141964\n\n\n# python安装psycopg2包\n\n\n# 环境准备\n\n由于python在centos7下连接postgresql数据库报错:\n\npython scram authentication requires libpq version 10 or above\n\n大概意思是libpg的版本低了，但使用 yum install postgresql-devel 只能更新到 9.2.24版本\n\n需要安装高版本的postgresql相关的rpm 包\n\n 1. 添加pg包的源\n    \n    rpm -uvh https://download.postgresql.org/pub/repos/yum/reporpms/el-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm\n\n 2. 下载pg包，准备离线安装\n    \n    yumdownloader --destdir ./ --resolve postgresql10 postgresql10-dev postgresql10-libs\n\n 3. 生产环境中安装rpm包\n    \n    rpm -ivh postgresql10*.rpm\n\n 4. 查找pg_config的位置\n    \n    find / -type f -name \"pg_config\"\n\n 5. 先删除/usr/bin下的pg_config\n\n 6. 建立软链接\n    \n    ln -s /usr/pgsql-10/bin/pg_config pg_config\n\n 7. 查看pg_config版本\n    \n    #pg_config --version\n    \n    postgresql 10.18\n    \n    \n    1\n    2\n    3\n    \n\n参考url：\n\n下载rpm包的两种方法\n\npython scram authentication requires libpq version 10 or above\n\n\n# 源码安装psycopg2\n\n 1. 下载psycopg2\n    \n    pip download -d /usr/local/download/pip/ psycopg2\n\n 2. 离线安装\n    \n    tar -xzvf psycopg2-2.9.3.tar.gz\n    cd psycopg2-2.9.3\n    python setup.py build\n    python setup.py install\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 3. 测试psycopg2\n    \n    python\n    > import psycopg2\n    \n    \n    1\n    2\n    \n\n离线安装psycopg2",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"《从Python开始学编程》读书笔记",frontmatter:{title:"《从Python开始学编程》读书笔记",date:"2022-01-20T14:24:13.000Z",permalink:"/pages/63a425/",categories:["编程","Python"],tags:[null]},regularPath:"/03.%E7%BC%96%E7%A8%8B/10.Python/03.%E3%80%8A%E4%BB%8EPython%E5%BC%80%E5%A7%8B%E5%AD%A6%E7%BC%96%E7%A8%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html",relativePath:"03.编程/10.Python/03.《从Python开始学编程》读书笔记.md",key:"v-01596c50",path:"/pages/63a425/",headers:[{level:2,title:"先做键盘侠",slug:"先做键盘侠",normalizedTitle:"先做键盘侠",charIndex:103},{level:2,title:"运算符",slug:"运算符",normalizedTitle:"运算符",charIndex:113},{level:3,title:"数值运算",slug:"数值运算",normalizedTitle:"数值运算",charIndex:121},{level:3,title:"逻辑运算",slug:"逻辑运算",normalizedTitle:"逻辑运算",charIndex:362},{level:3,title:"判断表达式",slug:"判断表达式",normalizedTitle:"判断表达式",charIndex:392},{level:3,title:"运算优先级",slug:"运算优先级",normalizedTitle:"运算优先级",charIndex:579},{level:2,title:"数据结构",slug:"数据结构",normalizedTitle:"数据结构",charIndex:679},{level:3,title:"变量的类型",slug:"变量的类型",normalizedTitle:"变量的类型",charIndex:688},{level:3,title:"序列",slug:"序列",normalizedTitle:"序列",charIndex:1060},{level:3,title:"词典",slug:"词典",normalizedTitle:"词典",charIndex:1075},{level:2,title:"IF",slug:"if",normalizedTitle:"if",charIndex:3073},{level:3,title:"if结构",slug:"if结构",normalizedTitle:"if结构",charIndex:3080},{level:3,title:"if结构中的缩进",slug:"if结构中的缩进",normalizedTitle:"if结构中的缩进",charIndex:3494},{level:3,title:"if的嵌套与elif",slug:"if的嵌套与elif",normalizedTitle:"if的嵌套与elif",charIndex:3676},{level:2,title:"循环",slug:"循环",normalizedTitle:"循环",charIndex:4200},{level:3,title:"for循环",slug:"for循环",normalizedTitle:"for循环",charIndex:4247},{level:3,title:"while循环",slug:"while循环",normalizedTitle:"while循环",charIndex:4646},{level:3,title:"跳出或终止",slug:"跳出或终止",normalizedTitle:"跳出或终止",charIndex:4780},{level:2,title:"代码规范",slug:"代码规范",normalizedTitle:"代码规范",charIndex:5036},{level:2,title:"过程大于结果",slug:"过程大于结果",normalizedTitle:"过程大于结果",charIndex:5507},{level:2,title:"函数",slug:"函数",normalizedTitle:"函数",charIndex:74},{level:3,title:"函数是什么",slug:"函数是什么",normalizedTitle:"函数是什么",charIndex:5630},{level:3,title:"定义函数",slug:"定义函数",normalizedTitle:"定义函数",charIndex:5787},{level:3,title:"调用函数",slug:"调用函数",normalizedTitle:"调用函数",charIndex:6298},{level:3,title:"函数文档",slug:"函数文档",normalizedTitle:"函数文档",charIndex:6493},{level:2,title:"参数传递",slug:"参数传递",normalizedTitle:"参数传递",charIndex:6396},{level:3,title:"基本传参",slug:"基本传参",normalizedTitle:"基本传参",charIndex:6771},{level:3,title:"包裹传参",slug:"包裹传参",normalizedTitle:"包裹传参",charIndex:7597},{level:3,title:"解包裹",slug:"解包裹",normalizedTitle:"解包裹",charIndex:8468},{level:2,title:"递归",slug:"递归",normalizedTitle:"递归",charIndex:8924},{level:3,title:"为什么要用递归",slug:"为什么要用递归",normalizedTitle:"为什么要用递归",charIndex:8931},{level:3,title:"什么是递归",slug:"什么是递归",normalizedTitle:"什么是递归",charIndex:9028},{level:3,title:"如何写递归",slug:"如何写递归",normalizedTitle:"如何写递归",charIndex:9194},{level:3,title:"函数栈",slug:"函数栈",normalizedTitle:"函数栈",charIndex:9630},{level:3,title:"变量的作用域",slug:"变量的作用域",normalizedTitle:"变量的作用域",charIndex:10310},{level:2,title:"模块",slug:"模块",normalizedTitle:"模块",charIndex:5565},{level:3,title:"为什么要有模块",slug:"为什么要有模块",normalizedTitle:"为什么要有模块",charIndex:11056},{level:3,title:"什么是模块",slug:"什么是模块",normalizedTitle:"什么是模块",charIndex:11130},{level:3,title:"如何使用模块",slug:"如何使用模块",normalizedTitle:"如何使用模块",charIndex:11210},{level:3,title:"搜索路径",slug:"搜索路径",normalizedTitle:"搜索路径",charIndex:11713},{level:2,title:"异常处理",slug:"异常处理",normalizedTitle:"异常处理",charIndex:11827},{level:3,title:"异常分类",slug:"异常分类",normalizedTitle:"异常分类",charIndex:11836},{level:3,title:"异常处理",slug:"异常处理-2",normalizedTitle:"异常处理",charIndex:11827},{level:2,title:"朝思暮想是对象",slug:"朝思暮想是对象",normalizedTitle:"朝思暮想是对象",charIndex:13295},{level:2,title:"类",slug:"类",normalizedTitle:"类",charIndex:691},{level:3,title:"什么是类",slug:"什么是类",normalizedTitle:"什么是类",charIndex:13459},{level:3,title:"如何定义类",slug:"如何定义类",normalizedTitle:"如何定义类",charIndex:13534},{level:3,title:"定义类的方法",slug:"定义类的方法",normalizedTitle:"定义类的方法",charIndex:13747},{level:2,title:"对象",slug:"对象",normalizedTitle:"对象",charIndex:69},{level:3,title:"什么是对象",slug:"什么是对象",normalizedTitle:"什么是对象",charIndex:14161},{level:3,title:"展示对象的属性和方法",slug:"展示对象的属性和方法",normalizedTitle:"展示对象的属性和方法",charIndex:14230},{level:3,title:"定义对象的个性属性",slug:"定义对象的个性属性",normalizedTitle:"定义对象的个性属性",charIndex:14529},{level:2,title:"子类",slug:"子类",normalizedTitle:"子类",charIndex:15720},{level:3,title:"什么是子类",slug:"什么是子类",normalizedTitle:"什么是子类",charIndex:15727},{level:2,title:"属性覆盖",slug:"属性覆盖",normalizedTitle:"属性覆盖",charIndex:16472},{level:3,title:"子类中调用父类中被覆盖的方法",slug:"子类中调用父类中被覆盖的方法",normalizedTitle:"子类中调用父类中被覆盖的方法",charIndex:16968},{level:2,title:"回头再看对象",slug:"回头再看对象",normalizedTitle:"回头再看对象",charIndex:17539},{level:3,title:"列表对象",slug:"列表对象",normalizedTitle:"列表对象",charIndex:17639},{level:3,title:"元组与字符串对象",slug:"元组与字符串对象",normalizedTitle:"元组与字符串对象",charIndex:18587},{level:3,title:"词典对象",slug:"词典对象",normalizedTitle:"词典对象",charIndex:19963},{level:2,title:"意想不到的对象",slug:"意想不到的对象",normalizedTitle:"意想不到的对象",charIndex:20451},{level:3,title:"循环对象",slug:"循环对象",normalizedTitle:"循环对象",charIndex:20463},{level:3,title:"函数对象",slug:"函数对象",normalizedTitle:"函数对象",charIndex:21410},{level:3,title:"模块对象",slug:"模块对象",normalizedTitle:"模块对象",charIndex:21839},{level:3,title:"异常对象",slug:"异常对象",normalizedTitle:"异常对象",charIndex:22951},{level:2,title:"对象带你飞",slug:"对象带你飞",normalizedTitle:"对象带你飞",charIndex:23328},{level:2,title:"存储",slug:"存储",normalizedTitle:"存储",charIndex:2599},{level:3,title:"文件(文本对象)",slug:"文件-文本对象",normalizedTitle:"文件(文本对象)",charIndex:23345},{level:3,title:"上下文管理器",slug:"上下文管理器",normalizedTitle:"上下文管理器",charIndex:24318},{level:3,title:"pickle包",slug:"pickle包",normalizedTitle:"pickle包",charIndex:24984},{level:2,title:"时间类的包",slug:"时间类的包",normalizedTitle:"时间类的包",charIndex:26993},{level:3,title:"time包",slug:"time包",normalizedTitle:"time包",charIndex:27218},{level:3,title:"datetime包",slug:"datetime包",normalizedTitle:"datetime包",charIndex:28396},{level:3,title:"日期格式",slug:"日期格式",normalizedTitle:"日期格式",charIndex:29286},{level:2,title:"正则表达式",slug:"正则表达式",normalizedTitle:"正则表达式",charIndex:29718},{level:3,title:"什么是正则表达式",slug:"什么是正则表达式",normalizedTitle:"什么是正则表达式",charIndex:29728},{level:3,title:"如何使用正则表达式",slug:"如何使用正则表达式",normalizedTitle:"如何使用正则表达式",charIndex:29883},{level:3,title:"正则表达式有哪些",slug:"正则表达式有哪些",normalizedTitle:"正则表达式有哪些",charIndex:30688},{level:3,title:"利用group来查看正则表达搜索结果",slug:"利用group来查看正则表达搜索结果",normalizedTitle:"利用group来查看正则表达搜索结果",charIndex:30749},{level:3,title:"给group命名",slug:"给group命名",normalizedTitle:"给group命名",charIndex:31058},{level:2,title:"Python的网络访问",slug:"python的网络访问",normalizedTitle:"python的网络访问",charIndex:31266},{level:3,title:"HTTP通信简介",slug:"http通信简介",normalizedTitle:"http通信简介",charIndex:31282},{level:3,title:"http.client包",slug:"http-client包",normalizedTitle:"http.client包",charIndex:31732},{level:2,title:"与对象的深入交往",slug:"与对象的深入交往",normalizedTitle:"与对象的深入交往",charIndex:32140},{level:2,title:"一切皆对象",slug:"一切皆对象",normalizedTitle:"一切皆对象",charIndex:13441},{level:3,title:"运算符",slug:"运算符-2",normalizedTitle:"运算符",charIndex:113},{level:3,title:"元素引用",slug:"元素引用",normalizedTitle:"元素引用",charIndex:32166},{level:3,title:"内置函数的实现",slug:"内置函数的实现",normalizedTitle:"内置函数的实现",charIndex:33091},{level:2,title:"属性管理",slug:"属性管理",normalizedTitle:"属性管理",charIndex:33222},{level:3,title:"属性覆盖的背后",slug:"属性覆盖的背后",normalizedTitle:"属性覆盖的背后",charIndex:33231},{level:3,title:"特性",slug:"特性",normalizedTitle:"特性",charIndex:8954},{level:3,title:"_getattr_()方法",slug:"getattr-方法",normalizedTitle:"<em>getattr</em>()方法",charIndex:null},{level:2,title:"我是风儿，我是沙",slug:"我是风儿-我是沙",normalizedTitle:"我是风儿，我是沙",charIndex:34903},{level:3,title:"动态类型",slug:"动态类型",normalizedTitle:"动态类型",charIndex:716},{level:3,title:"可变与不可变对象",slug:"可变与不可变对象",normalizedTitle:"可变与不可变对象",charIndex:35522},{level:3,title:"从动态类型引用角度看函数的参数传递",slug:"从动态类型引用角度看函数的参数传递",normalizedTitle:"从动态类型引用角度看函数的参数传递",charIndex:35711},{level:2,title:"内存管理",slug:"内存管理",normalizedTitle:"内存管理",charIndex:36032},{level:3,title:"引用管理",slug:"引用管理",normalizedTitle:"引用管理",charIndex:36041},{level:3,title:"对象引用对象",slug:"对象引用对象",normalizedTitle:"对象引用对象",charIndex:36367},{level:3,title:"垃圾回收",slug:"垃圾回收",normalizedTitle:"垃圾回收",charIndex:32278},{level:3,title:"孤立的引用环",slug:"孤立的引用环",normalizedTitle:"孤立的引用环",charIndex:37599},{level:2,title:"函数式编程",slug:"函数式编程",normalizedTitle:"函数式编程",charIndex:37937},{level:2,title:"又见函数",slug:"又见函数",normalizedTitle:"又见函数",charIndex:38103},{level:3,title:"Python中的函数式",slug:"python中的函数式",normalizedTitle:"python中的函数式",charIndex:38112},{level:3,title:"并行运算",slug:"并行运算",normalizedTitle:"并行运算",charIndex:39064},{level:2,title:"被解放的函数",slug:"被解放的函数",normalizedTitle:"被解放的函数",charIndex:39583},{level:3,title:"函数作为参数",slug:"函数作为参数",normalizedTitle:"函数作为参数",charIndex:39594},{level:3,title:"函数作为返回值",slug:"函数作为返回值",normalizedTitle:"函数作为返回值",charIndex:40021},{level:3,title:"闭包",slug:"闭包",normalizedTitle:"闭包",charIndex:40612},{level:2,title:"小女子的梳妆匣",slug:"小女子的梳妆匣",normalizedTitle:"小女子的梳妆匣",charIndex:41743},{level:3,title:"装饰器",slug:"装饰器",normalizedTitle:"装饰器",charIndex:41755},{level:3,title:"带参装饰器",slug:"带参装饰器",normalizedTitle:"带参装饰器",charIndex:43333},{level:3,title:"装饰类",slug:"装饰类",normalizedTitle:"装饰类",charIndex:44194},{level:2,title:"高阶函数",slug:"高阶函数",normalizedTitle:"高阶函数",charIndex:38491},{level:3,title:"lambda与map",slug:"lambda与map",normalizedTitle:"lambda与map",charIndex:45253},{level:3,title:"filter函数",slug:"filter函数",normalizedTitle:"filter函数",charIndex:46418},{level:3,title:"reduce函数",slug:"reduce函数",normalizedTitle:"reduce函数",charIndex:46870},{level:3,title:"并行处理",slug:"并行处理",normalizedTitle:"并行处理",charIndex:39187},{level:2,title:"自上而下",slug:"自上而下",normalizedTitle:"自上而下",charIndex:38524},{level:3,title:"便携表达式",slug:"便携表达式",normalizedTitle:"便携表达式",charIndex:47546},{level:3,title:"懒惰求值",slug:"懒惰求值",normalizedTitle:"懒惰求值",charIndex:48097},{level:3,title:"itertools包",slug:"itertools包",normalizedTitle:"itertools包",charIndex:48268}],headersStr:"先做键盘侠 运算符 数值运算 逻辑运算 判断表达式 运算优先级 数据结构 变量的类型 序列 词典 IF if结构 if结构中的缩进 if的嵌套与elif 循环 for循环 while循环 跳出或终止 代码规范 过程大于结果 函数 函数是什么 定义函数 调用函数 函数文档 参数传递 基本传参 包裹传参 解包裹 递归 为什么要用递归 什么是递归 如何写递归 函数栈 变量的作用域 模块 为什么要有模块 什么是模块 如何使用模块 搜索路径 异常处理 异常分类 异常处理 朝思暮想是对象 类 什么是类 如何定义类 定义类的方法 对象 什么是对象 展示对象的属性和方法 定义对象的个性属性 子类 什么是子类 属性覆盖 子类中调用父类中被覆盖的方法 回头再看对象 列表对象 元组与字符串对象 词典对象 意想不到的对象 循环对象 函数对象 模块对象 异常对象 对象带你飞 存储 文件(文本对象) 上下文管理器 pickle包 时间类的包 time包 datetime包 日期格式 正则表达式 什么是正则表达式 如何使用正则表达式 正则表达式有哪些 利用group来查看正则表达搜索结果 给group命名 Python的网络访问 HTTP通信简介 http.client包 与对象的深入交往 一切皆对象 运算符 元素引用 内置函数的实现 属性管理 属性覆盖的背后 特性 _getattr_()方法 我是风儿，我是沙 动态类型 可变与不可变对象 从动态类型引用角度看函数的参数传递 内存管理 引用管理 对象引用对象 垃圾回收 孤立的引用环 函数式编程 又见函数 Python中的函数式 并行运算 被解放的函数 函数作为参数 函数作为返回值 闭包 小女子的梳妆匣 装饰器 带参装饰器 装饰类 高阶函数 lambda与map filter函数 reduce函数 并行处理 自上而下 便携表达式 懒惰求值 itertools包",content:'# 《从Python开始学编程》读书笔记\n\n本书以Python为样本，不仅介绍了编程的基本概念，还着重讲解了编程语言的范式(面向过程、面向对象、面向函数)，并把编程语言的范式糅合在Python中。\n\n\n# 先做键盘侠\n\n\n# 运算符\n\n\n# 数值运算\n\n具体的算术如下：\n\n>>> 1 + 9\n>>> 1.3 - 4\n>>> 3*5\n>>> 4.5/1.5\n>>>3**2            #乘方，即求3的二次方\n>>>10%3            #求余数，就求10除以3的余数。结果为1\n>>>"vamei say:" + "Hello World" #字符串加法运算，也就是把字符串进行连接\n>>>"Vamei"*2       #结果为"VameiVamei"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 逻辑运算\n\nand 与 / or 或 /not 非\n\n\n# 判断表达式\n\n>>>1 == 1          # 相等\n>>>8.0 != 8.0      # 不等于\n>>>4 < 5           # <, 小于\n>>>3 <= 3          # <=，小于或等于\n>>>4 > 5           # >，大于\n>>>4 >=0           # >=,大于等于\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 运算优先级\n\n运算符的优先级高低如下所示：\n\n乘方：**\n乘除: *  /\n加减: + -\n判断: ==   >  >=  <  <=\n逻辑: !  and  or\n\n\n1\n2\n3\n4\n5\n\n\n\n# 数据结构\n\n\n# 变量的类型\n\npython能自由改变变量类型的特征被称为动态类型。并不是所有语言都支持动态类型。\n\n在静态类型的语言中，变量有事先说明好的类型。特定类型的数据必须存入特定类型的变量。\n\n变量有int整型、浮点数float、字符串string、布尔值bool。\n\n动态类型的语言看起来不需要说明类型，但其实是把区分类型的工作交给解释器。当改变变量的值的时候，python解释器会自动分辨出新数据的类型，再为数据开辟相应类型的内存空间。\n\nPython解释器的贴心的服务让编程更加方便，但也把计算机的一部分能力用于支持动态类型上。这也是Python的速度不如C语言等静态类型语言的一个原因。\n\n可以使用type()这一函数来查看变量的类型\n\nvar_integer = 10\nprint(type(var_integer))\n\n\n1\n2\n\n\n\n# 序列\n\nPython中的序列和词典，都是特殊类型的变量。能像一个容器一样，都是容器型变量，收纳多个数据。\n\n序列是有顺序的数据集合，就好像一列排好队的士兵。序列包含的一个数据被称为序列的一个元素。序列可以包含一个或多个元素，也可以是完全没有任何元素的空序列。\n\n# 元组和列表\n\n序列有两种，元组(tuple)和列表(list)。两者的主要区别在于，一旦建立，元组的各个元素不可再变更，而列表元素可以变更。\n\n所以，元组看起来就像一种特殊的表，有固定的数据。\n\n>>> example_tuple = (2, 1.3, "love", 5.6, False)  #一个元组，括号\n>>> example_list = [True, 5, "smile"]             #一个列表，中括号\n>>> example_list1 = [1, [3,4,5]]                  #列表中嵌套另一个列表\n>>> type(example_tuple)                           #结果为\'tuple\'\n>>> type(example_list)                            #结果为\'list\'\n\n\n1\n2\n3\n4\n5\n\n\n# 序列下标\n\n序列元素的位置索引称为下标，可以通过下标来找到序列中的对应的元素。\n\nPython中序列的下标从0开始，即第一个元素的对应下标为0。\n\n>>> example_tuple[0]          #显示序列example_tuple元组中第一个元素\n>>> nest_list = [1,[3,4,5]]   #列表中嵌套另一个列表\n>>> nest_list[1][2]           #显示的是第二个元素中，第二个元素是一个嵌套的列表，该嵌套列表中的第三个元素\n\n\n1\n2\n3\n\n\n元组一旦建立就不能改变，所以不能对元组的元素进行赋值操作。\n\n# 序列中范围引用\n\n序列中可以通过范围引用，来找到多个元素。范围引用的基本样式是：\n\n序列名[下限:上限:步长]\n\n下限表示起始下标，上限表示结尾下标。在起始下标和结尾下标之间，按照步长的间隔来找到元素。默认的步长是1。\n\n>>> example_tuple[:5]             # 从下标0到下标4，不包含下标5的元素\n>>> example_tuple[2:]             # 从下标2到最后一个元素\n>>> example_tuple[0:5:2]          # 下标为0，2，4的元素\n>>> sliced =  example_tuple[2:0:-1] # 从下标2到下标1\n>>> type(sliced)                  # 范围引用的结果还是一个元组\n>>> example_tuple[-1]             # 序列最后一个元素\n>>> example_tuple[-3]             # 序列倒数第三个元素\n>>> example_tuple[1:-1]           # 序列的第二个到倒数第二个元素\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 词典\n\n词典从很多方面都和序列中的列表比较相似。同样是一个可以容纳多个元素的容器。但是词典不是以位置来作为索引的。词典允许用自定义的方式来建立数据的索引。\n\n词典包含多个元素，每个元素以逗号分隔。词典的元素包含两部分，键(key)和值(value)。键是数据的索引，值是数据本身。\n\n词典的元素可以通过键来引用。\n\n词典不具备序列那样的连续有序性，所以适用于存储结构松散的一组数据。\n\n大部分场景中，我们都是用字符串来作为词典的键。其他类型，例如数字和布尔值，也可以作为词典的键值。\n\n>>> example_dict = {"tom":11, "sam":57, "lily":100}\n>>> type(example_dict)                                    #结果为\'dict\'\n>>> example_dict["tom"]                                   #结果为11，引用\n>>> example_dict["tom"] = 30                              #修改一个元素值\n>>> example_dict["lilei"] = 99                            #添加一个元素值\n>>> example_dict                                          #查看所有的元素值\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# IF\n\n\n# if结构\n\n关键字if和else分别有隶属于它们的一行或多行代码，从属代码的开头会有四个空格的缩进。程序最终会根据if后的条件是否成立，选择是执行if的从属代码，还是执行else的从属代码。if结构在程序中实现了分支。\n\nelse也并非必须的，可以写只有if的程序；没有else，实际上与空的else等价。如果if后的条件不成立，那么计算机什么都不用执行。\n\ntotal = 980000\nif total > 500000:\n    print ("总价超过了50万")\n    transaction_rate = 0.01\nelse:\n    print ("总价不超过50W")\n    transaction_rate = 0.02\n    \ntotal = 980000\nif total > 500000:\n    print ("总价超过50W")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# if结构中的缩进\n\n用缩进来表明代码的从属关系，是Python的特色。\n\n在python中，去掉了if条件后面i>0周围的括号，去除了每个语句句尾的分号，表示块的花括号也消失了。多出来了if...之后的:(冒号)，还有就是从属代码前面有四个空格的缩进。通过缩进，Python识别出这两个语句是隶属于if的。为了区分隶属关系，python中的缩进是强制的。\n\n\n# if的嵌套与elif\n\nelif的定义：\n\n我们可以在if和else之间增加多个elif，从而给程序开出更多的分支。\n\nelif说白了就是在if后面的增加的，另外的多个条件，多个选择的分支。\n\n在出现if、elif和else三个块的代码中。Python先检测if的条件，如果发现if的条件为假，则跳过隶属于if的程序块，检测elif的条件；如果elif的条件还是假的，则执行else块。程序会根据跳进，只执行三个分支中的一个。\n\nif i > 0:\n    i = i + 1\nelif i == 0:\n    i = i*10\nelse:\n    i = i - 1\n\n\n1\n2\n3\n4\n5\n6\n\n\nif的嵌套：\n\n意思是一个if结构中嵌套了另一个if结构，意思是进入一个选择分支后，又进入了一个选择分支。同样涉及到if的从属语句，要和if空4个空格的缩进。\n\n在进行完第一个if判断后，如果条件成立，那么程序依次运行，会遇到第二个if结构。程序将继续根据条件选择并决定是否执行。\n\nif i > 1:\n    print("good")\n    if i > 2:\n        print("better")\n\n\n1\n2\n3\n4\n\n\n\n# 循环\n\n循环用于重复执行一些代码块，在Python中，循环有for和while两种。\n\n\n# for循环\n\n属于循环结构的、需要重复的程序，也要同if结构一样，要被缩进四个空格。\n\nfor a in [3,4.4,"life"]:\n    print(a)\n\n\n1\n2\n\n\n# for循环基本用法一\n\nfor的一个基本用法是在in后面跟一个序列，序列中元素的个数决定了循环重复的次数。\n\n从序列中取出元素，再赋予给一个变量并在隶属程序中使用。\n\nfor 元素 in 序列:\n    statement\n\n\n1\n2\n\n\n# for循环基本用法二\n\n有时候，我只是想简单重复特定的次数，不想建立序列，我们可以使用Python提供的range()函数,range中间的数字就是说明了需要重复的次数。需要注意的是，range()提供的计数也是从0开始的，和序列列表中的下标一样。\n\nfor i in range(5):\n    print(i, "Hello World!")\n\n\n1\n2\n\n\n\n# while循环\n\nwhile后面紧跟着一个条件。如果条件为真，则while会不停地循环执行隶属于它的语句。只有条件为假的时候，程序才会停止。\n\ni = 0\nwhile i < 10:\n    print(i)\n    i = i + 1\n\n\n1\n2\n3\n4\n\n\n\n# 跳出或终止\n\n循环结构还提供了两个有用的语句，可以在循环结构内部使用，用于跳出或终止循环。\n\ncontinue用于跳出循环的这一次循环，进行下一次的循环操作。\n\nbreak则是停止执行整个循环。\n\nfor i in range(10):\n    if i == 2:\n        continue\n    print(i)\n    \nfor i in range(10):\n    if i == 2:\n        break\n    print(i)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 代码规范\n\n由于强制缩进的规定，Python代码看起来相对比较整齐。\n\nPython官方文档中提供了一套代码规范，即PEP8。\n\n 1. 在下列运算符的前后各保留一个空格\n    \n    = + - > == >= < <= and or not\n    \n    \n    1\n    \n\n 2. 下列运算符的前后不用保留空格\n    \n    * / **\n    \n    \n    1\n    \n\n 3. 如果有多行赋值，那么将上下的赋值号=对齐\n    \n    num      = 1\n    secNum   = 2\n    \n    \n    1\n    2\n    \n\n 4. 变量的所有字母小写，单词之间用下画线连接\n    \n    example_number = 10\n    \n    \n    1\n    \n\n 5. 类的命名采用首字母大写的英文单词。如果由多个单词连接而成，则每个单词的首字母都大写。单词之间不出现下画线。\n\n 6. 对象名、属性名和方法名，全部用小写字母。单词之间用下画线连接。\n\n\n# 过程大于结果\n\n这一章我们将完成面向过程的编程范式的学习。\n\n在这一章中，我们将看到其他面向过程的封装方法，即函数和模块。函数和模块把成块的指令封装成可以重复调用的代码块，并借着函数名和模块名整理出一套接口，方便未来调用。\n\n\n# 函数\n\n\n# 函数是什么\n\n从数学上来看，函数代表了集合之间的对应关系。例如定义域集合和值域集合之间的定义关系。\n\n从程序来看，函数是一种语法结构。它把一些指令封装在一起，形成一个组合拳。一旦定义好了函数，我们就可以通过对函数的调用，来启动这套组合拳。函数是对封装理念的实践。输入数据被称为参数，参数能影响函数的行为。\n\n\n# 定义函数\n\n在python中使用def这个关键字来定义函数，关键字def后面跟着的是函数的名字。\n\n在函数名后面，还有一个括号，用来说明函数有哪些参数，参数可以有多个，也可以完全没有。\n\n函数中定义的参数是一个形式代表，并非真正数据，所以又称为形参。\n\n根据Python语法规定，即使没有输入数据，函数后面的括号也要保留。\n\n括号结束后，第一行的末尾，有一个冒号，后面的代码和之前的一样，也要有4个空格缩进。\n\n最后一句return，关键字return用于说明函数的返回值，即函数的输出数据。\n\n函数执行到return时就会结束，不管它后面是否还有其他函数定义语句。\n\nreturn启动了中止函数和制定返回值的功能。\n\n在Python的语法中，return并不是必需的。如果没有return，或者return后面没有返回值，则函数将返回None。\n\nNone是Python中的空数据，用来表示什么都没有。关键字return也返回多个值。\n\ndef square_sum(a,b):\n    a = a**2\n    b = b**2\n    c = a + b\n    return c\n\n\n1\n2\n3\n4\n5\n\n\n\n# 调用函数\n\n使用函数的过程叫做调用函数。\n\n在函数调用时出现的参数称为实参。\n\n函数调用的写法，其实与函数定义第一行def后面的内容相仿。只不过在调用函数的时候，我们把真实的数据填入到括号中，作为参数传递给函数。\n\n除具体的数据表达式外，参数还可以是程序中已经存在的变量。\n\na = 5\nb = 6\nx = square_sum(a, b)\nprint(x)\n\n\n1\n2\n3\n4\n\n\n\n# 函数文档\n\n在定义函数的时候，要加上清晰的说明文档，说明函数的功能和用法分别是什么。\n\n查看函数文档，可以用help()来查看。注意的是这个多行注释同样是有缩进的。\n\ndef square_sum(a,b):\n    """ return the square sum of two arguments"""\n    a = a**2\n    b = b**2\n    c = a + b\n    return c\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 参数传递\n\n把数据用参数的形式输入到函数，被称为参数传递。\n\n在定义函数的时候就说明了函数的个数，这是基本传参。\n\n如果在定义函数的时候，我们并不知道参数的个数；有时需要在程序运行的时候才能知道参数的个数；有时是希望函数定义的更加松散，以便于函数能运用于不同形式的调用。这个时候会用到包裹(packing)传参的方式来进行参数传递。\n\n\n# 基本传参\n\n# 位置确定参数\n\n有多个参数，在调用函数的时候，Python会根据位置来确定数据对应哪个参数。确定实参与形参的对应关系。\n\ndef print_arguments(a, b, c):\n    """print arguments according to their sequence"""\n    print(a, b, c)\n    \nprint_arguments(1, 3, 5)\n\n\n1\n2\n3\n4\n5\n\n\n# 关键字传参\n\n根据位置传参比较死板，可以利用关键字(keyword)的方式来传递参数。\n\n在定义函数的时候，给形参一个符号标记，即参数名。\n\n关键字传递是根据参数名来让数据与符号对应上。\n\nprint_arguments(c=5, b=3, a=1)\n\n\n1\n\n\n位置传递与关键字传递混用，即一部分的参数传递根据位置，另一部分根据参数名。\n\n在调用函数的时候，所有的位置参数都要出现在关键字参数之前。\n\n如果位置参数放在关键字参数后面，python会报错。\n\nprint_arguments(1, c=5, b=3)\n\n\n1\n\n\n# 形参默认值\n\n在函数定义的时候，可以设置某些形参的默认值。如果我们在调用的时候不提供这些形参的具体数据，那么我们将采用定义时的默认值。\n\ndef f(a, b, c=10):\n    return a + b + c      \n\nprint(f(3,2,1))           # 参数c取传入的1，结果打印6\nprint(f(3,2))             # 参数c取默认值10。结果打印15\n\n\n1\n2\n3\n4\n5\n\n\n\n# 包裹传参\n\n所谓"包裹传参"是指，我们在定义函数的函数的时候，我们并不知道参数的个数，需要在程序运行时才能知道。所以我们在定义函数的函数更加松散，以便于函数能运用于不同形式的调用。\n\n包裹传参也有位置和关键字两种形式。\n\n# 元组方式\n\n在调用package_position()的时候，所有的数据都根据先后顺序，收集到一个元组。\n\n在函数内部，我们可以通过元组来读取传入的数据。\n\n在定义package_position()时要在元组名all_arguments前加*号，来表示使用的元组方式的包裹传递参数的方式。\n\ndef package_position(*all_arguments):\n    print(type(all_arguments))\n    print(all_arguments)\n\npackage_position(1,4,6)\npackage_position(5,6,7,8,9)\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 词典方式\n\n下面包裹关键字传递的例子中，参数传递方法把传入的数据收集为一个词典。\n\n每个关键字形式的参数调用，都会成为字典的一个元素。\n\n参数名成为元素的键，而数据成为元素的值。\n\n字典all_arguments收集了所有的参数，把数据传递给函数使用。我们在all_arguments前加上**，表示all_arguments的参数传递使用的包裹关键字字典的方式。\n\ndef package_keyword(**all_arguments):\n    print(type(all_arguments))\n    print(all_arguments)\n    \npackage_keyword(a=1, b=9)\npackage_keyword(m=2, n=1, c=11)\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 多种传参方式混用\n\n包裹位置传参和包括关键字传参可以混合使用。\n\n包裹传参和基本传参也可以混合使用。它们出现的先后顺序是：位置->关键字->包裹位置->包裹关键字。\n\n\n# 解包裹\n\n之前的包裹传参，是用于参数定义的阶段。主要的应用场景是：我们不确定后续有多个参数要传入，只能定义为元组或字典的方式的函数，后续调用的时候，不用考虑传入多少个参数了。\n\n解包裹并不是包裹的相反操作，解包裹的意思是，调用函数的时候，传入的参数并不是一个直接的传递的这个参数值，而是一个元组(用*表示)，或是一个字典(用**表示)，来提醒Python把对应的元组或字典去拆成一个一个对应的元素到具体的参数中去。\n\n解包裹用于函数调用。在调用函数的时候，几种参数的传递方式也可以混合。依然是相同的基本原则：位置 -> 关键字 -> 位置解包裹 -> 关键字解包裹\n\ndef unpackage(a,b,c):\n    print(a,b,c)\n\nargs = (1,3,4)\nunpackage(*args)       #结果为1 3 4\n\nargs = {"a":1,"b":2,"c":3}\nunpackage(**args)      # 打印1、2、3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 递归\n\n\n# 为什么要用递归\n\n有些应用场景，利用递归的算法特性，会使得逻辑更清晰。\n\n例如，如何比对一个json串的时候，可能一个key value值里面又包含一个key value值，层层嵌套。\n\n\n# 什么是递归\n\n递归是函数调用其自身的操作。\n\n# 数学归纳法\n\n递归的思想源于数学归纳法，常用于证明命题在自然数范围内成立。\n\n数学归纳法本身非常简单。如果我们想要证明某个命题对于自然数n成立，那么：\n\n第一步，证明命题对于n=1成立。\n\n第二步，假设命题对于n成立，n为任意自然数，则证明在此假设下，命题对于n+1成立。\n\n\n# 如何写递归\n\n递归的写法，要注意三个方面：初始条件、终止条件及衔接。从下面的列子来看衔接就是函数gasssian_sum(n)与gasssian_sum(n-1)的关系。\n\n为了保证计算机不陷入死循环，递归要求程序有一个能够达到的终止条件。还有递归的关键是说明衔接的两个步骤之间的衔接条件。\n\n设计递归程序的时候，我们从最终结果入手，要想求得gasssian_sum(100)，计算机会把这个计算拆解为求得gasssian_sum(99)的运算，以及gasssian_sum(99)加上100的运算。以此类推，直到拆解为gasssian_sum(1)的运算，就触发终止条件。\n\ndef gasssian_sum(n):\n    if n == 1:\n        return 1\n    else:\n        return n + gasssian_sum(n-1)\n\nprint(gasssian_sum(100))\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 函数栈\n\n程序中的递归需要用到栈这一数据结构。\n\n栈最显著的特征是"后进先出"(LIFO)。栈中的每一个元素，被称为一个帧(frame)。栈只支持两个操作：pop和push。栈用弹出(pop)操作来取出栈顶元素，用推入(push)操作将一个新的元素存入栈顶。\n\n上面的例子中，为了计算gasssian_sum(100)，我们需要先暂停gasssian_sum(100)，开始gasssian_sum(99)的计算。为了计算gasssian_sum(99)，需要先暂停gasssian_sum(99)，调用gasssian_sum(98) ......。在触发终止条件前，会有很多次为完成的函数调用。每次函数调用时，我们在栈中推入一个新的帧，用来保存这次函数调用的相关信息。栈不断增长，知道计算出gasssian_sum(1)后，我们又会恢复计算gasssian_sum(2)、gasssian_sum(3), ......。由于栈"后进先出"的特点，所以每次只需要弹出栈的帧，就正好是我们所需要的gasssian_sum(2)、gasssian_sum(3)......直到弹出藏在最底层的帧gasssian_sum(100)。\n\n所以，程序运行的过程，可以看做是一个先增长栈后消灭栈的过程。每次函数调用，都伴随着一个帧入栈。如果函数内部还有函数调用，那么又会多一个帧入栈。当函数返回时，相应的帧会出栈。等到程序的最后，栈清空，程序就完成了。\n\n要注意栈溢出(stack overflow)，由于直到栈底才会去释放之前栈的空间，需要消耗很多的栈内存。\n\n\n# 变量的作用域\n\n 1. python寻找变量的范围不止是当前的函数帧(当前函数内的变量)，它还会寻找函数外部，也就是Python主程序中定义的变量。\n\n 2. 当主程序中已经有一个变量，函数调用内部可以通过赋值的方式再创建了一个同名变量。函数会优先使用自己函数帧中的那个变量。这个时候，函数内部使用的变量的操作，不会影响到外部的那个同名变量。\n\n 3. 函数的参数与函数内部变量类似。可以把参数理解为函数内部的变量。在函数调用的时候，会把数据赋值给这些变量。等到函数返回的时候，这些参数相关的变量会被清空。\n\n 4. 但是将一个列表传递给函数，作为函数的参数的时候。在对函数进行操作后，函数外部的列表会发生变化。也就是说参数是一个数据容器的时候，函数内外部只存在一个数据容器，所以对函数内部对该数据容器的操作，会影响到函数外部。对于数据容器，函数内部的更改会影响到外部\n\ninfo = "hello"\ndef external_var():\n    info = "python"\n    print(info)              # 结果为python\nexternal_var()\nprint(info)                  # 结果还是为hello\n\nb = [1,2,3]\ndef change_list(b):\n    b[0] = b[0] + 1\n    return b\n\nprint(change_list(b))       # 打印[2,2,3]\nprint(b)                    # 打印[2,2,3],已经改变了外部的list\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 模块\n\n\n# 为什么要有模块\n\n通过模块，我们可以调用其他文件中的函数。而引入(import)模块，就是为了在新的程序中重复利用已有的Python程序。\n\n\n# 什么是模块\n\n对于面向过程语言来说，模块是比函数更高一层的封装模式。程序可以以文件为单位实现复用。\n\n在Python中，一个.py文件就构成一个模块。\n\n\n# 如何使用模块\n\n调用模块中的函数：\n\n先写一个first.py文件，里面定义了一个laugh()的函数；然后再在同一个目录下写一个second.py文件，在这个python程序文件中引入first模块中的laugh函数；然后就可以在first模块中调用laugh函数了。\n\nfirst.py文件如下：\n\ndef laugh():\n    print("hahaha")\n\n\n1\n2\n\n\nsecond.py文件如下：\n\nfrom first import laugh\nfor i in range(10):\n    laugh()\n\n\n1\n2\n3\n\n\n调用模块中的变量：\n\n除了函数，我们还可以引入其他文件中包含的数据。\n\n比如我们在module_var.py中有如下变量，我们在import_demo.py中，引入了这一变量：\n\nmodule_var.py文件如下：\n\ntext = "Hello!"\n\n\n1\n\n\nimport_demo.py中，我们引入这一变量：\n\nfrom module_var import text\nprint(text)       #打印"Hello!"\n\n\n1\n2\n\n\n\n# 搜索路径\n\nPython寻找相应的模块，有着自己的查找路径：\n\n 1. Python会自动在当前文件夹下搜索它想要引入的模块。\n 2. 查找标准库的安装路径。\n 3. 操作系统环境变量PYTHONPATH所包含的路径。\n\n\n# 异常处理\n\n\n# 异常分类\n\n 1. 语法错误\n    \n    这类错误，是Python在没有运行代码的时候就发现了这类语法错误。例如，for循环中没有冒号。\n\n 2. 运行时错误\n    \n    只有在Python编译器运行的时候，才会发现的错误，被称为运行时错误。由于Python是动态语言，许多操作必须在运行时才会执行，比如确定变量的类型等等。因此，Python要比静态语言更容易产生运行时错误。\n\n 3. 语义错误\n    \n    还有一种错误，叫做语义错误。编译器认为你的程序没有问题，可以正常运行。但是当检查程序的时候，却发现程序并非我们想要的结果。这种错误最为隐蔽，也最难纠正。\n\n\n# 异常处理\n\n# 为什么需要异常处理\n\n对于运行时可能产生的错误，我们可以提前在程序中处理。\n\n这样做有两个可能的目的：一个是让程序中止前进行更多的操作，比如提供更多的关于错误的信息。另一个则是让程序在犯错后依然能运行下去。\n\n异常处理还可以提高程序的容错性。\n\n# 异常的基本结构\n\n异常处理完整的语法形式为：\n\ntry:\n    ...\nexcept exception1:\n    ...\nexcept exception2:\nelse:\n    ...\nfinally:\n    ...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n如果try中有异常发生时，将执行异常的归属，执行except。\n\n异常层层比较，看是否是exception1、exception2 ...... 直到找到其归属，执行相应的except中的语句。\n\n如果try中没有异常，那么except部分将跳过，执行else中的语句。\n\nfinally是无论是否有异常，最后都要做的一些事情。\n\n# 所有异常都捕获\n\n如果except后面没有任何参数，那么表示所有的exception都交给这段程序处理：\n\nwhile True:\n    inputStr = input("Please input a number:")\n    try:\n        num = float(inputStr)\n        print("Input number:", num)\n        print("result:", 10/num)\n    except:\n        print("Something Wrong. Try Again.")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 子程序上抛异常至主程序\n\n如果子程序无法将异常交给合适的对象，那么异常将继续向上层抛出，直到被捕捉或者造成主程序报错。比如下面的程序，子程序的try...except...结构无法处理相应的除以0的错误，所以错误被抛给上层的主程序。\n\ndef test_func():\n    try:\n        m = 1/0\n    except ValueError:\n        print("Catch ValueError in the sub-function")\n\ntry:\n    test_func()\nexcept ZeroDivisionError:\n    print("Catch error in the main program")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 主动抛异常\n\n使用raise关键字，我们也可以在程序中主动抛出异常。\n\nraise ZeroDivisioError()\n\n\n1\n\n\n\n# 朝思暮想是对象\n\n为什么要有面向对象？\n\n在科学实践过程中，发现传统的面向过程的方法的编程方式无法去模拟真实世界中的个体。\n\n面向对象的出现就是为了解决上述的困难，通过类和对象这两种语法结构，加强了程序模拟真实世界的能力。而"模拟"，正是面向对象编程的核心。\n\nPython的一条哲学理念是"一切皆对象"。\n\n\n# 类\n\n\n# 什么是类\n\n和我们认识的日常生活中的"类"的概念差不多。在日常生活中，我们把相近的东西归为一类，而且给这个类起一个名字。比如鸟类、鱼类等等。\n\n\n# 如何定义类\n\n定义类的时候，我们使用class关键字。类的名字的括号里面有一个关键字object，也就是"东西"的意思，即某一个个体。我们把个体称为对象。\n\n在下面的例子中，我们定义了两个量，一个用于说明鸟类有羽毛，另一个用于说明鸟类的繁殖方式，这两个量称为类的属性。\n\nclass Bird(object):\n    feather = True\n    reproduction = "egg"\n\n\n1\n2\n3\n\n\n\n# 定义类的方法\n\n除了用数据性的属性来分辨类别外，有时也会根据这类东西能做什么事情来区分。比如，鸟会移动。\n\n这样的一些"行为"属性称为方法。\n\nPython中，一般通过在类的内部定义函数来说明方法。\n\n在下面的例子中，我们给鸟类新增了一个方法属性，表示鸟叫的方法chirp()。这个方法chirp()看起来很像一个函数。它的第一个参数是self，是为了在方法内部引用对象自身。无论该参数是否用到，方法的第一个参数必须是用于指代对象自身的self。剩下的sound是为了满足我们的需求设计的，代表了鸟叫的内容。方法chirp()会把sound打印出来。\n\nclass Bird(object):\n    feather = True\n    reproduction = "egg"\n    def chirp(self, sound):\n        print(sound)\n\n\n1\n2\n3\n4\n5\n\n\n\n# 对象\n\n\n# 什么是对象\n\n对象是一个个体，它是某一类的实例化。调用类，我们可以创造出这个类下面的一个对象。\n\nsummer = Bird()\n\n\n# 展示对象的属性和方法\n\n初始化完一个对象后，这个对象就有了这个类的属性和方法。\n\n对于属性的引用，我们可以通过对象.属性的形式来实现。\n\nprint(summer.reproduction) #打印\'egg\'\n\n对于方法的调用，我们可以让该对象执行鸟类允许的动作。\n\nsummer.chirp("jijijij") #打印\'jijijij\'\n\n> 注意，在调用方法的时候，我们只传递了一个参数，也就是"jijijij"。这是方法与函数有所区别的地方。尽管我们在定义类的方法的时候，必须加上这个self参数，但是self只用能在类定义的内部，所以在调用方法的时候不需要对self传入参数了。\n\n\n# 定义对象的个性属性\n\n每个对象个体，除了拥有共性的类属性外，还需要用于说明个性的对象属性。\n\n# self配置个性对象属性\n\n这个时候，我们可以在类中，通过self来操作对象的属性。\n\n下面的例子中，我们在Bird类中，定义了一个set_color的方法，在这个方法中，我们通过self参数设定了对象的属性color。\n\n和对象.类属性方式一样，我们能通过对象.对象属性的方式来操作对象属性。由于对象属性依赖于self，我们必须在某个方法内部才能操作类属性。因此，对象属性没办法像类属性一样，在类下方直接赋初值。\n\nclass Bird(object):\n    def chirp(self, sound):\n        print(sound)\n    def set_color(self, color):\n        self.color = color\n\nsummer = Bird()\nsummer.set_color("yellow")\nprint(summer.color)               # 打印\'yellow\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# _init_()方法初始化对象属性\n\n上面是自己在类里面定义一个方法来满足，对象的个性化属性。\n\n这里是利用Python自带的初始化对象属性的办法，这是特殊方法。名字很特别，前后有两个下划线。比如_init_()、_add_()、_dict_()等。对于类中的_init_()方法，Python会在每次创建对象时自动调用。和上面的方式不同的地方，上面需要手动调用。\n\nclass Bird(object):\n    def __init__(self, sound):\n        self.sound = sound\n        print("my sound is:", sound)\n    def chirp(self):\n        print(self.sound)\n\nsummer = Bird("ji")\nsummer.chirp()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# self调用类中其他方法\n\n这里只是描述了self的另外一种功能，也就是说除了上面两种可以通过self自定义个性化的对象属性，还可以利用self实现在一个方法内部调用同一类的其他方法。\n\nclass Bird(object):\n    def chirp(self, sound):\n        print(sound)\n    \n    def chirp_repeat(self, sound, n):\n        for i in range(n):\n            self.chirp(sound)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 子类\n\n\n# 什么是子类\n\n为了更为确切的用程序语言的方式来模拟现实世界，出现了子类的数据结构。\n\n通过子类继承父类的方式，很减少程序中的重复信息和重复语句。继承提高了程序的可重复使用性。\n\n类别本身还可以进一步细分为子类。例如，鸟类可以进一步分为鸡、天鹅等。在面向对象的编程中，我们通过子类继承父类的方式来表达这个概念。\n\n在下面的案例中，我们定义了一个鸡类，和一个天鹅类，它们都继承来着鸟类这个父类。鸡类和天鹅类也就同时拥有鸟类所有定义的数据性属性和方法性的属性。\n\n需要注意的是，在定义子类的时候，子类名字后面的括号里面，不在是object，而是父类的名字。\n\n最基础的情况，是类定义的括号中是object。类object其实是Python中的一个内置类。它充当了所有类的祖先。\n\nclass Bird(object):\n    feather = True\n    reproduction = "egg"\n    def chirp(self, sound):\n        print(sound)\n\nclass Chicken(Bird):\n    how_to_move = "walk"\n    edible = True\n\nclass Swan(Bird):\n    how_to_move = \'swim\'\n    edible = False\n\nsummer = Chicken()\nprint(summer.feather)                   # 打印True\nsummer.chirp("ji")                      # 打印\'ji\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 属性覆盖\n\n我们可以在子类中替换父类已经存在了的属性。\n\n通过对方法的覆盖，我们可以改变子类的行为。\n\n在下面的案例中，鸡类是鸟类的子类。在鸡类中，我们同样定义了一个chirp()方法，这个方法在鸟类中也是有定义的。\n\n调用鸡类中的这个chirp()方法来看，鸡类会调用自身定义的chirp()方法，而不是父类中的chirp()方法。从效果上来看，这就好像是父类中的方法chirp()被子类中的同名属性覆盖(override)了一样。\n\nclass Bird(object):\n    def chirp(self):\n        print("make sound")\n\nclass Chicken(Bird):\n    def chirp(self):\n        print("ji")\n\nbird = Bird()\nbird.chirp()            # 打印\'make sound\'\n\nsummer = Chicken()\nsummer.chirp()          #打印\'ji\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 子类中调用父类中被覆盖的方法\n\n上面的案例中，我们可以通过对方法的覆盖，可以彻底地改变子类的行为。但有时候，子类的行为是父类行为的拓展。这个时候，我们可以通过super关键字在子类中调用父类中被覆盖的方法。\n\n案例中，在鸡类的chirp()方法中，使用了super。这个是一个内置类，能产生一个指代父类的对象。通过super，我们在子类的同名方法中调用了父类的方法。这样，子类的方法既能执行父类中相关操作，又能定义属于自己的额外操作。\n\nclass Bird(object):\n    def chirp(self):\n        print("make sound")\n\nclass Chicken(Bird):\n    def chirp(self):\n        super().chirp()      #在子类的同名方法中调用了父类的同名方法\n        print("ji")\n\nbird = Bird()\nbird.chirp()                # 打印"make sound"\n\nsummer = Chicken()\nsummer.chirp()              # 打印"make sound"和"ji"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 回头再看对象\n\n在对对象的基本概念了解后，我们在回头看看了解下之前我们已经熟悉列表、元组与字符串、词典、循环、函数。\n\n在Python中，一切皆对象。我们要以面向对象的思维，再次来思考学习。\n\n\n# 列表对象\n\n# 什么是列表对象?\n\n我们在描述一个对象的时候，先考虑一下它所从属的类。list类，类是模拟现实世界的数据结构。它有自定义的静态的属性，和动态的可调用的方法。当我们新建了一个list的时候，实际上是在创建了list类的一个对象。\n\n# 创建list对象\n\na = [1,2,5,3,5]\ntype(a)\n\n\n1\n2\n\n\n从新建的一个list的过程来看，实际上我们是创建了list类的一个对象。list类是Python自带的，已经提前定义好的，被称为内置类。\n\n# list类的属性和方法\n\n 1.  利用函数dir()来查询一个类或对象的所有属性。dir(list)\n\n 2.  利用函数help()来查询类的说明文档。help(list)\n\n 3.  调用list类中的count方法\n     \n     a = [1,2,3,5,9.0,"Good",-1,True,False,"Bye"]\n     a.count(5)\n     \n     \n     1\n     2\n     \n\n 4.  调用list类中的index方法\n     \n     a.index(3) 查询元素3第一次出现时的下标\n\n 5.  调用list类中的append方法来添加一个新元素\n     \n     a.append(6) 在列表的最后增添一个新元素6\n\n 6.  调用list类中的sort方法来排序\n     \n     a.sort()\n\n 7.  调用list类中的reverse方法来颠倒次序\n     \n     a.reverse()\n\n 8.  调用list类中的pop方法去除最后一个元素\n     \n     a.pop() 去除最后一个元素，并将该元素返回\n\n 9.  调用list类中的remove方法去除第一次出现的某个元素\n     \n     a.remove(2) 去除第一次出现的元素2\n\n 10. 调用list类中的insert方法来插入指定位置的元素\n     \n     a.insert(0,9) 在下标为0的位置插入9\n\n 11. 调用list类中的clear方法来清空列表\n     \n     a.clear() 清空列表\n\n\n# 元组与字符串对象\n\n# 元组对象\n\n元组与列表一样，都是序列。但是元组不能变更内容。所以，元组只能进行查询操作，不能进行修改操作。\n\na = (1,3,5)\na.count(5)     #计数，看总共有多少个元素5\na.index(3)     #查询元素3第一次出现时的小标\n\n\n1\n2\n3\n\n\n# 字符串对象\n\n字符串是特殊的元组，因此可以执行元组的方法。\n\n尽管字符串是元组的一种，但是字符串有一些方法能改变字符串。其实这些方法并不是修改字符串对象，而是删除原有字符串，再建立一个新的字符串，所以并没有违背元组的不可变性。\n\n# 字符串对象方法\n\n下面的示例中，str是一个字符串，sub是str的一个子字符串。s为一个序列，它的元素都是字符串。width为一个整数，用于说明新生成字符串的宽度。\n\nstr = "Hello World!"\nsub = "World"\nstr.count(sub)         #返回：sub在str中出现的次数\nstr.find(sub)          #返回：从左开始，查找sub在str中第一次出现的位置。\n                       #如果str中不包含sub，返回-1\nstr.index(sub)         #和上面的一样的意思\nstr.rfind(sub)         #返回：从右开始，查找sub在str中第一次出现的位置。\n                       #如果str中不包含sub，举出错误\nstr.rindex(sub)        #和上面的一样的意思\nstr.isalnum()          #返回：True，如果所有的字符都是字母或数字\n                       #注意上面空格，标点符号也不能出现\nstr.isalpha()          #返回：True，如果所有的字符都是字母\nstr.isdigit()          #返回：True，如果所有的字符都是数字\nstr.istitle()          #返回：True，如果所有的词的首字母都是大写\nstr.isspace()          #返回：True，如果所有的字符都是空格\nstr.islower()          #返回:True,如果所有的字符都是小写字母\nstr.isupper()          #返回：True，如果所有的字符都是大写字母\nstr.split([sep, [max]]) # 返回：从左开始，以空格为分隔符，\n                        # 将str分隔为多个子字符串，总共分隔max次\nstr.rplit([sep, [max]]) # 返回：从右开始，以空格为分隔符，\n                        # 将str分隔为多个子字符串，总共分隔max次\nstr.joins(s)            # 返回：将s中的元素，以str为分隔符\n                        # 合并为一个字符串\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 词典对象\n\n词典同样是一个类，我们定义了一个词典对象后，可以充分利用下词典类中的各种方法。\n\n# 词典对象方法\n\nkeys()方法来循环遍历每个元素的键，keys()的方法实现的是把这个词典的所有的key塞到一个list中。\n\nexample_dict = {"a":1, "b":2}\ntype(example_dict)\nfor k in example_dict.keys():\n    print(example_dict[k])\n\n\n1\n2\n3\n4\n\n\nvalues()方法循环遍历每个元素的值，values()的方法实现的是把这个词典的所有的value塞到一个list中。\n\nfor v in example_dict.values():\n    print(v)\n\n\n1\n2\n\n\nitems()方法直接遍历每个元素：\n\nfor k,v in example_dict.items():\n    print(k,v)\n\n\n1\n2\n\n\nclean()方法，清空整个词典：\n\nexample_dict.clean()     #清理完后，变成了{}\n\n\n1\n\n\n\n# 意想不到的对象\n\n\n# 循环对象\n\n在Python中循环数据结构也是由循环对象来实现的。\n\n# 什么是循环对象\n\n所谓的循环对象包含有一个_next_()方法。这个方法的目的是生成循环的下一个结果。在生成过循环的所有结果之后，刚方法将抛出StopIteration异常。\n\n例如一个for这样的循环语法调用循环对象时，它会在每次循环的时候调用/next()方法，知道StopIteration出现。循环接收到这个异常，就会知道循环已经结束，将停止调用_next_()。\n\n下面的案例主要验证的是_next_()方法,内置函数iter()会把一个列表转变为循环对象。在真实的for循环中，我们可以省去内置函数iter的转换。这是因为，for结构会自动执行这一转换。\n\nexample_iter = iter([1,2])\nexample_iter.__next__()         # 显示1\nexample_iter.__next__()         # 显示2\nexample_iter.__next__()         # 出现StopIteration异常\n\nfor item in iter([1,2]):\n    print(item)\n\nfor item in (1,2):\n    print(item)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 生成器\n\n我们同样可以借助于生成器(generator)来自定义循环对象。生成器的编写方法和函数定义类似，只是在return的地方改为yield。\n\n生成器中可以有多个yield。当生成器遇到一个yield时，会暂停运行生成器，返回yield后面的值。当再次调用生成器的时候，会从刚才暂停的地方继续运行，直到下一个yield。生成器自身又构成一个循环对象，每次循环使用一个yield返回的值。Python中的内置函数range()返回的是一个循环对象，而不是一个序列。\n\ndef gen():\n    a = 100\n    yield a\n    a = a*8\n    yield a \n    yield 1000\n\nfor i in gen():\n    print(i)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 函数对象\n\n在Python中，函数也是一种对象。实际上，任何一个有_call_()特殊方法的对象都被当作是函数。\n\n通过type()方法来查看一个函数的类型，可以看到我们定义的一个函数其实也是一个函数类。我们在调用函数的过程，实际上是初始化了一个函数对象；而调用函数，加入实参的过程，就是对这个函数对象调用了__call__的内置方法，实现了函数传参。\n\nclass SampleMore(object):\n    def __call__(self, a):\n        return a + 5\n\nadd_five = SampleMore()    #生成函数对象\nprint(add_five(2))         #像一个函数一样调用函数对象，结果为7.\n\n#原先我们定义函数的过程如下；\ndef call(a):\n    return a + 5\nprint(call(2))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 模块对象\n\n在Python中的模块对应一个.py文件，模块也是对象。在前面的定义中，我们看到在调用模块中各个属性和方法的过程，实际上就是调用对象的属性或方法的过程。\n\n# 调用模块对象中方法的几种方式\n\n 1. 第一种方法，from ... import ...的方式\n    \n    from time import sleep    #单独从time模块中引入sleep动态属性\n    sleep(10)\n    \n    \n    1\n    2\n    \n\n 2. 第二种方法，from ... import * 的方式\n    \n    from time import *        #使用简单暴力的方法，一次性引入模块的所有属性\n    sleep(10)\n    \n    \n    1\n    2\n    \n\n 3. 第三钟方法，import ... 的方式\n    \n    import time               #引入time模块\n    time.sleep(10)            #利用对象.属性的方式来调用它\n                              #带上对象名的方式，好处可以避免模块中的属性引用时候，\n                              #出现同名\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 4. 第四种方法，import ... as xxx的方式\n    \n    import time as t         #我们还可以给模块换个名字\n    t.sleep(10)\n    \n    \n    1\n    2\n    \n\n 5. 第五种方法，引入文件夹中的模块\n    \n    import this_dir.module       #"this_dir"是目录名称,module是模块的名字\n                                 #模块的名字也就是xxx.py前面的名字\n                                 #需要在"this_dir"下建立一个__init__.py空文件\n    \n    \n    1\n    2\n    3\n    \n\n# 模块对象的名字\n\n每个模块对象都有一个__name__属性，用来记录模块的名字。\n\n当一个.py文件作为主程序运行的时候，比如python foo.py，这个文件也会有一个对应的模块对象。但这个模块对象的_name__属性会是 "_main"\n\n\n# 异常对象\n\n我们在程序中加入异常处理的try结构，捕捉程序中出现的异常。\n\n实际上，我们捕捉到的也是一个对象。\n\n利用except ... as ...的语法，我们在except结果中用e来代表捕获到的类型对象。关键字except直接跟随ZeroDivisionError实际上是异常对象的类。\n\ntry:\n    m = 1/0\nexcept ZeroDivisionError as e:\n    print("Catch NameError in the sub-function")\n    print(type(e))        #查看到该异常类的类型\n    print(dir(e))         # 异常对象的属性\n    print(e.message)      # 打印的异常信息\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 对象带你飞\n\n\n# 存储\n\n\n# 文件(文本对象)\n\n磁盘以文件为单位来存储数据。\n\n对于计算机来说，数据的本质就是有序的二进制数序列。\n\n如果以字节为单位，也就是每8位二进制数序列为单位，那么这个数据序列就称为文本。\n\n这是因为，8位二进制数序列正好对应ASCII编码中的一个字符。\n\n而Python能够借助文本对象来读写文件。\n\n# 创建文本对象\n\n在Python中，我们可以通过内置函数open来创建文件对象。在调用open的时候，需要说明文件名，以及打开文件的方式。\n\nf = open(文件名，方式)\n"r" #读取已经存在的文件\n"w" #新建文件，并写入\n"a" #如果文件存在，那么写入到文件的结尾\n    #如果文件不存在，则新建文件并写入\n\n\n1\n2\n3\n4\n5\n\n\n 1. 只读的方式打开文件\n    \n    f = open("test.txt", "r")\n    \n    \n    1\n    \n\n 2. 读取文件\n    \n    content = f.read(10)  #读取10个字节的数据\n    content = f.readline() #读取一行\n    content = f.readlines() #读取所有行，存储在列表中，每个元素都是一行\n    \n    \n    1\n    2\n    3\n    \n\n 3. 写或追加方式打开文件\n    \n    f = open("test.txt", "w")\n    f = open("test.txt", "a")\n    \n    \n    1\n    2\n    \n\n 4. 写入文件\n    \n    f.write("I like apple")\n    f.write("I like apple\\n")    #Unix系统中加入换行符\n    f.write("I like apple\\r\\n")    #windows系统中加入换行符\n    \n    \n    1\n    2\n    3\n    \n\n 5. 关闭文件\n    \n    打开文件端口将占用计算机资源，因此，\n    在读写完成后，应该及时用文件对象的close方法关闭文件：\n    f.close()\n    \n    \n    1\n    2\n    3\n    \n\n\n# 上下文管理器\n\n文件操作常常和上下文管理器一起使用。\n\n# 什么是上下文管理器\n\n上下文管理器(context manager)用于规定某个对象的使用范围。一旦进入或离开该使用范围，则会有特殊操作被调用，比如为对象分配或者释放内存。\n\n# 为什么需要上下文管理器\n\n对于文件操作，我们需要在读写结束时关闭文件。我们经常会忘记关闭文件，无谓的占用资源。上下文管理器可以在不需要文件的时候，自动关闭文件。\n\n# 如何使用上下文管理器\n\n下面有两个案例，一个案例是常规的文件操作，一个案例是使用上下文管理器的方式来进行的文件操作。\n\n第二段程序就使用了with...as...的结构。上下文管理器有隶属于它的程序块，当隶属的程序块执行结束时，也就是语句不再缩进时，上下文管理器就会自动关闭文件。在程序中，我们使用f.closed属性来验证是否已经关闭。通过上下午管理器，我们相当于用缩进来表达对象的打开范围。\n\n# 常规使用\nf = open("new.txt","w")\nprint(f.closed)               # 检查文件是否打开\nf.write("Hello World!")\nf.close()\nprint(f.closed)               # 打印true\n\n# 使用上下文管理器\nwith open("new.txt","w") as f:\n    f.write("Hello World!")\nprint(f.closed)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# pickle包\n\n# pickle包是用来干什么的\n\n从文字的理解来看，该模块是用来把某个对象保存下来，再存在磁盘里的文件。\n\n使用pickle包中的dump()或dumps()方法来进行序列化。\n\n使用pickle包中的load()方法进行反序列化\n\n# 为什么需要pickle包(序列化和反序列化)\n\n序列化:将变量从内存中变成可以存储或传输的过程称之为序列化，在Python中叫做pickling，在其它语言中也称之为 serialization、marshaling、flattening等等，说的都是一个意思。\n\n反序列化：反之，则为反序列化，称之为unpickling，把变量内容从序列化的对象重新读取到内存中。\n\n# 为什么需要序列化\n\n不序列化时，对象存储存在的问题：\n\n比如：我要将对象写入一个磁盘文件而后再将其读出来会有什么问题吗？别急，其中一个最大的问题就是对象引用！ 举个例子来说：假如我有两个类，分别是A和B，B类中含有一个指向A类对象的引用，现在我们对两个类进行实例化{ A a = new A(); B b = new B(); }。 这时在内存中实际上分配了两个空间，一个存储对象a，一个存储对象b。 接下来我们想将它们写入到磁盘的一个文件中去，就在写入文件时出现了问题！因为对象b包含对对象a的引用，所以系统会自动的将a的数据复制一份到b中，这样的话当我们从文件中恢复对象时(也就是重新加载到内存中)时，内存分配了三个空间，而对象a同时在内存中存在两份，想一想后果吧，如果我想修改对象a的数据的话，那不是还要搜索它的每一份拷贝来达到对象数据的一致性，这不是我们所希望的！\n\n序列化的解决方案：\n\n 1. 保存到磁盘的所有对象都获得一个序列号(1, 2, 3等等)\n 2. 当要保存一个对象时，先检查该对象是否被保存了\n 3. 如果以前保存过，只需写入"与已经保存的具有序列号x的对象相同"的标记，否则，保存该对象通过以上的步骤序列化机制解决了对象引用的问题！\n\n> https://blog.csdn.net/qq_37160773/article/details/95060070\n> \n> https://blog.csdn.net/u011215133/article/details/51177843\n\n# 保存对象到文件\n\n下面有2个示例，一个普通的例子。分别为创建对象、序列化对象、字节文本的存储。\n\n第二个示例，是上面的例子的简化。\n\n# 第一种示例\nimport pickle\nclass Bird(object):\n    have_feather = True\n    reprodution_method = "egg"\nsummer = Bird()                       #创建对象\npickle_string = pickle.dumps(summer)  #序列化对象\n\nwith open("summer.pkl", "wb") as f:\n    f.write(pickle_string)\n# 第二种示例\nimport pickle\nclass Bird(object):\n    have_feather = True\n    reprodution_method = "egg"\nsummer = Bird()\nwith open("summer.pkl", "w") as f:\n    pickle.dump(summer, f)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 从文件读取对象\n\n读取对象与存储对象的过程正好相反。首先，我们从文件中读出文本。然后使用pickle的loads()方法，将字符串形式的文本转换为对象。也可以使用pickle中的load()方法，将上面两步合并。\n\n有时候，仅仅是反向恢复还不够。对象依赖于它的类，所以Python在创建对象时，需要找到相应的类。\n\n所以，当我们从文本中读取对象的时候，程序中必须已经定义过类。对于Python总是存在的内置类，如列表、词典、字符串等，不需要再在程序中定义。但对于用于自定义的类，就必须要先定义类，然后才能从文件中载入该类的对象。\n\nimport pickle\nclass Bird(object):\n    have_feather = True\n    reproduction_method = "egg"\nwith open("summer.pkl", "rb") as f:\n    summer = pickle.load(f)\nprint(summer.have_feather)               # 打印True\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 时间类的包\n\n挂钟时间：\n\n在硬件的基础上，计算机可以提供挂钟时间(Wall Clock Time)。挂钟时间是从某个固定时间起点到现在的时间间隔。对于Uninx系统来说，起点时间是1970年1月1日的0点0分0秒。其他的日志信息都是从挂钟时间计算得到的。\n\n处理器时间：\n\n计算机还可以测量CPU实际运行的时间，也就是处理器时间(Processor Clock Time)，以测量计算机性能。当CPU处于闲置状态的时候，处理器时间会暂停。\n\n\n# time包\n\n 1. time()方法\n    \n    time包中的time()方法，显示挂钟时间。\n    \n    import time\n    print(time.time())  # 挂钟时间，单位是秒\n    \n    \n    1\n    2\n    \n\n 2. clock()方法\n    \n    time包中的clock()用来测量程序运行的时间，在程序中两次调用clock()方法，从而测量出镶嵌其间的程序所用的时间。在不同的计算机系统上，clock()的返回值会有所不同。在Unix系统上，返回的是处理器时间。当CPU处于闲置状态时，处理器时间会暂停。因此，我们获取的是CPU运行时间。在Windows系统上，返回的则是挂钟时间。\n    \n    import time\n    start = time.clock()\n    for i in range(1000000):\n        print(i**2)\n    end = time.clock()\n    print(end - start)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. sleep()方法\n    \n    sleep()方法可以让程序休眠。根据sleep()接收到的参数，程序会在某时间间隔之后醒来继续运行。\n    \n    import time\n    print("start")\n    time.sleep(10)    # 休眠10秒\n    print("wake up")\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 4. stuct_time对象\n    \n    time包中还定义了stuct_time对象。该对象将挂钟时间转换为年、月、日、时、分、秒等，存储在该对象的各个属性中，比如tm_year、tm_mon、tm_mday .... 下面几种方法可以将挂钟时间转换为struct_time对象。\n    \n    st = time.gmtime()     # 返回struct_time格式的UTC时间\n    st = time.localtim()   # 返回struct_time格式的当地时间，\n                           # 当地时区根据系统环境设定\n    \n    \n    1\n    2\n    3\n    \n    \n    也可以把一个stuct_time对象转换为time对象：\n    \n    s = time.mktime(st)    # 将struct_time格式转换成挂钟时间\n    \n    \n    1\n    \n\n\n# datetime包\n\n# 什么是datetime模块\n\ndatetime包是基于time包的一个高级包，用起来更加便利。datetime可以理解为由date和time两个部分组成。date是年月日，time是时、分、秒、毫秒。\n\n# 如何使用datetime模块\n\ndatetime模块下面有两个类：datetime.date类和datetime.time类。如果要想得到年月日是分秒，可以直接调用datetime.datetime类。\n\n显示时间：该对象t有时、分、秒、年、月、日等属性\n\nimport datetime\nt  = datetime.datetime(2012,9,3,21,30)\nprint(t)\nt2 = datetime.datetime.now()    # 显示当前时间\nprint(t2)\n\n\n1\n2\n3\n4\n5\n\n\n时间间隔计算：\n\n在datetime包中，有个专门代表时间间隔对象的类，即stimedelta。\n\n在一个datetime.datetime的时间点加上一个时间间隔，就可以得到一个新的时间点。\n\ndatetime.timedelta传递参数时，除了秒和星期外，还可以是天、小时、毫秒、微妙。\n\nimport datetime\nt        = datetime.datetime(2012,9,3,21,30)\nt_next   = datetime.datetime(2012,5,3,23,30)\ndelta1   = datetime.timedelta(seconds = 600)\ndelta2   = datetime.timedelta(weeks = 3)\nprint(t + delta1)        \nprint(t + delta2)\nprint(t_next - t)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\ndatetime对象比较：\n\n两个datetime对象进行比较运算，以确定哪个时间间隔更长。\n\nprint(t > t_next)          # 打印False\n\n\n1\n\n\n\n# 日期格式\n\n这里特意强调在datetime模块中日期格式的问题，主要是为了说明下面的日期格式转换。例如用strptime的方法实现日期的字符串转换为datetime的对象；用strftime方法实现把datetime对象转换为日期的字符串。\n\nfrom datetime import datetime\nstr        = "output-1997-12-23-030000.txt"\nformat     = "output-%Y-%m-%d-%H%M%S.txt"\nt          = datetime.strptime(str, format)\nprint(t) \n\nfrom datetime import datetime\nformat = "%Y-%m-%d %H:%M"\nt = datetime(2012,9,5,23,30)\nprint(t.strftime(format))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 正则表达式\n\n\n# 什么是正则表达式\n\n正则表达式的主要功能是从字符串中通过特定的模式，搜索希望找到的内容。\n\n这个功能有点和字符串对象中的搜索查找的方法，但是字符串对象中的查找是指，从一个大的字符串中，找出一个子字符串。而我们这里的使用正则表达式的目的在于，只是想要找到符合某种格式的字符串，而不是具体的那个子字符串。\n\n\n# 如何使用正则表达式\n\n# 从字符串中找出相应子字符串\n\nimport re\nm      = re.search("[0-9]", "dfsd789ddsfad")\nprint(m.group(0))\n\n\n1\n2\n3\n\n\nre.search()接收两个参数，第一个参数"[0-9]"就是我们所说的正则表达式，告诉Python去从第二个字符串中找出从0到9的任意一个数字字符。\n\nre.search()中的第二个参数，是指需要去查找的那个字符串。整体而言，就是从后面的字符串中，找出符合要求的子字符串，就返回一个对象m。可以通过m.group()的方法查看搜索到的结果。如果没有找到符合要求的字符，则re.search()会返回None.\n\n归纳为如下的使用模式：\n\nm = re.search(pattern, string) # 搜索整个字符串，直到发现符合的子字符串\n\n\n1\n\n\n# 从字符串中匹配相应的正则表达式\n\n下面的使用方式是指，从头开始检查字符串是否符合正则表达式。\n\n必须从字符串的第一个字符开始就相符。\n\nm = re.match(pattern, string)\n\n\n1\n\n\n# 替换符合正则规则的字符串\n\n下面的sub()利用正则pattern在字符串string中进行搜索，对于搜索到的字符串，用另一个字符串replacement进行替换。函数将返回替换后的字符串。\n\nstr = re.sub(pattern, replacement, string)\n\n\n1\n\n\n# 根据正则分隔字符串\n\n下面的用法是，根据正则表达式来分割字符串，将分割后的所有子字符串放在一个表(list)中返回。\n\nre.split()\n\n\n1\n\n\n# 根据正则返回所有子字符串\n\n根据正则表达式搜索字符串，将所有符合条件的子字符串放在一个表(list)中返回。\n\nre.findall()\n\n\n1\n\n\n\n# 正则表达式有哪些\n\n 1. 用某些符号代表单个字符\n 2. 用某些符合代表某种形式的重复\n 3. 位置相关的符号\n\n\n# 利用group来查看正则表达搜索结果\n\n有时候，我们想要查看匹配到的正则表达式的那个具体的，符合正则表达式的部分结果。我们可以在正则表达式上给目标加上括号。\n\n用括号()圈起来的正则表达式的一部分，称为群。一个正则表达式中可以有多个群。\n\n我们可以用group(number)的方法来查询群。需要注意的是，group(0)是整个正则表达式的搜索结果。group(1)是第一个群，以此类推。\n\nimport re\nm = re.search("output_(\\d{4})", "output_1986.txt")\nprint(m.group(1))     # 将找到4个数字组成的1896\n\n\n1\n2\n3\n\n\n\n# 给group命名\n\n我们还可以将群命名，以便更好地使用group查询：\n\n下面的(?P<year>...)括住一个群，并把它命名为year。用这种方式来产生群，就可以通过"year"这个键来提取结果了。\n\nimport re\nm = re.search("output_(?P<year>\\d{4})", "output_1986.txt")\nprint(m.group("year"))\n\n\n1\n2\n3\n\n\n\n# Python的网络访问\n\n\n# HTTP通信简介\n\nHTTP的request请求信息如下：\n\nGET /index.html  HTTP/1.1\nHost: www.example.com\n\n\n1\n2\n\n\n起始行中，有三段信息：\n\n * GET方法，用于说明想要服务器执行的操作。\n * /index.html资源的路径。这里指向服务器上的index.html文件\n * HTTP/1.1协议的版本。\n\nHTTP的response回复信息如下：\n\nHTTP/1.1 200 OK\nContent-type: text/plain\nContent-length: 12\n    \nHelllo World!\n\n\n1\n2\n3\n4\n5\n\n * HTTP/1.1：协议版本\n * 200：状态码(status code)\n * OK：状态描述\n * Content-type说明了主体所包含的资源的类型，有普通文件，HTML文本，jpeg图片等。\n * Content-length说明了主体部分的长度，以字节(byte)为单位。\n\n\n# http.client包\n\nhttp.client包(模块)中，包含了很多的http的方法，我们可以利用这些方法来实现http的通信。\n\n从上面的http请求中看出，在HTTP请求中最重要的一些信息是主机信息、请求方法和资源路径。\n\nimport http.client\nconn     = http.client.HTTPConnection("www.example.com")  #主机地址\nconn.request("GET", "/")  #请求方法和资源路径\nresponse = conn.getresponse()  # 获得回复\nprint(response.status, response.reason)  # 回复的状态码和状态描述\ncontent =  response.read()      # 回复的主体内容\nprint(content)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 与对象的深入交往\n\n在本章的前半部分，介绍了运算符、元素引用、内置函数，这些其实都是来自于一些特殊的对象。这样的设计既满足了Python的多范式的需求，又能以简单的体系满足丰富的语法需求，如运算符重载与即时特性等。\n\n本章的后半部分，将深入了解对象相关的重要机制，如动态类型和垃圾回收。\n\n\n# 一切皆对象\n\n\n# 运算符\n\n我们常见的运算符，比如+、-、>、<、and、or等，都是通过特殊方法实现的。或者说是通过归属类中的定义的相应的内置方法来实现的。\n\n在子类中重新定义了相应的运算符的方法后，子类中的方法会覆盖父类的同名发你规范。即运算符将被重新定义。\n\n我们以list列表类为例，如果我们用dir(list)查看list的属性，能看到一个属性时_add_()是特殊方法。这个方法定义了"+"运算符对于list对象的意义，两个list的对象相加时，会进行合并列表的操作。\n\n+号，对应的类中的方法是: "_add_()"\n\n-号，对应的类中的方法是: "_sub_()"\n\n*号，对应的类中的方法是: "_mul_()"\n\nor号，对应的类中的方法是："_or_()"\n\n定义运算符对于复杂的对象非常有用。例如，人类有多个属性，比如姓名、年龄和身高。我们可以把人类的比较(>、<、=)定义成只看年龄。\n\n\n# 元素引用\n\n序列包含的一个数据被称为序列的一个元素。\n\n序列是有顺序的数据集合，就好像一列排好队的士兵。序列分为元组和列表两种。\n\n我的理解是这里的"元素引用"是指，在序列(元组或列表)或词典中，单独对其中的一个元素，进行方法的运算。\n\n下面的案例中，展示了元素的获取的方法，更新的方法、删除的方法。\n\nli    = [1, 2, 3, 4, 5, 6]\nprint(li[3])\nprint(li.__getitem__(3))     # 获取元素\nli.__setitem__(3,0)          # 更新元素\nprint(li)\n\nexample_dict   = {"a":1, "b":2}\nexample_dict.__delitem__("a")    # 删除元素\nprint(example_dict)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 内置函数的实现\n\n与运算符类似，许多内置函数也都是调用对象的特殊方法。\n\n例如，下面的例子中的_len_()方法，实际上和执行内置函数len()一样的作用，起到了简化的作用。\n\nlen([1,2,3])\n[1,2,3].__len__()\n\n\n1\n2\n\n\n\n# 属性管理\n\n\n# 属性覆盖的背后\n\n当我们调用对象的属性的时候，这个属性可能有很多来源。除了来自对象属性和类属性，这个属性还可能是从祖先类那里继承来的。\n\n一个类或对象拥有的属性，会记录在_dict__中。这个_dict__是一个词典，键为属性名，对应的值为某个属性。Python在寻找对象的属性的时候，会按照继承关系依次寻找__dict。\n\n这个顺序是按照与summer对象的亲近关系排列的。\n\n我们用内置函数dir来查看对象summmer的属性的话，可以看到summer对象包含了全部四个部分。也就是说，对象的属性时分层管理的。对象summer能接触到的所有属性，分别存在summer/Chicken/Bird/object这四层。当我们需要调用某个属性的时候，Python会一层层向下遍历，直到找到那个属性。由于对象不需要重复存储其祖先类的属性，所以分层管理的机制可以节省存储空间。\n\n某个属性可能在不同层被重复定义。Python在向下遍历的过程中，会选取先遇到的那一个。这正是属性覆盖的原理。\n\n但是如果是进行赋值，那么Python就不会分层深入查找了。\n\n\n# 特性\n\n同一个对象的不同属性之间可能存在依赖关系。当某个属性被修改时，我们希望依赖于该属性的其他属性也同时变化。\n\n这时，我们不能通过__dict__的静态词典方式来存储属性。Python提供了多种即时生成属性的方法。其中一种称为特性(property)。特性是特殊的属性。\n\n特性使用内置函数property()来创建。property()最多可以加载四个参数。前三个参数为函数，分别用于设置获取、修改和删除特性时，Python应该执行的操作。最后一个参数为特性的文档，可以为一个字符串，起说明作用。\n\n下面的案例中，num为一个数字，而neg为一个特性，用来表示数字的负数。当一个数字确定的时候，它的负数总是确定的。而当我们修改了一个数的负数的时候，它本身的值也应该变化。这两点由get_neg()和set_neg()来实现。而del_neg()\n\nclass num(object):\n    def __init__(self, value):\n        self.value = value\n    def get_neg(self):\n        return -self.value\n    def set_neg(self, value):\n        self.value = -value\n    def del_neg(self):\n        print("value also deleted")\n        del self.value\n    neg = property(get_neg, set_neg, del_neg, "I am negative")\n\nx = num(1.1)\nprint(x.neg)\n\nx.neg = -22 \nprint(x.value)\nprint(num.neg.__doc__)\ndel x.neg\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# _getattr_()方法\n\n除了内置函数property外，还可以使用_gettr_(self,name)来查询即时生成的属性。\n\n当我们调用对象的一个属性的时候，如果通过_dict__机制无法找到该属性，那么Python就会调用对象的_getattr()方法，来即时生成该属性。\n\n和上面property特性的区别：\n\n每个特性都需要有自己的处理函数，而_getatr_()可以将所有的即时生成属性放在同一个函数中处理。而_getattr_()可以根据函数名来区别处理不同的属性。\n\n_getattr_()只能用于查询不在__dict__系统中的属性。\n\n_setattr_(self,name,value)和 _delattr_(self,name)可用于修改和删除属性。\n\n\n# 我是风儿，我是沙\n\n\n# 动态类型\n\nPython的变量不需要声明，在赋值的时候，变量可以重新赋值为其他任意值。\n\nPython变量这种变化的能力，就是动态类型的体现。\n\n# 对象名是指向对象的一个引用\n\n在下面的例子a =1说明了，在Python中，整数1是一个对象。对象的名字是"a"，对象名其实是指向对象的一个引用。对象是存储在内存中的实体。我们必须使用对象名，来执行这一对象的"引用"。\n\n通过内置函数id()，能查看到引用指向的是哪个对象。\n\na = 1\nprint(id(1))\nprint(id(a))\n\n\n1\n2\n3\n\n\n# 变量名重新赋值\n\n在Python中，赋值其实就是用对象名这个筷子去夹住其他的食物。每次赋值，我们让左侧的引用指向右侧的对象。引用能随时指向一个新的对象。\n\n下面的例子中，3是存储在内存中的一个整数对象。通过赋值，引用a指向对象3。这时建立了一个字符串对象"at"，通过赋值，我们将引用a指向"at"。\n\n既然变量名是个随时可以变更指向的引用，那么它的类型自然可以在程序中动态变化。\n\na = 3\nprint(id(a))\na = "at"\nprint(id(a))\n\n\n1\n2\n3\n4\n\n\n# 判断两个引用是否指向同一个对象\n\n除了直接打印id外，我们还可以用is运算来判断两个引用是否指向同一个对象。\n\na    = 3\nb    = 3\nprint(a is b)\n\n\n1\n2\n3\n\n\n\n# 可变与不可变对象\n\n不可变对象：\n\n意思是该对象，通过不同的变量名进行引用后，如果我们对引用后的变量名进行运算，不会改变原有的对象的值或内容，这就叫做是不可变对象。一般有：列表、词典。\n\n可变对象：\n\n意思是该对象，通过不同的变量名进行引用后，如果我们对引用后的变量名进行运算，结果改变了原有的对象的值或内容，这就叫做是可变对象。一般有：整数、浮点数、字符串、元组等。\n\n\n# 从动态类型引用角度看函数的参数传递\n\n函数的参数传递，本质上传递的是引用。每个参数或变量都是一个引用，当我们调用函数的时候，函数需要的形参，实际上通过引用的方式来指向实际的对象的值。\n\n下面的例子，分别从可变对象和不可变对象两个例子来说明：\n\ndef f(x):\n    print(id(x))\n    x = 100\n    print(id(x))\na = 1\nprint(id(a))\nf(a)\nprint(a)\n\ndef f(x):\n    x[0] = 100\n    print(x)\na = [1, 2, 3]\nf(a)\nprint(a)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 内存管理\n\n\n# 引用管理\n\n对象内存管理是基于对引用的管理。在Python中，引用与对象是分离的。\n\n一个对象可以有多个引用，而每个对象中都存有指向该对象的引用总数，即引用计数。我们可以使用标准库中sys包中的getrefcount()，来查看某个对象的引用计数。\n\n注意下面的例子中，我们想要查看对[1, 2, 3]这个列表对象的引用次数。这里我们使用了某个引用作为参数，传递给了getrefcount()时，参数实际上是创建了一个临时的引用。\n\nfrom sys import getrefcount\na = [1, 2, 3]\nprint(getrefcount(a))\nb = a\nprint(getrefcount(b))\n\n\n1\n2\n3\n4\n5\n\n\n\n# 对象引用对象\n\n容器对象的引用可能会构成很复杂的拓扑结构。我们可以用objgraph包来绘制其引用关系。\n\n两个对象可能相互引用，从而构成所谓的引用环。\n\n引用环会给垃圾回收机制带来很大的麻烦。\n\n我们可以通过del关键字删除某个引用，或者当这个引用被重新定向到某个其他对象时，也会减少对象的引用。\n\n\n# 垃圾回收\n\n# 为什么需要垃圾回收\n\n当Python中的对象越来越多的时候，它们将占用越来越大的内存。Python会在适当的时候启动垃圾回收，将没用的对象清除。\n\n# 什么是垃圾回收\n\n原理上，当Python的某个对象的引用计数降为0，即没有任何引用指向该对象的时候，该对象就称为要回收的垃圾了。\n\n垃圾回收费时费力，当垃圾回收的时候，Python不能进行其他的任务。频繁的垃圾回收将大大降低Python的工作效率。\n\n如果内存中的对象不多，就没有必要频繁启动垃圾回收。所以，Python只会在特定条件下，自动启动垃圾回收。当Python运行时，会记录其中分配对象(Object Allocation)和取消分配对象(Object Deallocation)的次数。当两者的差值高于某个阀值的时候，垃圾回收才会启动。\n\n# Python分代回收策略\n\n下面的700的阀值，是基础回收方式。除了基础回收方式外，Python同时还采用了分代回收的策略。\n\n这一策略的基本假设是，存活时间越久的对象，越不可能在后面的程序中变成垃圾。我们程序往往会产生大量的对象，许多对象很快产生和消失，但也有一些对象长期被使用。出于信任和效率，对于这样一些"长寿"对象，我们相信它们还有用处，所以减少在垃圾回收中扫描它们的频率。\n\nPython将所有的对象分为0、1、2三代。所有的新建对象都是0代对象。当某一代对象经历过垃圾回收，依然存活，那么它就被归入下一代对象。垃圾回收启动时，一定会扫描所有的0代对象。如果0代经过一定次数垃圾回收，那么就启动对0代和1代的扫描清理。当1代也经历了一定次数的垃圾回收后，就会启动对0、1、2代的扫描，即对所有对象进行扫描。\n\n# 如何配置垃圾回收\n\n我们可以通过gc模块的get_threshold()方法，来查看该阀值。一般返回的是(700,10,10)\n\nimport gc\nprint(gc.get_threshold())\n\n\n1\n2\n\n\n700即垃圾回收启动的阀值。可以通过gc中的set_threshold()方法重新设置。也可以手动使用gc.collect()来手动启动垃圾回收。\n\n后面两个10，与分代回收相关的阀值。也就是说，每10次0代垃圾回收，会配合1次1代的垃圾回收；而每10次1代的垃圾回收，才会有1次2代的垃圾回收。\n\n可以使用set_threshold()来调整次数，比如下面对2代对象进行更频繁的扫描。\n\nimport gc\ngc.set_threshold(700, 10, 5)\n\n\n1\n2\n\n\n\n# 孤立的引用环\n\n引用环的存在会给垃圾回收机制带来很大的困难。这些引用环可能构成无法使用，但引用计数不为0的一些对象。\n\n在下面的例子中，由于引用环的存在，这两个对象的引用计数都没有降到0，所以不会被垃圾回收。\n\na = []\nb = [a]\na.append(b)\ndel a\ndel b\n\n\n1\n2\n3\n4\n5\n\n\n为了回收这样的引用环，Python会复制每个对象的引用计数，可以记为gc_ref。假设，每个对象i，该计数为gc_ref_i。Python会遍历所有的对象i。对于每个对象i所引用的对象j，将相应的gc_ref_j减去1。\n\n在结束遍历后，gc_ref不为0的对象，和这些对象引用的对象，以及继续更下游引用的对象，需要被保留，而其他对象则被垃圾回收。\n\n\n# 函数式编程\n\nPython虽然不是纯粹的函数式编程，但是包含了不少函数式编程的语法。\n\n面向过程编程中，利用选择和循环结构，以及函数、模块等，对指令进行封装。\n\n面向对象编程中，实现了另外一种形式的封装，包含有数据的对象的一系列方法，这些方法能造成对象的状态改变。\n\n面向函数编程中，本质上也在于以函数为中心进行代码封装。\n\n\n# 又见函数\n\n\n# Python中的函数式\n\nPython中的函数实际上是一些特殊的对象。这一条已经符合函数式编程的一个重要方面：函数是第一级对象，能像普通对象一样使用。\n\n# 函数式编程中函数纯粹性\n\n函数式编程强调了函数的纯粹性。一个纯函数是没有副作用的，即这个函数的运行不会影响其他函数。纯函数像一个沙盒，把函数带来的效果控制在内部，从而不影响程序的其他部分。\n\n为了达到纯函数的标准，函数式编程要求其变量都是不可变更的。\n\n# Python与函数式编程\n\nPython并非完全的函数式编程语言。在Python中借鉴了函数式编程，尽量在编程中避免副作用。函数式编程的好处在于：纯函数相互独立，不会相互影响；纯函数方便进行并行化运算。\n\n早期的Python版本中，并没有函数式编程的相关语法。后来Python中加入了lambda函数，以及map、filter、reduce等高阶函数，从而加入了函数式编程的特征。\n\n函数式编程的思维方式，是自上而下的。它先提出一个大问题，在最高层用一个函数来解决这个大问题。在这个函数内部，再用其他函数来解决小问题。在这样递归式的分解下，直到问题得到解决。\n\n# 消除竞跑条件\n\n在下面的案例中，当多个进行同时修改一个变量的时候，进程的先后顺序会影响最终结果。\n\n下面的两个函数中使用了关键字global。global说明了x是一个全局变量。函数对全局变量的修改能被其他函数看到，因此有副作用。函数的执行顺序不确定，最终结果也不一定。这就被称为竞跑条件,是并行编程中需要极力避免的。\n\nfrom threading import Thread\nx = 5\ndef double():\n    global x\n    x = x * 2\ndef plus_ten():\n    global x\n    x = x + 10\nthread1 = Thread(target=double)\nthread2 = Thread(target=plus_ten)\nthread1.start()\nthread2.start()\nthread1.join()\nthread2.join()\nprint(x)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 并行运算\n\n# 什么是串行运算\n\n一台单处理器的计算机同一时间内只能执行一条指令。这种每次执行一条指令的工作方式称为串行运算。\n\n# 什么是并行运算\n\n所谓的并行运算，是指多条指令同时执行。我们可以在单机上通过多进程或多线程的方式，来模拟多主机的并行处理。\n\n事实上，单机的处理器按照"分时复用"的方式，把运算能力分配给多个进程。处理器在进程间频繁切换。因此，即使处理器同一时间只能处理一个指令，但通过在进程间的切换，也能造成多个进程齐头并进的效果。\n\n# 进程与线程\n\n进程有自己的内存空间，用来存储自身的运行状态、数据和相关代码。\n\n一个进程一般不会直接读取其他进程的内存空间。进程运行过程中，可以完成程序描述的工作。但在一个进程内部，又可以有多个称为"线程"的任务，处理器可以在多个线程之间切换，从而形成并行的多线程处理。\n\n线程看起来和进程类似，但是线程之前可以共享同一个进程的内存空间。\n\n# 多进程编程案例\n\n在下面的案例中是个多进程编程的例子。程序用了两个进程，进程的工作包含在函数中，分别是函数proc1()和函数proc2()。方法start()用于启动进程，而join()方法用于在主程序中等待相应进程完成。\n\n\n# 被解放的函数\n\n\n# 函数作为参数\n\n在函数式编程中，函数是第一级对象。所谓"第一级对象"，即函数能像普通对象一样使用。\n\n函数可以像一个普通对象一样，成为其他函数的参数。和称为其他函数的返回值。\n\n在下面的程序中，函数就充当了参数。函数argument_demo()的第一个参数f就是一个函数对象。按照位置传参，square_sum()传递给函数argument_demo()，对应参数列表中的f。\n\ndef square_sum(a, b):\n    return a**2 + b**2\ndef cubic_sum(a, b):\n    return a**3 + b**3\ndef argument_demo(f, a, b):\n    return f(a, b)\nprint(argument_demo(square_sum, 3, 5))\nprint(argument_demo(cubic_sum, 3, 5))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 函数作为返回值\n\n既然函数是一个对象，那么它就可以成为另一个函数的返回结果。\n\n下面的line_conf()函数的返回结果被赋予line对象，看到了在一个函数内部定义的函数。\n\ndef line_conf():\n    def line(x):\n        return 2*x+1\n    return line\nmy_line = line_conf()   # 函数对象\nprint(my_line(5))\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 函数对象作用域\n\n和函数内部的对象一样，函数对象也有存活范围，也就是函数对象的作用域。Python的缩进形式很容易让我们看到函数对象的作用域。\n\n下面的例子中，我们在line_conf()函数的隶属范围内定义的函数line()，就只能在line_conf()的隶属范围内调用。超出了函数line()的作用域，Python对该函数的调用失败。\n\ndef line_conf():\n    def line(x):\n        return 2*x + 1\n    print(line(5))          # 作用域内\nif __name__=="__main__":\n    line_conf()\n    print(line(5))          # 作用域外，报错\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 闭包\n\n# 什么是闭包\n\n一个函数和它的环境变量合在一起，就构成了一个闭包。\n\n在Python中，所谓的闭包是一个包含有环境变量取值的函数对象。首先闭包是一个函数对象，另外这个函数对象有些特殊，里面包含了环境变量取值。\n\n环境变量取值被复制到函数对象的__closure__属性中。在下面的案例中，my_line()的 __closure__属性中包含了一个元组。这个元组中的每个元素都是cell类型的对象。第一个cell包含的就是整数5，也就是我们返回闭包时的环境变量b的取值。\n\ndef line_conf():\n    b = 15\n    def line(x):\n        return 2*x + b\n    b = 5\n    return line      # 返回函数对象\nif __name__ == "__main__":\n    my_line = line_conf()\n    print(my_line.__closure__)\n    print(my_line.__closure__[0].cell_contents)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 为什么需要闭包\n\n提高了代码的可复用性。从下面的例子中，函数line()与环境变量a、b构成闭包。在创建闭包的时候，我们通过line_conf()的参数a、b说明直线的参量。\n\n这样，我们就能复用同一个闭包，通过代入不同的数据来获得不同的直线函数，如y=x+1和y=4x+5。闭包实际上创建了一群形式相似的函数。\n\ndef line_conf(a, b):\n    def line(x):\n        return a*x + b\n    return line\n\nline1 = line_conf(1, 1)\nline2 = line_conf(4, 5)\nline3 = line_conf(5, 10)\nline4 = line_conf(-2, -6)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n使用闭包，还可以起到减少函数参数的作用：\n\n下面的例子中，函数curve()是一个二次函数。它除了自变量x外，还有a、b、c三个参数。通过curve_closure()这个闭包，我们可以预设a、b、c三个参数的值。从而起到减少参数的作用。\n\ndef curve_closure(a, b, c):\n    def curve(x):\n        return a*x**2 + b*x + c\n    return curve\ncurve1 = curve_closure(1, 2, 1)\n\n\n1\n2\n3\n4\n5\n\n\n\n# 小女子的梳妆匣\n\n\n# 装饰器\n\n# 什么是装饰器\n\n装饰器是一种高级Python的语法。装饰器可以对一个函数、方法或者类进行加工。装饰器从操作上入手，为函数增加额外的指令。\n\n在Python中的变量名和对象是分离的，变量名其实是指向一个对象的引用。从本质上，装饰器起到的作用就是名称绑定，让同一个变量名指向一个新返回的函数对象，从而达到修改函数对象的目的。\n\n只不过，我们很少彻底地更改函数对象。在使用装饰器时，我们往往会在新函数内部调用旧的函数，以便保留旧函数的功能。\n\n函数装饰器是，接收一个函数，并返回一个函数，从而起到加工函数的效果。\n\n# 为什么需要装饰器\n\n在下面的例子中，我们想要为函数增加其他的功能，比如打印输入。我们重新修改函数的定义，为函数增加了功能。但是也可以改用装饰器，定义功能拓展本身，再把装饰器用于两个函数。\n\ndef decorator_demo(old_function):\n    def new_function(a, b):\n        print("input", a, b)\n        return old_function(a, b)\n    return new_function\n\n\n@decorator_demo\ndef square_sum(a, b):\n    return a**2 + b**2\n\n@decorator_demo\ndef square_diff(a, b):\n    return a**2 - b**2\n\nif __name__ == "__main__":\n    print(square_sum(3, 4))\n    print(square_diff(3, 4))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 如何使用装饰器\n\n在上面的例子中，装饰器可以用def的形式定义，如上面代码中的decorator_demo()。装饰器可以接受一个可调用对象作为输入参数，并返回一个新的可调用对象。\n\n装饰器新建了一个函数对象，也就是上面的new_function()。在new_function()中，我们增加了打印的功能，并通过old_function(a,b)来保留原有函数的功能。\n\n定义好装饰器后，我们就可以通过@语法使用了。在函数square_sum()和square_diff()定义之前调用@decorator_demo，实际上就是将square_sum()或square_diff()传递给了decorator_demo()，并将decorator_demo()返回的新的函数对象赋给原来的函数名square_sum()和square_diff()。\n\n所以当我们调用squre_sum(3,4)的时候，实际上发生的是：\n\nsquare_sum = decorator_demo(square_sum)\nsquare_sum(3,4)\n\n\n1\n2\n\n\n# time包装饰器\n\n在下面的案例中，我们利用time包来测量程序运行的时间。把测量程序运行时间的功能做成一个装饰器，将这个装饰器运用于其他函数，将显示函数的实际运行时间。\n\n在new_function()中，除了调用旧函数外，还前后额外调用了一次time.time()。由于time.time()返回挂钟时间，它们的差值反映了旧函数的运行时间。此外，我们通过打包参数的办法，可以在新函数和旧函数之间传递所有的参数。\n\n# 装饰器的其他好处\n\n装饰器可以实现代码的可复用性。我们可以用同一个装饰器修饰多个函数，以便实现相同的附加功能。例如，我们在每次处理HTTP请求前，都想附加一个客户验证功能时，那么就可以定义一个统一的装饰器，作用于每一个处理函数。\n\n\n# 带参装饰器\n\n# 什么是带参装饰器\n\n下面的例子中pre_str是一个带参装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有环境参数的闭包。\n\n当我们使用@pre_str("^_^")调用的时候，Python能够发现这一层的封装，并把参数传递到装饰器的环境中。\n\n该调用相当于：square_sum = pre_str("^_^")(square_sum)\n\ndef pre_str(pre=""):\n    def decorator(old_function):\n        def new_function(a, b):\n            print(pre + "input", a, b)\n            return old_function(a, b)\n        return new_function\n    return decorator\n\n@pre_str("^_^")\ndef square_sum(a, b):\n    return a**2 + b**2 \n\n@pre_str("T_T")\ndef square_diff(a, b):\n    return a**2 - b**2\n\nif __name__ == "__main__":\n    print(square_sum(3, 4))\n    print(square_diff(3, 4))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 带参装饰器的好处\n\n根据参数不同，带参装饰器会对函数进行不同的加工，进一步提高了装饰器的适用范围。我们可以把"管理员"和"用户"作为参数，传递给验证装饰器。\n\n对于那些负责关键HTTP请求的函数，我们可以把"管理员"参数传给装饰器。\n\n对于那些负责普通HTTP请求的函数，我们可以把"用户"参数传给它们的装饰器。\n\n这样，同一个装饰器就可以满足不同的需求了。\n\n\n# 装饰类\n\n# 什么是装饰类\n\n和函数的装饰器的概念比较类似，在类的装饰器中，一个装饰器可以接收一个类，并返回一个类，从而起到加工类的效果。\n\n无论是装饰函数，还是装饰类，装饰器的核心作用都是名称绑定。\n\n# 如何使用装饰器\n\n在下面的案例中，我们在装饰器decorator_class中，返回了一个新类NewClass。在新类的构造器中，我们用一个属性self.wrapped记录了原来类生成的对象，并附加了新的属性total_display，用于记录调用display()的次数。我们同时更改了display方法。通过装饰，我们的Bird类可以显示调用display()的次数。\n\ndef decorator_class(SomeClass):\n    class NewClass(object):\n        def __init__(self, age):\n            self.total_display = 0\n            self.wrapped = SomeClass(age)\n        def display(self):\n            self.total_display +=1\n            print("total display", self.total_display)\n            self.wrapped.display()\n    return NewClass\n\n\n@decorator_class\nclass Bird:\n    def __init__(self, age):\n        self.age = age\n    def display(self):\n        print("My age is", self.age)\n\nif __name__ == "__main__":\n    eagle_lord = Bird(5)\n    for i in range(3):\n        eagle_lord.display()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 高阶函数\n\n什么是高阶函数？\n\n能接收其他函数作为参数的函数，被称为高阶函数。\n\n高阶函数是函数式编程的一个重要组成部分。\n\n本节我们讲解最具有代表性的高阶函数：map()、filter()和reduce()。\n\n\n# lambda与map\n\n# 什么是lambda表达式\n\n这是一种新的定义函数的方式，之前我们定义函数的方式都是使用def关键字。\n\n# 为什么需要lambda表达式\n\n通过lambda表达式，我们可以创建一个匿名的函数对象。\n\n通常的def定义函数的方式，比较繁琐，有时候我们不需要来定义函数的名称。\n\n# 如何使用lambda表达式\n\n在下面的例子中，我们实现的是定义个函数，实现两个参数的和的功能。通过lambda表达式，我们可以创建一个匿名的函数对象。借着赋值语句，这个匿名函数赋予给函数名lambda_sum。\n\n函数的参数为x、y，返回值为x与y的和。函数lambda_sum()的调用与正常函数一样。这种lambda来产生匿名函数的方式适用于简短函数的定义。\n\ndef sum(x, y):\n    return x+y\n改为lambda表达式\nlambda_sum = lambda x, y: x + y\nprint(lambda_sum(3, 5))\n\n\n1\n2\n3\n4\n5\n\n\n# map函数\n\n所谓高阶函数，就是能处理函数的函数。\n\nmap()是Python的内置函数，它的第一个参数就是一个函数对象；map()的第二个参数是一个可循环对象。对于data_list的每个元素，lambda函数都会调用一次。\n\n那个元素会成为lambda函数的参数。换个角度说，map()把接收到的函数对象依次作用于每一个元素。最终，map()会返回一个迭代器。迭代器中的元素，就是多次调用lambda函数的结果。\n\ndef equivalent_generator(func, iter):\n    for item in iter:\n        yield func(item)\ndata_list = [1,3,5,6]\nresult =  map(lambda x: x+3, data_list)\n\n\n1\n2\n3\n4\n5\n\n\nmap()的多参数函数：\n\n可以定义多个参数，这个时候，map()的参数列表中就需要提供相应数目的可循环对象。\n\n下面的案例中，map()接收了square_sum()作为第一个参数。square_sum()要求有两个参数。因此，map()调用的时候需要两个可循环对象。第一个循环对象提供了square_sum()中对应于x的参数，第二个循环对象提供了对应于y的参数。\n\ndef square_sum(x, y):\n    return x**2 + y**2\ndata_list1 = [1,3,5,7]\ndata_list2 = [2,4,6,8]\nresult = map(square_sum, data_list1, data_list2)\n\n\n1\n2\n3\n4\n5\n\n\n\n# filter函数\n\nmap()函数和filter()函数的功能有很多相似的地方，都是把同一个函数应用于多个数据。\n\n和map()函数一样，内置函数filter()的第一个参数也是一个函数对象。它也将这个函数对象作用于可循环对象的多个元素。如果函数对象返回的是True，则该次的元素被放到返回的迭代器中。也就是说，filter()通过调用函数来筛选数据。\n\n类似的，filter()用于多参数的函数时，也可以在参数中增加更多的可循环对象。\n\n下面的案例中，使用的是filter()函数的一个例子。作为参数的larger100()函数用于判断元素是否比100大。\n\ndef larger1(a):\n    if a > 100:\n        return True\n    else:\n        return False\nfor item in filter(larger100, [10, 56, 101, 500]):\n    print(item)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# reduce函数\n\n# 什么是reduce函数\n\nreduce()函数是一个常见的高阶函数。函数reduce()在标准库的functools包中，使用之前需要引入。\n\n和map()、reduce()一样，reduce()函数的第一个参数是函数，但是reduce()对作为参数的函数对象有一个特殊要求，就是这个作为参数的函数必须能接收两个参数。\n\nreduce()可以把函数对象累进的作用于各个参数。\n\n# 如何使用reduce函数\n\n在下面的案例中，reduce()函数的第一个参数是求和的sum()函数，它接收两个参数x和y。在功能上，reduce()累进的运用传给它的二参函数。上一次运算的结构将作为下一次调用的第一个参数。上面过程不断重复，直到列表中的元素耗尽。\n\nfrom functools import reduce\ndata_list = [1,2,5,7,9]\nresult =  reduce(lambda x,y: x + y, data_list)\nprint(result)\n\n\n1\n2\n3\n4\n\n\n# map&reduce 并行运算\n\n函数reduce()通过某种形式的二元运算，把多个元素收集起来，形成一个单一的结果。map()函数和reduce()函数都是单线程的，所以运行效果和循环差不多。\n\n但map()、reduce()可以方便地移植到并行化的运行环境中。在并行运算中，reduce运算紧接着map运算。map运算的结果分布在多个主机上，reduce运算把结果收集起来。\n\n\n# 并行处理\n\n\n# 自上而下\n\n\n# 便携表达式\n\n函数式变的思维是自上而下式的。Python中也有不少体现了这一思维的语法，如生成器表达式、列表解析和词典解析。\n\n# 生成器表达式\n\n生成器表达式是构建生成器的便捷方法。\n\n下面的例子分别是用原有的方式用生成器定义循环对象，和用生成器表达式来定义循环对象。\n\ndef gen():\n    for i in range(4):\n        yield i\n等价使用：\ngen = (x for x in range(4))\n\n\n1\n2\n3\n4\n5\n\n\n# 列表解析\n\n利用列表解析的方式是快速生成列表的方法。\n\n列表解析的语法和生成器的表达式很像，只不过把小括号换成了中括号。\n\n语法直观，直截了当地说明了想要的是元素的平方，然后再通过for来增加限定条件，即哪些元素的平方。\n\nl = []\nfor x in range(10):\n    l.append(x**2)\n等价使用：\nl = [x**2 for x in range(10)]\n\n\n1\n2\n3\n4\n5\n\n\n# 词典解析\n\n词典解析可用于快捷的生成词典。它的语法也与之前的类似：\n\nd = {k: v for k,v in enumerate("Vamei") if val not in "Vi"}\n\n\n1\n\n\n\n# 懒惰求值\n\nPython中的迭代器的工作方式正是函数式编程中的懒惰求值。我们可以对迭代器进行各种各样的操作。\n\n懒惰求值可以最小化计算机要做的工作。\n\n除了运算资源，懒惰求值还能节约内存空间。对于即时求值来说，其运算过程的中间结果都需要占用不少额内存空间。而懒惰求值可以现在迭代器层面上进行操作，在获得最终迭代器以后一次性完成计算。\n\n\n# itertools包\n\n标准库中的itertools包提供了更加灵活的生成迭代器的工具，这些工具的输入大都是以后的迭代器。另一方面，这些工具完全可以自行使用Python实现，该包只是提供了一种比较标准、高效的实现方式。',normalizedContent:'# 《从python开始学编程》读书笔记\n\n本书以python为样本，不仅介绍了编程的基本概念，还着重讲解了编程语言的范式(面向过程、面向对象、面向函数)，并把编程语言的范式糅合在python中。\n\n\n# 先做键盘侠\n\n\n# 运算符\n\n\n# 数值运算\n\n具体的算术如下：\n\n>>> 1 + 9\n>>> 1.3 - 4\n>>> 3*5\n>>> 4.5/1.5\n>>>3**2            #乘方，即求3的二次方\n>>>10%3            #求余数，就求10除以3的余数。结果为1\n>>>"vamei say:" + "hello world" #字符串加法运算，也就是把字符串进行连接\n>>>"vamei"*2       #结果为"vameivamei"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 逻辑运算\n\nand 与 / or 或 /not 非\n\n\n# 判断表达式\n\n>>>1 == 1          # 相等\n>>>8.0 != 8.0      # 不等于\n>>>4 < 5           # <, 小于\n>>>3 <= 3          # <=，小于或等于\n>>>4 > 5           # >，大于\n>>>4 >=0           # >=,大于等于\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 运算优先级\n\n运算符的优先级高低如下所示：\n\n乘方：**\n乘除: *  /\n加减: + -\n判断: ==   >  >=  <  <=\n逻辑: !  and  or\n\n\n1\n2\n3\n4\n5\n\n\n\n# 数据结构\n\n\n# 变量的类型\n\npython能自由改变变量类型的特征被称为动态类型。并不是所有语言都支持动态类型。\n\n在静态类型的语言中，变量有事先说明好的类型。特定类型的数据必须存入特定类型的变量。\n\n变量有int整型、浮点数float、字符串string、布尔值bool。\n\n动态类型的语言看起来不需要说明类型，但其实是把区分类型的工作交给解释器。当改变变量的值的时候，python解释器会自动分辨出新数据的类型，再为数据开辟相应类型的内存空间。\n\npython解释器的贴心的服务让编程更加方便，但也把计算机的一部分能力用于支持动态类型上。这也是python的速度不如c语言等静态类型语言的一个原因。\n\n可以使用type()这一函数来查看变量的类型\n\nvar_integer = 10\nprint(type(var_integer))\n\n\n1\n2\n\n\n\n# 序列\n\npython中的序列和词典，都是特殊类型的变量。能像一个容器一样，都是容器型变量，收纳多个数据。\n\n序列是有顺序的数据集合，就好像一列排好队的士兵。序列包含的一个数据被称为序列的一个元素。序列可以包含一个或多个元素，也可以是完全没有任何元素的空序列。\n\n# 元组和列表\n\n序列有两种，元组(tuple)和列表(list)。两者的主要区别在于，一旦建立，元组的各个元素不可再变更，而列表元素可以变更。\n\n所以，元组看起来就像一种特殊的表，有固定的数据。\n\n>>> example_tuple = (2, 1.3, "love", 5.6, false)  #一个元组，括号\n>>> example_list = [true, 5, "smile"]             #一个列表，中括号\n>>> example_list1 = [1, [3,4,5]]                  #列表中嵌套另一个列表\n>>> type(example_tuple)                           #结果为\'tuple\'\n>>> type(example_list)                            #结果为\'list\'\n\n\n1\n2\n3\n4\n5\n\n\n# 序列下标\n\n序列元素的位置索引称为下标，可以通过下标来找到序列中的对应的元素。\n\npython中序列的下标从0开始，即第一个元素的对应下标为0。\n\n>>> example_tuple[0]          #显示序列example_tuple元组中第一个元素\n>>> nest_list = [1,[3,4,5]]   #列表中嵌套另一个列表\n>>> nest_list[1][2]           #显示的是第二个元素中，第二个元素是一个嵌套的列表，该嵌套列表中的第三个元素\n\n\n1\n2\n3\n\n\n元组一旦建立就不能改变，所以不能对元组的元素进行赋值操作。\n\n# 序列中范围引用\n\n序列中可以通过范围引用，来找到多个元素。范围引用的基本样式是：\n\n序列名[下限:上限:步长]\n\n下限表示起始下标，上限表示结尾下标。在起始下标和结尾下标之间，按照步长的间隔来找到元素。默认的步长是1。\n\n>>> example_tuple[:5]             # 从下标0到下标4，不包含下标5的元素\n>>> example_tuple[2:]             # 从下标2到最后一个元素\n>>> example_tuple[0:5:2]          # 下标为0，2，4的元素\n>>> sliced =  example_tuple[2:0:-1] # 从下标2到下标1\n>>> type(sliced)                  # 范围引用的结果还是一个元组\n>>> example_tuple[-1]             # 序列最后一个元素\n>>> example_tuple[-3]             # 序列倒数第三个元素\n>>> example_tuple[1:-1]           # 序列的第二个到倒数第二个元素\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 词典\n\n词典从很多方面都和序列中的列表比较相似。同样是一个可以容纳多个元素的容器。但是词典不是以位置来作为索引的。词典允许用自定义的方式来建立数据的索引。\n\n词典包含多个元素，每个元素以逗号分隔。词典的元素包含两部分，键(key)和值(value)。键是数据的索引，值是数据本身。\n\n词典的元素可以通过键来引用。\n\n词典不具备序列那样的连续有序性，所以适用于存储结构松散的一组数据。\n\n大部分场景中，我们都是用字符串来作为词典的键。其他类型，例如数字和布尔值，也可以作为词典的键值。\n\n>>> example_dict = {"tom":11, "sam":57, "lily":100}\n>>> type(example_dict)                                    #结果为\'dict\'\n>>> example_dict["tom"]                                   #结果为11，引用\n>>> example_dict["tom"] = 30                              #修改一个元素值\n>>> example_dict["lilei"] = 99                            #添加一个元素值\n>>> example_dict                                          #查看所有的元素值\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# if\n\n\n# if结构\n\n关键字if和else分别有隶属于它们的一行或多行代码，从属代码的开头会有四个空格的缩进。程序最终会根据if后的条件是否成立，选择是执行if的从属代码，还是执行else的从属代码。if结构在程序中实现了分支。\n\nelse也并非必须的，可以写只有if的程序；没有else，实际上与空的else等价。如果if后的条件不成立，那么计算机什么都不用执行。\n\ntotal = 980000\nif total > 500000:\n    print ("总价超过了50万")\n    transaction_rate = 0.01\nelse:\n    print ("总价不超过50w")\n    transaction_rate = 0.02\n    \ntotal = 980000\nif total > 500000:\n    print ("总价超过50w")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# if结构中的缩进\n\n用缩进来表明代码的从属关系，是python的特色。\n\n在python中，去掉了if条件后面i>0周围的括号，去除了每个语句句尾的分号，表示块的花括号也消失了。多出来了if...之后的:(冒号)，还有就是从属代码前面有四个空格的缩进。通过缩进，python识别出这两个语句是隶属于if的。为了区分隶属关系，python中的缩进是强制的。\n\n\n# if的嵌套与elif\n\nelif的定义：\n\n我们可以在if和else之间增加多个elif，从而给程序开出更多的分支。\n\nelif说白了就是在if后面的增加的，另外的多个条件，多个选择的分支。\n\n在出现if、elif和else三个块的代码中。python先检测if的条件，如果发现if的条件为假，则跳过隶属于if的程序块，检测elif的条件；如果elif的条件还是假的，则执行else块。程序会根据跳进，只执行三个分支中的一个。\n\nif i > 0:\n    i = i + 1\nelif i == 0:\n    i = i*10\nelse:\n    i = i - 1\n\n\n1\n2\n3\n4\n5\n6\n\n\nif的嵌套：\n\n意思是一个if结构中嵌套了另一个if结构，意思是进入一个选择分支后，又进入了一个选择分支。同样涉及到if的从属语句，要和if空4个空格的缩进。\n\n在进行完第一个if判断后，如果条件成立，那么程序依次运行，会遇到第二个if结构。程序将继续根据条件选择并决定是否执行。\n\nif i > 1:\n    print("good")\n    if i > 2:\n        print("better")\n\n\n1\n2\n3\n4\n\n\n\n# 循环\n\n循环用于重复执行一些代码块，在python中，循环有for和while两种。\n\n\n# for循环\n\n属于循环结构的、需要重复的程序，也要同if结构一样，要被缩进四个空格。\n\nfor a in [3,4.4,"life"]:\n    print(a)\n\n\n1\n2\n\n\n# for循环基本用法一\n\nfor的一个基本用法是在in后面跟一个序列，序列中元素的个数决定了循环重复的次数。\n\n从序列中取出元素，再赋予给一个变量并在隶属程序中使用。\n\nfor 元素 in 序列:\n    statement\n\n\n1\n2\n\n\n# for循环基本用法二\n\n有时候，我只是想简单重复特定的次数，不想建立序列，我们可以使用python提供的range()函数,range中间的数字就是说明了需要重复的次数。需要注意的是，range()提供的计数也是从0开始的，和序列列表中的下标一样。\n\nfor i in range(5):\n    print(i, "hello world!")\n\n\n1\n2\n\n\n\n# while循环\n\nwhile后面紧跟着一个条件。如果条件为真，则while会不停地循环执行隶属于它的语句。只有条件为假的时候，程序才会停止。\n\ni = 0\nwhile i < 10:\n    print(i)\n    i = i + 1\n\n\n1\n2\n3\n4\n\n\n\n# 跳出或终止\n\n循环结构还提供了两个有用的语句，可以在循环结构内部使用，用于跳出或终止循环。\n\ncontinue用于跳出循环的这一次循环，进行下一次的循环操作。\n\nbreak则是停止执行整个循环。\n\nfor i in range(10):\n    if i == 2:\n        continue\n    print(i)\n    \nfor i in range(10):\n    if i == 2:\n        break\n    print(i)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 代码规范\n\n由于强制缩进的规定，python代码看起来相对比较整齐。\n\npython官方文档中提供了一套代码规范，即pep8。\n\n 1. 在下列运算符的前后各保留一个空格\n    \n    = + - > == >= < <= and or not\n    \n    \n    1\n    \n\n 2. 下列运算符的前后不用保留空格\n    \n    * / **\n    \n    \n    1\n    \n\n 3. 如果有多行赋值，那么将上下的赋值号=对齐\n    \n    num      = 1\n    secnum   = 2\n    \n    \n    1\n    2\n    \n\n 4. 变量的所有字母小写，单词之间用下画线连接\n    \n    example_number = 10\n    \n    \n    1\n    \n\n 5. 类的命名采用首字母大写的英文单词。如果由多个单词连接而成，则每个单词的首字母都大写。单词之间不出现下画线。\n\n 6. 对象名、属性名和方法名，全部用小写字母。单词之间用下画线连接。\n\n\n# 过程大于结果\n\n这一章我们将完成面向过程的编程范式的学习。\n\n在这一章中，我们将看到其他面向过程的封装方法，即函数和模块。函数和模块把成块的指令封装成可以重复调用的代码块，并借着函数名和模块名整理出一套接口，方便未来调用。\n\n\n# 函数\n\n\n# 函数是什么\n\n从数学上来看，函数代表了集合之间的对应关系。例如定义域集合和值域集合之间的定义关系。\n\n从程序来看，函数是一种语法结构。它把一些指令封装在一起，形成一个组合拳。一旦定义好了函数，我们就可以通过对函数的调用，来启动这套组合拳。函数是对封装理念的实践。输入数据被称为参数，参数能影响函数的行为。\n\n\n# 定义函数\n\n在python中使用def这个关键字来定义函数，关键字def后面跟着的是函数的名字。\n\n在函数名后面，还有一个括号，用来说明函数有哪些参数，参数可以有多个，也可以完全没有。\n\n函数中定义的参数是一个形式代表，并非真正数据，所以又称为形参。\n\n根据python语法规定，即使没有输入数据，函数后面的括号也要保留。\n\n括号结束后，第一行的末尾，有一个冒号，后面的代码和之前的一样，也要有4个空格缩进。\n\n最后一句return，关键字return用于说明函数的返回值，即函数的输出数据。\n\n函数执行到return时就会结束，不管它后面是否还有其他函数定义语句。\n\nreturn启动了中止函数和制定返回值的功能。\n\n在python的语法中，return并不是必需的。如果没有return，或者return后面没有返回值，则函数将返回none。\n\nnone是python中的空数据，用来表示什么都没有。关键字return也返回多个值。\n\ndef square_sum(a,b):\n    a = a**2\n    b = b**2\n    c = a + b\n    return c\n\n\n1\n2\n3\n4\n5\n\n\n\n# 调用函数\n\n使用函数的过程叫做调用函数。\n\n在函数调用时出现的参数称为实参。\n\n函数调用的写法，其实与函数定义第一行def后面的内容相仿。只不过在调用函数的时候，我们把真实的数据填入到括号中，作为参数传递给函数。\n\n除具体的数据表达式外，参数还可以是程序中已经存在的变量。\n\na = 5\nb = 6\nx = square_sum(a, b)\nprint(x)\n\n\n1\n2\n3\n4\n\n\n\n# 函数文档\n\n在定义函数的时候，要加上清晰的说明文档，说明函数的功能和用法分别是什么。\n\n查看函数文档，可以用help()来查看。注意的是这个多行注释同样是有缩进的。\n\ndef square_sum(a,b):\n    """ return the square sum of two arguments"""\n    a = a**2\n    b = b**2\n    c = a + b\n    return c\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 参数传递\n\n把数据用参数的形式输入到函数，被称为参数传递。\n\n在定义函数的时候就说明了函数的个数，这是基本传参。\n\n如果在定义函数的时候，我们并不知道参数的个数；有时需要在程序运行的时候才能知道参数的个数；有时是希望函数定义的更加松散，以便于函数能运用于不同形式的调用。这个时候会用到包裹(packing)传参的方式来进行参数传递。\n\n\n# 基本传参\n\n# 位置确定参数\n\n有多个参数，在调用函数的时候，python会根据位置来确定数据对应哪个参数。确定实参与形参的对应关系。\n\ndef print_arguments(a, b, c):\n    """print arguments according to their sequence"""\n    print(a, b, c)\n    \nprint_arguments(1, 3, 5)\n\n\n1\n2\n3\n4\n5\n\n\n# 关键字传参\n\n根据位置传参比较死板，可以利用关键字(keyword)的方式来传递参数。\n\n在定义函数的时候，给形参一个符号标记，即参数名。\n\n关键字传递是根据参数名来让数据与符号对应上。\n\nprint_arguments(c=5, b=3, a=1)\n\n\n1\n\n\n位置传递与关键字传递混用，即一部分的参数传递根据位置，另一部分根据参数名。\n\n在调用函数的时候，所有的位置参数都要出现在关键字参数之前。\n\n如果位置参数放在关键字参数后面，python会报错。\n\nprint_arguments(1, c=5, b=3)\n\n\n1\n\n\n# 形参默认值\n\n在函数定义的时候，可以设置某些形参的默认值。如果我们在调用的时候不提供这些形参的具体数据，那么我们将采用定义时的默认值。\n\ndef f(a, b, c=10):\n    return a + b + c      \n\nprint(f(3,2,1))           # 参数c取传入的1，结果打印6\nprint(f(3,2))             # 参数c取默认值10。结果打印15\n\n\n1\n2\n3\n4\n5\n\n\n\n# 包裹传参\n\n所谓"包裹传参"是指，我们在定义函数的函数的时候，我们并不知道参数的个数，需要在程序运行时才能知道。所以我们在定义函数的函数更加松散，以便于函数能运用于不同形式的调用。\n\n包裹传参也有位置和关键字两种形式。\n\n# 元组方式\n\n在调用package_position()的时候，所有的数据都根据先后顺序，收集到一个元组。\n\n在函数内部，我们可以通过元组来读取传入的数据。\n\n在定义package_position()时要在元组名all_arguments前加*号，来表示使用的元组方式的包裹传递参数的方式。\n\ndef package_position(*all_arguments):\n    print(type(all_arguments))\n    print(all_arguments)\n\npackage_position(1,4,6)\npackage_position(5,6,7,8,9)\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 词典方式\n\n下面包裹关键字传递的例子中，参数传递方法把传入的数据收集为一个词典。\n\n每个关键字形式的参数调用，都会成为字典的一个元素。\n\n参数名成为元素的键，而数据成为元素的值。\n\n字典all_arguments收集了所有的参数，把数据传递给函数使用。我们在all_arguments前加上**，表示all_arguments的参数传递使用的包裹关键字字典的方式。\n\ndef package_keyword(**all_arguments):\n    print(type(all_arguments))\n    print(all_arguments)\n    \npackage_keyword(a=1, b=9)\npackage_keyword(m=2, n=1, c=11)\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 多种传参方式混用\n\n包裹位置传参和包括关键字传参可以混合使用。\n\n包裹传参和基本传参也可以混合使用。它们出现的先后顺序是：位置->关键字->包裹位置->包裹关键字。\n\n\n# 解包裹\n\n之前的包裹传参，是用于参数定义的阶段。主要的应用场景是：我们不确定后续有多个参数要传入，只能定义为元组或字典的方式的函数，后续调用的时候，不用考虑传入多少个参数了。\n\n解包裹并不是包裹的相反操作，解包裹的意思是，调用函数的时候，传入的参数并不是一个直接的传递的这个参数值，而是一个元组(用*表示)，或是一个字典(用**表示)，来提醒python把对应的元组或字典去拆成一个一个对应的元素到具体的参数中去。\n\n解包裹用于函数调用。在调用函数的时候，几种参数的传递方式也可以混合。依然是相同的基本原则：位置 -> 关键字 -> 位置解包裹 -> 关键字解包裹\n\ndef unpackage(a,b,c):\n    print(a,b,c)\n\nargs = (1,3,4)\nunpackage(*args)       #结果为1 3 4\n\nargs = {"a":1,"b":2,"c":3}\nunpackage(**args)      # 打印1、2、3\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 递归\n\n\n# 为什么要用递归\n\n有些应用场景，利用递归的算法特性，会使得逻辑更清晰。\n\n例如，如何比对一个json串的时候，可能一个key value值里面又包含一个key value值，层层嵌套。\n\n\n# 什么是递归\n\n递归是函数调用其自身的操作。\n\n# 数学归纳法\n\n递归的思想源于数学归纳法，常用于证明命题在自然数范围内成立。\n\n数学归纳法本身非常简单。如果我们想要证明某个命题对于自然数n成立，那么：\n\n第一步，证明命题对于n=1成立。\n\n第二步，假设命题对于n成立，n为任意自然数，则证明在此假设下，命题对于n+1成立。\n\n\n# 如何写递归\n\n递归的写法，要注意三个方面：初始条件、终止条件及衔接。从下面的列子来看衔接就是函数gasssian_sum(n)与gasssian_sum(n-1)的关系。\n\n为了保证计算机不陷入死循环，递归要求程序有一个能够达到的终止条件。还有递归的关键是说明衔接的两个步骤之间的衔接条件。\n\n设计递归程序的时候，我们从最终结果入手，要想求得gasssian_sum(100)，计算机会把这个计算拆解为求得gasssian_sum(99)的运算，以及gasssian_sum(99)加上100的运算。以此类推，直到拆解为gasssian_sum(1)的运算，就触发终止条件。\n\ndef gasssian_sum(n):\n    if n == 1:\n        return 1\n    else:\n        return n + gasssian_sum(n-1)\n\nprint(gasssian_sum(100))\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 函数栈\n\n程序中的递归需要用到栈这一数据结构。\n\n栈最显著的特征是"后进先出"(lifo)。栈中的每一个元素，被称为一个帧(frame)。栈只支持两个操作：pop和push。栈用弹出(pop)操作来取出栈顶元素，用推入(push)操作将一个新的元素存入栈顶。\n\n上面的例子中，为了计算gasssian_sum(100)，我们需要先暂停gasssian_sum(100)，开始gasssian_sum(99)的计算。为了计算gasssian_sum(99)，需要先暂停gasssian_sum(99)，调用gasssian_sum(98) ......。在触发终止条件前，会有很多次为完成的函数调用。每次函数调用时，我们在栈中推入一个新的帧，用来保存这次函数调用的相关信息。栈不断增长，知道计算出gasssian_sum(1)后，我们又会恢复计算gasssian_sum(2)、gasssian_sum(3), ......。由于栈"后进先出"的特点，所以每次只需要弹出栈的帧，就正好是我们所需要的gasssian_sum(2)、gasssian_sum(3)......直到弹出藏在最底层的帧gasssian_sum(100)。\n\n所以，程序运行的过程，可以看做是一个先增长栈后消灭栈的过程。每次函数调用，都伴随着一个帧入栈。如果函数内部还有函数调用，那么又会多一个帧入栈。当函数返回时，相应的帧会出栈。等到程序的最后，栈清空，程序就完成了。\n\n要注意栈溢出(stack overflow)，由于直到栈底才会去释放之前栈的空间，需要消耗很多的栈内存。\n\n\n# 变量的作用域\n\n 1. python寻找变量的范围不止是当前的函数帧(当前函数内的变量)，它还会寻找函数外部，也就是python主程序中定义的变量。\n\n 2. 当主程序中已经有一个变量，函数调用内部可以通过赋值的方式再创建了一个同名变量。函数会优先使用自己函数帧中的那个变量。这个时候，函数内部使用的变量的操作，不会影响到外部的那个同名变量。\n\n 3. 函数的参数与函数内部变量类似。可以把参数理解为函数内部的变量。在函数调用的时候，会把数据赋值给这些变量。等到函数返回的时候，这些参数相关的变量会被清空。\n\n 4. 但是将一个列表传递给函数，作为函数的参数的时候。在对函数进行操作后，函数外部的列表会发生变化。也就是说参数是一个数据容器的时候，函数内外部只存在一个数据容器，所以对函数内部对该数据容器的操作，会影响到函数外部。对于数据容器，函数内部的更改会影响到外部\n\ninfo = "hello"\ndef external_var():\n    info = "python"\n    print(info)              # 结果为python\nexternal_var()\nprint(info)                  # 结果还是为hello\n\nb = [1,2,3]\ndef change_list(b):\n    b[0] = b[0] + 1\n    return b\n\nprint(change_list(b))       # 打印[2,2,3]\nprint(b)                    # 打印[2,2,3],已经改变了外部的list\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 模块\n\n\n# 为什么要有模块\n\n通过模块，我们可以调用其他文件中的函数。而引入(import)模块，就是为了在新的程序中重复利用已有的python程序。\n\n\n# 什么是模块\n\n对于面向过程语言来说，模块是比函数更高一层的封装模式。程序可以以文件为单位实现复用。\n\n在python中，一个.py文件就构成一个模块。\n\n\n# 如何使用模块\n\n调用模块中的函数：\n\n先写一个first.py文件，里面定义了一个laugh()的函数；然后再在同一个目录下写一个second.py文件，在这个python程序文件中引入first模块中的laugh函数；然后就可以在first模块中调用laugh函数了。\n\nfirst.py文件如下：\n\ndef laugh():\n    print("hahaha")\n\n\n1\n2\n\n\nsecond.py文件如下：\n\nfrom first import laugh\nfor i in range(10):\n    laugh()\n\n\n1\n2\n3\n\n\n调用模块中的变量：\n\n除了函数，我们还可以引入其他文件中包含的数据。\n\n比如我们在module_var.py中有如下变量，我们在import_demo.py中，引入了这一变量：\n\nmodule_var.py文件如下：\n\ntext = "hello!"\n\n\n1\n\n\nimport_demo.py中，我们引入这一变量：\n\nfrom module_var import text\nprint(text)       #打印"hello!"\n\n\n1\n2\n\n\n\n# 搜索路径\n\npython寻找相应的模块，有着自己的查找路径：\n\n 1. python会自动在当前文件夹下搜索它想要引入的模块。\n 2. 查找标准库的安装路径。\n 3. 操作系统环境变量pythonpath所包含的路径。\n\n\n# 异常处理\n\n\n# 异常分类\n\n 1. 语法错误\n    \n    这类错误，是python在没有运行代码的时候就发现了这类语法错误。例如，for循环中没有冒号。\n\n 2. 运行时错误\n    \n    只有在python编译器运行的时候，才会发现的错误，被称为运行时错误。由于python是动态语言，许多操作必须在运行时才会执行，比如确定变量的类型等等。因此，python要比静态语言更容易产生运行时错误。\n\n 3. 语义错误\n    \n    还有一种错误，叫做语义错误。编译器认为你的程序没有问题，可以正常运行。但是当检查程序的时候，却发现程序并非我们想要的结果。这种错误最为隐蔽，也最难纠正。\n\n\n# 异常处理\n\n# 为什么需要异常处理\n\n对于运行时可能产生的错误，我们可以提前在程序中处理。\n\n这样做有两个可能的目的：一个是让程序中止前进行更多的操作，比如提供更多的关于错误的信息。另一个则是让程序在犯错后依然能运行下去。\n\n异常处理还可以提高程序的容错性。\n\n# 异常的基本结构\n\n异常处理完整的语法形式为：\n\ntry:\n    ...\nexcept exception1:\n    ...\nexcept exception2:\nelse:\n    ...\nfinally:\n    ...\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n如果try中有异常发生时，将执行异常的归属，执行except。\n\n异常层层比较，看是否是exception1、exception2 ...... 直到找到其归属，执行相应的except中的语句。\n\n如果try中没有异常，那么except部分将跳过，执行else中的语句。\n\nfinally是无论是否有异常，最后都要做的一些事情。\n\n# 所有异常都捕获\n\n如果except后面没有任何参数，那么表示所有的exception都交给这段程序处理：\n\nwhile true:\n    inputstr = input("please input a number:")\n    try:\n        num = float(inputstr)\n        print("input number:", num)\n        print("result:", 10/num)\n    except:\n        print("something wrong. try again.")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 子程序上抛异常至主程序\n\n如果子程序无法将异常交给合适的对象，那么异常将继续向上层抛出，直到被捕捉或者造成主程序报错。比如下面的程序，子程序的try...except...结构无法处理相应的除以0的错误，所以错误被抛给上层的主程序。\n\ndef test_func():\n    try:\n        m = 1/0\n    except valueerror:\n        print("catch valueerror in the sub-function")\n\ntry:\n    test_func()\nexcept zerodivisionerror:\n    print("catch error in the main program")\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 主动抛异常\n\n使用raise关键字，我们也可以在程序中主动抛出异常。\n\nraise zerodivisioerror()\n\n\n1\n\n\n\n# 朝思暮想是对象\n\n为什么要有面向对象？\n\n在科学实践过程中，发现传统的面向过程的方法的编程方式无法去模拟真实世界中的个体。\n\n面向对象的出现就是为了解决上述的困难，通过类和对象这两种语法结构，加强了程序模拟真实世界的能力。而"模拟"，正是面向对象编程的核心。\n\npython的一条哲学理念是"一切皆对象"。\n\n\n# 类\n\n\n# 什么是类\n\n和我们认识的日常生活中的"类"的概念差不多。在日常生活中，我们把相近的东西归为一类，而且给这个类起一个名字。比如鸟类、鱼类等等。\n\n\n# 如何定义类\n\n定义类的时候，我们使用class关键字。类的名字的括号里面有一个关键字object，也就是"东西"的意思，即某一个个体。我们把个体称为对象。\n\n在下面的例子中，我们定义了两个量，一个用于说明鸟类有羽毛，另一个用于说明鸟类的繁殖方式，这两个量称为类的属性。\n\nclass bird(object):\n    feather = true\n    reproduction = "egg"\n\n\n1\n2\n3\n\n\n\n# 定义类的方法\n\n除了用数据性的属性来分辨类别外，有时也会根据这类东西能做什么事情来区分。比如，鸟会移动。\n\n这样的一些"行为"属性称为方法。\n\npython中，一般通过在类的内部定义函数来说明方法。\n\n在下面的例子中，我们给鸟类新增了一个方法属性，表示鸟叫的方法chirp()。这个方法chirp()看起来很像一个函数。它的第一个参数是self，是为了在方法内部引用对象自身。无论该参数是否用到，方法的第一个参数必须是用于指代对象自身的self。剩下的sound是为了满足我们的需求设计的，代表了鸟叫的内容。方法chirp()会把sound打印出来。\n\nclass bird(object):\n    feather = true\n    reproduction = "egg"\n    def chirp(self, sound):\n        print(sound)\n\n\n1\n2\n3\n4\n5\n\n\n\n# 对象\n\n\n# 什么是对象\n\n对象是一个个体，它是某一类的实例化。调用类，我们可以创造出这个类下面的一个对象。\n\nsummer = bird()\n\n\n# 展示对象的属性和方法\n\n初始化完一个对象后，这个对象就有了这个类的属性和方法。\n\n对于属性的引用，我们可以通过对象.属性的形式来实现。\n\nprint(summer.reproduction) #打印\'egg\'\n\n对于方法的调用，我们可以让该对象执行鸟类允许的动作。\n\nsummer.chirp("jijijij") #打印\'jijijij\'\n\n> 注意，在调用方法的时候，我们只传递了一个参数，也就是"jijijij"。这是方法与函数有所区别的地方。尽管我们在定义类的方法的时候，必须加上这个self参数，但是self只用能在类定义的内部，所以在调用方法的时候不需要对self传入参数了。\n\n\n# 定义对象的个性属性\n\n每个对象个体，除了拥有共性的类属性外，还需要用于说明个性的对象属性。\n\n# self配置个性对象属性\n\n这个时候，我们可以在类中，通过self来操作对象的属性。\n\n下面的例子中，我们在bird类中，定义了一个set_color的方法，在这个方法中，我们通过self参数设定了对象的属性color。\n\n和对象.类属性方式一样，我们能通过对象.对象属性的方式来操作对象属性。由于对象属性依赖于self，我们必须在某个方法内部才能操作类属性。因此，对象属性没办法像类属性一样，在类下方直接赋初值。\n\nclass bird(object):\n    def chirp(self, sound):\n        print(sound)\n    def set_color(self, color):\n        self.color = color\n\nsummer = bird()\nsummer.set_color("yellow")\nprint(summer.color)               # 打印\'yellow\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# _init_()方法初始化对象属性\n\n上面是自己在类里面定义一个方法来满足，对象的个性化属性。\n\n这里是利用python自带的初始化对象属性的办法，这是特殊方法。名字很特别，前后有两个下划线。比如_init_()、_add_()、_dict_()等。对于类中的_init_()方法，python会在每次创建对象时自动调用。和上面的方式不同的地方，上面需要手动调用。\n\nclass bird(object):\n    def __init__(self, sound):\n        self.sound = sound\n        print("my sound is:", sound)\n    def chirp(self):\n        print(self.sound)\n\nsummer = bird("ji")\nsummer.chirp()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# self调用类中其他方法\n\n这里只是描述了self的另外一种功能，也就是说除了上面两种可以通过self自定义个性化的对象属性，还可以利用self实现在一个方法内部调用同一类的其他方法。\n\nclass bird(object):\n    def chirp(self, sound):\n        print(sound)\n    \n    def chirp_repeat(self, sound, n):\n        for i in range(n):\n            self.chirp(sound)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 子类\n\n\n# 什么是子类\n\n为了更为确切的用程序语言的方式来模拟现实世界，出现了子类的数据结构。\n\n通过子类继承父类的方式，很减少程序中的重复信息和重复语句。继承提高了程序的可重复使用性。\n\n类别本身还可以进一步细分为子类。例如，鸟类可以进一步分为鸡、天鹅等。在面向对象的编程中，我们通过子类继承父类的方式来表达这个概念。\n\n在下面的案例中，我们定义了一个鸡类，和一个天鹅类，它们都继承来着鸟类这个父类。鸡类和天鹅类也就同时拥有鸟类所有定义的数据性属性和方法性的属性。\n\n需要注意的是，在定义子类的时候，子类名字后面的括号里面，不在是object，而是父类的名字。\n\n最基础的情况，是类定义的括号中是object。类object其实是python中的一个内置类。它充当了所有类的祖先。\n\nclass bird(object):\n    feather = true\n    reproduction = "egg"\n    def chirp(self, sound):\n        print(sound)\n\nclass chicken(bird):\n    how_to_move = "walk"\n    edible = true\n\nclass swan(bird):\n    how_to_move = \'swim\'\n    edible = false\n\nsummer = chicken()\nprint(summer.feather)                   # 打印true\nsummer.chirp("ji")                      # 打印\'ji\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 属性覆盖\n\n我们可以在子类中替换父类已经存在了的属性。\n\n通过对方法的覆盖，我们可以改变子类的行为。\n\n在下面的案例中，鸡类是鸟类的子类。在鸡类中，我们同样定义了一个chirp()方法，这个方法在鸟类中也是有定义的。\n\n调用鸡类中的这个chirp()方法来看，鸡类会调用自身定义的chirp()方法，而不是父类中的chirp()方法。从效果上来看，这就好像是父类中的方法chirp()被子类中的同名属性覆盖(override)了一样。\n\nclass bird(object):\n    def chirp(self):\n        print("make sound")\n\nclass chicken(bird):\n    def chirp(self):\n        print("ji")\n\nbird = bird()\nbird.chirp()            # 打印\'make sound\'\n\nsummer = chicken()\nsummer.chirp()          #打印\'ji\'\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 子类中调用父类中被覆盖的方法\n\n上面的案例中，我们可以通过对方法的覆盖，可以彻底地改变子类的行为。但有时候，子类的行为是父类行为的拓展。这个时候，我们可以通过super关键字在子类中调用父类中被覆盖的方法。\n\n案例中，在鸡类的chirp()方法中，使用了super。这个是一个内置类，能产生一个指代父类的对象。通过super，我们在子类的同名方法中调用了父类的方法。这样，子类的方法既能执行父类中相关操作，又能定义属于自己的额外操作。\n\nclass bird(object):\n    def chirp(self):\n        print("make sound")\n\nclass chicken(bird):\n    def chirp(self):\n        super().chirp()      #在子类的同名方法中调用了父类的同名方法\n        print("ji")\n\nbird = bird()\nbird.chirp()                # 打印"make sound"\n\nsummer = chicken()\nsummer.chirp()              # 打印"make sound"和"ji"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 回头再看对象\n\n在对对象的基本概念了解后，我们在回头看看了解下之前我们已经熟悉列表、元组与字符串、词典、循环、函数。\n\n在python中，一切皆对象。我们要以面向对象的思维，再次来思考学习。\n\n\n# 列表对象\n\n# 什么是列表对象?\n\n我们在描述一个对象的时候，先考虑一下它所从属的类。list类，类是模拟现实世界的数据结构。它有自定义的静态的属性，和动态的可调用的方法。当我们新建了一个list的时候，实际上是在创建了list类的一个对象。\n\n# 创建list对象\n\na = [1,2,5,3,5]\ntype(a)\n\n\n1\n2\n\n\n从新建的一个list的过程来看，实际上我们是创建了list类的一个对象。list类是python自带的，已经提前定义好的，被称为内置类。\n\n# list类的属性和方法\n\n 1.  利用函数dir()来查询一个类或对象的所有属性。dir(list)\n\n 2.  利用函数help()来查询类的说明文档。help(list)\n\n 3.  调用list类中的count方法\n     \n     a = [1,2,3,5,9.0,"good",-1,true,false,"bye"]\n     a.count(5)\n     \n     \n     1\n     2\n     \n\n 4.  调用list类中的index方法\n     \n     a.index(3) 查询元素3第一次出现时的下标\n\n 5.  调用list类中的append方法来添加一个新元素\n     \n     a.append(6) 在列表的最后增添一个新元素6\n\n 6.  调用list类中的sort方法来排序\n     \n     a.sort()\n\n 7.  调用list类中的reverse方法来颠倒次序\n     \n     a.reverse()\n\n 8.  调用list类中的pop方法去除最后一个元素\n     \n     a.pop() 去除最后一个元素，并将该元素返回\n\n 9.  调用list类中的remove方法去除第一次出现的某个元素\n     \n     a.remove(2) 去除第一次出现的元素2\n\n 10. 调用list类中的insert方法来插入指定位置的元素\n     \n     a.insert(0,9) 在下标为0的位置插入9\n\n 11. 调用list类中的clear方法来清空列表\n     \n     a.clear() 清空列表\n\n\n# 元组与字符串对象\n\n# 元组对象\n\n元组与列表一样，都是序列。但是元组不能变更内容。所以，元组只能进行查询操作，不能进行修改操作。\n\na = (1,3,5)\na.count(5)     #计数，看总共有多少个元素5\na.index(3)     #查询元素3第一次出现时的小标\n\n\n1\n2\n3\n\n\n# 字符串对象\n\n字符串是特殊的元组，因此可以执行元组的方法。\n\n尽管字符串是元组的一种，但是字符串有一些方法能改变字符串。其实这些方法并不是修改字符串对象，而是删除原有字符串，再建立一个新的字符串，所以并没有违背元组的不可变性。\n\n# 字符串对象方法\n\n下面的示例中，str是一个字符串，sub是str的一个子字符串。s为一个序列，它的元素都是字符串。width为一个整数，用于说明新生成字符串的宽度。\n\nstr = "hello world!"\nsub = "world"\nstr.count(sub)         #返回：sub在str中出现的次数\nstr.find(sub)          #返回：从左开始，查找sub在str中第一次出现的位置。\n                       #如果str中不包含sub，返回-1\nstr.index(sub)         #和上面的一样的意思\nstr.rfind(sub)         #返回：从右开始，查找sub在str中第一次出现的位置。\n                       #如果str中不包含sub，举出错误\nstr.rindex(sub)        #和上面的一样的意思\nstr.isalnum()          #返回：true，如果所有的字符都是字母或数字\n                       #注意上面空格，标点符号也不能出现\nstr.isalpha()          #返回：true，如果所有的字符都是字母\nstr.isdigit()          #返回：true，如果所有的字符都是数字\nstr.istitle()          #返回：true，如果所有的词的首字母都是大写\nstr.isspace()          #返回：true，如果所有的字符都是空格\nstr.islower()          #返回:true,如果所有的字符都是小写字母\nstr.isupper()          #返回：true，如果所有的字符都是大写字母\nstr.split([sep, [max]]) # 返回：从左开始，以空格为分隔符，\n                        # 将str分隔为多个子字符串，总共分隔max次\nstr.rplit([sep, [max]]) # 返回：从右开始，以空格为分隔符，\n                        # 将str分隔为多个子字符串，总共分隔max次\nstr.joins(s)            # 返回：将s中的元素，以str为分隔符\n                        # 合并为一个字符串\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 词典对象\n\n词典同样是一个类，我们定义了一个词典对象后，可以充分利用下词典类中的各种方法。\n\n# 词典对象方法\n\nkeys()方法来循环遍历每个元素的键，keys()的方法实现的是把这个词典的所有的key塞到一个list中。\n\nexample_dict = {"a":1, "b":2}\ntype(example_dict)\nfor k in example_dict.keys():\n    print(example_dict[k])\n\n\n1\n2\n3\n4\n\n\nvalues()方法循环遍历每个元素的值，values()的方法实现的是把这个词典的所有的value塞到一个list中。\n\nfor v in example_dict.values():\n    print(v)\n\n\n1\n2\n\n\nitems()方法直接遍历每个元素：\n\nfor k,v in example_dict.items():\n    print(k,v)\n\n\n1\n2\n\n\nclean()方法，清空整个词典：\n\nexample_dict.clean()     #清理完后，变成了{}\n\n\n1\n\n\n\n# 意想不到的对象\n\n\n# 循环对象\n\n在python中循环数据结构也是由循环对象来实现的。\n\n# 什么是循环对象\n\n所谓的循环对象包含有一个_next_()方法。这个方法的目的是生成循环的下一个结果。在生成过循环的所有结果之后，刚方法将抛出stopiteration异常。\n\n例如一个for这样的循环语法调用循环对象时，它会在每次循环的时候调用/next()方法，知道stopiteration出现。循环接收到这个异常，就会知道循环已经结束，将停止调用_next_()。\n\n下面的案例主要验证的是_next_()方法,内置函数iter()会把一个列表转变为循环对象。在真实的for循环中，我们可以省去内置函数iter的转换。这是因为，for结构会自动执行这一转换。\n\nexample_iter = iter([1,2])\nexample_iter.__next__()         # 显示1\nexample_iter.__next__()         # 显示2\nexample_iter.__next__()         # 出现stopiteration异常\n\nfor item in iter([1,2]):\n    print(item)\n\nfor item in (1,2):\n    print(item)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 生成器\n\n我们同样可以借助于生成器(generator)来自定义循环对象。生成器的编写方法和函数定义类似，只是在return的地方改为yield。\n\n生成器中可以有多个yield。当生成器遇到一个yield时，会暂停运行生成器，返回yield后面的值。当再次调用生成器的时候，会从刚才暂停的地方继续运行，直到下一个yield。生成器自身又构成一个循环对象，每次循环使用一个yield返回的值。python中的内置函数range()返回的是一个循环对象，而不是一个序列。\n\ndef gen():\n    a = 100\n    yield a\n    a = a*8\n    yield a \n    yield 1000\n\nfor i in gen():\n    print(i)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 函数对象\n\n在python中，函数也是一种对象。实际上，任何一个有_call_()特殊方法的对象都被当作是函数。\n\n通过type()方法来查看一个函数的类型，可以看到我们定义的一个函数其实也是一个函数类。我们在调用函数的过程，实际上是初始化了一个函数对象；而调用函数，加入实参的过程，就是对这个函数对象调用了__call__的内置方法，实现了函数传参。\n\nclass samplemore(object):\n    def __call__(self, a):\n        return a + 5\n\nadd_five = samplemore()    #生成函数对象\nprint(add_five(2))         #像一个函数一样调用函数对象，结果为7.\n\n#原先我们定义函数的过程如下；\ndef call(a):\n    return a + 5\nprint(call(2))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 模块对象\n\n在python中的模块对应一个.py文件，模块也是对象。在前面的定义中，我们看到在调用模块中各个属性和方法的过程，实际上就是调用对象的属性或方法的过程。\n\n# 调用模块对象中方法的几种方式\n\n 1. 第一种方法，from ... import ...的方式\n    \n    from time import sleep    #单独从time模块中引入sleep动态属性\n    sleep(10)\n    \n    \n    1\n    2\n    \n\n 2. 第二种方法，from ... import * 的方式\n    \n    from time import *        #使用简单暴力的方法，一次性引入模块的所有属性\n    sleep(10)\n    \n    \n    1\n    2\n    \n\n 3. 第三钟方法，import ... 的方式\n    \n    import time               #引入time模块\n    time.sleep(10)            #利用对象.属性的方式来调用它\n                              #带上对象名的方式，好处可以避免模块中的属性引用时候，\n                              #出现同名\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 4. 第四种方法，import ... as xxx的方式\n    \n    import time as t         #我们还可以给模块换个名字\n    t.sleep(10)\n    \n    \n    1\n    2\n    \n\n 5. 第五种方法，引入文件夹中的模块\n    \n    import this_dir.module       #"this_dir"是目录名称,module是模块的名字\n                                 #模块的名字也就是xxx.py前面的名字\n                                 #需要在"this_dir"下建立一个__init__.py空文件\n    \n    \n    1\n    2\n    3\n    \n\n# 模块对象的名字\n\n每个模块对象都有一个__name__属性，用来记录模块的名字。\n\n当一个.py文件作为主程序运行的时候，比如python foo.py，这个文件也会有一个对应的模块对象。但这个模块对象的_name__属性会是 "_main"\n\n\n# 异常对象\n\n我们在程序中加入异常处理的try结构，捕捉程序中出现的异常。\n\n实际上，我们捕捉到的也是一个对象。\n\n利用except ... as ...的语法，我们在except结果中用e来代表捕获到的类型对象。关键字except直接跟随zerodivisionerror实际上是异常对象的类。\n\ntry:\n    m = 1/0\nexcept zerodivisionerror as e:\n    print("catch nameerror in the sub-function")\n    print(type(e))        #查看到该异常类的类型\n    print(dir(e))         # 异常对象的属性\n    print(e.message)      # 打印的异常信息\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 对象带你飞\n\n\n# 存储\n\n\n# 文件(文本对象)\n\n磁盘以文件为单位来存储数据。\n\n对于计算机来说，数据的本质就是有序的二进制数序列。\n\n如果以字节为单位，也就是每8位二进制数序列为单位，那么这个数据序列就称为文本。\n\n这是因为，8位二进制数序列正好对应ascii编码中的一个字符。\n\n而python能够借助文本对象来读写文件。\n\n# 创建文本对象\n\n在python中，我们可以通过内置函数open来创建文件对象。在调用open的时候，需要说明文件名，以及打开文件的方式。\n\nf = open(文件名，方式)\n"r" #读取已经存在的文件\n"w" #新建文件，并写入\n"a" #如果文件存在，那么写入到文件的结尾\n    #如果文件不存在，则新建文件并写入\n\n\n1\n2\n3\n4\n5\n\n\n 1. 只读的方式打开文件\n    \n    f = open("test.txt", "r")\n    \n    \n    1\n    \n\n 2. 读取文件\n    \n    content = f.read(10)  #读取10个字节的数据\n    content = f.readline() #读取一行\n    content = f.readlines() #读取所有行，存储在列表中，每个元素都是一行\n    \n    \n    1\n    2\n    3\n    \n\n 3. 写或追加方式打开文件\n    \n    f = open("test.txt", "w")\n    f = open("test.txt", "a")\n    \n    \n    1\n    2\n    \n\n 4. 写入文件\n    \n    f.write("i like apple")\n    f.write("i like apple\\n")    #unix系统中加入换行符\n    f.write("i like apple\\r\\n")    #windows系统中加入换行符\n    \n    \n    1\n    2\n    3\n    \n\n 5. 关闭文件\n    \n    打开文件端口将占用计算机资源，因此，\n    在读写完成后，应该及时用文件对象的close方法关闭文件：\n    f.close()\n    \n    \n    1\n    2\n    3\n    \n\n\n# 上下文管理器\n\n文件操作常常和上下文管理器一起使用。\n\n# 什么是上下文管理器\n\n上下文管理器(context manager)用于规定某个对象的使用范围。一旦进入或离开该使用范围，则会有特殊操作被调用，比如为对象分配或者释放内存。\n\n# 为什么需要上下文管理器\n\n对于文件操作，我们需要在读写结束时关闭文件。我们经常会忘记关闭文件，无谓的占用资源。上下文管理器可以在不需要文件的时候，自动关闭文件。\n\n# 如何使用上下文管理器\n\n下面有两个案例，一个案例是常规的文件操作，一个案例是使用上下文管理器的方式来进行的文件操作。\n\n第二段程序就使用了with...as...的结构。上下文管理器有隶属于它的程序块，当隶属的程序块执行结束时，也就是语句不再缩进时，上下文管理器就会自动关闭文件。在程序中，我们使用f.closed属性来验证是否已经关闭。通过上下午管理器，我们相当于用缩进来表达对象的打开范围。\n\n# 常规使用\nf = open("new.txt","w")\nprint(f.closed)               # 检查文件是否打开\nf.write("hello world!")\nf.close()\nprint(f.closed)               # 打印true\n\n# 使用上下文管理器\nwith open("new.txt","w") as f:\n    f.write("hello world!")\nprint(f.closed)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# pickle包\n\n# pickle包是用来干什么的\n\n从文字的理解来看，该模块是用来把某个对象保存下来，再存在磁盘里的文件。\n\n使用pickle包中的dump()或dumps()方法来进行序列化。\n\n使用pickle包中的load()方法进行反序列化\n\n# 为什么需要pickle包(序列化和反序列化)\n\n序列化:将变量从内存中变成可以存储或传输的过程称之为序列化，在python中叫做pickling，在其它语言中也称之为 serialization、marshaling、flattening等等，说的都是一个意思。\n\n反序列化：反之，则为反序列化，称之为unpickling，把变量内容从序列化的对象重新读取到内存中。\n\n# 为什么需要序列化\n\n不序列化时，对象存储存在的问题：\n\n比如：我要将对象写入一个磁盘文件而后再将其读出来会有什么问题吗？别急，其中一个最大的问题就是对象引用！ 举个例子来说：假如我有两个类，分别是a和b，b类中含有一个指向a类对象的引用，现在我们对两个类进行实例化{ a a = new a(); b b = new b(); }。 这时在内存中实际上分配了两个空间，一个存储对象a，一个存储对象b。 接下来我们想将它们写入到磁盘的一个文件中去，就在写入文件时出现了问题！因为对象b包含对对象a的引用，所以系统会自动的将a的数据复制一份到b中，这样的话当我们从文件中恢复对象时(也就是重新加载到内存中)时，内存分配了三个空间，而对象a同时在内存中存在两份，想一想后果吧，如果我想修改对象a的数据的话，那不是还要搜索它的每一份拷贝来达到对象数据的一致性，这不是我们所希望的！\n\n序列化的解决方案：\n\n 1. 保存到磁盘的所有对象都获得一个序列号(1, 2, 3等等)\n 2. 当要保存一个对象时，先检查该对象是否被保存了\n 3. 如果以前保存过，只需写入"与已经保存的具有序列号x的对象相同"的标记，否则，保存该对象通过以上的步骤序列化机制解决了对象引用的问题！\n\n> https://blog.csdn.net/qq_37160773/article/details/95060070\n> \n> https://blog.csdn.net/u011215133/article/details/51177843\n\n# 保存对象到文件\n\n下面有2个示例，一个普通的例子。分别为创建对象、序列化对象、字节文本的存储。\n\n第二个示例，是上面的例子的简化。\n\n# 第一种示例\nimport pickle\nclass bird(object):\n    have_feather = true\n    reprodution_method = "egg"\nsummer = bird()                       #创建对象\npickle_string = pickle.dumps(summer)  #序列化对象\n\nwith open("summer.pkl", "wb") as f:\n    f.write(pickle_string)\n# 第二种示例\nimport pickle\nclass bird(object):\n    have_feather = true\n    reprodution_method = "egg"\nsummer = bird()\nwith open("summer.pkl", "w") as f:\n    pickle.dump(summer, f)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 从文件读取对象\n\n读取对象与存储对象的过程正好相反。首先，我们从文件中读出文本。然后使用pickle的loads()方法，将字符串形式的文本转换为对象。也可以使用pickle中的load()方法，将上面两步合并。\n\n有时候，仅仅是反向恢复还不够。对象依赖于它的类，所以python在创建对象时，需要找到相应的类。\n\n所以，当我们从文本中读取对象的时候，程序中必须已经定义过类。对于python总是存在的内置类，如列表、词典、字符串等，不需要再在程序中定义。但对于用于自定义的类，就必须要先定义类，然后才能从文件中载入该类的对象。\n\nimport pickle\nclass bird(object):\n    have_feather = true\n    reproduction_method = "egg"\nwith open("summer.pkl", "rb") as f:\n    summer = pickle.load(f)\nprint(summer.have_feather)               # 打印true\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 时间类的包\n\n挂钟时间：\n\n在硬件的基础上，计算机可以提供挂钟时间(wall clock time)。挂钟时间是从某个固定时间起点到现在的时间间隔。对于uninx系统来说，起点时间是1970年1月1日的0点0分0秒。其他的日志信息都是从挂钟时间计算得到的。\n\n处理器时间：\n\n计算机还可以测量cpu实际运行的时间，也就是处理器时间(processor clock time)，以测量计算机性能。当cpu处于闲置状态的时候，处理器时间会暂停。\n\n\n# time包\n\n 1. time()方法\n    \n    time包中的time()方法，显示挂钟时间。\n    \n    import time\n    print(time.time())  # 挂钟时间，单位是秒\n    \n    \n    1\n    2\n    \n\n 2. clock()方法\n    \n    time包中的clock()用来测量程序运行的时间，在程序中两次调用clock()方法，从而测量出镶嵌其间的程序所用的时间。在不同的计算机系统上，clock()的返回值会有所不同。在unix系统上，返回的是处理器时间。当cpu处于闲置状态时，处理器时间会暂停。因此，我们获取的是cpu运行时间。在windows系统上，返回的则是挂钟时间。\n    \n    import time\n    start = time.clock()\n    for i in range(1000000):\n        print(i**2)\n    end = time.clock()\n    print(end - start)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n 3. sleep()方法\n    \n    sleep()方法可以让程序休眠。根据sleep()接收到的参数，程序会在某时间间隔之后醒来继续运行。\n    \n    import time\n    print("start")\n    time.sleep(10)    # 休眠10秒\n    print("wake up")\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 4. stuct_time对象\n    \n    time包中还定义了stuct_time对象。该对象将挂钟时间转换为年、月、日、时、分、秒等，存储在该对象的各个属性中，比如tm_year、tm_mon、tm_mday .... 下面几种方法可以将挂钟时间转换为struct_time对象。\n    \n    st = time.gmtime()     # 返回struct_time格式的utc时间\n    st = time.localtim()   # 返回struct_time格式的当地时间，\n                           # 当地时区根据系统环境设定\n    \n    \n    1\n    2\n    3\n    \n    \n    也可以把一个stuct_time对象转换为time对象：\n    \n    s = time.mktime(st)    # 将struct_time格式转换成挂钟时间\n    \n    \n    1\n    \n\n\n# datetime包\n\n# 什么是datetime模块\n\ndatetime包是基于time包的一个高级包，用起来更加便利。datetime可以理解为由date和time两个部分组成。date是年月日，time是时、分、秒、毫秒。\n\n# 如何使用datetime模块\n\ndatetime模块下面有两个类：datetime.date类和datetime.time类。如果要想得到年月日是分秒，可以直接调用datetime.datetime类。\n\n显示时间：该对象t有时、分、秒、年、月、日等属性\n\nimport datetime\nt  = datetime.datetime(2012,9,3,21,30)\nprint(t)\nt2 = datetime.datetime.now()    # 显示当前时间\nprint(t2)\n\n\n1\n2\n3\n4\n5\n\n\n时间间隔计算：\n\n在datetime包中，有个专门代表时间间隔对象的类，即stimedelta。\n\n在一个datetime.datetime的时间点加上一个时间间隔，就可以得到一个新的时间点。\n\ndatetime.timedelta传递参数时，除了秒和星期外，还可以是天、小时、毫秒、微妙。\n\nimport datetime\nt        = datetime.datetime(2012,9,3,21,30)\nt_next   = datetime.datetime(2012,5,3,23,30)\ndelta1   = datetime.timedelta(seconds = 600)\ndelta2   = datetime.timedelta(weeks = 3)\nprint(t + delta1)        \nprint(t + delta2)\nprint(t_next - t)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\ndatetime对象比较：\n\n两个datetime对象进行比较运算，以确定哪个时间间隔更长。\n\nprint(t > t_next)          # 打印false\n\n\n1\n\n\n\n# 日期格式\n\n这里特意强调在datetime模块中日期格式的问题，主要是为了说明下面的日期格式转换。例如用strptime的方法实现日期的字符串转换为datetime的对象；用strftime方法实现把datetime对象转换为日期的字符串。\n\nfrom datetime import datetime\nstr        = "output-1997-12-23-030000.txt"\nformat     = "output-%y-%m-%d-%h%m%s.txt"\nt          = datetime.strptime(str, format)\nprint(t) \n\nfrom datetime import datetime\nformat = "%y-%m-%d %h:%m"\nt = datetime(2012,9,5,23,30)\nprint(t.strftime(format))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 正则表达式\n\n\n# 什么是正则表达式\n\n正则表达式的主要功能是从字符串中通过特定的模式，搜索希望找到的内容。\n\n这个功能有点和字符串对象中的搜索查找的方法，但是字符串对象中的查找是指，从一个大的字符串中，找出一个子字符串。而我们这里的使用正则表达式的目的在于，只是想要找到符合某种格式的字符串，而不是具体的那个子字符串。\n\n\n# 如何使用正则表达式\n\n# 从字符串中找出相应子字符串\n\nimport re\nm      = re.search("[0-9]", "dfsd789ddsfad")\nprint(m.group(0))\n\n\n1\n2\n3\n\n\nre.search()接收两个参数，第一个参数"[0-9]"就是我们所说的正则表达式，告诉python去从第二个字符串中找出从0到9的任意一个数字字符。\n\nre.search()中的第二个参数，是指需要去查找的那个字符串。整体而言，就是从后面的字符串中，找出符合要求的子字符串，就返回一个对象m。可以通过m.group()的方法查看搜索到的结果。如果没有找到符合要求的字符，则re.search()会返回none.\n\n归纳为如下的使用模式：\n\nm = re.search(pattern, string) # 搜索整个字符串，直到发现符合的子字符串\n\n\n1\n\n\n# 从字符串中匹配相应的正则表达式\n\n下面的使用方式是指，从头开始检查字符串是否符合正则表达式。\n\n必须从字符串的第一个字符开始就相符。\n\nm = re.match(pattern, string)\n\n\n1\n\n\n# 替换符合正则规则的字符串\n\n下面的sub()利用正则pattern在字符串string中进行搜索，对于搜索到的字符串，用另一个字符串replacement进行替换。函数将返回替换后的字符串。\n\nstr = re.sub(pattern, replacement, string)\n\n\n1\n\n\n# 根据正则分隔字符串\n\n下面的用法是，根据正则表达式来分割字符串，将分割后的所有子字符串放在一个表(list)中返回。\n\nre.split()\n\n\n1\n\n\n# 根据正则返回所有子字符串\n\n根据正则表达式搜索字符串，将所有符合条件的子字符串放在一个表(list)中返回。\n\nre.findall()\n\n\n1\n\n\n\n# 正则表达式有哪些\n\n 1. 用某些符号代表单个字符\n 2. 用某些符合代表某种形式的重复\n 3. 位置相关的符号\n\n\n# 利用group来查看正则表达搜索结果\n\n有时候，我们想要查看匹配到的正则表达式的那个具体的，符合正则表达式的部分结果。我们可以在正则表达式上给目标加上括号。\n\n用括号()圈起来的正则表达式的一部分，称为群。一个正则表达式中可以有多个群。\n\n我们可以用group(number)的方法来查询群。需要注意的是，group(0)是整个正则表达式的搜索结果。group(1)是第一个群，以此类推。\n\nimport re\nm = re.search("output_(\\d{4})", "output_1986.txt")\nprint(m.group(1))     # 将找到4个数字组成的1896\n\n\n1\n2\n3\n\n\n\n# 给group命名\n\n我们还可以将群命名，以便更好地使用group查询：\n\n下面的(?p<year>...)括住一个群，并把它命名为year。用这种方式来产生群，就可以通过"year"这个键来提取结果了。\n\nimport re\nm = re.search("output_(?p<year>\\d{4})", "output_1986.txt")\nprint(m.group("year"))\n\n\n1\n2\n3\n\n\n\n# python的网络访问\n\n\n# http通信简介\n\nhttp的request请求信息如下：\n\nget /index.html  http/1.1\nhost: www.example.com\n\n\n1\n2\n\n\n起始行中，有三段信息：\n\n * get方法，用于说明想要服务器执行的操作。\n * /index.html资源的路径。这里指向服务器上的index.html文件\n * http/1.1协议的版本。\n\nhttp的response回复信息如下：\n\nhttp/1.1 200 ok\ncontent-type: text/plain\ncontent-length: 12\n    \nhelllo world!\n\n\n1\n2\n3\n4\n5\n\n * http/1.1：协议版本\n * 200：状态码(status code)\n * ok：状态描述\n * content-type说明了主体所包含的资源的类型，有普通文件，html文本，jpeg图片等。\n * content-length说明了主体部分的长度，以字节(byte)为单位。\n\n\n# http.client包\n\nhttp.client包(模块)中，包含了很多的http的方法，我们可以利用这些方法来实现http的通信。\n\n从上面的http请求中看出，在http请求中最重要的一些信息是主机信息、请求方法和资源路径。\n\nimport http.client\nconn     = http.client.httpconnection("www.example.com")  #主机地址\nconn.request("get", "/")  #请求方法和资源路径\nresponse = conn.getresponse()  # 获得回复\nprint(response.status, response.reason)  # 回复的状态码和状态描述\ncontent =  response.read()      # 回复的主体内容\nprint(content)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 与对象的深入交往\n\n在本章的前半部分，介绍了运算符、元素引用、内置函数，这些其实都是来自于一些特殊的对象。这样的设计既满足了python的多范式的需求，又能以简单的体系满足丰富的语法需求，如运算符重载与即时特性等。\n\n本章的后半部分，将深入了解对象相关的重要机制，如动态类型和垃圾回收。\n\n\n# 一切皆对象\n\n\n# 运算符\n\n我们常见的运算符，比如+、-、>、<、and、or等，都是通过特殊方法实现的。或者说是通过归属类中的定义的相应的内置方法来实现的。\n\n在子类中重新定义了相应的运算符的方法后，子类中的方法会覆盖父类的同名发你规范。即运算符将被重新定义。\n\n我们以list列表类为例，如果我们用dir(list)查看list的属性，能看到一个属性时_add_()是特殊方法。这个方法定义了"+"运算符对于list对象的意义，两个list的对象相加时，会进行合并列表的操作。\n\n+号，对应的类中的方法是: "_add_()"\n\n-号，对应的类中的方法是: "_sub_()"\n\n*号，对应的类中的方法是: "_mul_()"\n\nor号，对应的类中的方法是："_or_()"\n\n定义运算符对于复杂的对象非常有用。例如，人类有多个属性，比如姓名、年龄和身高。我们可以把人类的比较(>、<、=)定义成只看年龄。\n\n\n# 元素引用\n\n序列包含的一个数据被称为序列的一个元素。\n\n序列是有顺序的数据集合，就好像一列排好队的士兵。序列分为元组和列表两种。\n\n我的理解是这里的"元素引用"是指，在序列(元组或列表)或词典中，单独对其中的一个元素，进行方法的运算。\n\n下面的案例中，展示了元素的获取的方法，更新的方法、删除的方法。\n\nli    = [1, 2, 3, 4, 5, 6]\nprint(li[3])\nprint(li.__getitem__(3))     # 获取元素\nli.__setitem__(3,0)          # 更新元素\nprint(li)\n\nexample_dict   = {"a":1, "b":2}\nexample_dict.__delitem__("a")    # 删除元素\nprint(example_dict)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 内置函数的实现\n\n与运算符类似，许多内置函数也都是调用对象的特殊方法。\n\n例如，下面的例子中的_len_()方法，实际上和执行内置函数len()一样的作用，起到了简化的作用。\n\nlen([1,2,3])\n[1,2,3].__len__()\n\n\n1\n2\n\n\n\n# 属性管理\n\n\n# 属性覆盖的背后\n\n当我们调用对象的属性的时候，这个属性可能有很多来源。除了来自对象属性和类属性，这个属性还可能是从祖先类那里继承来的。\n\n一个类或对象拥有的属性，会记录在_dict__中。这个_dict__是一个词典，键为属性名，对应的值为某个属性。python在寻找对象的属性的时候，会按照继承关系依次寻找__dict。\n\n这个顺序是按照与summer对象的亲近关系排列的。\n\n我们用内置函数dir来查看对象summmer的属性的话，可以看到summer对象包含了全部四个部分。也就是说，对象的属性时分层管理的。对象summer能接触到的所有属性，分别存在summer/chicken/bird/object这四层。当我们需要调用某个属性的时候，python会一层层向下遍历，直到找到那个属性。由于对象不需要重复存储其祖先类的属性，所以分层管理的机制可以节省存储空间。\n\n某个属性可能在不同层被重复定义。python在向下遍历的过程中，会选取先遇到的那一个。这正是属性覆盖的原理。\n\n但是如果是进行赋值，那么python就不会分层深入查找了。\n\n\n# 特性\n\n同一个对象的不同属性之间可能存在依赖关系。当某个属性被修改时，我们希望依赖于该属性的其他属性也同时变化。\n\n这时，我们不能通过__dict__的静态词典方式来存储属性。python提供了多种即时生成属性的方法。其中一种称为特性(property)。特性是特殊的属性。\n\n特性使用内置函数property()来创建。property()最多可以加载四个参数。前三个参数为函数，分别用于设置获取、修改和删除特性时，python应该执行的操作。最后一个参数为特性的文档，可以为一个字符串，起说明作用。\n\n下面的案例中，num为一个数字，而neg为一个特性，用来表示数字的负数。当一个数字确定的时候，它的负数总是确定的。而当我们修改了一个数的负数的时候，它本身的值也应该变化。这两点由get_neg()和set_neg()来实现。而del_neg()\n\nclass num(object):\n    def __init__(self, value):\n        self.value = value\n    def get_neg(self):\n        return -self.value\n    def set_neg(self, value):\n        self.value = -value\n    def del_neg(self):\n        print("value also deleted")\n        del self.value\n    neg = property(get_neg, set_neg, del_neg, "i am negative")\n\nx = num(1.1)\nprint(x.neg)\n\nx.neg = -22 \nprint(x.value)\nprint(num.neg.__doc__)\ndel x.neg\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# _getattr_()方法\n\n除了内置函数property外，还可以使用_gettr_(self,name)来查询即时生成的属性。\n\n当我们调用对象的一个属性的时候，如果通过_dict__机制无法找到该属性，那么python就会调用对象的_getattr()方法，来即时生成该属性。\n\n和上面property特性的区别：\n\n每个特性都需要有自己的处理函数，而_getatr_()可以将所有的即时生成属性放在同一个函数中处理。而_getattr_()可以根据函数名来区别处理不同的属性。\n\n_getattr_()只能用于查询不在__dict__系统中的属性。\n\n_setattr_(self,name,value)和 _delattr_(self,name)可用于修改和删除属性。\n\n\n# 我是风儿，我是沙\n\n\n# 动态类型\n\npython的变量不需要声明，在赋值的时候，变量可以重新赋值为其他任意值。\n\npython变量这种变化的能力，就是动态类型的体现。\n\n# 对象名是指向对象的一个引用\n\n在下面的例子a =1说明了，在python中，整数1是一个对象。对象的名字是"a"，对象名其实是指向对象的一个引用。对象是存储在内存中的实体。我们必须使用对象名，来执行这一对象的"引用"。\n\n通过内置函数id()，能查看到引用指向的是哪个对象。\n\na = 1\nprint(id(1))\nprint(id(a))\n\n\n1\n2\n3\n\n\n# 变量名重新赋值\n\n在python中，赋值其实就是用对象名这个筷子去夹住其他的食物。每次赋值，我们让左侧的引用指向右侧的对象。引用能随时指向一个新的对象。\n\n下面的例子中，3是存储在内存中的一个整数对象。通过赋值，引用a指向对象3。这时建立了一个字符串对象"at"，通过赋值，我们将引用a指向"at"。\n\n既然变量名是个随时可以变更指向的引用，那么它的类型自然可以在程序中动态变化。\n\na = 3\nprint(id(a))\na = "at"\nprint(id(a))\n\n\n1\n2\n3\n4\n\n\n# 判断两个引用是否指向同一个对象\n\n除了直接打印id外，我们还可以用is运算来判断两个引用是否指向同一个对象。\n\na    = 3\nb    = 3\nprint(a is b)\n\n\n1\n2\n3\n\n\n\n# 可变与不可变对象\n\n不可变对象：\n\n意思是该对象，通过不同的变量名进行引用后，如果我们对引用后的变量名进行运算，不会改变原有的对象的值或内容，这就叫做是不可变对象。一般有：列表、词典。\n\n可变对象：\n\n意思是该对象，通过不同的变量名进行引用后，如果我们对引用后的变量名进行运算，结果改变了原有的对象的值或内容，这就叫做是可变对象。一般有：整数、浮点数、字符串、元组等。\n\n\n# 从动态类型引用角度看函数的参数传递\n\n函数的参数传递，本质上传递的是引用。每个参数或变量都是一个引用，当我们调用函数的时候，函数需要的形参，实际上通过引用的方式来指向实际的对象的值。\n\n下面的例子，分别从可变对象和不可变对象两个例子来说明：\n\ndef f(x):\n    print(id(x))\n    x = 100\n    print(id(x))\na = 1\nprint(id(a))\nf(a)\nprint(a)\n\ndef f(x):\n    x[0] = 100\n    print(x)\na = [1, 2, 3]\nf(a)\nprint(a)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 内存管理\n\n\n# 引用管理\n\n对象内存管理是基于对引用的管理。在python中，引用与对象是分离的。\n\n一个对象可以有多个引用，而每个对象中都存有指向该对象的引用总数，即引用计数。我们可以使用标准库中sys包中的getrefcount()，来查看某个对象的引用计数。\n\n注意下面的例子中，我们想要查看对[1, 2, 3]这个列表对象的引用次数。这里我们使用了某个引用作为参数，传递给了getrefcount()时，参数实际上是创建了一个临时的引用。\n\nfrom sys import getrefcount\na = [1, 2, 3]\nprint(getrefcount(a))\nb = a\nprint(getrefcount(b))\n\n\n1\n2\n3\n4\n5\n\n\n\n# 对象引用对象\n\n容器对象的引用可能会构成很复杂的拓扑结构。我们可以用objgraph包来绘制其引用关系。\n\n两个对象可能相互引用，从而构成所谓的引用环。\n\n引用环会给垃圾回收机制带来很大的麻烦。\n\n我们可以通过del关键字删除某个引用，或者当这个引用被重新定向到某个其他对象时，也会减少对象的引用。\n\n\n# 垃圾回收\n\n# 为什么需要垃圾回收\n\n当python中的对象越来越多的时候，它们将占用越来越大的内存。python会在适当的时候启动垃圾回收，将没用的对象清除。\n\n# 什么是垃圾回收\n\n原理上，当python的某个对象的引用计数降为0，即没有任何引用指向该对象的时候，该对象就称为要回收的垃圾了。\n\n垃圾回收费时费力，当垃圾回收的时候，python不能进行其他的任务。频繁的垃圾回收将大大降低python的工作效率。\n\n如果内存中的对象不多，就没有必要频繁启动垃圾回收。所以，python只会在特定条件下，自动启动垃圾回收。当python运行时，会记录其中分配对象(object allocation)和取消分配对象(object deallocation)的次数。当两者的差值高于某个阀值的时候，垃圾回收才会启动。\n\n# python分代回收策略\n\n下面的700的阀值，是基础回收方式。除了基础回收方式外，python同时还采用了分代回收的策略。\n\n这一策略的基本假设是，存活时间越久的对象，越不可能在后面的程序中变成垃圾。我们程序往往会产生大量的对象，许多对象很快产生和消失，但也有一些对象长期被使用。出于信任和效率，对于这样一些"长寿"对象，我们相信它们还有用处，所以减少在垃圾回收中扫描它们的频率。\n\npython将所有的对象分为0、1、2三代。所有的新建对象都是0代对象。当某一代对象经历过垃圾回收，依然存活，那么它就被归入下一代对象。垃圾回收启动时，一定会扫描所有的0代对象。如果0代经过一定次数垃圾回收，那么就启动对0代和1代的扫描清理。当1代也经历了一定次数的垃圾回收后，就会启动对0、1、2代的扫描，即对所有对象进行扫描。\n\n# 如何配置垃圾回收\n\n我们可以通过gc模块的get_threshold()方法，来查看该阀值。一般返回的是(700,10,10)\n\nimport gc\nprint(gc.get_threshold())\n\n\n1\n2\n\n\n700即垃圾回收启动的阀值。可以通过gc中的set_threshold()方法重新设置。也可以手动使用gc.collect()来手动启动垃圾回收。\n\n后面两个10，与分代回收相关的阀值。也就是说，每10次0代垃圾回收，会配合1次1代的垃圾回收；而每10次1代的垃圾回收，才会有1次2代的垃圾回收。\n\n可以使用set_threshold()来调整次数，比如下面对2代对象进行更频繁的扫描。\n\nimport gc\ngc.set_threshold(700, 10, 5)\n\n\n1\n2\n\n\n\n# 孤立的引用环\n\n引用环的存在会给垃圾回收机制带来很大的困难。这些引用环可能构成无法使用，但引用计数不为0的一些对象。\n\n在下面的例子中，由于引用环的存在，这两个对象的引用计数都没有降到0，所以不会被垃圾回收。\n\na = []\nb = [a]\na.append(b)\ndel a\ndel b\n\n\n1\n2\n3\n4\n5\n\n\n为了回收这样的引用环，python会复制每个对象的引用计数，可以记为gc_ref。假设，每个对象i，该计数为gc_ref_i。python会遍历所有的对象i。对于每个对象i所引用的对象j，将相应的gc_ref_j减去1。\n\n在结束遍历后，gc_ref不为0的对象，和这些对象引用的对象，以及继续更下游引用的对象，需要被保留，而其他对象则被垃圾回收。\n\n\n# 函数式编程\n\npython虽然不是纯粹的函数式编程，但是包含了不少函数式编程的语法。\n\n面向过程编程中，利用选择和循环结构，以及函数、模块等，对指令进行封装。\n\n面向对象编程中，实现了另外一种形式的封装，包含有数据的对象的一系列方法，这些方法能造成对象的状态改变。\n\n面向函数编程中，本质上也在于以函数为中心进行代码封装。\n\n\n# 又见函数\n\n\n# python中的函数式\n\npython中的函数实际上是一些特殊的对象。这一条已经符合函数式编程的一个重要方面：函数是第一级对象，能像普通对象一样使用。\n\n# 函数式编程中函数纯粹性\n\n函数式编程强调了函数的纯粹性。一个纯函数是没有副作用的，即这个函数的运行不会影响其他函数。纯函数像一个沙盒，把函数带来的效果控制在内部，从而不影响程序的其他部分。\n\n为了达到纯函数的标准，函数式编程要求其变量都是不可变更的。\n\n# python与函数式编程\n\npython并非完全的函数式编程语言。在python中借鉴了函数式编程，尽量在编程中避免副作用。函数式编程的好处在于：纯函数相互独立，不会相互影响；纯函数方便进行并行化运算。\n\n早期的python版本中，并没有函数式编程的相关语法。后来python中加入了lambda函数，以及map、filter、reduce等高阶函数，从而加入了函数式编程的特征。\n\n函数式编程的思维方式，是自上而下的。它先提出一个大问题，在最高层用一个函数来解决这个大问题。在这个函数内部，再用其他函数来解决小问题。在这样递归式的分解下，直到问题得到解决。\n\n# 消除竞跑条件\n\n在下面的案例中，当多个进行同时修改一个变量的时候，进程的先后顺序会影响最终结果。\n\n下面的两个函数中使用了关键字global。global说明了x是一个全局变量。函数对全局变量的修改能被其他函数看到，因此有副作用。函数的执行顺序不确定，最终结果也不一定。这就被称为竞跑条件,是并行编程中需要极力避免的。\n\nfrom threading import thread\nx = 5\ndef double():\n    global x\n    x = x * 2\ndef plus_ten():\n    global x\n    x = x + 10\nthread1 = thread(target=double)\nthread2 = thread(target=plus_ten)\nthread1.start()\nthread2.start()\nthread1.join()\nthread2.join()\nprint(x)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 并行运算\n\n# 什么是串行运算\n\n一台单处理器的计算机同一时间内只能执行一条指令。这种每次执行一条指令的工作方式称为串行运算。\n\n# 什么是并行运算\n\n所谓的并行运算，是指多条指令同时执行。我们可以在单机上通过多进程或多线程的方式，来模拟多主机的并行处理。\n\n事实上，单机的处理器按照"分时复用"的方式，把运算能力分配给多个进程。处理器在进程间频繁切换。因此，即使处理器同一时间只能处理一个指令，但通过在进程间的切换，也能造成多个进程齐头并进的效果。\n\n# 进程与线程\n\n进程有自己的内存空间，用来存储自身的运行状态、数据和相关代码。\n\n一个进程一般不会直接读取其他进程的内存空间。进程运行过程中，可以完成程序描述的工作。但在一个进程内部，又可以有多个称为"线程"的任务，处理器可以在多个线程之间切换，从而形成并行的多线程处理。\n\n线程看起来和进程类似，但是线程之前可以共享同一个进程的内存空间。\n\n# 多进程编程案例\n\n在下面的案例中是个多进程编程的例子。程序用了两个进程，进程的工作包含在函数中，分别是函数proc1()和函数proc2()。方法start()用于启动进程，而join()方法用于在主程序中等待相应进程完成。\n\n\n# 被解放的函数\n\n\n# 函数作为参数\n\n在函数式编程中，函数是第一级对象。所谓"第一级对象"，即函数能像普通对象一样使用。\n\n函数可以像一个普通对象一样，成为其他函数的参数。和称为其他函数的返回值。\n\n在下面的程序中，函数就充当了参数。函数argument_demo()的第一个参数f就是一个函数对象。按照位置传参，square_sum()传递给函数argument_demo()，对应参数列表中的f。\n\ndef square_sum(a, b):\n    return a**2 + b**2\ndef cubic_sum(a, b):\n    return a**3 + b**3\ndef argument_demo(f, a, b):\n    return f(a, b)\nprint(argument_demo(square_sum, 3, 5))\nprint(argument_demo(cubic_sum, 3, 5))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 函数作为返回值\n\n既然函数是一个对象，那么它就可以成为另一个函数的返回结果。\n\n下面的line_conf()函数的返回结果被赋予line对象，看到了在一个函数内部定义的函数。\n\ndef line_conf():\n    def line(x):\n        return 2*x+1\n    return line\nmy_line = line_conf()   # 函数对象\nprint(my_line(5))\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 函数对象作用域\n\n和函数内部的对象一样，函数对象也有存活范围，也就是函数对象的作用域。python的缩进形式很容易让我们看到函数对象的作用域。\n\n下面的例子中，我们在line_conf()函数的隶属范围内定义的函数line()，就只能在line_conf()的隶属范围内调用。超出了函数line()的作用域，python对该函数的调用失败。\n\ndef line_conf():\n    def line(x):\n        return 2*x + 1\n    print(line(5))          # 作用域内\nif __name__=="__main__":\n    line_conf()\n    print(line(5))          # 作用域外，报错\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 闭包\n\n# 什么是闭包\n\n一个函数和它的环境变量合在一起，就构成了一个闭包。\n\n在python中，所谓的闭包是一个包含有环境变量取值的函数对象。首先闭包是一个函数对象，另外这个函数对象有些特殊，里面包含了环境变量取值。\n\n环境变量取值被复制到函数对象的__closure__属性中。在下面的案例中，my_line()的 __closure__属性中包含了一个元组。这个元组中的每个元素都是cell类型的对象。第一个cell包含的就是整数5，也就是我们返回闭包时的环境变量b的取值。\n\ndef line_conf():\n    b = 15\n    def line(x):\n        return 2*x + b\n    b = 5\n    return line      # 返回函数对象\nif __name__ == "__main__":\n    my_line = line_conf()\n    print(my_line.__closure__)\n    print(my_line.__closure__[0].cell_contents)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 为什么需要闭包\n\n提高了代码的可复用性。从下面的例子中，函数line()与环境变量a、b构成闭包。在创建闭包的时候，我们通过line_conf()的参数a、b说明直线的参量。\n\n这样，我们就能复用同一个闭包，通过代入不同的数据来获得不同的直线函数，如y=x+1和y=4x+5。闭包实际上创建了一群形式相似的函数。\n\ndef line_conf(a, b):\n    def line(x):\n        return a*x + b\n    return line\n\nline1 = line_conf(1, 1)\nline2 = line_conf(4, 5)\nline3 = line_conf(5, 10)\nline4 = line_conf(-2, -6)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n使用闭包，还可以起到减少函数参数的作用：\n\n下面的例子中，函数curve()是一个二次函数。它除了自变量x外，还有a、b、c三个参数。通过curve_closure()这个闭包，我们可以预设a、b、c三个参数的值。从而起到减少参数的作用。\n\ndef curve_closure(a, b, c):\n    def curve(x):\n        return a*x**2 + b*x + c\n    return curve\ncurve1 = curve_closure(1, 2, 1)\n\n\n1\n2\n3\n4\n5\n\n\n\n# 小女子的梳妆匣\n\n\n# 装饰器\n\n# 什么是装饰器\n\n装饰器是一种高级python的语法。装饰器可以对一个函数、方法或者类进行加工。装饰器从操作上入手，为函数增加额外的指令。\n\n在python中的变量名和对象是分离的，变量名其实是指向一个对象的引用。从本质上，装饰器起到的作用就是名称绑定，让同一个变量名指向一个新返回的函数对象，从而达到修改函数对象的目的。\n\n只不过，我们很少彻底地更改函数对象。在使用装饰器时，我们往往会在新函数内部调用旧的函数，以便保留旧函数的功能。\n\n函数装饰器是，接收一个函数，并返回一个函数，从而起到加工函数的效果。\n\n# 为什么需要装饰器\n\n在下面的例子中，我们想要为函数增加其他的功能，比如打印输入。我们重新修改函数的定义，为函数增加了功能。但是也可以改用装饰器，定义功能拓展本身，再把装饰器用于两个函数。\n\ndef decorator_demo(old_function):\n    def new_function(a, b):\n        print("input", a, b)\n        return old_function(a, b)\n    return new_function\n\n\n@decorator_demo\ndef square_sum(a, b):\n    return a**2 + b**2\n\n@decorator_demo\ndef square_diff(a, b):\n    return a**2 - b**2\n\nif __name__ == "__main__":\n    print(square_sum(3, 4))\n    print(square_diff(3, 4))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 如何使用装饰器\n\n在上面的例子中，装饰器可以用def的形式定义，如上面代码中的decorator_demo()。装饰器可以接受一个可调用对象作为输入参数，并返回一个新的可调用对象。\n\n装饰器新建了一个函数对象，也就是上面的new_function()。在new_function()中，我们增加了打印的功能，并通过old_function(a,b)来保留原有函数的功能。\n\n定义好装饰器后，我们就可以通过@语法使用了。在函数square_sum()和square_diff()定义之前调用@decorator_demo，实际上就是将square_sum()或square_diff()传递给了decorator_demo()，并将decorator_demo()返回的新的函数对象赋给原来的函数名square_sum()和square_diff()。\n\n所以当我们调用squre_sum(3,4)的时候，实际上发生的是：\n\nsquare_sum = decorator_demo(square_sum)\nsquare_sum(3,4)\n\n\n1\n2\n\n\n# time包装饰器\n\n在下面的案例中，我们利用time包来测量程序运行的时间。把测量程序运行时间的功能做成一个装饰器，将这个装饰器运用于其他函数，将显示函数的实际运行时间。\n\n在new_function()中，除了调用旧函数外，还前后额外调用了一次time.time()。由于time.time()返回挂钟时间，它们的差值反映了旧函数的运行时间。此外，我们通过打包参数的办法，可以在新函数和旧函数之间传递所有的参数。\n\n# 装饰器的其他好处\n\n装饰器可以实现代码的可复用性。我们可以用同一个装饰器修饰多个函数，以便实现相同的附加功能。例如，我们在每次处理http请求前，都想附加一个客户验证功能时，那么就可以定义一个统一的装饰器，作用于每一个处理函数。\n\n\n# 带参装饰器\n\n# 什么是带参装饰器\n\n下面的例子中pre_str是一个带参装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有环境参数的闭包。\n\n当我们使用@pre_str("^_^")调用的时候，python能够发现这一层的封装，并把参数传递到装饰器的环境中。\n\n该调用相当于：square_sum = pre_str("^_^")(square_sum)\n\ndef pre_str(pre=""):\n    def decorator(old_function):\n        def new_function(a, b):\n            print(pre + "input", a, b)\n            return old_function(a, b)\n        return new_function\n    return decorator\n\n@pre_str("^_^")\ndef square_sum(a, b):\n    return a**2 + b**2 \n\n@pre_str("t_t")\ndef square_diff(a, b):\n    return a**2 - b**2\n\nif __name__ == "__main__":\n    print(square_sum(3, 4))\n    print(square_diff(3, 4))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 带参装饰器的好处\n\n根据参数不同，带参装饰器会对函数进行不同的加工，进一步提高了装饰器的适用范围。我们可以把"管理员"和"用户"作为参数，传递给验证装饰器。\n\n对于那些负责关键http请求的函数，我们可以把"管理员"参数传给装饰器。\n\n对于那些负责普通http请求的函数，我们可以把"用户"参数传给它们的装饰器。\n\n这样，同一个装饰器就可以满足不同的需求了。\n\n\n# 装饰类\n\n# 什么是装饰类\n\n和函数的装饰器的概念比较类似，在类的装饰器中，一个装饰器可以接收一个类，并返回一个类，从而起到加工类的效果。\n\n无论是装饰函数，还是装饰类，装饰器的核心作用都是名称绑定。\n\n# 如何使用装饰器\n\n在下面的案例中，我们在装饰器decorator_class中，返回了一个新类newclass。在新类的构造器中，我们用一个属性self.wrapped记录了原来类生成的对象，并附加了新的属性total_display，用于记录调用display()的次数。我们同时更改了display方法。通过装饰，我们的bird类可以显示调用display()的次数。\n\ndef decorator_class(someclass):\n    class newclass(object):\n        def __init__(self, age):\n            self.total_display = 0\n            self.wrapped = someclass(age)\n        def display(self):\n            self.total_display +=1\n            print("total display", self.total_display)\n            self.wrapped.display()\n    return newclass\n\n\n@decorator_class\nclass bird:\n    def __init__(self, age):\n        self.age = age\n    def display(self):\n        print("my age is", self.age)\n\nif __name__ == "__main__":\n    eagle_lord = bird(5)\n    for i in range(3):\n        eagle_lord.display()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 高阶函数\n\n什么是高阶函数？\n\n能接收其他函数作为参数的函数，被称为高阶函数。\n\n高阶函数是函数式编程的一个重要组成部分。\n\n本节我们讲解最具有代表性的高阶函数：map()、filter()和reduce()。\n\n\n# lambda与map\n\n# 什么是lambda表达式\n\n这是一种新的定义函数的方式，之前我们定义函数的方式都是使用def关键字。\n\n# 为什么需要lambda表达式\n\n通过lambda表达式，我们可以创建一个匿名的函数对象。\n\n通常的def定义函数的方式，比较繁琐，有时候我们不需要来定义函数的名称。\n\n# 如何使用lambda表达式\n\n在下面的例子中，我们实现的是定义个函数，实现两个参数的和的功能。通过lambda表达式，我们可以创建一个匿名的函数对象。借着赋值语句，这个匿名函数赋予给函数名lambda_sum。\n\n函数的参数为x、y，返回值为x与y的和。函数lambda_sum()的调用与正常函数一样。这种lambda来产生匿名函数的方式适用于简短函数的定义。\n\ndef sum(x, y):\n    return x+y\n改为lambda表达式\nlambda_sum = lambda x, y: x + y\nprint(lambda_sum(3, 5))\n\n\n1\n2\n3\n4\n5\n\n\n# map函数\n\n所谓高阶函数，就是能处理函数的函数。\n\nmap()是python的内置函数，它的第一个参数就是一个函数对象；map()的第二个参数是一个可循环对象。对于data_list的每个元素，lambda函数都会调用一次。\n\n那个元素会成为lambda函数的参数。换个角度说，map()把接收到的函数对象依次作用于每一个元素。最终，map()会返回一个迭代器。迭代器中的元素，就是多次调用lambda函数的结果。\n\ndef equivalent_generator(func, iter):\n    for item in iter:\n        yield func(item)\ndata_list = [1,3,5,6]\nresult =  map(lambda x: x+3, data_list)\n\n\n1\n2\n3\n4\n5\n\n\nmap()的多参数函数：\n\n可以定义多个参数，这个时候，map()的参数列表中就需要提供相应数目的可循环对象。\n\n下面的案例中，map()接收了square_sum()作为第一个参数。square_sum()要求有两个参数。因此，map()调用的时候需要两个可循环对象。第一个循环对象提供了square_sum()中对应于x的参数，第二个循环对象提供了对应于y的参数。\n\ndef square_sum(x, y):\n    return x**2 + y**2\ndata_list1 = [1,3,5,7]\ndata_list2 = [2,4,6,8]\nresult = map(square_sum, data_list1, data_list2)\n\n\n1\n2\n3\n4\n5\n\n\n\n# filter函数\n\nmap()函数和filter()函数的功能有很多相似的地方，都是把同一个函数应用于多个数据。\n\n和map()函数一样，内置函数filter()的第一个参数也是一个函数对象。它也将这个函数对象作用于可循环对象的多个元素。如果函数对象返回的是true，则该次的元素被放到返回的迭代器中。也就是说，filter()通过调用函数来筛选数据。\n\n类似的，filter()用于多参数的函数时，也可以在参数中增加更多的可循环对象。\n\n下面的案例中，使用的是filter()函数的一个例子。作为参数的larger100()函数用于判断元素是否比100大。\n\ndef larger1(a):\n    if a > 100:\n        return true\n    else:\n        return false\nfor item in filter(larger100, [10, 56, 101, 500]):\n    print(item)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# reduce函数\n\n# 什么是reduce函数\n\nreduce()函数是一个常见的高阶函数。函数reduce()在标准库的functools包中，使用之前需要引入。\n\n和map()、reduce()一样，reduce()函数的第一个参数是函数，但是reduce()对作为参数的函数对象有一个特殊要求，就是这个作为参数的函数必须能接收两个参数。\n\nreduce()可以把函数对象累进的作用于各个参数。\n\n# 如何使用reduce函数\n\n在下面的案例中，reduce()函数的第一个参数是求和的sum()函数，它接收两个参数x和y。在功能上，reduce()累进的运用传给它的二参函数。上一次运算的结构将作为下一次调用的第一个参数。上面过程不断重复，直到列表中的元素耗尽。\n\nfrom functools import reduce\ndata_list = [1,2,5,7,9]\nresult =  reduce(lambda x,y: x + y, data_list)\nprint(result)\n\n\n1\n2\n3\n4\n\n\n# map&reduce 并行运算\n\n函数reduce()通过某种形式的二元运算，把多个元素收集起来，形成一个单一的结果。map()函数和reduce()函数都是单线程的，所以运行效果和循环差不多。\n\n但map()、reduce()可以方便地移植到并行化的运行环境中。在并行运算中，reduce运算紧接着map运算。map运算的结果分布在多个主机上，reduce运算把结果收集起来。\n\n\n# 并行处理\n\n\n# 自上而下\n\n\n# 便携表达式\n\n函数式变的思维是自上而下式的。python中也有不少体现了这一思维的语法，如生成器表达式、列表解析和词典解析。\n\n# 生成器表达式\n\n生成器表达式是构建生成器的便捷方法。\n\n下面的例子分别是用原有的方式用生成器定义循环对象，和用生成器表达式来定义循环对象。\n\ndef gen():\n    for i in range(4):\n        yield i\n等价使用：\ngen = (x for x in range(4))\n\n\n1\n2\n3\n4\n5\n\n\n# 列表解析\n\n利用列表解析的方式是快速生成列表的方法。\n\n列表解析的语法和生成器的表达式很像，只不过把小括号换成了中括号。\n\n语法直观，直截了当地说明了想要的是元素的平方，然后再通过for来增加限定条件，即哪些元素的平方。\n\nl = []\nfor x in range(10):\n    l.append(x**2)\n等价使用：\nl = [x**2 for x in range(10)]\n\n\n1\n2\n3\n4\n5\n\n\n# 词典解析\n\n词典解析可用于快捷的生成词典。它的语法也与之前的类似：\n\nd = {k: v for k,v in enumerate("vamei") if val not in "vi"}\n\n\n1\n\n\n\n# 懒惰求值\n\npython中的迭代器的工作方式正是函数式编程中的懒惰求值。我们可以对迭代器进行各种各样的操作。\n\n懒惰求值可以最小化计算机要做的工作。\n\n除了运算资源，懒惰求值还能节约内存空间。对于即时求值来说，其运算过程的中间结果都需要占用不少额内存空间。而懒惰求值可以现在迭代器层面上进行操作，在获得最终迭代器以后一次性完成计算。\n\n\n# itertools包\n\n标准库中的itertools包提供了更加灵活的生成迭代器的工具，这些工具的输入大都是以后的迭代器。另一方面，这些工具完全可以自行使用python实现，该包只是提供了一种比较标准、高效的实现方式。',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"华为CKA认证",frontmatter:{title:"华为CKA认证",date:"2022-01-20T11:40:52.000Z",permalink:"/pages/5cdc7e/",categories:["中间件","K8S"],tags:[null]},regularPath:"/02.%E4%B8%AD%E9%97%B4%E4%BB%B6/01.K8S/80.%E5%8D%8E%E4%B8%BACKA%E8%AE%A4%E8%AF%81.html",relativePath:"02.中间件/01.K8S/80.华为CKA认证.md",key:"v-65da262e",path:"/pages/5cdc7e/",headers:[{level:2,title:"CKA认证介绍",slug:"cka认证介绍",normalizedTitle:"cka认证介绍",charIndex:32},{level:2,title:"考纲解读",slug:"考纲解读",normalizedTitle:"考纲解读",charIndex:263},{level:2,title:"K8S架构和工作原理",slug:"k8s架构和工作原理",normalizedTitle:"k8s架构和工作原理",charIndex:705},{level:3,title:"K8S架构图如下",slug:"k8s架构图如下",normalizedTitle:"k8s架构图如下",charIndex:720},{level:3,title:"K8s工作原理",slug:"k8s工作原理",normalizedTitle:"k8s工作原理",charIndex:1130},{level:2,title:"理解K8S API原语",slug:"理解k8s-api原语",normalizedTitle:"理解k8s api原语",charIndex:1336},{level:3,title:"K8S中的基本概念",slug:"k8s中的基本概念",normalizedTitle:"k8s中的基本概念",charIndex:1352},{level:3,title:"K8S中API对象的基本构成",slug:"k8s中api对象的基本构成",normalizedTitle:"k8s中api对象的基本构成",charIndex:1602},{level:2,title:"使用kubectl",slug:"使用kubectl",normalizedTitle:"使用kubectl",charIndex:1849},{level:3,title:"基本命令",slug:"基本命令",normalizedTitle:"基本命令",charIndex:1863},{level:3,title:"部署命令",slug:"部署命令",normalizedTitle:"部署命令",charIndex:2314},{level:3,title:"集群管理命令",slug:"集群管理命令",normalizedTitle:"集群管理命令",charIndex:2548},{level:3,title:"Troubleshooting and Debugging命令",slug:"troubleshooting-and-debugging命令",normalizedTitle:"troubleshooting and debugging命令",charIndex:2745},{level:3,title:"高级命令",slug:"高级命令",normalizedTitle:"高级命令",charIndex:2950},{level:3,title:"Setting命令",slug:"setting命令",normalizedTitle:"setting命令",charIndex:3082},{level:3,title:"其他命令",slug:"其他命令",normalizedTitle:"其他命令",charIndex:3184},{level:3,title:"kubectl实用技巧",slug:"kubectl实用技巧",normalizedTitle:"kubectl实用技巧",charIndex:3326},{level:2,title:"K8S调度管理",slug:"k8s调度管理",normalizedTitle:"k8s调度管理",charIndex:3825},{level:3,title:"大纲",slug:"大纲",normalizedTitle:"大纲",charIndex:3837},{level:3,title:"K8S调度相关基础概念",slug:"k8s调度相关基础概念",normalizedTitle:"k8s调度相关基础概念",charIndex:4042},{level:3,title:"Node定义",slug:"node定义",normalizedTitle:"node定义",charIndex:4255},{level:3,title:"Pod中主要属性字段",slug:"pod中主要属性字段",normalizedTitle:"pod中主要属性字段",charIndex:4501},{level:3,title:"K8S中的资源分配",slug:"k8s中的资源分配",normalizedTitle:"k8s中的资源分配",charIndex:5116},{level:3,title:"调度策略",slug:"调度策略",normalizedTitle:"调度策略",charIndex:5375},{level:3,title:"预选策略-必选全部满足",slug:"预选策略-必选全部满足",normalizedTitle:"预选策略-必选全部满足",charIndex:5543},{level:3,title:"优选策略",slug:"优选策略",normalizedTitle:"优选策略",charIndex:5494},{level:3,title:"调度分配机制的要点",slug:"调度分配机制的要点",normalizedTitle:"调度分配机制的要点",charIndex:7987},{level:3,title:"Pod所需资源的计算",slug:"pod所需资源的计算",normalizedTitle:"pod所需资源的计算",charIndex:8561},{level:2,title:"K8S中的高级调度及用法",slug:"k8s中的高级调度及用法",normalizedTitle:"k8s中的高级调度及用法",charIndex:8745},{level:3,title:"nodeSelector",slug:"nodeselector",normalizedTitle:"nodeselector",charIndex:5060},{level:3,title:"nodeAffinity",slug:"nodeaffinity",normalizedTitle:"nodeaffinity",charIndex:9048},{level:3,title:"podAffinity",slug:"podaffinity",normalizedTitle:"podaffinity",charIndex:10184},{level:3,title:"手动调度和DaemonSet",slug:"手动调度和daemonset",normalizedTitle:"手动调度和daemonset",charIndex:11567},{level:3,title:"污点和容忍度",slug:"污点和容忍度",normalizedTitle:"污点和容忍度",charIndex:11928},{level:3,title:"调度结果和失败原因分析",slug:"调度结果和失败原因分析",normalizedTitle:"调度结果和失败原因分析",charIndex:14406},{level:3,title:"多调度器及调度器配置",slug:"多调度器及调度器配置",normalizedTitle:"多调度器及调度器配置",charIndex:14686},{level:2,title:"K8S日志/监控/应用管理",slug:"k8s日志-监控-应用管理",normalizedTitle:"k8s日志/监控/应用管理",charIndex:14855},{level:3,title:"监控集群组件",slug:"监控集群组件",normalizedTitle:"监控集群组件",charIndex:14873},{level:3,title:"监控应用",slug:"监控应用",normalizedTitle:"监控应用",charIndex:15678},{level:3,title:"管理组件日志",slug:"管理组件日志",normalizedTitle:"管理组件日志",charIndex:16075},{level:3,title:"管理应用日志",slug:"管理应用日志",normalizedTitle:"管理应用日志",charIndex:16430},{level:2,title:"Deployment升级和回滚",slug:"deployment升级和回滚",normalizedTitle:"deployment升级和回滚",charIndex:17279},{level:3,title:"创建Deployment",slug:"创建deployment",normalizedTitle:"创建deployment",charIndex:17299},{level:3,title:"升级Deployment",slug:"升级deployment",normalizedTitle:"升级deployment",charIndex:17498},{level:3,title:"升级策略",slug:"升级策略",normalizedTitle:"升级策略",charIndex:17695},{level:3,title:"暂停Deployment",slug:"暂停deployment",normalizedTitle:"暂停deployment",charIndex:18047},{level:3,title:"恢复Deployment",slug:"恢复deployment",normalizedTitle:"恢复deployment",charIndex:18225},{level:3,title:"查询升级状态",slug:"查询升级状态",normalizedTitle:"查询升级状态",charIndex:18417},{level:3,title:"查看升级历史",slug:"查看升级历史",normalizedTitle:"查看升级历史",charIndex:18512},{level:3,title:"回滚的命令",slug:"回滚的命令",normalizedTitle:"回滚的命令",charIndex:18666},{level:2,title:"应用弹性伸缩",slug:"应用弹性伸缩",normalizedTitle:"应用弹性伸缩",charIndex:18760},{level:2,title:"应用自恢复",slug:"应用自恢复",normalizedTitle:"应用自恢复",charIndex:18969},{level:2,title:"K8S中的安全",slug:"k8s中的安全",normalizedTitle:"k8s中的安全",charIndex:19519},{level:3,title:"基本概念罗列",slug:"基本概念罗列",normalizedTitle:"基本概念罗列",charIndex:19787},{level:3,title:"公钥/私钥/CA/数字签名",slug:"公钥-私钥-ca-数字签名",normalizedTitle:"公钥/私钥/ca/数字签名",charIndex:22477},{level:3,title:"什么是X509",slug:"什么是x509",normalizedTitle:"什么是x509",charIndex:23810},{level:3,title:"Openssl生成SSL证书过程",slug:"openssl生成ssl证书过程",normalizedTitle:"openssl生成ssl证书过程",charIndex:24161},{level:3,title:"SSL/TLS认证",slug:"ssl-tls认证",normalizedTitle:"ssl/tls认证",charIndex:25761},{level:3,title:"什么是Token",slug:"什么是token",normalizedTitle:"什么是token",charIndex:26275},{level:3,title:"kubeconfig的配置",slug:"kubeconfig的配置",normalizedTitle:"kubeconfig的配置",charIndex:26408},{level:3,title:"实现自主控制Pod对象资源的访问权限",slug:"实现自主控制pod对象资源的访问权限",normalizedTitle:"实现自主控制pod对象资源的访问权限",charIndex:28270},{level:3,title:"创建自定义权限用户",slug:"创建自定义权限用户",normalizedTitle:"创建自定义权限用户",charIndex:28724},{level:2,title:"CKA考题整理",slug:"cka考题整理",normalizedTitle:"cka考题整理",charIndex:29911},{level:3,title:"第一题",slug:"第一题",normalizedTitle:"第一题",charIndex:29923},{level:3,title:"第二题",slug:"第二题",normalizedTitle:"第二题",charIndex:30147},{level:3,title:"第三题",slug:"第三题",normalizedTitle:"第三题",charIndex:30418},{level:3,title:"第四题",slug:"第四题",normalizedTitle:"第四题",charIndex:30784},{level:3,title:"第五题",slug:"第五题",normalizedTitle:"第五题",charIndex:31260},{level:3,title:"第六题",slug:"第六题",normalizedTitle:"第六题",charIndex:31530},{level:3,title:"第七题",slug:"第七题",normalizedTitle:"第七题",charIndex:31698},{level:3,title:"第八题",slug:"第八题",normalizedTitle:"第八题",charIndex:32114},{level:3,title:"第九题",slug:"第九题",normalizedTitle:"第九题",charIndex:32339},{level:3,title:"第十题",slug:"第十题",normalizedTitle:"第十题",charIndex:32555},{level:3,title:"第十一题",slug:"第十一题",normalizedTitle:"第十一题",charIndex:32966},{level:3,title:"第十二题",slug:"第十二题",normalizedTitle:"第十二题",charIndex:33236},{level:3,title:"第十三题",slug:"第十三题",normalizedTitle:"第十三题",charIndex:33652},{level:3,title:"第十四题",slug:"第十四题",normalizedTitle:"第十四题",charIndex:33980},{level:3,title:"第十五题",slug:"第十五题",normalizedTitle:"第十五题",charIndex:34113},{level:3,title:"第十六题",slug:"第十六题",normalizedTitle:"第十六题",charIndex:34320},{level:3,title:"第十七题",slug:"第十七题",normalizedTitle:"第十七题",charIndex:34582},{level:3,title:"第十八题",slug:"第十八题",normalizedTitle:"第十八题",charIndex:35099},{level:3,title:"第十九题",slug:"第十九题",normalizedTitle:"第十九题",charIndex:35608},{level:3,title:"第二十题",slug:"第二十题",normalizedTitle:"第二十题",charIndex:35802},{level:3,title:"第二十一题",slug:"第二十一题",normalizedTitle:"第二十一题",charIndex:36281},{level:3,title:"第二十二题",slug:"第二十二题",normalizedTitle:"第二十二题",charIndex:36810},{level:3,title:"第二十三题",slug:"第二十三题",normalizedTitle:"第二十三题",charIndex:39433},{level:3,title:"第二十四题",slug:"第二十四题",normalizedTitle:"第二十四题",charIndex:40084}],headersStr:"CKA认证介绍 考纲解读 K8S架构和工作原理 K8S架构图如下 K8s工作原理 理解K8S API原语 K8S中的基本概念 K8S中API对象的基本构成 使用kubectl 基本命令 部署命令 集群管理命令 Troubleshooting and Debugging命令 高级命令 Setting命令 其他命令 kubectl实用技巧 K8S调度管理 大纲 K8S调度相关基础概念 Node定义 Pod中主要属性字段 K8S中的资源分配 调度策略 预选策略-必选全部满足 优选策略 调度分配机制的要点 Pod所需资源的计算 K8S中的高级调度及用法 nodeSelector nodeAffinity podAffinity 手动调度和DaemonSet 污点和容忍度 调度结果和失败原因分析 多调度器及调度器配置 K8S日志/监控/应用管理 监控集群组件 监控应用 管理组件日志 管理应用日志 Deployment升级和回滚 创建Deployment 升级Deployment 升级策略 暂停Deployment 恢复Deployment 查询升级状态 查看升级历史 回滚的命令 应用弹性伸缩 应用自恢复 K8S中的安全 基本概念罗列 公钥/私钥/CA/数字签名 什么是X509 Openssl生成SSL证书过程 SSL/TLS认证 什么是Token kubeconfig的配置 实现自主控制Pod对象资源的访问权限 创建自定义权限用户 CKA考题整理 第一题 第二题 第三题 第四题 第五题 第六题 第七题 第八题 第九题 第十题 第十一题 第十二题 第十三题 第十四题 第十五题 第十六题 第十七题 第十八题 第十九题 第二十题 第二十一题 第二十二题 第二十三题 第二十四题",content:'华为云CKA认证考试知识点整理\n\n\n# 华为CKA认证\n\n\n# CKA认证介绍\n\nCKA，也就是Certificated Kubernetes Administrator，这是面向K8S管理员的认证项目，用于考核日常运维K8S集群所需的知识、技能，以及熟练度。\n\n关键信息：\n\n * 费用：$300(含一次补考的机会)\n * 在线远程监考、3小时上机实操、开卷(可查K8S手册)\n * 有效期是2年\n * 网络连通性、熟练度\n\n报名的链接：https://www.cncf.io/certification/cka/\n\n\n# 考纲解读\n\n可以通过https://github.com/cncf/curriculum来查看考纲最新版本。\n\n下面是一张关于涉及到的各个知识点所占的分值比例的介绍。\n\n\n\n这几大知识点，作者分别通过八次课程讲解的方式来全面讲解，也就是本次CKA知识点的整理脉络所在。\n\n\n\n第一节，讲解的是基础概念。对应考纲中的是"核心概念19%"\n\n第二节，讲解的是调度管理实训。对应考纲中的是"调度5%"\n\n第三节，讲解的是日志、监控与应用管理实训。对应考纲中的是"日志/监控5%"，"应用生命周期管理8%"。\n\n第四节，讲解的是K8S网络实训。对应考纲中的"网络11%"。\n\n第五节，讲解的是K8S存储管理实训。对应考纲中的是"存储7%"。\n\n第六节，讲解的是K8S安全管理实训。对应考纲中的是"安全12%"。\n\n第七节，讲解的是K8S集群运维实训。对应考纲中的是"集群运维11%"和"安装、配置和验证12%"。\n\n第八节，讲解的是K8S故障排查实训。对应考纲中的是"排错10%"。\n\n\n# K8S架构和工作原理\n\n\n# K8S架构图如下\n\n\n\n整体来说，K8S的架构是Master、Node的模式，Master节点上通常部署有scheduler、controller-manager、api-server，以及etcd分布式数据库。Node节点上通常运行着kubelet、kube-proxy组件，以及真正运行docker实例的Pod容器。\n\n用户通过kubectl，经过一些列的集群的安全认证后，将所需要执行的资源清单配置文件提交给api-server，api-server会将此信息写入etcd，写入完成后，etcd向api-server发起一系列的事件信息，通过list-watch的机制，先后由controller-manager执行副本配置，scheduler完成调度node，kubelet完成落实到各自node上启动相应的pod。而kube-proxy则用于相应的Server资源转发到所关联的一系列的Pod对象上。\n\n\n# K8s工作原理\n\n如下的图示是完整的工作原理的解析，其中核心的重点概念就是list-watch的机制。\n\n\n\n上面所有的"0"示意图，都是表示的是从k8s环境部署完毕后，各个api-server的客户端(api-server、scheduler、kubelet)都对于各个所订阅的资源对象，进行watch。\n\nlist-watch的异步消息通信机制，有效的保证了消息的可靠性、及时性、顺序性、高性能等。\n\n\n# 理解K8S API原语\n\n\n# K8S中的基本概念\n\nPod：是一组功能相关的Container的封装；共享存储和Network Namespace；是K8S调度和作业运行的基本单位(Scheduler调度，Kubelet运行)；Pod容易"走失"，需要workload和Service的保障。\n\nWorkloads: 有Deployment，Statefulset，DaemonSet，Job ....，实际上是一组功能相关的Pod的封装。\n\nService: 主要是为了让Pod "防失联"，给一组Pod设置反向代理。\n\n\n# K8S中API对象的基本构成\n\n整体来说，在K8S中一个完整的API对象由四大部分组成。\n\ntypeMeta: 主要由apiVersion和kind两个构成。主要描述的是该类API资源对象的类别和版本。\n\nobjectMeta: metadata类别，涵盖该类型资源对象下的基本元数据的信息，包含一些名称、label、注解等。\n\nspec: 这是关键的信息内容，涵盖定义的所有的期望状态信息。\n\nstatus: 这个不是客户端所需要配置的，反映的是当前该类资源对象的实际的状态值。\n\n\n\n\n# 使用kubectl\n\n\n# 基本命令\n\ncreate: 从文件或stdin创建资源。\n\nexpose: 为deployment，pod创建Service。\n\nrun: Run a particular image on the Cluster.\n\nset: Set sepcific features on objects.\n\nget: 最基本的查询命令。如kubectl get rs，kubectl get deploy，kubectl get svc，kubectl get rs。\n\nexplain: 查看资源定义。如kubectl explain replicaset。\n\nedit: 使用系统编辑器编辑资源。如 kubectl edit deploy。使用系统编辑器来加载yaml文件，当get查看一个yaml文件的时候很可能会翻过头，使用edit就可以自由多了。\n\ndelete: 删除指定资源，支持文件名、资源名、label selector。如kubectl delete po -l foo=bar。\n\n\n# 部署命令\n\nrollout: Deployment，DaemonSet的升级过程管理(查看状态、操作历史、暂停升级、恢复升级、回滚等)\n\nrolling-update: 客户端滚动升级，仅限ReplicationController\n\nscale: 修改Deployment，ReplicaSet，ReplicationController，Job的实例数。\n\nautoscale: 为Deploy，RS，RC配置自动伸缩规则(依赖heapster和HPA)\n\n\n# 集群管理命令\n\ncertificate: Modify certificate resources.\n\ncluster-info: 查看集群信息。\n\ntop: 查看资源占用率。\n\ncordon: 标记节点为unschedulable。\n\nuncordon: 标记节点为schedulable。\n\ndrain: 驱逐节点上的而应用，准备下线维护。\n\ntaint: 修改节点taint标记\n\n\n# Troubleshooting and Debugging命令\n\ndescribe: 查看资源详情。\n\nlogs: 查看pod内容器的日志。\n\nattach: Attach到pod内的一个容器。\n\nexec: 在指定容器内执行命令。\n\nport-forward: 为pod创建本地端口映射。\n\nproxy: 为Kubernetes API Server创建代理。\n\ncp: 容器内外/容器间文件拷贝。\n\n\n# 高级命令\n\napply: 从文件或stdin创建/更新资源。\n\npatch: 使用strategic merge patch语法更新对象的某些字段。\n\nreplace: 从文件或stdin更新资源。\n\nconvert: 在不同API版本之间转换对象定义。\n\n\n# Setting命令\n\nlabel: 给资源设置label。\n\nannotate: 给资源设置annotation。\n\ncompletion: 获取shell自动补全脚本(支持bash和zsh)。\n\n\n# 其他命令\n\napi-versions: 打印该集群支持的API的版本，在"group/version"中。\n\nconfig: 修改kubectl配置(kubeconfig文件)，如context。\n\nhelp: 帮助\n\nversion: 查看客户端和Server端K8S版本\n\n\n# kubectl实用技巧\n\n当kubectl命令太多太长记不住该怎么办？\n\n * 查看资源缩写\n   \n   $ kubectl api-resources\n   \n   \n   1\n   \n\n * 配置kubectl 自动完成\n   \n   $ source <(kubectl completion bash)\n   \n   \n   1\n   \n\n当kubectl写yaml文件太累了，找样例太麻烦？\n\n * 用run 命令生成deployment的yaml配置文件\n   \n   $ kubectl run --image=nginx my-deploy -o yaml --dry-run > my-deploy.yaml\n   \n   \n   1\n   \n\n * 用get 命令导出\n   \n   $ kubectl get statefulset/foo -o=yaml --export > new.yaml\n   \n   \n   1\n   \n\n * 用explain命令\n   \n   $ kubectl explain xxxx.xxxx\n   \n   \n   1\n   \n\n\n# K8S调度管理\n\n\n# 大纲\n\n整体从考试的大纲来看，有如下的考点要求。\n\n * 理解资源限制对Pod调度的影响\n * 使用label selector调度Pod\n * 手动调度Pod\n * 理解DaemonSet\n * 调度失败原因分析\n * 使用多调度器\n * 了解调度器的配置\n\n好像大致来看，也就是对K8S中scheduler调度的理解，分为资源调度、label selector调度、手动调度、高级调度等相关内容。\n\n\n# K8S调度相关基础概念\n\n调度器scheduler就是从现有的node中选择一个，来运行所需要的pod。\n\n从下面的图示可以看出，pod m/n/j通过K8S的scheduler调度以后，分别为pod m指定了节点y，pod n指定了节点z，pod l指定了节点x。从pod的yaml清单文件中也可以观察掉，调度前nodeName选项的value是空，调度后nodeName选项的value是相应的节点的名称了。\n\n\n\n\n# Node定义\n\n既然我们了解到scheduler的目的就是为pod指向一个node，那么就必须先了解下node的定义和pod的定义。\n\n\n\n通过执行kubectl get node/xxxx -o yaml,其中"allocatable"表示的是一个node可以分配的资源量，作为一个节点来接收Pod资源的依据。而"capacity"表示的是一个节点固有的资源量，一般来说capacity要大于等于allocatable的资源量。毕竟node除了给pod用，还有预留一些资源给其他。\n\n\n# Pod中主要属性字段\n\n我们同样要了解pod的具体资源配置文件的定义，毕竟资源的调度是由pod的需求来发起，通过schduler来调度node来满足。\n\n可以通过kubectl explain pod.spec来查看到相关的字段信息，或者和前面一样通过kubectl get po/xxxx -o yaml来查看一个既有的pod的配置信息。\n\n\n\n其中pod.spec.containers.resources字段中"requests"字段表示的是该Pod提出的要求就是这个node最少就得满足这么多资源，可能真实运行情况下，还用不到这么多的资源，可能要比这个少，也可可能比这个多，但是在Pod用到时，必须提供这么多的资源。"requests"可以理解为请求量，是schedule资源调度的依据。\n\n而"limits"的意思是，就是限制该Pod可以用到的最大值，实际在node上的资源使用时，node把这个pod的资源给限制死了。主要是防止pod自身的bug把node上其他资源给拖累了。\n\n"schedulerName"字段显示的是该Pod用的是哪种调度器管理的，默认就是k8s自带的"default-scheduler"。\n\n"nodeName"字段之前介绍过了，显示的value值就是调度后的结果。\n\n接下来的"nodeSelector"/"affinity"/"tolerations"就是高级调度的策略用法了。\n\n\n# K8S中的资源分配\n\nK8S中默认的调度器的核心目标就是基于资源可用性将各Pod资源公平地分布于集群节点之上。目前默认的调度器通过三个步骤来完成调度操作：节点预选(Predicate)、节点优先级排序(Priority)以及节点择优(Select)。\n\n> 下面有些调度内容，摘录自网上https://blog.csdn.net/qq_34857250/article/details/90259693\n> \n> https://www.cnblogs.com/L-dongf/p/12327401.html\n\n\n# 调度策略\n\n * 预选策略，predicate是强制性规则，会遍历所有的node节点，依据具体的预选策略筛选出符合要求的node列表，如果没有node符合predicates策略规则，那么Pod就会被挂起，直到有node能够满足。\n * 优选策略，这一步会在第一步筛选的基础上，按照优选策略为待选node打分排序，获取最优者。\n\n\n# 预选策略-必选全部满足\n\n * CheckNodeConditon: 检查node是否正常。\n * GeneralPredicates: 普通判断策略\n   * HostName: 检测Pod对象是否定义了pod.spec.hostname，并且检查节点中是否有同名的pod而冲突。\n   * PodFitHostPorts: 检查pod.spec.containers.ports.hostPort属性(绑定节点上的某个端口)是否定义，并且检查节点中的节点端口是否冲突。\n   * MatchNodeSelector: pods.spec.nodeSelector，检查节点选择器。\n   * PodFitsResources: 检查Pod的资源需求request是否能被节点所满足。\n * NoDiskConflict: 检测pod依赖的存储卷是否能满足需求，默认不检查。\n * PodToleratesNodeTaints: pods.spec.tolerations可容忍的污点，检查Pod是否能容忍节点上的污点。\n * PodToleratesNodeExecuteTanits: pod.tolerations属性中是否能接纳容忍NoExecute级别的污点，默认没有启用。\n * CheckNodeLablePresence: 检测node上的标签的存在与否，默认没有启用。\n * CheckServiceAffinity: 根据Pod所属的service，将相同所属的service尽可能放在同一个节点，默认不检查。\n * CheckVolumeBinding: 检查节点上已绑定和未绑定的PVC是否能够满足Pod对象的存储卷需求。\n * NoVolumeZoneConflict: 如果给定了区域限制，检查在此节点上部署Pod对象是否存在存储卷冲突。\n * CheckNodeMemoryPressure: 检测节点内存是否存在压力，如果节点内存压力过大，则检查当前Pod是否可以调度此节点上。\n * CheckNodePIDPressure: 检查节点PID数量是否存在压力。\n * CheckNodeDiskPressure: 检查节点磁盘资源的压力情况。\n * MatchInterPodAffinity: 检查给定节点是否能够满足Pod对象的亲和性或反亲和性条件，以用于实现Pod亲和性调度或反亲和性调度。\n\n\n# 优选策略\n\n优选过程中，调度器向每个通过预选的节点传递一系列的优选函数，来计算每个节点上各个优选函数后得到的值，调度器会给每个优选函数设定一个权重，大多数优先级默认为1。将所有优选函数得分乘以权重，然后相加从而得出节点的最终优先级分值。finaSoreNode=(weight1*priorityFunc1)+(weight2*priorityFunc2)+ ...\n\n下面是各个优选函数的相关说明：\n\n * LeastRequested: 节点的资源空闲率高的优选，是节点空闲资源与节点总容量的比值计算而来的。即由CPU或内存资源的总容量减去节点上已有Pod对象需求的容量总和，再减去当前要创建的Pod对象的需求容量的结果除以总容量。计算公式是：(cpu((capacity-sum(requested))*10 / capacity)+memory((capacity-sum(requested))*10 / capacity)) / 2\n * BalancedResourceAllocation: 计算节点上面的CPU和内存资源被占用的比率相近程度，越接近，比分越高，平衡节点的资源使用情况。计算公式：cpu=cpu((capacity-sum(requested))*10 / capacity) mem=memory((capacity-sum(requested))*10 / capacity)\n * NodePreferAvoidPodsPriority: 如果node上不存在"scheduler.alpha.kubernetes.io/preferAvoidPods"这个注解，那么不管什么pod都没有影响；如果node上存在相关的注解，那么注解中关联的Pod对象名称正好是要去调度的Pod，那么此类node分值会很低，如果关联的Pod对象名称和要调度的Pod名称没有任何关系，那么和没有注解是一样的效果。需要注意的是，在这个优先级中，优先级最高，得分会非常高。\n * NodeAffinityPriority: 节点的亲和性，亲和性高，得分高。基于节点亲和性调度偏好进行的优选级评估，根据Pod资源中的nodeSelector对给定节点进行匹配度检查，成功匹配到的条目越多则节点得分越高。\n * TaintTolerationPriority: 将Pod对象的spec.tolertions与节点的taints列表项进行匹配度检测，匹配的条目越多，得分越低。\n * SelectorSpreading: 尽可能的把Pod分散开，也就是没有启动这个Pod的node，得分会越高。\n * InterPodAffinityPriority: 遍历Pod对象的亲和性条目，匹配项越多，得分就越多。\n * MostRequestedPriority: 节点中空限量越少的，得分越高，与LeastRequested不能同时使用，集中各一个机器上面跑Pod，默认没有启用。\n * NodeLabelPriority: 根据node上面是否拥有特定的标签来评估得分，有标签就有分，而无论其值为何。默认没有启用。\n * ImageLocalityPriority: 一个node的得分高低，是根据node上面是否有镜像，有镜像就有得分，反之就没有(根据node上已有满足需求的image的size大小之和来计算)，默认没有启用。\n\n\n# 调度分配机制的要点\n\n * 基于Pod中容器request资源"总和"调度\n   \n   * resources.limits影响Pod的运行资源上限，但是不会影响调度。\n   * initContainer取最大值，container取累加值，最后取大值。即Max(Max(initContainers.requests), Sum(containers.requests))\n   * 未指定request资源时，按0资源需求进行调度，也就是任何node都满足该pod资源。\n\n * 基于资源声明量的调度，而非实际占用\n   \n   * 不依赖监控，系统不会过于敏感\n   * 能否调度成功：能否调度成功：pod.request < node.allocatable - node.requested\n\n * K8S node的资源盒子模型\n   \n   也就是说一个node的资源，要满足于很多种类型的需求，不仅仅是提供给pod的资源。\n   \n   \n\n * 重要的一些资源分配相关算法\n   \n   * GeneralPredicated(主要是PodFitsReources)\n   * LeastRequesterPriority\n   * BalancedResourceAllocation，平衡CPU/MEM的消耗比例\n\n\n# Pod所需资源的计算\n\n上面已经说到了，关于resource.requests是资源调度的依据。在计算一个Pod所需要的资源的时候，同时考虑pod中的initcontainers的资源量和containers的资源量，其中initcontainers的资源量取单个容器的最大值，而containers的资源量去所有容量的累加值。然后这两个值比较取最大值。\n\n\n\n\n# K8S中的高级调度及用法\n\n\n# nodeSelector\n\n将Pod调度到特定的Node上\n\n\n\n首选要在node上定义相应的labels，然后在pod.spec.nodeSelector中定义需要调度到的相应标签的node。类似于RDBMS中通过select node.name from xxx where node.disktype=ssd and node-flavor=s3.large.2 ，也就是说pod中nodeSelector要完全匹配node上的标签。\n\n这个匹配调度的逻辑，是之前描述的MatchNodeSelector预选调度算法，预选调度算法是必须要满足项的node。\n\n\n# nodeAffinity\n\n节点亲和性，是调度程序用来确定pod对象调度到哪个node上的一组规则，和nodeselecotr一样，也是基于节点上的自定义标签和Pod对象上指定的标签选择器来进行定义的，这是nodeSelector的升级版本。\n\nnodeAffinity属于优选函数算法中一种。\n\n总体来说，节点亲和性调度可以分为硬亲和(required)和软亲和(preferred)。顾名思义，硬亲和性实现的是强制性规则，它是Pod调度时必须要满足的规则，如果不满足，则Pod对象会被置于Pending状态；而软亲和则是一种柔性调度限制，它倾向于将Pod对象运行于某类特定的节点之上，而调度器也将尽量满足此需求，但是在无法满足调度需求的时候，它将退而求其次地选择一个不匹配规则的节点。\n\n\n\n# 硬亲和\n\n硬亲和的策略在pod中位置于pod.spec.affinity.nodeAffinity.requireDuringSchedulingIgnoreDuringExecution。"IgnoreDuringExecution"表明了，该调度策略只会对当前的pod和node标签，以及相应规则，做个调度匹配。以后要是节点标签发生了变化了，那么已经调度到了该node上的Pod对象不会做出改变，只会对新建的Pod对象生效。老人老办法，新人新规定。\n\n在上述map的中可以包含多个value(nodeSelectorTerms字段)，多个nodeSelecorTerm之间是"逻辑或"的关系，意思是有节点满足其中的一个nodeSelecorTerm就算这个node满足了。\n\nnodeSelectorTerms下面需要可以定义多个matchExpression，多个规则彼此之间为"逻辑与"的关系，也就是说只有该nodeSelecorTerm下面的所有matchExpression定义的规则都满足，这个node才算是满足了。\n\n标签选择器表达式(matchExpression下定义的规则)，支持使用的操作符号有In、NotIn、Exsits、DoesNotExists、Lt和Gt等。In表示的是只要满足后面的集合中的一个就算是满足了条件。\n\n# 软亲和\n\n软亲和的策略在pod中位置于pod.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoreDuringExecution。同理是"IgnoreDuringExecution"。\n\n在上述的map中可以包含多个规则，每个规则都可以配置weight属性以便用于定义其优先级。对于相关的节点分别计算出多个规则的权重值，最后分值高的节点胜出。\n\n\n# podAffinity\n\npod亲和性，顾名思义，调度和判断的主体是之前已经存在的pod，而非上面所说的node。是根据已调度或将要调度的pod的所位于的node的情况，来决定后续的pod将要部署在哪些node上，反映的是后一种pod对已存在pod的一种亲和性的关系的，调度管理。\n\npodAffinity也分为亲和性和反亲和性，每种亲和策略下又分为硬(required)亲和、软(preferred)亲和。\n\nK8S调度器通过内建的MatchInterPodAffinity预选策略为这种调度方式完成节点预选，并基于InterPodAffinityPriority优选函数进行各节点的优选级评估。\n\n# 为什么要有pod亲和性\n\n出于高效通信的需要，偶尔需要把一些pod对象组织在相近的位置(同一节点、机架、区域或地区等)，如某业务的前端Pod和后端Pod(表现为pod亲和性)。或者说要出于安全性或分布式的原因，需要将一些Pod对象在其运行的位置上隔离开来(表现为pod反亲和性)。\n\n# 什么是位置拓扑topologykey\n\nPod亲和性调度需要各相关的Pod对象运行于"同一位置"，而topologykey就恰恰定义了这个一个什么样的类别，比如区域的类别、机架的类别、主机的类别等等。\n\n在定义Pod对象的亲和性与反亲和性时，需要借助于标签选择器来选择被依赖的Pod对象，并根据选出的Pod对象所在节点的标签来判断"同一位置"的 具体意义。\n\n# pod亲和性和node亲和性有什么区别\n\n * 在pod.sepc.affinity存在podAffinity和podAntiAffinity，这两种配置都是对称的。\n * pod亲和性调度中labelSelector的匹配对象是Pod，而node亲和性调度中匹配的是node。\n * pod亲和性调度中匹配到的是根据topologykey定义的一组node，topologykey定义了分组是什么样的一个级别，相同topologykey中的key和value的值为一组。\n * 在pod亲和性中硬亲和过滤规则中，条件间只有逻辑与运算。\n\n\n\n# pod硬亲和调度\n\npod的硬亲和调度的API定义于pod.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution下，通过labelSelector 这个map定义多个匹配表达式。条件间只有逻辑与的运算，这一点和node亲和性有所不同。\n\ntopologyKey: kubernetes.io/hostname是K8S中节点的内建标签，它的值就是当前节点的节点主机名称。同理还有region(地区)、zone(区域)、rack(机架)的拓扑位置的定义。\n\n# pod软亲和调度\n\npod的软亲和调度的API定义于pod.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution下，和node软亲和性调度类型，可以定义多个调度规则，每个规则都可以定义一个权重。\n\n# podAntiAffinity反亲和性\n\n与podAffinity匹配过程相同，只是最终的结果取反。\n\n\n\n\n# 手动调度和DaemonSet\n\n当遇到调度器不工作时，需要考虑到手动调度Pod。\n\n我们只需要在pod.spec.nodeName中直接填上需要调度到的node的名称就可以了。\n\n\n\n# DaemonSet调度\n\n老的版本中DaemonSet中的pod调度都是由controller-manager直接指定pod的运行节点，不经过调度器。直到1.11版本开始，DaemonSet的pod才由scheduler引入调度。\n\nDaemonSet实际上是要求每个节点都部署一个相同的pod，通常用于部署集群中的agent，例如网络插件等。在下图的Daemonset的配置中，可以看出类似于定义了一个Deployment的清单配置文件中，对于要求一个主机上不存在相同的pod label，也就是对于Pod的反亲和性。\n\n\n\n\n# 污点和容忍度\n\n污点(taints)是定义在节点之上的键值型属性数据，用于让节点拒绝将pod调度运行于其上，除非该pod对象具有容纳节点污点的容忍度。\n\n而容忍度(tolerations)是定义在Pod对象上的键值型属性数据，用于配置其可容忍的节点污点，而且调度器仅能将Pod对象调度至其能够容忍该节点污点的节点之上。\n\nK8S中使用PodToleratesNodeTaints预选策略和TaintTolerationPriority优选函数来完成此类高级调度机制。\n\n# 污点容忍度/节点选择器/节点亲和性区别\n\n上面描述的节点选择器(nodeSelector)和节点亲和性两种调度方式都是通过在Pod对象上添加标签选择器来完成对特定类型节点标签的匹配，实现的是由Pod选择节点的机制。\n\n而污点和容忍度则是通过向节点添加污点信息来控制Pod对象的调度结果，从而赋予了节点控制何种Pod对象能够调度与其上的主控权。\n\n节点亲和性是使得Pod对象被吸引到一类特定的节点，而污点则相反，它提供了让节点排斥特定Pod对象的能力。\n\n# 污点\n\n污点(taint)的定义在 node.spec.taints下，是键值型数据，但又额外支持一个效果(effect)标识，语法格式为"key=value:effect"，其中key和value的用法及格式与资源注解信息相似，而effect则用于定义对pod对象的排斥等级，它主要包含以下三种类型。\n\n * NoSchedule: 不能容忍此污点的新Pod对象，不可调度至当前节点，属于强制型约束关系，节点上现存的Pod对象不受影响。\n * PreferNoSchedule: NoSchedule的柔性约束版本，即不能容忍此污点的新Pod对象尽量不要调度至当前节点，不过无其他节点可供调度时也允许接受相应的Pod对象。节点上现存的Pod对象不受影响。\n * NoExecute: 不能容忍此污点的新Pod对象，不可调度至当前节点，属于强制性约束关系，而且节点上现存的Pod对象会因节点污点变动或Pod容忍度变动而不再满足匹配规则时，Pod对象将被驱逐。\n\n\n\n给节点添加污点标识：\n\n$ kubectl taint nodes xxx <key>=<value>:<effect>\n例如：\n$ kubectl taint nodes node1 node-type=production:NoSchedule\n\n\n1\n2\n3\n\n\n即便是同一个键值数据，若其效用标识不同，则也分属于不同的污点信息，也就是说会增加一条污点信息，和之前的区别只是在于效用标识不同。\n\n$  kubectl taint nodes node1 node-type=production:PreferNoSchedule\n\n\n1\n\n\n删除节点上某特定键名，特定标识的污点信息。\n\n$ kubectl taint nodes node1 node-type=NoSchedule-\n\n\n1\n\n\n删除节点上某特定键名的所有污点信息。(也就是省略效用标识)\n\n$ kubectl taint nodes node1 node-type-\n\n\n1\n\n\n删除节点上所有的全部的污点信息，可以使用kubectl patch命令将节点属性spec.taints的值直接置为空即可。\n\n$ kubectl patch nodes node1 -p \'{"spec":{"taints":{}}}\'\n\n\n1\n\n\n# 容忍度\n\nPod对象的容忍度可通过其pod.spec.tolerations字段进行添加，根据使用的操作符不同，主要有两种不同的形式：一种是与污点信息完全匹配的等值关系"Equal"；另一种是判断污点信息存在性的匹配方式"Exists"。其中tolerationSeconds用于定义延迟驱逐当前Pod对象的时长。\n\n\n\n需要注意的如下信息：\n\n * 如果节点node上有多个污点信息，那么就必须该Pod对此节点上的所有污点信息都能容忍，才能调度上去。\n * 匹配逻辑和之前的pod中的nodeselector正好相反，之前的逻辑是只要是node上的一个标签满足于pod中定义的node selector就进行匹配。\n * pod中定义的operator为"Equal"时，就是需要在pod的toerations中完整填写所有的key、value、effect。\n * pod中定义的operator为"Exists"时(key/effect/operator项是必填的，而value项留空)，可以填写value为空，表示的是匹配容忍节点node中所有关于这个key中的相应的"effect"的，所有value的污点的信息。\n * pod中定义的operator为"Exists"时(key/operator项是必填，value和effect为空)，表示的是匹配容忍节点node中所有关于这个key值与之相同的，所有value的所有effect的污点信息。\n\n# 污点和容忍度的调度逻辑\n\n一个节点可以配置使用多个污点，一个Pod对象也可以有多个容忍度，不过二者在进行逻辑检查时会遵循如下逻辑。\n\n 1. 首先处理每个有着与之匹配的容忍度的污点。\n\n 2. 不能匹配到的污点上，如果存在一个污点使用NoSchedule效用标识，则拒绝调度Pod对象至此节点上。\n\n 3. 不能匹配到的污点上，若没有任何一个使用NoSchedule效用标识，但至少有一个使用了PreferNoScheduler，则应尽量避免将Pod对象调度至此节点。\n\n 4. 如果至少有一个不匹配的污点使用了NoExecute效用标识，则节点将立即驱逐Pod对象，或者不予调度至给给定节点；另外，即便容忍度可以匹配到使用了NoExecute效用标识的污点，若在定义容忍度时还同时使用了tolerationSeconds属性定义了容忍时限，则超出时限后其也将被节点驱逐。\n\n\n# 调度结果和失败原因分析\n\n * 查看调度结果\n   \n   $ kubectl get pod xxx -o wide\n   \n   \n   1\n   \n\n * 查看调度失败原因\n   \n   $ kubectl describe pod xxxx\n   \n   \n   1\n   \n\n * 调度失败错误列表(可以选择相应的版本)\n   \n   https://github.com/kubernetes/kubernetes/blob/release-1.17/pkg/scheduler/algorithm/predicates/error.go\n\n\n# 多调度器及调度器配置\n\n如果集群中已经启动了另外的一个调度器，那我们可以在Pod对象的清单配置文件中，指定相应的"schedulerName"的值。如果集群中存在多个调度器，那么建议将node节点划分资源池来管理，避免多个调度器管理混乱。\n\n\n\n我们可以在新配置的调取器中加载个性化的配置文件，来修改调整我们想要的排序算法。\n\n\n\n\n# K8S日志/监控/应用管理\n\n\n# 监控集群组件\n\n 1. 查看集群的整体状态\n    \n    $ kubectl cluster-info\n    \n    \n    1\n    \n\n 2. 查看更多更全的信息\n    \n    该信息下会收集所有的已有的pod等资源对象的信息，event信息，调度信息等。\n    \n    $ kubectl cluster-info dump > xxx.txt\n    \n    \n    1\n    \n\n 3. 查看master组件(插件部署的)\n    \n    $ kubectl get pod etcd -n kube-system\n    $ kubectl describe pod kube-apiserver -n kube-system\n    \n    \n    1\n    2\n    \n\n 4. 查看组件metrics\n    \n    $ curl localhost:10250/stats/summary\n    \n    \n    1\n    \n\n 5. 查看组件的健康状况\n    \n    $ curl localhost:10250/healthz\n    \n    \n    1\n    \n\n 6. Heapster + cAdvisor监控集群组件\n    \n    早期的版本中，k8s官方提供了Heapster、InfluxDB和Grafana的组合来对系统进行，Heapster以Pod的形式运行于集群中，已被metrics-server替代。\n    \n    cAdvistor既能收集容器CPU、内存、文件系统和网络使用统计信息，还能采集节点资源使用情况。使用下面的命令，展示Node CPU/内存/存储资源消耗\n    \n    $ kubectl top node {node name}\n    \n    \n    1\n    \n\n\n# 监控应用\n\n 1. 监控应用pod详细信息\n    \n    $ kubectl describe pod\n    \n    \n    1\n    \n\n 2. 展示Pod CPU/内存/存储资源消耗\n    \n    对接了heapster或metrics-server后，可以展示这些数据。相比于在pod内输入top看到的数据可能有所不同，一方面如果pod内没有做资源隔离的化，看到的是整个节点的资源情况；另一方便这个命令能看到一个区间内的资源消耗情况，而不是只能实时显示当时的情况。\n    \n    $ kubectl top pod {pod name}\n    \n    \n    1\n    \n\n 3. 持续性的观察k8s中资源的变化情况\n    \n    $ kubectl get pod {pod name} --watch\n    \n    \n    1\n    \n\n\n# 管理组件日志\n\n 1. 当位于特定的目录下的组件日志查看：\n    \n    /var/log/kube-apiserver.log\n    /var/log/kube-proxy.log\n    /var/log/kube-controller-manager.log\n    /var/log/kubelet.log\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 使用systemd管理的时候\n    \n    $ journalctl -u kubelet\n    \n    \n    1\n    \n\n 3. 使用K8S插件部署的时候\n    \n    $ kubectl logs -f kube-proxy\n    \n    \n    1\n    \n\n\n# 管理应用日志\n\n 1. 从K8S的容器标准输出中截取\n    \n    $ kubectl logs -f {pod name} -c {container name}\n    \n    \n    1\n    \n\n 2. 从docker容器标准输出中截取\n    \n    $ docker logs -f {docker name}\n    \n    \n    1\n    \n\n 3. 进入到容器内查看\n    \n    将日志文件挂载到主机目录中:\n    \n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: test-pd\n    spec:\n      containers:\n      - image: xxxxxx/xxxx\n        name: test-container\n        volumeMounts:\n        - mountPath: /log\n          name: log-volume\n        volumes:\n        - name: log-volume\n          hostPath:\n            # 主机上面的目录\n            path: /var/k8s/log\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n    \n    这样可以直接进入容器内来查看日志\n    \n    $ kubectl exec -it {pod} -c {container} /bin/sh\n    $ docker exec -it {container} /bin/sh\n    \n    \n    1\n    2\n    \n\n\n# Deployment升级和回滚\n\n\n# 创建Deployment\n\n$ kubectl run {deployment_name} --image={image} --replicas={rep.}\n$ kubectl run --image=nginx my-deploy -o yaml --dry-run > my-deploy.yaml\n\n\n1\n2\n\n\n或者使用yaml文件形式，重点配置replicas和image字段\n\n\n# 升级Deployment\n\n升级deployment的镜像文件\n\n$ kubectl set image deploy/nginx   nginx=nginx:1.9.1\n\n\n1\n\n\n修改deployment的资源限制\n\n$ kubectl set resources deploy/nginx -c=nginx --limits=cpu=200m,memory=512Mi\n\n\n1\n\n\n\n# 升级策略\n\n修改deployment中的升级策略\n\nmaxSurge: 是指升级期间存在的总Pod对象数量最多可以超出期望值的个数，如果spec是3，而maxSurge为1，那么则表示Pod对象的总数不能超过4个。\n\nminReadySeconds: 5\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 1 # 默认25%\n    maxUnavailable: 1 #默认25%\n\n\n1\n2\n3\n4\n5\n6\n\n\nmaxUnavailable: 升级期间正常可用的Pod副本数(包括新的旧的版本)最多不能低于期望值的个数。默认值为1，如果期望值spec是3，那么升级期间至少要有2个Pod对象处于正常提供服务的状态。\n\n\n# 暂停Deployment\n\n通过以下的命令来暂停deployment，暂停deployment，意思不是说我们把deployment冻住了，而是暂停这个deploy之后，所对这个deploy做的改动不会记录到升级日志版本中去了。\n\n$ kubectl roullout pause deployment/nginx-deployment\n\n\n1\n\n\n\n# 恢复Deployment\n\n当已经处于暂停状态下的deploy，可以通过如下命令来恢复。比如说现在我们想把处于暂停状态下的deploy修改另外一个image版本，但是是想把这个记录到升级日志中去。那么可以先把这个暂停了的deploy做个恢复，随后再做修改。\n\n$ kubectl rollout resume deployment/nginx-deployment\n\n\n1\n\n\n\n# 查询升级状态\n\n通过如下的命令来查看deploy对象的升级状态\n\n$ kubectl rollout status deployment/nginx-deployment\n\n\n1\n\n\n\n# 查看升级历史\n\n查看所有的版本\n\n$ kubectl rollout history deploy/nginx-deployment\n\n\n1\n\n\n查看指定的版本\n\n$ kubectl rollout history deploy/nginx-deployment --revision=2\n\n\n1\n\n\n\n# 回滚的命令\n\n回滚到指定的版本上\n\n$ kubectl rollout undo deployment/nginx-deployment --t0-revision=2\n\n\n1\n\n\n\n# 应用弹性伸缩\n\n直接通过scale命令来弹性伸缩副本数。\n\n$ kubectl scale deployment nginx-deployment --replicas=10\n\n\n1\n\n\n当对接了heapster，和HPA联动后。\n\n$ kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-persent=80\n\n\n1\n\n\n\n# 应用自恢复\n\n这里配置应用pod自恢复策略，通过restartPolicy(重启策略)和livenessProbe(存活性探针)来做的。\n\nPod Restart Policy有：Always、OnFailure、Never。\n\nlivessProbe: http/https Get、shell exec、topSocket。\n\n下面是一个tcp socket的liveness探针+always restart例子。\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: goproxy\nspec:\n  restartPolicy: Always\n  containers:\n  - name: goproxy\n    image: k8s.gcr.io/goproxy:0.1\n    ports:\n    - containerPort: 8080\n    livenessProbe:\n      tcpSocket:\n        port:8080\n      initialDelaySeconds: 15\n      periodSeconds: 20\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# K8S中的安全\n\n在K8S中安全方面，主要可以分为两大类：部署态的安全控制和运行态的安全控制。\n\n部署态的安全控制主要体现在：当"人为"(常规账户) 或pod对象(service account)通过 kubectl 、客户端库或直接使用REST接口进行请求时，需要通过认证、鉴权、Admission(准入控制)。以及正常我们所配置的Pod的安全上下文中，用于限定pod启动的用户、组、以及特权用户等。\n\n运行态的安全控制主要体现在：Network policy上，一般是指namespaces或pod对象之间的网络访问控制。\n\n\n# 基本概念罗列\n\n# 什么是User Account(用户账户)\n\n是k8s的客户端，此类客户端或通过kubectl、客户端库或REST接口进行请求通信时，对应的是现实中的"人"。\n\nUser Account一般是指由独立于k8s之外的其他服务管理的用户账号，K8S中不存在此类用户账号的对象。\n\nUser Account通常用于复杂的业务逻辑管控，它作用于系统全局，所以名称必须是全局唯一。\n\n# 什么是Service Account(服务账户)\n\nService Account也是K8S的客户端的类型，是指由K8S API管理的账号，用于为Pod之中的服务进程访问K8S API时提供的身份标识。\n\nService Account属于namespace scope，它们由API Server创建，或通过API调用手动创建，附带着一组存储为Secret的用于访问API Server的凭据。\n\n# 用户组的概念\n\nUser Account和Service Account都可以隶属于一个或多个用户组。用户组只是用户账号的逻辑集合，它本身并没有操作权限，但是附加于组上的权限(这个用户组所拥有的权限)，可以被其内部的所有用户继承。\n\nK8S有着如下几个内建的用于特殊目的的组：\n\n * system:unauthenticated: 未能通过任何一个插件检验的账号，即未通过认证测试的用户所属的组。\n * system:authenticated: 认证成功后的用户自动加入的一个组，用于快捷引用所有正常通过认证的用户账号。\n * system:serviceaccounts: 当前系统上的所有Service Account对象。\n * system:serviceaccounts: <namespace>: 特定名称空间内所有的Service Account对象。\n\n# 什么是Authentication(认证)\n\nAPI Server处理请求的过程中，认证插件负责鉴定用户身份。\n\nK8S中支持的认证方式包括客户端证书、承载令牌(bearer tokens)、身份验证打理(authenticating proxy)或HTTP basic认证。认证的过程中会以串行的方式进行，直到一种认证机制成功完成即结束。\n\nAPI Server接收到访问请求时，它将调用认证插件尝试将以下属性与访问请求相关联。\n\n * Username: 用户名，如kubenetes-admin等。\n * UID: 用户的数字标签符，用于确保用户身份的唯一性。\n * Groups: 用户所属的组，用于权限指派和继承。\n * Extra: 键值数据类型的字符串，用于提供认证时需要用到的额外信息。\n\nAPI Server支持以下几种具体的认证方式。\n\n 1. X509客户端证书认证：\n\n客户端在请求报文中携带X509格式的数字证书用于认证，认证通过后，证书中的主体标识(Subject)将被识别为用户标识，其中的CN(Common Name)字段是用户名，O(Organization)是用户所属的组，如"/CN=ilinux/O=opmasters/O=admin"中，用户名为ilinux，其属于opmasters和admin两个组。\n\n 2. 静态令牌文件(static Token File):\n\n即保存着令牌信息的文件，由kube-apiserver的命令行选项--token-auth-file加载，且服务器启动后不可更改；HTTP客户端也能使用承载令牌进行身份验证，将令牌进行编码后，通过请求报文中的Authorization首部承载传递给API Server即可。\n\n 3. 引导令牌(Bootstrap Tokens):\n\n一种动态管理承载令牌进行身份认证的方式，常用于简化新建K8S集群的节点认证过程，需要通过--experimental-bootstrap-token-auth选项启用；有新的工作节点首次加入时，Master使用引导令牌确认节点身份的合法性之后自动为其签署数字证书以用于后续的安全通信，使用kubeadm join命令将节点加入kubeadm初始化的集群时使用的就是这种认证方式；这些令牌作为Secrets存储在kube-system命名空间中时，可以动态管理和创建它们，而Controller Manager 包含一个TokenCleaner控制器，用于删除过期的引导令牌。\n\n# 什么是Authorization(授权)\n\n授权插件用于操作权限许可鉴别，最常用的就是RBAC(基于角色的访问控制)。\n\nAPI Server是一种REST API，除了身份认证信息之外，一个请求报文还需要提供操作方法及其目标对象。如针对某Pod资源对象进行的创建、查看、修改或删除操作等。\n\n为了核验用户的操作许可，成功通过身份认证后的操作请求还需要转交给授权插件进行许可权限检查，以确保其拥有执行相应的操作的许可。这里主要介绍的就是RBAC的授权插件。\n\n# 什么是RBAC\n\n简而言之，就是K8S将权限授予"角色"(role)之上，有别于传统访问控制机制中将权限直接赋予使用者的方式。\n\n用户(User)就是一个可以独立访问计算机系统中的数据或用数据表示的其他资源的主体(Subject)。\n\n角色是一个组织或任务中的工作或位置，代表了一种权利、资格和责任。\n\n许可(Permission)就是允许对一个或多个客体(Object)执行的操作。\n\n一个用户可经授权而拥有多个角色，一个角色可由多个用户构成；每个角色可以拥有多种许可，每个许可也可授权给多个不同的角色。\n\n重点内容：\n\n * RBAC授权插件，实现了主体(用户账号或服务账号)，操作(各种权限)，客体(目标实体)。\n * RBAC授权插件支持Role和ClusterRole两类角色，其中Role作用于名称空间级别，ClusterRole作用于集群级别。\n * Role角色需要用RoleBinding这个资源类型，将Role上面的权限绑定到一个或一组用户至上。同样作用于名称空间级别。\n * ClusterRole角色需要用ClusterRoleBinding这个资源类型，将ClusterRole上面的权限绑定在一个或一组用户至上。\n\n# 什么是Admission Control(准入控制)\n\n准入控制则是用于在资源对象的创建、删除、更新或连接(proxy)操作时实现精细的许可检查。\n\n\n# 公钥/私钥/CA/数字签名\n\n> 参考的网络URL如下\n> \n> https://zhuanlan.zhihu.com/p/31477508\n> \n> https://zhuanlan.zhihu.com/p/113522792\n> \n> https://www.jianshu.com/p/939b3039478c\n> \n> https://www.cnblogs.com/handsomeBoys/p/6556336.html\n\n# 公钥和私钥\n\n首先是加密算法分为两种，一种是对称加密算法(私钥加密算法)，一种是非对称加密算法(公钥算法)。\n\n对称加密算法：主要有DES/AES，特点是这种算法的加密/解密用的密钥key都是一样的。也就是说用一个密钥key可以加密内容，同样也可以用这把key来解密内容。举个例子：如果总部A派出了两个间谍B和C打入敌方内部，如果他们用的都是对称加密算法的话，那么如果其中一个间谍的密钥被获得的话，那么其他间谍的所有信息都会受到暴露。\n\n非对称加密算法：这种算法加密和解密的密码不一样，一把是公钥，一把是私钥。有如下的特征：\n\n * 公钥和私钥成对出现。\n * 公开的密钥叫做公钥，只有自己知道的叫私钥。\n * 用公钥加密的数据只有对应的私钥可以解密。(B在获得了A的发出的公钥后，可以例如A的公钥对要写给A的信件进行加密，那么A收到B发出的信件后，利用A自己的私钥进行解密。理论上，只要A自己的私钥不泄露，信件都是安全的。因为其他人即使截获了B发出的加密信件内容，也由于没有A的私钥而解密)\n * 用私钥加密的数据只有对应的公钥可以解密。(这一点用在了数字签名上，还是上面的案例中，如果B要求 A收到了B的信件后，要给B签名签收以示确认。那么就是A收到信件后，用A对信件中的文本进行hash函数，得到文本信件摘要，然后对摘要，用自己的私钥进行加密，进行加密得到密文(签名)。而B收到这个密文后，拿出原来A给的公钥进行解密，如果得到了文本摘要hash值，和自己用同样的hash函数文本内容得到的hash值进行比对，如果相等，那么就可以确定这是A的签名无误了，A是不能抵赖的)\n\n# CA证书\n\n在A向B发送信息的过程中，如果有个人C图谋不轨，偷偷把B的公钥换成了自己的公钥，然后截获了A发往B的内容，这个时候C就可以通过自己的私钥解密这个内容了。\n\n那么这个时候，就需要一个中立的权威的机构来指明，A所拿到的B的公钥真的是B的公钥。这个权威的机构就是"证书中心CA"，说白了就是为公钥做个公证。证书中心用自己的私钥，对需要公证的公钥和一些相关信息一起加密(也就是CA对这个公钥进行签名认证了)，生成"数字证书"。\n\nCA证书包括以下信息：申请者的公钥、申请者的组织信息和个人信息、签发机构CA的信息、有效时间、证书序列号等信息的明文，同时要包含一个CA机构的签名。\n\nA向B发起请求时，B返回的是一个证书文件。A读取这个证书中的明文信息，就得到了B的公钥内容；同时A也会去相应的认可的CA中心，用CA中心提供的公钥来解析这个数字证书中CA的签名信息，以此来得到B的公钥内容。两者做个比较，来判断，是否B的公钥是否正确。\n\n\n# 什么是X509\n\n是一种行业标准或者行业解决方案，在X.509方案中，默认的加密体制是公钥密码体制，一种非常通用的证书格式。\n\n在K8S中支持的HTTPS客户端证书认证、token认证以及HTTP basic认证几种认证方式中，基于SSL/TLS协议的客户端证书认证(遵循于X.509数字证书标准)以安全性高、易于实现，成为主要使用的认证方式。\n\nX509证书一般会用到三类文件，key，csr，crt。\n\nkey是私用密钥，openssl格式，通常是rsa算法。\n\ncsr是证书请求文件，用于申请证书。在制作csr文件的时候，必须使用自己的私钥来签署申请，还可以设定一个密钥。\n\ncrt是CA认证后的证书文件(windows下面的csr，其实是crt)，签署人用自己的key给你签署的凭证。\n\n\n# Openssl生成SSL证书过程\n\n> 参考网上如下URL文档整理：\n> \n> https://www.cnblogs.com/qiumingcheng/p/12024280.html\n\n# 基本概念\n\n首先要有一个CA根证书，然后用CA根证书来签发用户证书。\n\n用户进行证书申请：一般先生成一个私钥，然后用私钥生成证书请求(证书请求里应含有公钥信息)，再利用证书服务器的CA根证书来签发证书。\n\n特别说明：\n\n 1. 自签名证书(一般用于顶级证书、根证书)：证书的名称和认证机构的名称相同。\n 2. 根证书：根证书时CA认证中心给自己颁发的证书，是信任链的起始点。任何安装CA根证书的服务器都意味着对这个CA认证中心是信任的。\n 3. 数字证书：数字证书则是由证书认证机构(CA)对证书申请者真实身份验证之后，用CA的根证书对申请人的一些基本信息以及申请人的公钥进行签名(相当于加盖证书机构的公章)后形成的一个数字文件。数字证书包含证书中所标识的实体的公钥(就是说你的证书里有你的公钥)，由于证书将公钥与特定的个人匹配，并且该证书的真实性由颁发机构保证(就是说可以让大家相信你的证书时真的)，因此，数字证书为如何找到用户的公钥并知道它是否有效这一问题提供了解决方案。\n\n# openssl中有如下后缀名的文件\n\n.key格式：私有的密钥\n\n.csr格式：证书签名请求(证书请求文件)，含有公钥信息，certficate signing requests的缩写。\n\n.crl格式：证书吊销列表，Certificate Revocation List的缩写。\n\n.pem格式：用于导出，导入证书时候的证书的格式，有证书开头，结尾的格式。\n\n# CA根证书的生成步骤\n\n根证书，是CA自己的证书，需要在部署了CA服务的服务器上操作。\n\n一般的流程是：生成CA私钥(.key) --\x3e生成CA证书请求(.csr) --\x3e 自签名得到根证书(.crt)(实际上就是CA给自己颁发的证书)。\n\n# Generate CA private key\nopenssl genrsa -out ca.key 2048\n# Generate CSR \nopenssl req -new -key ca.key -out ca.csr\n# Generate Self Signed certificate（CA 根证书）\nopenssl x509 -req -days 365 -in ca.csr -signkey ca.key -out ca.crt\n\n\n1\n2\n3\n4\n5\n6\n\n\n在实际的软件开发工作中，往往服务器就采用这种自签名的方式，因为毕竟找第三方签名机构是要给钱的，也需要时间。\n\n# 用户证书的生成步骤\n\n生成用户证书的过程，一般如下：\n\n生成私钥(.key) --\x3e 生成证书请求(.csr) --\x3e 用CA根证书签名得到证书(.crt)\n\n# private key\nopenssl genrsa -des3 -out server.key 1024\n# generate csr\nopenssl req -new -key server.key -out server.csr\n# generate certificate\nopenssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 生成pem格式证书\n\n有时候需要用到pem格式的证书，可以用以下方式合并证书(crt)和私钥文件(key)来生成。\n\ncat client.crt client.key > client.pem\ncat server.crt server.key > server.pem\n\n\n1\n2\n\n\n\n# SSL/TLS认证\n\n# etcd集群内部对等节点通信：\n\netcd集群内各节点间的集群事务通信，默认监听于TCP的2380端口，基于SSL/TLS通信时需要peer类型的数字证书，可实现节点间的身份认证及通信安全，这些证书需要由一个专用的CA进行管理。\n\n# etcd服务器与客户端通信：\n\nkube-apiserver是etcd服务的主要客户端。etcd的REST API服务，默认监听于TCP的2379端口，用于接收并响应客户端请求，基于SSL/TLS通信，支持服务端认证和双向认证，而且要使用一个专用的CA来管理此类证书。\n\n# API Server与其客户端通信：\n\n它们之间的通信的证书可由同一个CA进行管理，其客户端大体上可以分为如下三类：\n\n * 控制平面的kube-scheduler和kube-controller-manager。\n * 工作节点组件kubelet和kube-proxy：初次接入集群时，kubelet可自动生成私钥和证书签署请求，并由Master为其自动进行证书签名和颁发，这就是所谓的tls bootstraping。\n * kubelet及其他形式的客户端，如Pod对象等。\n\n\n# 什么是Token\n\nToken的意思是"令牌"，是服务端生成的一串字符串，作为客户端进行请求的一个标识。当用户第一次登录后，服务器生成一个token并将此token返回给客户端，以后客户端只需要带上这个token前来请求数据即可，无需再次带上用户名和密码。\n\n\n# kubeconfig的配置\n\n# kubeconfig是干什么用的\n\nkubeconfig是一个配置文件，里面写了有哪些k8s的集群(多套K8S集群)，用户列表(不同用户，包含的ApiServer的认证信息)，上下文列表(不同用户对应不同的K8S环境的列表)，以及当前使用的上下文(指当前默认使用的用户对应相应集群的配置信息)，集群参数和用户参数可以同时设置多对，在上下文参数中将集群参数和用户参数关联起来。\n\n通过kubeconfig，apiserver的客户端可以通过简单切换上下文列表，来完成不同用户访问不同K8S集群的目的。\n\n# 常见命令\n\n 1. 打印kubeconfig文件内容\n    \n    $ kubectl config view\n    \n    \n    1\n    \n\n 2. 设置kubeconfig的clusters配置段。\n    \n    kubectl config set-cluster myk8s \\\n    --certificate-authority=/path/to/ca  \\\n    --embed-certs=true   \\\n    --server=${KUBE_APISERVER} \\\n    --kubeconfig=/kubeconfig/filename\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n    \n    myk8s只是一个自定义的k8s集群的名字；\n    \n    --certificate-authority设置了该集群的公钥；\n    \n    --embed-certs表示将--certificate-authority证书写入到kubeconfig中；也就是默认是false，也就是不会在这个定义的kubeconfig文件中写入上面的证书文件的具体内容，而是写一个对应的路径，如果改为了true，那么就会在这个定义的kubeconfig文件中写上上面的定义的具体的证书文件的内容。\n    \n    --server表示的是该集群的kube-apiserver的地址；\n    \n    --kubeconfig指定保存的kubeconfig文件的路径。如果没有这个文件的话，会新增，如果有的话，就会在已有的这个文件中，添加集群的信息内容。\n\n 3. 设置kubeconfig中的用户配置参数。\n    \n    kubectl config set-credentials chuck \\\n    --client-certificate=/path/to/cert  \\\n    --client-key=/path/to/private_key  \\\n    --embed-certs=true \\\n    --kubeconfig=/kubeconfig/filename \n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n    \n    chuck是定义的用户名；\n    \n    --client-certificate是定义的一个以.pem结尾的证书文件；\n    \n    --client-key是定义的一个私钥文件，注意客户端的证书首先要经过集群CA的签署，否则不会被集群认可。此处使用的是ca认证方式，也可以使用token认证，如kubelet的TLS Bootstrap机制下的bootstrapping使用的就是token认证方式。这里的kubectl 使用的是CA认证的方式，不需要token字段。\n\n 4. 修改context的参数。\n    \n    kubectl config set-context chuck@myk8s \\\n    --cluster=myk8s \\\n    --user=chuck \\\n    --kubeconfig=/kubeconfig/filename\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    chuck@myk8s是定义的上下文名称；\n    \n    --cluster是定义的集群名称；\n    \n    --user是定义的用户名称；\n\n 5. 设置默认上下文\n    \n    kubectl config use-context xxxx\n    \n    \n    1\n    \n\n\n# 实现自主控制Pod对象资源的访问权限\n\n解析一下：首先要做访问权限的控制，必须要用到鉴权插件中RBAC中的内容，也就是说需要定义不同的Role/ClusterRole，然后通过RoleBinding或ClusterRolebinding的方式，将权限依附于对应的service account上。那就是意味着pod在创建的时候，必须指定对应的service account。\n\n手动创建service account的时候，K8S会自动提供认证令牌Secret对象创建完成。通常挂载点有三个文件：ca.crt、namespace和token。其中token文件是保存了service account的认证token，容器就是通过它向API Server发起连接请求，进而由认证插件完成用户认证并将其用户名传递给授权插件。随后为这个service account进入RoleBinding或ClusterRolebinding，然后在创建Pod的时候，指定这个service account就可以了。\n\n\n# 创建自定义权限用户\n\n创建一个自定义基于SSL/TLS认证的的用户，配置其访问k8s集群的上下文的config，以授予非管理员级的集群资源使用权限，其配置过程由两部分组成：一是为用户创建专用的私钥和证书文件，二是将其配置于某kubeconfig中。\n\n# 创建自定义用户\n\n创建一个自定义用户所需要的私钥和证书文件，根据上面总结的利用openssl命令生成用户证书的过程。\n\n生成证书的过程如下：生成私钥(.key) --\x3e 生成证书请求(.csr) --\x3e 用CA根证书签名得到证书(.crt)。所有的这些操作都需要在master节点上以root用户的身份来执行，注意最后用到的根证书ca.crt，以及ca的私钥ca.key的位置，默认位于master节点的/etc/kubernetes/pki的路径下。\n\n 1. 生成私钥文件，注意其权限为600以阻止其他用户随意获取，这里在Master节点上以root用户进行操作，并将文件放置于/etc/kubernetes/pki专用目录中：\n    \n    # cd /etc/kubernetes/pki\n    # (umask 077; openssl genrsa -out kube-user1.key 2048)\n    \n    \n    1\n    2\n    \n    \n    这里openssl是生成私钥的命令，-out是指定输出的文件名，2048指的是生成2048位的rsa私钥，默认是X509编码。\n\n 2. 创建证书签署请求，-subj选项中CN的值将被kubeconfig作为用户名使用，O的值将被识别为用户组。\n    \n    # openssl req -new -key kube-usr1.key -out kube-user1.csr -subj "/CN=kube-user1/O=kuberusers"\n    \n    \n    1\n    \n    \n    req 的命令大致有3个功能：生成证书请求文件、验证证书请求文件和创建根CA。这里用到的创建证书请求文件。\n    \n    -new表示的是新生成一个新的证书请求文件。\n    \n    -key表示的是指定私钥文件。\n    \n    -out指定输出文件。\n\n 3. 基于kubeadm安装K8S集群时生成的CA来签署证书(对于上面创建的证书签署请求予以签署)，这里设置的有效时长为3650天。\n    \n    openssl x509 -req -in kube-user1.csr -CA ca.crt -CAkey  ca.key \\\n    -CAcreateserial -out kube-user1.crt -days 3650\n    \n    \n    1\n    2\n    \n\n\n# CKA考题整理\n\n\n# 第一题\n\nSet configuration context $kubectl config use-context k8s\n\nMonitor the logs of Pod foobar and\n\n 1. Extract log lines corresponding to error file-not-found\n 2. Write them to /opt/KULM00201/foobar\n\nQuestion Weight 5%\n\n\n# 第二题\n\nSet configuration context $ kubectl config use-context k8s\n\nList all PVs sorted by name saving the full kubectl output to /opt/KUCC0010/my_volumes.\n\nUse kubectl\'s own functionally for sorting the output, and do not manipulate if any further.\n\nQuestion weight 3%\n\n\n# 第三题\n\nSet configuration context $ kubectl config use-context k8s\n\nEnsure a single instance of Pod nginx is running on each node of the kubernetes cluster where nginx also represents the image name which has to be used. Do no override any taints currently in place.\n\nUse Daemonsets to complete this task and use ds.kusc00201 as Daemonset name.\n\nQuestion weight 3%\n\n\n# 第四题\n\nSet configuration context $ kubectl config use-context k8s Perform the following tasks.\n\n 1. Add an init container to lumpy--koala(Which has been defined in spec file /opt/kucc00100/pod-spec-KUCC00100.yaml)\n 2. The init container should create an empty file named /workdir/calm.txt\n 3. If /workdir/calm.txt is not detected, the Pod should exit\n 4. Once the spec file has been updated with the init container definition, the Pod should be created.\n\nQuestion weight 7%\n\n\n# 第五题\n\nSet configuration context $ kubectl config use-context k8s\n\nCreate a pod named kucc4 with a single container for each of the following images running inside (there may be betwen 1 and 4 images specified): nginx + redis + memcached + consul\n\nQuestion weight: 4%\n\n\n# 第六题\n\nSet configuration context $ kubectl config use-context k8s\n\nSchedule a Pod as follows:\n\n 1. Name: nginx-kusc00101\n 2. Image: nginx\n 3. Node selecotr: disk=ssd\n\n\n# 第七题\n\nSet configuration context $ kubectl config use-context k8s\n\nCreate a deployment as follows:\n\n 1. Name: nginx-app\n 2. Using container nginx with version 1.10.2-alpine\n 3. The deployment should contain 3 replicas\n\nNext, deploy the app with new version 1.13.0-alpine by perfoming a rolling update and record that update.\n\nFinally, rollback that update to the previous version 1.10.2-alpine\n\nQuesion weight: 4%\n\n\n# 第八题\n\nSet configuration context $ kubectl config use-context k8s\n\nCreate and configure the service front-end-service so it\'s accessible through NodePort and routes to the existing pod named front-end.\n\nQuestion weight: 4%\n\n\n# 第九题\n\nSet configuration context $ kubectl config use-context k8s\n\nCreate a Pod as follows:\n\n 1. Name: jenkins\n 2. Using image: jenkins\n 3. In a new Kubernetes namespace named website-frontend\n\nQuestion weight: 3%\n\n\n# 第十题\n\nSet configuration context $ kubectl config use-context k8s\n\nCreate a deployment spec file that will:\n\n 1. Lauch 7 replicas of the redis image with the label: app_env_stage=dev\n 2. Deployment name: kual00201\n\nSave a copy of this spec file to "/opt/KUAL00201/deploy_spec.yaml"(or.json)\n\nWhen you are done, clean up(delete) any new k8s API objects that you produced during this task.\n\nQuestion weight: 3%\n\n\n# 第十一题\n\nSet configuration context ```$ kubectl config use-context k8s``\n\nCreate a file "/opt/KUCC00303/kucc00302.txt" that lists all pods that implement Service foo in Namespace production.\n\nThe format of the file should be one pod name per line.\n\nQuestion weight: 3%\n\n\n# 第十二题\n\nSet configuration context $ kubectl config use-context k8s\n\nCreate a Kubernetes Secret as follows:\n\n 1. Name: super-secret\n 2. Credential: alice or username:bob\n\nCreate a Pod named pod-secrets-via-file using the redis image which mounts a secret named super-secret at /secrets\n\nCreate a second Pod named pod-secrets-via-env using the redis image, which exports credential as TOPSECRET\n\nQuestion weight: 9%\n\n\n# 第十三题\n\nSet configuration context $ kubectl config use-context k8s\n\nCreate a pod as follows:\n\n 1. Name: non-persistent-redis\n 2. Container image: redis\n 3. Named-volume with name: cache-control\n 4. Mount path: /data/redis\n\nIt should launch in the pre-prod namespace and the volume MUST NOT be persistent.\n\nQuestion weight: 4%\n\n\n# 第十四题\n\nSet configuration context $ kubectl config use-context k8s\n\nScale the deployment webserver to 6 pods.\n\nQuestion weight: 1%\n\n\n# 第十五题\n\nSet configuration context $ kubectl config use-context k8s\n\nCheck to see how many nodes are ready (not including nodes tainted NoSchedule) and write the number to /opt/nodenum\n\nQuestion weight: 2%\n\n\n# 第十六题\n\nSet configuration context $ kubectl config use-context k8s\n\nFrom the Pod label name=cpu-utilizer, find pods running high CPU workloads and write the name of the Pod consuming most CPU to the file /opt/cpu.txt(which already exists)\n\nQuestion weight: 2%\n\n\n# 第十七题\n\nSet configuration context $ kubectl config use-context k8s\n\nCreate a deployment as follows\n\n 1. Name: nginx-dns\n 2. Exposed via a service: nginx-dns\n 3. Ensure that the service & pod are accessible via their respective DNS records\n 4. The container(s) within any Pod(s) running as a part of this deployment should use the nginx image\n\nNext, use the utility nslookup to look up the DNS records of the service & pod and write the output to /opt/service.dns and /opt/pod.dns respectively.\n\nQuestion weight: 7%\n\n\n# 第十八题\n\nNo configuration context change required for this item\n\nCreate a snapshot of the etcd instance running at https://127.0.0.1:2379 saving the snapshot to the file path /data/backup/etcd-snapshot.db\n\nThe etcd instance is running etcd version 3.1.10\n\nThe following TLS certificates/key are supplied for connecting to the sever with ecdctl\n\n 1. CA certificate: /opt/KUCM00302/ca/crt\n 2. Client certificate: /opt/KUCM00303/etc-client.crt\n 3. Clientkey: /opt/KUCM00302/etcd-client.key\n\nQuestion weight: 7%\n\n\n# 第十九题\n\nSet configuration context $ kubectl config use-context ek8s\n\nSet the node labelled with name=ek8s-node-1 as unavailable and reschedule all the pods running on it.\n\nQuestion weight: 4%\n\n\n# 第二十题\n\nSet configuration context $ kubectl config use-context wk8s\n\nA Kubernetes worker node, labelled with name=wk8s-node-0 is in state NotReady.\n\nInvestigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made permanent.\n\nHints:\n\n 1. Yout can ssh to the failed node using $ ssh wk8s-node-0\n 2. You can assume elevated privileges on the node with the following command $ sudo -i\n\nQuestion weight: 4%\n\n\n# 第二十一题\n\nSet configuration context $ kubectl config use-context wk8s\n\nConfigure the kubelet systemd managed service, on the node labelled with name=w8s-node-1 , to launch a Pod containing a single container of image nginx named myservice automatically. Any spec files required should be placed in the /etc/kubernetes/manifests directory on the node.\n\nHints:\n\n 1. You can ssh to the failed node using $ ssh wk8s-node-1\n 2. You can assume elevated privileges on the node with the following command $ sudo -i\n\nQuestion weight: 4%\n\n\n# 第二十二题\n\nSet configuration context $ kubectl config use-context ik8s\n\nIn this task, you will configure a new Node, ik8s-node-0, to join a Kubernetes cluster as follows:\n\n 1. Configure kubelet for automatic certificate rotation and ensure that both server and client CSRS are automatically approved and signed as appropnate via the use of RBAC.\n 2. Ensure that the appropriate cluster-info ConfigMap is created and configured appropriately in the correct namespace so that future Nodes can easily join the cluster.\n 3. Your bootstrap kubeconfig should be created on the new Node at /etc/kubernetes/bootstrap-kubelet.conf(do not remove this file once your Node has successfully joined the cluster)\n 4. The appropriate cluser-wide CA certificate is located on the node at /etc/kubernetes/pki/ca.crt. You should ensure that any automatically issued certificates are installed to the node at /var/lib/kubelet/pki and that the kubeconfig file for kubelet will be rendered at /etc/kubernetes/kubelet.conf upon successful bootstrapping.\n 5. Use an additional group for bootstrapping Nodes attempting to join the cluster which should be called system:bootstrappers:cka:default-node-token\n 6. Solution should start automatically on boot, with the systemd service unit file for kubelet available at /etc/systemd/system/kubelet.service\n\nTo test your solution, create the appropriate resources from the spec file located at /opt/.../kube-flannel.yaml . This will create the necessary supporting resources as well as the kube-flannel-ds DaemonSet. You should ensure that this DaemonSet is correctly deployed to the single node in the cluster.\n\nHists:\n\n 1.  kubelet is not configured or running on ik8s-master-0 for this task, and you should not attempt to configure it.\n 2.  You will make use of TLS bootstrapping to complete this task.\n 3.  You can obtain the IP address of the Kubernetes API server via the following command $ ssh ik8s-node-0 getent hsots ik8s--master-0\n 4.  The API server is listening on the usual port, 6443/tcp, and will only server TLS requests .\n 5.  The kubelet binary is already installed on ik8s-node-0 at /usr/bin/kublet. You will not need to deploy kube-proxy to the cluster during this task.\n 6.  You can ssh to the new worker node using $ ssh ik8s-node-0\n 7.  You can ssh to the master node with the following command $ ssh ik8s-master-0\n 8.  No further configuration of control plane services running on ik8s-master-0 is required.\n 9.  You can assume elevated privileges on both nodes with the follwing command $ sudo -i\n 10. Docker is already installed and running on ik8s-node-0\n\nQuestion weight: 8%\n\n\n# 第二十三题\n\nSet configuration context $ kubectl config use-context bk8s\n\nGiven a partially-functioning Kubernetes cluster, identify symptoms of failure on the cluster. Determine the node, the failing service and take actions to bring up the failed service and restore the health of the cluster. Ensure that any changes are made permanently.\n\nThe worker node in this cluster is labelled with name=bk8s-node-0\n\nHints:\n\n 1. You can ssh to the relevant nodes using $ ssh $(NODE) where $(NODE) is one of bk8s-master-0 or bk8s-node-0\n 2. You can assume elevated privileges on any node in the cluster with the following command $ sudo -i\n\nQuestion weight: 4%\n\n\n# 第二十四题\n\nSet configuration context $ kubectl config use-context hk8s\n\nCreate a persistent volume with name app-config of capacity 1Gi and access mode ReadWriteOnce. The type of volume is hostPath and its location is /srv/app-config.\n\nQuestion weight: 3%',normalizedContent:'华为云cka认证考试知识点整理\n\n\n# 华为cka认证\n\n\n# cka认证介绍\n\ncka，也就是certificated kubernetes administrator，这是面向k8s管理员的认证项目，用于考核日常运维k8s集群所需的知识、技能，以及熟练度。\n\n关键信息：\n\n * 费用：$300(含一次补考的机会)\n * 在线远程监考、3小时上机实操、开卷(可查k8s手册)\n * 有效期是2年\n * 网络连通性、熟练度\n\n报名的链接：https://www.cncf.io/certification/cka/\n\n\n# 考纲解读\n\n可以通过https://github.com/cncf/curriculum来查看考纲最新版本。\n\n下面是一张关于涉及到的各个知识点所占的分值比例的介绍。\n\n\n\n这几大知识点，作者分别通过八次课程讲解的方式来全面讲解，也就是本次cka知识点的整理脉络所在。\n\n\n\n第一节，讲解的是基础概念。对应考纲中的是"核心概念19%"\n\n第二节，讲解的是调度管理实训。对应考纲中的是"调度5%"\n\n第三节，讲解的是日志、监控与应用管理实训。对应考纲中的是"日志/监控5%"，"应用生命周期管理8%"。\n\n第四节，讲解的是k8s网络实训。对应考纲中的"网络11%"。\n\n第五节，讲解的是k8s存储管理实训。对应考纲中的是"存储7%"。\n\n第六节，讲解的是k8s安全管理实训。对应考纲中的是"安全12%"。\n\n第七节，讲解的是k8s集群运维实训。对应考纲中的是"集群运维11%"和"安装、配置和验证12%"。\n\n第八节，讲解的是k8s故障排查实训。对应考纲中的是"排错10%"。\n\n\n# k8s架构和工作原理\n\n\n# k8s架构图如下\n\n\n\n整体来说，k8s的架构是master、node的模式，master节点上通常部署有scheduler、controller-manager、api-server，以及etcd分布式数据库。node节点上通常运行着kubelet、kube-proxy组件，以及真正运行docker实例的pod容器。\n\n用户通过kubectl，经过一些列的集群的安全认证后，将所需要执行的资源清单配置文件提交给api-server，api-server会将此信息写入etcd，写入完成后，etcd向api-server发起一系列的事件信息，通过list-watch的机制，先后由controller-manager执行副本配置，scheduler完成调度node，kubelet完成落实到各自node上启动相应的pod。而kube-proxy则用于相应的server资源转发到所关联的一系列的pod对象上。\n\n\n# k8s工作原理\n\n如下的图示是完整的工作原理的解析，其中核心的重点概念就是list-watch的机制。\n\n\n\n上面所有的"0"示意图，都是表示的是从k8s环境部署完毕后，各个api-server的客户端(api-server、scheduler、kubelet)都对于各个所订阅的资源对象，进行watch。\n\nlist-watch的异步消息通信机制，有效的保证了消息的可靠性、及时性、顺序性、高性能等。\n\n\n# 理解k8s api原语\n\n\n# k8s中的基本概念\n\npod：是一组功能相关的container的封装；共享存储和network namespace；是k8s调度和作业运行的基本单位(scheduler调度，kubelet运行)；pod容易"走失"，需要workload和service的保障。\n\nworkloads: 有deployment，statefulset，daemonset，job ....，实际上是一组功能相关的pod的封装。\n\nservice: 主要是为了让pod "防失联"，给一组pod设置反向代理。\n\n\n# k8s中api对象的基本构成\n\n整体来说，在k8s中一个完整的api对象由四大部分组成。\n\ntypemeta: 主要由apiversion和kind两个构成。主要描述的是该类api资源对象的类别和版本。\n\nobjectmeta: metadata类别，涵盖该类型资源对象下的基本元数据的信息，包含一些名称、label、注解等。\n\nspec: 这是关键的信息内容，涵盖定义的所有的期望状态信息。\n\nstatus: 这个不是客户端所需要配置的，反映的是当前该类资源对象的实际的状态值。\n\n\n\n\n# 使用kubectl\n\n\n# 基本命令\n\ncreate: 从文件或stdin创建资源。\n\nexpose: 为deployment，pod创建service。\n\nrun: run a particular image on the cluster.\n\nset: set sepcific features on objects.\n\nget: 最基本的查询命令。如kubectl get rs，kubectl get deploy，kubectl get svc，kubectl get rs。\n\nexplain: 查看资源定义。如kubectl explain replicaset。\n\nedit: 使用系统编辑器编辑资源。如 kubectl edit deploy。使用系统编辑器来加载yaml文件，当get查看一个yaml文件的时候很可能会翻过头，使用edit就可以自由多了。\n\ndelete: 删除指定资源，支持文件名、资源名、label selector。如kubectl delete po -l foo=bar。\n\n\n# 部署命令\n\nrollout: deployment，daemonset的升级过程管理(查看状态、操作历史、暂停升级、恢复升级、回滚等)\n\nrolling-update: 客户端滚动升级，仅限replicationcontroller\n\nscale: 修改deployment，replicaset，replicationcontroller，job的实例数。\n\nautoscale: 为deploy，rs，rc配置自动伸缩规则(依赖heapster和hpa)\n\n\n# 集群管理命令\n\ncertificate: modify certificate resources.\n\ncluster-info: 查看集群信息。\n\ntop: 查看资源占用率。\n\ncordon: 标记节点为unschedulable。\n\nuncordon: 标记节点为schedulable。\n\ndrain: 驱逐节点上的而应用，准备下线维护。\n\ntaint: 修改节点taint标记\n\n\n# troubleshooting and debugging命令\n\ndescribe: 查看资源详情。\n\nlogs: 查看pod内容器的日志。\n\nattach: attach到pod内的一个容器。\n\nexec: 在指定容器内执行命令。\n\nport-forward: 为pod创建本地端口映射。\n\nproxy: 为kubernetes api server创建代理。\n\ncp: 容器内外/容器间文件拷贝。\n\n\n# 高级命令\n\napply: 从文件或stdin创建/更新资源。\n\npatch: 使用strategic merge patch语法更新对象的某些字段。\n\nreplace: 从文件或stdin更新资源。\n\nconvert: 在不同api版本之间转换对象定义。\n\n\n# setting命令\n\nlabel: 给资源设置label。\n\nannotate: 给资源设置annotation。\n\ncompletion: 获取shell自动补全脚本(支持bash和zsh)。\n\n\n# 其他命令\n\napi-versions: 打印该集群支持的api的版本，在"group/version"中。\n\nconfig: 修改kubectl配置(kubeconfig文件)，如context。\n\nhelp: 帮助\n\nversion: 查看客户端和server端k8s版本\n\n\n# kubectl实用技巧\n\n当kubectl命令太多太长记不住该怎么办？\n\n * 查看资源缩写\n   \n   $ kubectl api-resources\n   \n   \n   1\n   \n\n * 配置kubectl 自动完成\n   \n   $ source <(kubectl completion bash)\n   \n   \n   1\n   \n\n当kubectl写yaml文件太累了，找样例太麻烦？\n\n * 用run 命令生成deployment的yaml配置文件\n   \n   $ kubectl run --image=nginx my-deploy -o yaml --dry-run > my-deploy.yaml\n   \n   \n   1\n   \n\n * 用get 命令导出\n   \n   $ kubectl get statefulset/foo -o=yaml --export > new.yaml\n   \n   \n   1\n   \n\n * 用explain命令\n   \n   $ kubectl explain xxxx.xxxx\n   \n   \n   1\n   \n\n\n# k8s调度管理\n\n\n# 大纲\n\n整体从考试的大纲来看，有如下的考点要求。\n\n * 理解资源限制对pod调度的影响\n * 使用label selector调度pod\n * 手动调度pod\n * 理解daemonset\n * 调度失败原因分析\n * 使用多调度器\n * 了解调度器的配置\n\n好像大致来看，也就是对k8s中scheduler调度的理解，分为资源调度、label selector调度、手动调度、高级调度等相关内容。\n\n\n# k8s调度相关基础概念\n\n调度器scheduler就是从现有的node中选择一个，来运行所需要的pod。\n\n从下面的图示可以看出，pod m/n/j通过k8s的scheduler调度以后，分别为pod m指定了节点y，pod n指定了节点z，pod l指定了节点x。从pod的yaml清单文件中也可以观察掉，调度前nodename选项的value是空，调度后nodename选项的value是相应的节点的名称了。\n\n\n\n\n# node定义\n\n既然我们了解到scheduler的目的就是为pod指向一个node，那么就必须先了解下node的定义和pod的定义。\n\n\n\n通过执行kubectl get node/xxxx -o yaml,其中"allocatable"表示的是一个node可以分配的资源量，作为一个节点来接收pod资源的依据。而"capacity"表示的是一个节点固有的资源量，一般来说capacity要大于等于allocatable的资源量。毕竟node除了给pod用，还有预留一些资源给其他。\n\n\n# pod中主要属性字段\n\n我们同样要了解pod的具体资源配置文件的定义，毕竟资源的调度是由pod的需求来发起，通过schduler来调度node来满足。\n\n可以通过kubectl explain pod.spec来查看到相关的字段信息，或者和前面一样通过kubectl get po/xxxx -o yaml来查看一个既有的pod的配置信息。\n\n\n\n其中pod.spec.containers.resources字段中"requests"字段表示的是该pod提出的要求就是这个node最少就得满足这么多资源，可能真实运行情况下，还用不到这么多的资源，可能要比这个少，也可可能比这个多，但是在pod用到时，必须提供这么多的资源。"requests"可以理解为请求量，是schedule资源调度的依据。\n\n而"limits"的意思是，就是限制该pod可以用到的最大值，实际在node上的资源使用时，node把这个pod的资源给限制死了。主要是防止pod自身的bug把node上其他资源给拖累了。\n\n"schedulername"字段显示的是该pod用的是哪种调度器管理的，默认就是k8s自带的"default-scheduler"。\n\n"nodename"字段之前介绍过了，显示的value值就是调度后的结果。\n\n接下来的"nodeselector"/"affinity"/"tolerations"就是高级调度的策略用法了。\n\n\n# k8s中的资源分配\n\nk8s中默认的调度器的核心目标就是基于资源可用性将各pod资源公平地分布于集群节点之上。目前默认的调度器通过三个步骤来完成调度操作：节点预选(predicate)、节点优先级排序(priority)以及节点择优(select)。\n\n> 下面有些调度内容，摘录自网上https://blog.csdn.net/qq_34857250/article/details/90259693\n> \n> https://www.cnblogs.com/l-dongf/p/12327401.html\n\n\n# 调度策略\n\n * 预选策略，predicate是强制性规则，会遍历所有的node节点，依据具体的预选策略筛选出符合要求的node列表，如果没有node符合predicates策略规则，那么pod就会被挂起，直到有node能够满足。\n * 优选策略，这一步会在第一步筛选的基础上，按照优选策略为待选node打分排序，获取最优者。\n\n\n# 预选策略-必选全部满足\n\n * checknodeconditon: 检查node是否正常。\n * generalpredicates: 普通判断策略\n   * hostname: 检测pod对象是否定义了pod.spec.hostname，并且检查节点中是否有同名的pod而冲突。\n   * podfithostports: 检查pod.spec.containers.ports.hostport属性(绑定节点上的某个端口)是否定义，并且检查节点中的节点端口是否冲突。\n   * matchnodeselector: pods.spec.nodeselector，检查节点选择器。\n   * podfitsresources: 检查pod的资源需求request是否能被节点所满足。\n * nodiskconflict: 检测pod依赖的存储卷是否能满足需求，默认不检查。\n * podtoleratesnodetaints: pods.spec.tolerations可容忍的污点，检查pod是否能容忍节点上的污点。\n * podtoleratesnodeexecutetanits: pod.tolerations属性中是否能接纳容忍noexecute级别的污点，默认没有启用。\n * checknodelablepresence: 检测node上的标签的存在与否，默认没有启用。\n * checkserviceaffinity: 根据pod所属的service，将相同所属的service尽可能放在同一个节点，默认不检查。\n * checkvolumebinding: 检查节点上已绑定和未绑定的pvc是否能够满足pod对象的存储卷需求。\n * novolumezoneconflict: 如果给定了区域限制，检查在此节点上部署pod对象是否存在存储卷冲突。\n * checknodememorypressure: 检测节点内存是否存在压力，如果节点内存压力过大，则检查当前pod是否可以调度此节点上。\n * checknodepidpressure: 检查节点pid数量是否存在压力。\n * checknodediskpressure: 检查节点磁盘资源的压力情况。\n * matchinterpodaffinity: 检查给定节点是否能够满足pod对象的亲和性或反亲和性条件，以用于实现pod亲和性调度或反亲和性调度。\n\n\n# 优选策略\n\n优选过程中，调度器向每个通过预选的节点传递一系列的优选函数，来计算每个节点上各个优选函数后得到的值，调度器会给每个优选函数设定一个权重，大多数优先级默认为1。将所有优选函数得分乘以权重，然后相加从而得出节点的最终优先级分值。finasorenode=(weight1*priorityfunc1)+(weight2*priorityfunc2)+ ...\n\n下面是各个优选函数的相关说明：\n\n * leastrequested: 节点的资源空闲率高的优选，是节点空闲资源与节点总容量的比值计算而来的。即由cpu或内存资源的总容量减去节点上已有pod对象需求的容量总和，再减去当前要创建的pod对象的需求容量的结果除以总容量。计算公式是：(cpu((capacity-sum(requested))*10 / capacity)+memory((capacity-sum(requested))*10 / capacity)) / 2\n * balancedresourceallocation: 计算节点上面的cpu和内存资源被占用的比率相近程度，越接近，比分越高，平衡节点的资源使用情况。计算公式：cpu=cpu((capacity-sum(requested))*10 / capacity) mem=memory((capacity-sum(requested))*10 / capacity)\n * nodepreferavoidpodspriority: 如果node上不存在"scheduler.alpha.kubernetes.io/preferavoidpods"这个注解，那么不管什么pod都没有影响；如果node上存在相关的注解，那么注解中关联的pod对象名称正好是要去调度的pod，那么此类node分值会很低，如果关联的pod对象名称和要调度的pod名称没有任何关系，那么和没有注解是一样的效果。需要注意的是，在这个优先级中，优先级最高，得分会非常高。\n * nodeaffinitypriority: 节点的亲和性，亲和性高，得分高。基于节点亲和性调度偏好进行的优选级评估，根据pod资源中的nodeselector对给定节点进行匹配度检查，成功匹配到的条目越多则节点得分越高。\n * tainttolerationpriority: 将pod对象的spec.tolertions与节点的taints列表项进行匹配度检测，匹配的条目越多，得分越低。\n * selectorspreading: 尽可能的把pod分散开，也就是没有启动这个pod的node，得分会越高。\n * interpodaffinitypriority: 遍历pod对象的亲和性条目，匹配项越多，得分就越多。\n * mostrequestedpriority: 节点中空限量越少的，得分越高，与leastrequested不能同时使用，集中各一个机器上面跑pod，默认没有启用。\n * nodelabelpriority: 根据node上面是否拥有特定的标签来评估得分，有标签就有分，而无论其值为何。默认没有启用。\n * imagelocalitypriority: 一个node的得分高低，是根据node上面是否有镜像，有镜像就有得分，反之就没有(根据node上已有满足需求的image的size大小之和来计算)，默认没有启用。\n\n\n# 调度分配机制的要点\n\n * 基于pod中容器request资源"总和"调度\n   \n   * resources.limits影响pod的运行资源上限，但是不会影响调度。\n   * initcontainer取最大值，container取累加值，最后取大值。即max(max(initcontainers.requests), sum(containers.requests))\n   * 未指定request资源时，按0资源需求进行调度，也就是任何node都满足该pod资源。\n\n * 基于资源声明量的调度，而非实际占用\n   \n   * 不依赖监控，系统不会过于敏感\n   * 能否调度成功：能否调度成功：pod.request < node.allocatable - node.requested\n\n * k8s node的资源盒子模型\n   \n   也就是说一个node的资源，要满足于很多种类型的需求，不仅仅是提供给pod的资源。\n   \n   \n\n * 重要的一些资源分配相关算法\n   \n   * generalpredicated(主要是podfitsreources)\n   * leastrequesterpriority\n   * balancedresourceallocation，平衡cpu/mem的消耗比例\n\n\n# pod所需资源的计算\n\n上面已经说到了，关于resource.requests是资源调度的依据。在计算一个pod所需要的资源的时候，同时考虑pod中的initcontainers的资源量和containers的资源量，其中initcontainers的资源量取单个容器的最大值，而containers的资源量去所有容量的累加值。然后这两个值比较取最大值。\n\n\n\n\n# k8s中的高级调度及用法\n\n\n# nodeselector\n\n将pod调度到特定的node上\n\n\n\n首选要在node上定义相应的labels，然后在pod.spec.nodeselector中定义需要调度到的相应标签的node。类似于rdbms中通过select node.name from xxx where node.disktype=ssd and node-flavor=s3.large.2 ，也就是说pod中nodeselector要完全匹配node上的标签。\n\n这个匹配调度的逻辑，是之前描述的matchnodeselector预选调度算法，预选调度算法是必须要满足项的node。\n\n\n# nodeaffinity\n\n节点亲和性，是调度程序用来确定pod对象调度到哪个node上的一组规则，和nodeselecotr一样，也是基于节点上的自定义标签和pod对象上指定的标签选择器来进行定义的，这是nodeselector的升级版本。\n\nnodeaffinity属于优选函数算法中一种。\n\n总体来说，节点亲和性调度可以分为硬亲和(required)和软亲和(preferred)。顾名思义，硬亲和性实现的是强制性规则，它是pod调度时必须要满足的规则，如果不满足，则pod对象会被置于pending状态；而软亲和则是一种柔性调度限制，它倾向于将pod对象运行于某类特定的节点之上，而调度器也将尽量满足此需求，但是在无法满足调度需求的时候，它将退而求其次地选择一个不匹配规则的节点。\n\n\n\n# 硬亲和\n\n硬亲和的策略在pod中位置于pod.spec.affinity.nodeaffinity.requireduringschedulingignoreduringexecution。"ignoreduringexecution"表明了，该调度策略只会对当前的pod和node标签，以及相应规则，做个调度匹配。以后要是节点标签发生了变化了，那么已经调度到了该node上的pod对象不会做出改变，只会对新建的pod对象生效。老人老办法，新人新规定。\n\n在上述map的中可以包含多个value(nodeselectorterms字段)，多个nodeselecorterm之间是"逻辑或"的关系，意思是有节点满足其中的一个nodeselecorterm就算这个node满足了。\n\nnodeselectorterms下面需要可以定义多个matchexpression，多个规则彼此之间为"逻辑与"的关系，也就是说只有该nodeselecorterm下面的所有matchexpression定义的规则都满足，这个node才算是满足了。\n\n标签选择器表达式(matchexpression下定义的规则)，支持使用的操作符号有in、notin、exsits、doesnotexists、lt和gt等。in表示的是只要满足后面的集合中的一个就算是满足了条件。\n\n# 软亲和\n\n软亲和的策略在pod中位置于pod.spec.affinity.nodeaffinity.preferredduringschedulingignoreduringexecution。同理是"ignoreduringexecution"。\n\n在上述的map中可以包含多个规则，每个规则都可以配置weight属性以便用于定义其优先级。对于相关的节点分别计算出多个规则的权重值，最后分值高的节点胜出。\n\n\n# podaffinity\n\npod亲和性，顾名思义，调度和判断的主体是之前已经存在的pod，而非上面所说的node。是根据已调度或将要调度的pod的所位于的node的情况，来决定后续的pod将要部署在哪些node上，反映的是后一种pod对已存在pod的一种亲和性的关系的，调度管理。\n\npodaffinity也分为亲和性和反亲和性，每种亲和策略下又分为硬(required)亲和、软(preferred)亲和。\n\nk8s调度器通过内建的matchinterpodaffinity预选策略为这种调度方式完成节点预选，并基于interpodaffinitypriority优选函数进行各节点的优选级评估。\n\n# 为什么要有pod亲和性\n\n出于高效通信的需要，偶尔需要把一些pod对象组织在相近的位置(同一节点、机架、区域或地区等)，如某业务的前端pod和后端pod(表现为pod亲和性)。或者说要出于安全性或分布式的原因，需要将一些pod对象在其运行的位置上隔离开来(表现为pod反亲和性)。\n\n# 什么是位置拓扑topologykey\n\npod亲和性调度需要各相关的pod对象运行于"同一位置"，而topologykey就恰恰定义了这个一个什么样的类别，比如区域的类别、机架的类别、主机的类别等等。\n\n在定义pod对象的亲和性与反亲和性时，需要借助于标签选择器来选择被依赖的pod对象，并根据选出的pod对象所在节点的标签来判断"同一位置"的 具体意义。\n\n# pod亲和性和node亲和性有什么区别\n\n * 在pod.sepc.affinity存在podaffinity和podantiaffinity，这两种配置都是对称的。\n * pod亲和性调度中labelselector的匹配对象是pod，而node亲和性调度中匹配的是node。\n * pod亲和性调度中匹配到的是根据topologykey定义的一组node，topologykey定义了分组是什么样的一个级别，相同topologykey中的key和value的值为一组。\n * 在pod亲和性中硬亲和过滤规则中，条件间只有逻辑与运算。\n\n\n\n# pod硬亲和调度\n\npod的硬亲和调度的api定义于pod.spec.affinity.podaffinity.requiredduringschedulingignoredduringexecution下，通过labelselector 这个map定义多个匹配表达式。条件间只有逻辑与的运算，这一点和node亲和性有所不同。\n\ntopologykey: kubernetes.io/hostname是k8s中节点的内建标签，它的值就是当前节点的节点主机名称。同理还有region(地区)、zone(区域)、rack(机架)的拓扑位置的定义。\n\n# pod软亲和调度\n\npod的软亲和调度的api定义于pod.spec.affinity.podaffinity.preferredduringschedulingignoredduringexecution下，和node软亲和性调度类型，可以定义多个调度规则，每个规则都可以定义一个权重。\n\n# podantiaffinity反亲和性\n\n与podaffinity匹配过程相同，只是最终的结果取反。\n\n\n\n\n# 手动调度和daemonset\n\n当遇到调度器不工作时，需要考虑到手动调度pod。\n\n我们只需要在pod.spec.nodename中直接填上需要调度到的node的名称就可以了。\n\n\n\n# daemonset调度\n\n老的版本中daemonset中的pod调度都是由controller-manager直接指定pod的运行节点，不经过调度器。直到1.11版本开始，daemonset的pod才由scheduler引入调度。\n\ndaemonset实际上是要求每个节点都部署一个相同的pod，通常用于部署集群中的agent，例如网络插件等。在下图的daemonset的配置中，可以看出类似于定义了一个deployment的清单配置文件中，对于要求一个主机上不存在相同的pod label，也就是对于pod的反亲和性。\n\n\n\n\n# 污点和容忍度\n\n污点(taints)是定义在节点之上的键值型属性数据，用于让节点拒绝将pod调度运行于其上，除非该pod对象具有容纳节点污点的容忍度。\n\n而容忍度(tolerations)是定义在pod对象上的键值型属性数据，用于配置其可容忍的节点污点，而且调度器仅能将pod对象调度至其能够容忍该节点污点的节点之上。\n\nk8s中使用podtoleratesnodetaints预选策略和tainttolerationpriority优选函数来完成此类高级调度机制。\n\n# 污点容忍度/节点选择器/节点亲和性区别\n\n上面描述的节点选择器(nodeselector)和节点亲和性两种调度方式都是通过在pod对象上添加标签选择器来完成对特定类型节点标签的匹配，实现的是由pod选择节点的机制。\n\n而污点和容忍度则是通过向节点添加污点信息来控制pod对象的调度结果，从而赋予了节点控制何种pod对象能够调度与其上的主控权。\n\n节点亲和性是使得pod对象被吸引到一类特定的节点，而污点则相反，它提供了让节点排斥特定pod对象的能力。\n\n# 污点\n\n污点(taint)的定义在 node.spec.taints下，是键值型数据，但又额外支持一个效果(effect)标识，语法格式为"key=value:effect"，其中key和value的用法及格式与资源注解信息相似，而effect则用于定义对pod对象的排斥等级，它主要包含以下三种类型。\n\n * noschedule: 不能容忍此污点的新pod对象，不可调度至当前节点，属于强制型约束关系，节点上现存的pod对象不受影响。\n * prefernoschedule: noschedule的柔性约束版本，即不能容忍此污点的新pod对象尽量不要调度至当前节点，不过无其他节点可供调度时也允许接受相应的pod对象。节点上现存的pod对象不受影响。\n * noexecute: 不能容忍此污点的新pod对象，不可调度至当前节点，属于强制性约束关系，而且节点上现存的pod对象会因节点污点变动或pod容忍度变动而不再满足匹配规则时，pod对象将被驱逐。\n\n\n\n给节点添加污点标识：\n\n$ kubectl taint nodes xxx <key>=<value>:<effect>\n例如：\n$ kubectl taint nodes node1 node-type=production:noschedule\n\n\n1\n2\n3\n\n\n即便是同一个键值数据，若其效用标识不同，则也分属于不同的污点信息，也就是说会增加一条污点信息，和之前的区别只是在于效用标识不同。\n\n$  kubectl taint nodes node1 node-type=production:prefernoschedule\n\n\n1\n\n\n删除节点上某特定键名，特定标识的污点信息。\n\n$ kubectl taint nodes node1 node-type=noschedule-\n\n\n1\n\n\n删除节点上某特定键名的所有污点信息。(也就是省略效用标识)\n\n$ kubectl taint nodes node1 node-type-\n\n\n1\n\n\n删除节点上所有的全部的污点信息，可以使用kubectl patch命令将节点属性spec.taints的值直接置为空即可。\n\n$ kubectl patch nodes node1 -p \'{"spec":{"taints":{}}}\'\n\n\n1\n\n\n# 容忍度\n\npod对象的容忍度可通过其pod.spec.tolerations字段进行添加，根据使用的操作符不同，主要有两种不同的形式：一种是与污点信息完全匹配的等值关系"equal"；另一种是判断污点信息存在性的匹配方式"exists"。其中tolerationseconds用于定义延迟驱逐当前pod对象的时长。\n\n\n\n需要注意的如下信息：\n\n * 如果节点node上有多个污点信息，那么就必须该pod对此节点上的所有污点信息都能容忍，才能调度上去。\n * 匹配逻辑和之前的pod中的nodeselector正好相反，之前的逻辑是只要是node上的一个标签满足于pod中定义的node selector就进行匹配。\n * pod中定义的operator为"equal"时，就是需要在pod的toerations中完整填写所有的key、value、effect。\n * pod中定义的operator为"exists"时(key/effect/operator项是必填的，而value项留空)，可以填写value为空，表示的是匹配容忍节点node中所有关于这个key中的相应的"effect"的，所有value的污点的信息。\n * pod中定义的operator为"exists"时(key/operator项是必填，value和effect为空)，表示的是匹配容忍节点node中所有关于这个key值与之相同的，所有value的所有effect的污点信息。\n\n# 污点和容忍度的调度逻辑\n\n一个节点可以配置使用多个污点，一个pod对象也可以有多个容忍度，不过二者在进行逻辑检查时会遵循如下逻辑。\n\n 1. 首先处理每个有着与之匹配的容忍度的污点。\n\n 2. 不能匹配到的污点上，如果存在一个污点使用noschedule效用标识，则拒绝调度pod对象至此节点上。\n\n 3. 不能匹配到的污点上，若没有任何一个使用noschedule效用标识，但至少有一个使用了prefernoscheduler，则应尽量避免将pod对象调度至此节点。\n\n 4. 如果至少有一个不匹配的污点使用了noexecute效用标识，则节点将立即驱逐pod对象，或者不予调度至给给定节点；另外，即便容忍度可以匹配到使用了noexecute效用标识的污点，若在定义容忍度时还同时使用了tolerationseconds属性定义了容忍时限，则超出时限后其也将被节点驱逐。\n\n\n# 调度结果和失败原因分析\n\n * 查看调度结果\n   \n   $ kubectl get pod xxx -o wide\n   \n   \n   1\n   \n\n * 查看调度失败原因\n   \n   $ kubectl describe pod xxxx\n   \n   \n   1\n   \n\n * 调度失败错误列表(可以选择相应的版本)\n   \n   https://github.com/kubernetes/kubernetes/blob/release-1.17/pkg/scheduler/algorithm/predicates/error.go\n\n\n# 多调度器及调度器配置\n\n如果集群中已经启动了另外的一个调度器，那我们可以在pod对象的清单配置文件中，指定相应的"schedulername"的值。如果集群中存在多个调度器，那么建议将node节点划分资源池来管理，避免多个调度器管理混乱。\n\n\n\n我们可以在新配置的调取器中加载个性化的配置文件，来修改调整我们想要的排序算法。\n\n\n\n\n# k8s日志/监控/应用管理\n\n\n# 监控集群组件\n\n 1. 查看集群的整体状态\n    \n    $ kubectl cluster-info\n    \n    \n    1\n    \n\n 2. 查看更多更全的信息\n    \n    该信息下会收集所有的已有的pod等资源对象的信息，event信息，调度信息等。\n    \n    $ kubectl cluster-info dump > xxx.txt\n    \n    \n    1\n    \n\n 3. 查看master组件(插件部署的)\n    \n    $ kubectl get pod etcd -n kube-system\n    $ kubectl describe pod kube-apiserver -n kube-system\n    \n    \n    1\n    2\n    \n\n 4. 查看组件metrics\n    \n    $ curl localhost:10250/stats/summary\n    \n    \n    1\n    \n\n 5. 查看组件的健康状况\n    \n    $ curl localhost:10250/healthz\n    \n    \n    1\n    \n\n 6. heapster + cadvisor监控集群组件\n    \n    早期的版本中，k8s官方提供了heapster、influxdb和grafana的组合来对系统进行，heapster以pod的形式运行于集群中，已被metrics-server替代。\n    \n    cadvistor既能收集容器cpu、内存、文件系统和网络使用统计信息，还能采集节点资源使用情况。使用下面的命令，展示node cpu/内存/存储资源消耗\n    \n    $ kubectl top node {node name}\n    \n    \n    1\n    \n\n\n# 监控应用\n\n 1. 监控应用pod详细信息\n    \n    $ kubectl describe pod\n    \n    \n    1\n    \n\n 2. 展示pod cpu/内存/存储资源消耗\n    \n    对接了heapster或metrics-server后，可以展示这些数据。相比于在pod内输入top看到的数据可能有所不同，一方面如果pod内没有做资源隔离的化，看到的是整个节点的资源情况；另一方便这个命令能看到一个区间内的资源消耗情况，而不是只能实时显示当时的情况。\n    \n    $ kubectl top pod {pod name}\n    \n    \n    1\n    \n\n 3. 持续性的观察k8s中资源的变化情况\n    \n    $ kubectl get pod {pod name} --watch\n    \n    \n    1\n    \n\n\n# 管理组件日志\n\n 1. 当位于特定的目录下的组件日志查看：\n    \n    /var/log/kube-apiserver.log\n    /var/log/kube-proxy.log\n    /var/log/kube-controller-manager.log\n    /var/log/kubelet.log\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 使用systemd管理的时候\n    \n    $ journalctl -u kubelet\n    \n    \n    1\n    \n\n 3. 使用k8s插件部署的时候\n    \n    $ kubectl logs -f kube-proxy\n    \n    \n    1\n    \n\n\n# 管理应用日志\n\n 1. 从k8s的容器标准输出中截取\n    \n    $ kubectl logs -f {pod name} -c {container name}\n    \n    \n    1\n    \n\n 2. 从docker容器标准输出中截取\n    \n    $ docker logs -f {docker name}\n    \n    \n    1\n    \n\n 3. 进入到容器内查看\n    \n    将日志文件挂载到主机目录中:\n    \n    apiversion: v1\n    kind: pod\n    metadata:\n      name: test-pd\n    spec:\n      containers:\n      - image: xxxxxx/xxxx\n        name: test-container\n        volumemounts:\n        - mountpath: /log\n          name: log-volume\n        volumes:\n        - name: log-volume\n          hostpath:\n            # 主机上面的目录\n            path: /var/k8s/log\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n    \n    这样可以直接进入容器内来查看日志\n    \n    $ kubectl exec -it {pod} -c {container} /bin/sh\n    $ docker exec -it {container} /bin/sh\n    \n    \n    1\n    2\n    \n\n\n# deployment升级和回滚\n\n\n# 创建deployment\n\n$ kubectl run {deployment_name} --image={image} --replicas={rep.}\n$ kubectl run --image=nginx my-deploy -o yaml --dry-run > my-deploy.yaml\n\n\n1\n2\n\n\n或者使用yaml文件形式，重点配置replicas和image字段\n\n\n# 升级deployment\n\n升级deployment的镜像文件\n\n$ kubectl set image deploy/nginx   nginx=nginx:1.9.1\n\n\n1\n\n\n修改deployment的资源限制\n\n$ kubectl set resources deploy/nginx -c=nginx --limits=cpu=200m,memory=512mi\n\n\n1\n\n\n\n# 升级策略\n\n修改deployment中的升级策略\n\nmaxsurge: 是指升级期间存在的总pod对象数量最多可以超出期望值的个数，如果spec是3，而maxsurge为1，那么则表示pod对象的总数不能超过4个。\n\nminreadyseconds: 5\nstrategy:\n  type: rollingupdate\n  rollingupdate:\n    maxsurge: 1 # 默认25%\n    maxunavailable: 1 #默认25%\n\n\n1\n2\n3\n4\n5\n6\n\n\nmaxunavailable: 升级期间正常可用的pod副本数(包括新的旧的版本)最多不能低于期望值的个数。默认值为1，如果期望值spec是3，那么升级期间至少要有2个pod对象处于正常提供服务的状态。\n\n\n# 暂停deployment\n\n通过以下的命令来暂停deployment，暂停deployment，意思不是说我们把deployment冻住了，而是暂停这个deploy之后，所对这个deploy做的改动不会记录到升级日志版本中去了。\n\n$ kubectl roullout pause deployment/nginx-deployment\n\n\n1\n\n\n\n# 恢复deployment\n\n当已经处于暂停状态下的deploy，可以通过如下命令来恢复。比如说现在我们想把处于暂停状态下的deploy修改另外一个image版本，但是是想把这个记录到升级日志中去。那么可以先把这个暂停了的deploy做个恢复，随后再做修改。\n\n$ kubectl rollout resume deployment/nginx-deployment\n\n\n1\n\n\n\n# 查询升级状态\n\n通过如下的命令来查看deploy对象的升级状态\n\n$ kubectl rollout status deployment/nginx-deployment\n\n\n1\n\n\n\n# 查看升级历史\n\n查看所有的版本\n\n$ kubectl rollout history deploy/nginx-deployment\n\n\n1\n\n\n查看指定的版本\n\n$ kubectl rollout history deploy/nginx-deployment --revision=2\n\n\n1\n\n\n\n# 回滚的命令\n\n回滚到指定的版本上\n\n$ kubectl rollout undo deployment/nginx-deployment --t0-revision=2\n\n\n1\n\n\n\n# 应用弹性伸缩\n\n直接通过scale命令来弹性伸缩副本数。\n\n$ kubectl scale deployment nginx-deployment --replicas=10\n\n\n1\n\n\n当对接了heapster，和hpa联动后。\n\n$ kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-persent=80\n\n\n1\n\n\n\n# 应用自恢复\n\n这里配置应用pod自恢复策略，通过restartpolicy(重启策略)和livenessprobe(存活性探针)来做的。\n\npod restart policy有：always、onfailure、never。\n\nlivessprobe: http/https get、shell exec、topsocket。\n\n下面是一个tcp socket的liveness探针+always restart例子。\n\napiversion: v1\nkind: pod\nmetadata:\n  name: goproxy\nspec:\n  restartpolicy: always\n  containers:\n  - name: goproxy\n    image: k8s.gcr.io/goproxy:0.1\n    ports:\n    - containerport: 8080\n    livenessprobe:\n      tcpsocket:\n        port:8080\n      initialdelayseconds: 15\n      periodseconds: 20\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# k8s中的安全\n\n在k8s中安全方面，主要可以分为两大类：部署态的安全控制和运行态的安全控制。\n\n部署态的安全控制主要体现在：当"人为"(常规账户) 或pod对象(service account)通过 kubectl 、客户端库或直接使用rest接口进行请求时，需要通过认证、鉴权、admission(准入控制)。以及正常我们所配置的pod的安全上下文中，用于限定pod启动的用户、组、以及特权用户等。\n\n运行态的安全控制主要体现在：network policy上，一般是指namespaces或pod对象之间的网络访问控制。\n\n\n# 基本概念罗列\n\n# 什么是user account(用户账户)\n\n是k8s的客户端，此类客户端或通过kubectl、客户端库或rest接口进行请求通信时，对应的是现实中的"人"。\n\nuser account一般是指由独立于k8s之外的其他服务管理的用户账号，k8s中不存在此类用户账号的对象。\n\nuser account通常用于复杂的业务逻辑管控，它作用于系统全局，所以名称必须是全局唯一。\n\n# 什么是service account(服务账户)\n\nservice account也是k8s的客户端的类型，是指由k8s api管理的账号，用于为pod之中的服务进程访问k8s api时提供的身份标识。\n\nservice account属于namespace scope，它们由api server创建，或通过api调用手动创建，附带着一组存储为secret的用于访问api server的凭据。\n\n# 用户组的概念\n\nuser account和service account都可以隶属于一个或多个用户组。用户组只是用户账号的逻辑集合，它本身并没有操作权限，但是附加于组上的权限(这个用户组所拥有的权限)，可以被其内部的所有用户继承。\n\nk8s有着如下几个内建的用于特殊目的的组：\n\n * system:unauthenticated: 未能通过任何一个插件检验的账号，即未通过认证测试的用户所属的组。\n * system:authenticated: 认证成功后的用户自动加入的一个组，用于快捷引用所有正常通过认证的用户账号。\n * system:serviceaccounts: 当前系统上的所有service account对象。\n * system:serviceaccounts: <namespace>: 特定名称空间内所有的service account对象。\n\n# 什么是authentication(认证)\n\napi server处理请求的过程中，认证插件负责鉴定用户身份。\n\nk8s中支持的认证方式包括客户端证书、承载令牌(bearer tokens)、身份验证打理(authenticating proxy)或http basic认证。认证的过程中会以串行的方式进行，直到一种认证机制成功完成即结束。\n\napi server接收到访问请求时，它将调用认证插件尝试将以下属性与访问请求相关联。\n\n * username: 用户名，如kubenetes-admin等。\n * uid: 用户的数字标签符，用于确保用户身份的唯一性。\n * groups: 用户所属的组，用于权限指派和继承。\n * extra: 键值数据类型的字符串，用于提供认证时需要用到的额外信息。\n\napi server支持以下几种具体的认证方式。\n\n 1. x509客户端证书认证：\n\n客户端在请求报文中携带x509格式的数字证书用于认证，认证通过后，证书中的主体标识(subject)将被识别为用户标识，其中的cn(common name)字段是用户名，o(organization)是用户所属的组，如"/cn=ilinux/o=opmasters/o=admin"中，用户名为ilinux，其属于opmasters和admin两个组。\n\n 2. 静态令牌文件(static token file):\n\n即保存着令牌信息的文件，由kube-apiserver的命令行选项--token-auth-file加载，且服务器启动后不可更改；http客户端也能使用承载令牌进行身份验证，将令牌进行编码后，通过请求报文中的authorization首部承载传递给api server即可。\n\n 3. 引导令牌(bootstrap tokens):\n\n一种动态管理承载令牌进行身份认证的方式，常用于简化新建k8s集群的节点认证过程，需要通过--experimental-bootstrap-token-auth选项启用；有新的工作节点首次加入时，master使用引导令牌确认节点身份的合法性之后自动为其签署数字证书以用于后续的安全通信，使用kubeadm join命令将节点加入kubeadm初始化的集群时使用的就是这种认证方式；这些令牌作为secrets存储在kube-system命名空间中时，可以动态管理和创建它们，而controller manager 包含一个tokencleaner控制器，用于删除过期的引导令牌。\n\n# 什么是authorization(授权)\n\n授权插件用于操作权限许可鉴别，最常用的就是rbac(基于角色的访问控制)。\n\napi server是一种rest api，除了身份认证信息之外，一个请求报文还需要提供操作方法及其目标对象。如针对某pod资源对象进行的创建、查看、修改或删除操作等。\n\n为了核验用户的操作许可，成功通过身份认证后的操作请求还需要转交给授权插件进行许可权限检查，以确保其拥有执行相应的操作的许可。这里主要介绍的就是rbac的授权插件。\n\n# 什么是rbac\n\n简而言之，就是k8s将权限授予"角色"(role)之上，有别于传统访问控制机制中将权限直接赋予使用者的方式。\n\n用户(user)就是一个可以独立访问计算机系统中的数据或用数据表示的其他资源的主体(subject)。\n\n角色是一个组织或任务中的工作或位置，代表了一种权利、资格和责任。\n\n许可(permission)就是允许对一个或多个客体(object)执行的操作。\n\n一个用户可经授权而拥有多个角色，一个角色可由多个用户构成；每个角色可以拥有多种许可，每个许可也可授权给多个不同的角色。\n\n重点内容：\n\n * rbac授权插件，实现了主体(用户账号或服务账号)，操作(各种权限)，客体(目标实体)。\n * rbac授权插件支持role和clusterrole两类角色，其中role作用于名称空间级别，clusterrole作用于集群级别。\n * role角色需要用rolebinding这个资源类型，将role上面的权限绑定到一个或一组用户至上。同样作用于名称空间级别。\n * clusterrole角色需要用clusterrolebinding这个资源类型，将clusterrole上面的权限绑定在一个或一组用户至上。\n\n# 什么是admission control(准入控制)\n\n准入控制则是用于在资源对象的创建、删除、更新或连接(proxy)操作时实现精细的许可检查。\n\n\n# 公钥/私钥/ca/数字签名\n\n> 参考的网络url如下\n> \n> https://zhuanlan.zhihu.com/p/31477508\n> \n> https://zhuanlan.zhihu.com/p/113522792\n> \n> https://www.jianshu.com/p/939b3039478c\n> \n> https://www.cnblogs.com/handsomeboys/p/6556336.html\n\n# 公钥和私钥\n\n首先是加密算法分为两种，一种是对称加密算法(私钥加密算法)，一种是非对称加密算法(公钥算法)。\n\n对称加密算法：主要有des/aes，特点是这种算法的加密/解密用的密钥key都是一样的。也就是说用一个密钥key可以加密内容，同样也可以用这把key来解密内容。举个例子：如果总部a派出了两个间谍b和c打入敌方内部，如果他们用的都是对称加密算法的话，那么如果其中一个间谍的密钥被获得的话，那么其他间谍的所有信息都会受到暴露。\n\n非对称加密算法：这种算法加密和解密的密码不一样，一把是公钥，一把是私钥。有如下的特征：\n\n * 公钥和私钥成对出现。\n * 公开的密钥叫做公钥，只有自己知道的叫私钥。\n * 用公钥加密的数据只有对应的私钥可以解密。(b在获得了a的发出的公钥后，可以例如a的公钥对要写给a的信件进行加密，那么a收到b发出的信件后，利用a自己的私钥进行解密。理论上，只要a自己的私钥不泄露，信件都是安全的。因为其他人即使截获了b发出的加密信件内容，也由于没有a的私钥而解密)\n * 用私钥加密的数据只有对应的公钥可以解密。(这一点用在了数字签名上，还是上面的案例中，如果b要求 a收到了b的信件后，要给b签名签收以示确认。那么就是a收到信件后，用a对信件中的文本进行hash函数，得到文本信件摘要，然后对摘要，用自己的私钥进行加密，进行加密得到密文(签名)。而b收到这个密文后，拿出原来a给的公钥进行解密，如果得到了文本摘要hash值，和自己用同样的hash函数文本内容得到的hash值进行比对，如果相等，那么就可以确定这是a的签名无误了，a是不能抵赖的)\n\n# ca证书\n\n在a向b发送信息的过程中，如果有个人c图谋不轨，偷偷把b的公钥换成了自己的公钥，然后截获了a发往b的内容，这个时候c就可以通过自己的私钥解密这个内容了。\n\n那么这个时候，就需要一个中立的权威的机构来指明，a所拿到的b的公钥真的是b的公钥。这个权威的机构就是"证书中心ca"，说白了就是为公钥做个公证。证书中心用自己的私钥，对需要公证的公钥和一些相关信息一起加密(也就是ca对这个公钥进行签名认证了)，生成"数字证书"。\n\nca证书包括以下信息：申请者的公钥、申请者的组织信息和个人信息、签发机构ca的信息、有效时间、证书序列号等信息的明文，同时要包含一个ca机构的签名。\n\na向b发起请求时，b返回的是一个证书文件。a读取这个证书中的明文信息，就得到了b的公钥内容；同时a也会去相应的认可的ca中心，用ca中心提供的公钥来解析这个数字证书中ca的签名信息，以此来得到b的公钥内容。两者做个比较，来判断，是否b的公钥是否正确。\n\n\n# 什么是x509\n\n是一种行业标准或者行业解决方案，在x.509方案中，默认的加密体制是公钥密码体制，一种非常通用的证书格式。\n\n在k8s中支持的https客户端证书认证、token认证以及http basic认证几种认证方式中，基于ssl/tls协议的客户端证书认证(遵循于x.509数字证书标准)以安全性高、易于实现，成为主要使用的认证方式。\n\nx509证书一般会用到三类文件，key，csr，crt。\n\nkey是私用密钥，openssl格式，通常是rsa算法。\n\ncsr是证书请求文件，用于申请证书。在制作csr文件的时候，必须使用自己的私钥来签署申请，还可以设定一个密钥。\n\ncrt是ca认证后的证书文件(windows下面的csr，其实是crt)，签署人用自己的key给你签署的凭证。\n\n\n# openssl生成ssl证书过程\n\n> 参考网上如下url文档整理：\n> \n> https://www.cnblogs.com/qiumingcheng/p/12024280.html\n\n# 基本概念\n\n首先要有一个ca根证书，然后用ca根证书来签发用户证书。\n\n用户进行证书申请：一般先生成一个私钥，然后用私钥生成证书请求(证书请求里应含有公钥信息)，再利用证书服务器的ca根证书来签发证书。\n\n特别说明：\n\n 1. 自签名证书(一般用于顶级证书、根证书)：证书的名称和认证机构的名称相同。\n 2. 根证书：根证书时ca认证中心给自己颁发的证书，是信任链的起始点。任何安装ca根证书的服务器都意味着对这个ca认证中心是信任的。\n 3. 数字证书：数字证书则是由证书认证机构(ca)对证书申请者真实身份验证之后，用ca的根证书对申请人的一些基本信息以及申请人的公钥进行签名(相当于加盖证书机构的公章)后形成的一个数字文件。数字证书包含证书中所标识的实体的公钥(就是说你的证书里有你的公钥)，由于证书将公钥与特定的个人匹配，并且该证书的真实性由颁发机构保证(就是说可以让大家相信你的证书时真的)，因此，数字证书为如何找到用户的公钥并知道它是否有效这一问题提供了解决方案。\n\n# openssl中有如下后缀名的文件\n\n.key格式：私有的密钥\n\n.csr格式：证书签名请求(证书请求文件)，含有公钥信息，certficate signing requests的缩写。\n\n.crl格式：证书吊销列表，certificate revocation list的缩写。\n\n.pem格式：用于导出，导入证书时候的证书的格式，有证书开头，结尾的格式。\n\n# ca根证书的生成步骤\n\n根证书，是ca自己的证书，需要在部署了ca服务的服务器上操作。\n\n一般的流程是：生成ca私钥(.key) --\x3e生成ca证书请求(.csr) --\x3e 自签名得到根证书(.crt)(实际上就是ca给自己颁发的证书)。\n\n# generate ca private key\nopenssl genrsa -out ca.key 2048\n# generate csr \nopenssl req -new -key ca.key -out ca.csr\n# generate self signed certificate（ca 根证书）\nopenssl x509 -req -days 365 -in ca.csr -signkey ca.key -out ca.crt\n\n\n1\n2\n3\n4\n5\n6\n\n\n在实际的软件开发工作中，往往服务器就采用这种自签名的方式，因为毕竟找第三方签名机构是要给钱的，也需要时间。\n\n# 用户证书的生成步骤\n\n生成用户证书的过程，一般如下：\n\n生成私钥(.key) --\x3e 生成证书请求(.csr) --\x3e 用ca根证书签名得到证书(.crt)\n\n# private key\nopenssl genrsa -des3 -out server.key 1024\n# generate csr\nopenssl req -new -key server.key -out server.csr\n# generate certificate\nopenssl ca -in server.csr -out server.crt -cert ca.crt -keyfile ca.key\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 生成pem格式证书\n\n有时候需要用到pem格式的证书，可以用以下方式合并证书(crt)和私钥文件(key)来生成。\n\ncat client.crt client.key > client.pem\ncat server.crt server.key > server.pem\n\n\n1\n2\n\n\n\n# ssl/tls认证\n\n# etcd集群内部对等节点通信：\n\netcd集群内各节点间的集群事务通信，默认监听于tcp的2380端口，基于ssl/tls通信时需要peer类型的数字证书，可实现节点间的身份认证及通信安全，这些证书需要由一个专用的ca进行管理。\n\n# etcd服务器与客户端通信：\n\nkube-apiserver是etcd服务的主要客户端。etcd的rest api服务，默认监听于tcp的2379端口，用于接收并响应客户端请求，基于ssl/tls通信，支持服务端认证和双向认证，而且要使用一个专用的ca来管理此类证书。\n\n# api server与其客户端通信：\n\n它们之间的通信的证书可由同一个ca进行管理，其客户端大体上可以分为如下三类：\n\n * 控制平面的kube-scheduler和kube-controller-manager。\n * 工作节点组件kubelet和kube-proxy：初次接入集群时，kubelet可自动生成私钥和证书签署请求，并由master为其自动进行证书签名和颁发，这就是所谓的tls bootstraping。\n * kubelet及其他形式的客户端，如pod对象等。\n\n\n# 什么是token\n\ntoken的意思是"令牌"，是服务端生成的一串字符串，作为客户端进行请求的一个标识。当用户第一次登录后，服务器生成一个token并将此token返回给客户端，以后客户端只需要带上这个token前来请求数据即可，无需再次带上用户名和密码。\n\n\n# kubeconfig的配置\n\n# kubeconfig是干什么用的\n\nkubeconfig是一个配置文件，里面写了有哪些k8s的集群(多套k8s集群)，用户列表(不同用户，包含的apiserver的认证信息)，上下文列表(不同用户对应不同的k8s环境的列表)，以及当前使用的上下文(指当前默认使用的用户对应相应集群的配置信息)，集群参数和用户参数可以同时设置多对，在上下文参数中将集群参数和用户参数关联起来。\n\n通过kubeconfig，apiserver的客户端可以通过简单切换上下文列表，来完成不同用户访问不同k8s集群的目的。\n\n# 常见命令\n\n 1. 打印kubeconfig文件内容\n    \n    $ kubectl config view\n    \n    \n    1\n    \n\n 2. 设置kubeconfig的clusters配置段。\n    \n    kubectl config set-cluster myk8s \\\n    --certificate-authority=/path/to/ca  \\\n    --embed-certs=true   \\\n    --server=${kube_apiserver} \\\n    --kubeconfig=/kubeconfig/filename\n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n    \n    myk8s只是一个自定义的k8s集群的名字；\n    \n    --certificate-authority设置了该集群的公钥；\n    \n    --embed-certs表示将--certificate-authority证书写入到kubeconfig中；也就是默认是false，也就是不会在这个定义的kubeconfig文件中写入上面的证书文件的具体内容，而是写一个对应的路径，如果改为了true，那么就会在这个定义的kubeconfig文件中写上上面的定义的具体的证书文件的内容。\n    \n    --server表示的是该集群的kube-apiserver的地址；\n    \n    --kubeconfig指定保存的kubeconfig文件的路径。如果没有这个文件的话，会新增，如果有的话，就会在已有的这个文件中，添加集群的信息内容。\n\n 3. 设置kubeconfig中的用户配置参数。\n    \n    kubectl config set-credentials chuck \\\n    --client-certificate=/path/to/cert  \\\n    --client-key=/path/to/private_key  \\\n    --embed-certs=true \\\n    --kubeconfig=/kubeconfig/filename \n    \n    \n    1\n    2\n    3\n    4\n    5\n    \n    \n    chuck是定义的用户名；\n    \n    --client-certificate是定义的一个以.pem结尾的证书文件；\n    \n    --client-key是定义的一个私钥文件，注意客户端的证书首先要经过集群ca的签署，否则不会被集群认可。此处使用的是ca认证方式，也可以使用token认证，如kubelet的tls bootstrap机制下的bootstrapping使用的就是token认证方式。这里的kubectl 使用的是ca认证的方式，不需要token字段。\n\n 4. 修改context的参数。\n    \n    kubectl config set-context chuck@myk8s \\\n    --cluster=myk8s \\\n    --user=chuck \\\n    --kubeconfig=/kubeconfig/filename\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    chuck@myk8s是定义的上下文名称；\n    \n    --cluster是定义的集群名称；\n    \n    --user是定义的用户名称；\n\n 5. 设置默认上下文\n    \n    kubectl config use-context xxxx\n    \n    \n    1\n    \n\n\n# 实现自主控制pod对象资源的访问权限\n\n解析一下：首先要做访问权限的控制，必须要用到鉴权插件中rbac中的内容，也就是说需要定义不同的role/clusterrole，然后通过rolebinding或clusterrolebinding的方式，将权限依附于对应的service account上。那就是意味着pod在创建的时候，必须指定对应的service account。\n\n手动创建service account的时候，k8s会自动提供认证令牌secret对象创建完成。通常挂载点有三个文件：ca.crt、namespace和token。其中token文件是保存了service account的认证token，容器就是通过它向api server发起连接请求，进而由认证插件完成用户认证并将其用户名传递给授权插件。随后为这个service account进入rolebinding或clusterrolebinding，然后在创建pod的时候，指定这个service account就可以了。\n\n\n# 创建自定义权限用户\n\n创建一个自定义基于ssl/tls认证的的用户，配置其访问k8s集群的上下文的config，以授予非管理员级的集群资源使用权限，其配置过程由两部分组成：一是为用户创建专用的私钥和证书文件，二是将其配置于某kubeconfig中。\n\n# 创建自定义用户\n\n创建一个自定义用户所需要的私钥和证书文件，根据上面总结的利用openssl命令生成用户证书的过程。\n\n生成证书的过程如下：生成私钥(.key) --\x3e 生成证书请求(.csr) --\x3e 用ca根证书签名得到证书(.crt)。所有的这些操作都需要在master节点上以root用户的身份来执行，注意最后用到的根证书ca.crt，以及ca的私钥ca.key的位置，默认位于master节点的/etc/kubernetes/pki的路径下。\n\n 1. 生成私钥文件，注意其权限为600以阻止其他用户随意获取，这里在master节点上以root用户进行操作，并将文件放置于/etc/kubernetes/pki专用目录中：\n    \n    # cd /etc/kubernetes/pki\n    # (umask 077; openssl genrsa -out kube-user1.key 2048)\n    \n    \n    1\n    2\n    \n    \n    这里openssl是生成私钥的命令，-out是指定输出的文件名，2048指的是生成2048位的rsa私钥，默认是x509编码。\n\n 2. 创建证书签署请求，-subj选项中cn的值将被kubeconfig作为用户名使用，o的值将被识别为用户组。\n    \n    # openssl req -new -key kube-usr1.key -out kube-user1.csr -subj "/cn=kube-user1/o=kuberusers"\n    \n    \n    1\n    \n    \n    req 的命令大致有3个功能：生成证书请求文件、验证证书请求文件和创建根ca。这里用到的创建证书请求文件。\n    \n    -new表示的是新生成一个新的证书请求文件。\n    \n    -key表示的是指定私钥文件。\n    \n    -out指定输出文件。\n\n 3. 基于kubeadm安装k8s集群时生成的ca来签署证书(对于上面创建的证书签署请求予以签署)，这里设置的有效时长为3650天。\n    \n    openssl x509 -req -in kube-user1.csr -ca ca.crt -cakey  ca.key \\\n    -cacreateserial -out kube-user1.crt -days 3650\n    \n    \n    1\n    2\n    \n\n\n# cka考题整理\n\n\n# 第一题\n\nset configuration context $kubectl config use-context k8s\n\nmonitor the logs of pod foobar and\n\n 1. extract log lines corresponding to error file-not-found\n 2. write them to /opt/kulm00201/foobar\n\nquestion weight 5%\n\n\n# 第二题\n\nset configuration context $ kubectl config use-context k8s\n\nlist all pvs sorted by name saving the full kubectl output to /opt/kucc0010/my_volumes.\n\nuse kubectl\'s own functionally for sorting the output, and do not manipulate if any further.\n\nquestion weight 3%\n\n\n# 第三题\n\nset configuration context $ kubectl config use-context k8s\n\nensure a single instance of pod nginx is running on each node of the kubernetes cluster where nginx also represents the image name which has to be used. do no override any taints currently in place.\n\nuse daemonsets to complete this task and use ds.kusc00201 as daemonset name.\n\nquestion weight 3%\n\n\n# 第四题\n\nset configuration context $ kubectl config use-context k8s perform the following tasks.\n\n 1. add an init container to lumpy--koala(which has been defined in spec file /opt/kucc00100/pod-spec-kucc00100.yaml)\n 2. the init container should create an empty file named /workdir/calm.txt\n 3. if /workdir/calm.txt is not detected, the pod should exit\n 4. once the spec file has been updated with the init container definition, the pod should be created.\n\nquestion weight 7%\n\n\n# 第五题\n\nset configuration context $ kubectl config use-context k8s\n\ncreate a pod named kucc4 with a single container for each of the following images running inside (there may be betwen 1 and 4 images specified): nginx + redis + memcached + consul\n\nquestion weight: 4%\n\n\n# 第六题\n\nset configuration context $ kubectl config use-context k8s\n\nschedule a pod as follows:\n\n 1. name: nginx-kusc00101\n 2. image: nginx\n 3. node selecotr: disk=ssd\n\n\n# 第七题\n\nset configuration context $ kubectl config use-context k8s\n\ncreate a deployment as follows:\n\n 1. name: nginx-app\n 2. using container nginx with version 1.10.2-alpine\n 3. the deployment should contain 3 replicas\n\nnext, deploy the app with new version 1.13.0-alpine by perfoming a rolling update and record that update.\n\nfinally, rollback that update to the previous version 1.10.2-alpine\n\nquesion weight: 4%\n\n\n# 第八题\n\nset configuration context $ kubectl config use-context k8s\n\ncreate and configure the service front-end-service so it\'s accessible through nodeport and routes to the existing pod named front-end.\n\nquestion weight: 4%\n\n\n# 第九题\n\nset configuration context $ kubectl config use-context k8s\n\ncreate a pod as follows:\n\n 1. name: jenkins\n 2. using image: jenkins\n 3. in a new kubernetes namespace named website-frontend\n\nquestion weight: 3%\n\n\n# 第十题\n\nset configuration context $ kubectl config use-context k8s\n\ncreate a deployment spec file that will:\n\n 1. lauch 7 replicas of the redis image with the label: app_env_stage=dev\n 2. deployment name: kual00201\n\nsave a copy of this spec file to "/opt/kual00201/deploy_spec.yaml"(or.json)\n\nwhen you are done, clean up(delete) any new k8s api objects that you produced during this task.\n\nquestion weight: 3%\n\n\n# 第十一题\n\nset configuration context ```$ kubectl config use-context k8s``\n\ncreate a file "/opt/kucc00303/kucc00302.txt" that lists all pods that implement service foo in namespace production.\n\nthe format of the file should be one pod name per line.\n\nquestion weight: 3%\n\n\n# 第十二题\n\nset configuration context $ kubectl config use-context k8s\n\ncreate a kubernetes secret as follows:\n\n 1. name: super-secret\n 2. credential: alice or username:bob\n\ncreate a pod named pod-secrets-via-file using the redis image which mounts a secret named super-secret at /secrets\n\ncreate a second pod named pod-secrets-via-env using the redis image, which exports credential as topsecret\n\nquestion weight: 9%\n\n\n# 第十三题\n\nset configuration context $ kubectl config use-context k8s\n\ncreate a pod as follows:\n\n 1. name: non-persistent-redis\n 2. container image: redis\n 3. named-volume with name: cache-control\n 4. mount path: /data/redis\n\nit should launch in the pre-prod namespace and the volume must not be persistent.\n\nquestion weight: 4%\n\n\n# 第十四题\n\nset configuration context $ kubectl config use-context k8s\n\nscale the deployment webserver to 6 pods.\n\nquestion weight: 1%\n\n\n# 第十五题\n\nset configuration context $ kubectl config use-context k8s\n\ncheck to see how many nodes are ready (not including nodes tainted noschedule) and write the number to /opt/nodenum\n\nquestion weight: 2%\n\n\n# 第十六题\n\nset configuration context $ kubectl config use-context k8s\n\nfrom the pod label name=cpu-utilizer, find pods running high cpu workloads and write the name of the pod consuming most cpu to the file /opt/cpu.txt(which already exists)\n\nquestion weight: 2%\n\n\n# 第十七题\n\nset configuration context $ kubectl config use-context k8s\n\ncreate a deployment as follows\n\n 1. name: nginx-dns\n 2. exposed via a service: nginx-dns\n 3. ensure that the service & pod are accessible via their respective dns records\n 4. the container(s) within any pod(s) running as a part of this deployment should use the nginx image\n\nnext, use the utility nslookup to look up the dns records of the service & pod and write the output to /opt/service.dns and /opt/pod.dns respectively.\n\nquestion weight: 7%\n\n\n# 第十八题\n\nno configuration context change required for this item\n\ncreate a snapshot of the etcd instance running at https://127.0.0.1:2379 saving the snapshot to the file path /data/backup/etcd-snapshot.db\n\nthe etcd instance is running etcd version 3.1.10\n\nthe following tls certificates/key are supplied for connecting to the sever with ecdctl\n\n 1. ca certificate: /opt/kucm00302/ca/crt\n 2. client certificate: /opt/kucm00303/etc-client.crt\n 3. clientkey: /opt/kucm00302/etcd-client.key\n\nquestion weight: 7%\n\n\n# 第十九题\n\nset configuration context $ kubectl config use-context ek8s\n\nset the node labelled with name=ek8s-node-1 as unavailable and reschedule all the pods running on it.\n\nquestion weight: 4%\n\n\n# 第二十题\n\nset configuration context $ kubectl config use-context wk8s\n\na kubernetes worker node, labelled with name=wk8s-node-0 is in state notready.\n\ninvestigate why this is the case, and perform any appropriate steps to bring the node to a ready state, ensuring that any changes are made permanent.\n\nhints:\n\n 1. yout can ssh to the failed node using $ ssh wk8s-node-0\n 2. you can assume elevated privileges on the node with the following command $ sudo -i\n\nquestion weight: 4%\n\n\n# 第二十一题\n\nset configuration context $ kubectl config use-context wk8s\n\nconfigure the kubelet systemd managed service, on the node labelled with name=w8s-node-1 , to launch a pod containing a single container of image nginx named myservice automatically. any spec files required should be placed in the /etc/kubernetes/manifests directory on the node.\n\nhints:\n\n 1. you can ssh to the failed node using $ ssh wk8s-node-1\n 2. you can assume elevated privileges on the node with the following command $ sudo -i\n\nquestion weight: 4%\n\n\n# 第二十二题\n\nset configuration context $ kubectl config use-context ik8s\n\nin this task, you will configure a new node, ik8s-node-0, to join a kubernetes cluster as follows:\n\n 1. configure kubelet for automatic certificate rotation and ensure that both server and client csrs are automatically approved and signed as appropnate via the use of rbac.\n 2. ensure that the appropriate cluster-info configmap is created and configured appropriately in the correct namespace so that future nodes can easily join the cluster.\n 3. your bootstrap kubeconfig should be created on the new node at /etc/kubernetes/bootstrap-kubelet.conf(do not remove this file once your node has successfully joined the cluster)\n 4. the appropriate cluser-wide ca certificate is located on the node at /etc/kubernetes/pki/ca.crt. you should ensure that any automatically issued certificates are installed to the node at /var/lib/kubelet/pki and that the kubeconfig file for kubelet will be rendered at /etc/kubernetes/kubelet.conf upon successful bootstrapping.\n 5. use an additional group for bootstrapping nodes attempting to join the cluster which should be called system:bootstrappers:cka:default-node-token\n 6. solution should start automatically on boot, with the systemd service unit file for kubelet available at /etc/systemd/system/kubelet.service\n\nto test your solution, create the appropriate resources from the spec file located at /opt/.../kube-flannel.yaml . this will create the necessary supporting resources as well as the kube-flannel-ds daemonset. you should ensure that this daemonset is correctly deployed to the single node in the cluster.\n\nhists:\n\n 1.  kubelet is not configured or running on ik8s-master-0 for this task, and you should not attempt to configure it.\n 2.  you will make use of tls bootstrapping to complete this task.\n 3.  you can obtain the ip address of the kubernetes api server via the following command $ ssh ik8s-node-0 getent hsots ik8s--master-0\n 4.  the api server is listening on the usual port, 6443/tcp, and will only server tls requests .\n 5.  the kubelet binary is already installed on ik8s-node-0 at /usr/bin/kublet. you will not need to deploy kube-proxy to the cluster during this task.\n 6.  you can ssh to the new worker node using $ ssh ik8s-node-0\n 7.  you can ssh to the master node with the following command $ ssh ik8s-master-0\n 8.  no further configuration of control plane services running on ik8s-master-0 is required.\n 9.  you can assume elevated privileges on both nodes with the follwing command $ sudo -i\n 10. docker is already installed and running on ik8s-node-0\n\nquestion weight: 8%\n\n\n# 第二十三题\n\nset configuration context $ kubectl config use-context bk8s\n\ngiven a partially-functioning kubernetes cluster, identify symptoms of failure on the cluster. determine the node, the failing service and take actions to bring up the failed service and restore the health of the cluster. ensure that any changes are made permanently.\n\nthe worker node in this cluster is labelled with name=bk8s-node-0\n\nhints:\n\n 1. you can ssh to the relevant nodes using $ ssh $(node) where $(node) is one of bk8s-master-0 or bk8s-node-0\n 2. you can assume elevated privileges on any node in the cluster with the following command $ sudo -i\n\nquestion weight: 4%\n\n\n# 第二十四题\n\nset configuration context $ kubectl config use-context hk8s\n\ncreate a persistent volume with name app-config of capacity 1gi and access mode readwriteonce. the type of volume is hostpath and its location is /srv/app-config.\n\nquestion weight: 3%',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"883数据结构和算法",frontmatter:{title:"883数据结构和算法",date:"2022-01-20T11:18:31.000Z",permalink:"/pages/349c56/",categories:["编程","数据结构和算法"],tags:[null]},regularPath:"/03.%E7%BC%96%E7%A8%8B/80.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/70.883%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95.html",relativePath:"03.编程/80.数据结构和算法/70.883数据结构和算法.md",key:"v-7a4a7900",path:"/pages/349c56/",headers:[{level:2,title:"考试科目和参考书是什么？",slug:"考试科目和参考书是什么",normalizedTitle:"考试科目和参考书是什么？",charIndex:29},{level:2,title:"883中试卷题型分部是什么样的？",slug:"_883中试卷题型分部是什么样的",normalizedTitle:"883中试卷题型分部是什么样的？",charIndex:85},{level:2,title:"各种题型中有哪些侧重点？",slug:"各种题型中有哪些侧重点",normalizedTitle:"各种题型中有哪些侧重点？",charIndex:221},{level:3,title:"选择填空",slug:"选择填空",normalizedTitle:"选择填空",charIndex:238},{level:3,title:"应用题",slug:"应用题",normalizedTitle:"应用题",charIndex:171},{level:3,title:"算法题",slug:"算法题",normalizedTitle:"算法题",charIndex:194},{level:2,title:"需要看883推荐参考书吗？",slug:"需要看883推荐参考书吗",normalizedTitle:"需要看883推荐参考书吗？",charIndex:483},{level:2,title:"如何准备883考试的？",slug:"如何准备883考试的",normalizedTitle:"如何准备883考试的？",charIndex:577},{level:2,title:"有哪些应试技巧？",slug:"有哪些应试技巧",normalizedTitle:"有哪些应试技巧？",charIndex:657},{level:3,title:"多总结",slug:"多总结",normalizedTitle:"多总结",charIndex:670},{level:3,title:"多练习",slug:"多练习",normalizedTitle:"多练习",charIndex:725},{level:3,title:"要背诵",slug:"要背诵",normalizedTitle:"要背诵",charIndex:893},{level:3,title:"提醒注意",slug:"提醒注意",normalizedTitle:"提醒注意",charIndex:1067},{level:2,title:"883知识点提纲",slug:"_883知识点提纲",normalizedTitle:"883知识点提纲",charIndex:1446}],headersStr:"考试科目和参考书是什么？ 883中试卷题型分部是什么样的？ 各种题型中有哪些侧重点？ 选择填空 应用题 算法题 需要看883推荐参考书吗？ 如何准备883考试的？ 有哪些应试技巧？ 多总结 多练习 要背诵 提醒注意 883知识点提纲",content:"883数据结构考试\n\n\n# 883数据结构和算法\n\n\n# 考试科目和参考书是什么？\n\n数据结构，150分\n\n《数据结构》 (C语言版) ，耿国华，高等教育出版社\n\n\n# 883中试卷题型分部是什么样的？\n\n题型    每题分值   题目数   分数\n选择题   2分     15    30分\n填空题   2分     15    30分\n应用题   10分    5     50分\n算法题   10分    4     40分\n\n\n# 各种题型中有哪些侧重点？\n\n\n# 选择填空\n\n选择填空基本上是考察每章的一些概念和性质，难题较少。\n\n唯一可能能难倒大家的应该是算法填空，大概10分左右。\n\n没有类似于名词解释的题目，所以在定义方面不需要特别去记忆，只需了解认识，可以做出选择填空即可。\n\n\n# 应用题\n\n这部分是考察看书的全面性，南林特爱考冷门的知识点，比如KMP算法，B树，数组之类的，一般考数据结构的学校都很少考这些冷门的知识点。\n\n\n# 算法题\n\n算法题一定要勤加练习，其中算法题包括线性表(包括排序)和树的算法题，又小概率会考图的算法题。\n\n\n# 需要看883推荐参考书吗？\n\n学长给的建议是不看，耿国华的书，内容太多，有很多根本不会考的东西而且很晦涩难懂。\n\n书后的题目，前两章有答案后面基本上就没有答案。只是作为补充使用。\n\n\n# 如何准备883考试的？\n\n用了王道数据结构，从头看到尾，外部排序没有看，这个肯定不考，其他基本都看了，里面的内容都比较经典，为考研准备的，没太多废话。\n\n\n# 有哪些应试技巧？\n\n\n# 多总结\n\n应试技巧的话就是平时多总结概念比如树的性质，这部分选择填空必考，多做题错了总结起来看一看。\n\n\n# 多练习\n\n应用题也基本上也就考那几块比如，kmp，排序，图的应用，数组，哈夫曼树，树的遍历这些，这些解题都是死方法，平时把可以把总结好的方法记在笔记本上，多练习。我基本上行看完一遍书有些知识点我就忘了，然后再练，时间长了就记住了。\n\n算法的话，每天尽量练习，线性表和树把王道上的题目做会了考试基本满分，上面基本上都是经典例题。\n\n\n# 要背诵\n\n图的话掌握王道书上的内容后可以再看我的笔记，里面我总结了3-4个模板，看不懂我的模板，也没关系，图本来就难，而且考的概念不高，你看我的模板和试卷上差不多的直接就抄上去，也会得分，能抄多少是多少。\n\n最后排序，每个排序都要自己会写，我半个与就用白纸默写一次，今年考试就考虑排序，我看见了就挺开心的，直接就默写上去，都不怎么动脑子。\n\n\n# 提醒注意\n\n千万要注意，算法题千万不能空着，不会也不要空着，不然老师想给分也给不了，有时候老师为了平衡分数看见有人写就会给点，但你不写肯定是0分。老师如果给你分，被抽查到就会担责任，但只要你写，基本不可能给0分。\n\n有些人会问，真的不会写怎么办？教你们一个办法，就是创造条件，比如一个指针指向一个循环链表的第二个节点，要求删除这个节点，但这个时候你忘了怎么做，那你可以直接定义一个结点指向第一个节点，然后删除第二个节点，但是要有注释，那么基本上会拿3-5分，但不写直接就是0分。\n\n还有就是算法题不一定要最优解，考试可能来不及想，那么可以用最笨的方法，并且注释，这样也可以得7-8分，把时间省出来做其他题目。\n\n此外自己想法比较特殊，也要注释，老师改试卷很快，你用的常规方法，老师一眼就看出来不用备注，比较奇怪的话，可能认为你乱写的，就给个错误分。\n\n\n# 883知识点提纲\n\n线性表(包括队列中，堆栈等特殊线性表)的基本逻辑结构以及物理存储结构特征理解与应用，线性表的推广(数组、广义表、矩阵)的内容。\n\n树、图等非线性结构的物理存储结构，排序与查找算法；一些算法的设计及时间复杂度分析。",normalizedContent:"883数据结构考试\n\n\n# 883数据结构和算法\n\n\n# 考试科目和参考书是什么？\n\n数据结构，150分\n\n《数据结构》 (c语言版) ，耿国华，高等教育出版社\n\n\n# 883中试卷题型分部是什么样的？\n\n题型    每题分值   题目数   分数\n选择题   2分     15    30分\n填空题   2分     15    30分\n应用题   10分    5     50分\n算法题   10分    4     40分\n\n\n# 各种题型中有哪些侧重点？\n\n\n# 选择填空\n\n选择填空基本上是考察每章的一些概念和性质，难题较少。\n\n唯一可能能难倒大家的应该是算法填空，大概10分左右。\n\n没有类似于名词解释的题目，所以在定义方面不需要特别去记忆，只需了解认识，可以做出选择填空即可。\n\n\n# 应用题\n\n这部分是考察看书的全面性，南林特爱考冷门的知识点，比如kmp算法，b树，数组之类的，一般考数据结构的学校都很少考这些冷门的知识点。\n\n\n# 算法题\n\n算法题一定要勤加练习，其中算法题包括线性表(包括排序)和树的算法题，又小概率会考图的算法题。\n\n\n# 需要看883推荐参考书吗？\n\n学长给的建议是不看，耿国华的书，内容太多，有很多根本不会考的东西而且很晦涩难懂。\n\n书后的题目，前两章有答案后面基本上就没有答案。只是作为补充使用。\n\n\n# 如何准备883考试的？\n\n用了王道数据结构，从头看到尾，外部排序没有看，这个肯定不考，其他基本都看了，里面的内容都比较经典，为考研准备的，没太多废话。\n\n\n# 有哪些应试技巧？\n\n\n# 多总结\n\n应试技巧的话就是平时多总结概念比如树的性质，这部分选择填空必考，多做题错了总结起来看一看。\n\n\n# 多练习\n\n应用题也基本上也就考那几块比如，kmp，排序，图的应用，数组，哈夫曼树，树的遍历这些，这些解题都是死方法，平时把可以把总结好的方法记在笔记本上，多练习。我基本上行看完一遍书有些知识点我就忘了，然后再练，时间长了就记住了。\n\n算法的话，每天尽量练习，线性表和树把王道上的题目做会了考试基本满分，上面基本上都是经典例题。\n\n\n# 要背诵\n\n图的话掌握王道书上的内容后可以再看我的笔记，里面我总结了3-4个模板，看不懂我的模板，也没关系，图本来就难，而且考的概念不高，你看我的模板和试卷上差不多的直接就抄上去，也会得分，能抄多少是多少。\n\n最后排序，每个排序都要自己会写，我半个与就用白纸默写一次，今年考试就考虑排序，我看见了就挺开心的，直接就默写上去，都不怎么动脑子。\n\n\n# 提醒注意\n\n千万要注意，算法题千万不能空着，不会也不要空着，不然老师想给分也给不了，有时候老师为了平衡分数看见有人写就会给点，但你不写肯定是0分。老师如果给你分，被抽查到就会担责任，但只要你写，基本不可能给0分。\n\n有些人会问，真的不会写怎么办？教你们一个办法，就是创造条件，比如一个指针指向一个循环链表的第二个节点，要求删除这个节点，但这个时候你忘了怎么做，那你可以直接定义一个结点指向第一个节点，然后删除第二个节点，但是要有注释，那么基本上会拿3-5分，但不写直接就是0分。\n\n还有就是算法题不一定要最优解，考试可能来不及想，那么可以用最笨的方法，并且注释，这样也可以得7-8分，把时间省出来做其他题目。\n\n此外自己想法比较特殊，也要注释，老师改试卷很快，你用的常规方法，老师一眼就看出来不用备注，比较奇怪的话，可能认为你乱写的，就给个错误分。\n\n\n# 883知识点提纲\n\n线性表(包括队列中，堆栈等特殊线性表)的基本逻辑结构以及物理存储结构特征理解与应用，线性表的推广(数组、广义表、矩阵)的内容。\n\n树、图等非线性结构的物理存储结构，排序与查找算法；一些算法的设计及时间复杂度分析。",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"《暗时间》读书笔记",frontmatter:{title:"《暗时间》读书笔记",date:"2022-01-20T11:49:47.000Z",permalink:"/pages/e5d8ed/",categories:["生活","学习方法"],tags:[null]},regularPath:"/04.%E7%94%9F%E6%B4%BB/01.%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/02.%E3%80%8A%E6%9A%97%E6%97%B6%E9%97%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html",relativePath:"04.生活/01.学习方法/02.《暗时间》读书笔记.md",key:"v-5dae9843",path:"/pages/e5d8ed/",headers:[{level:2,title:"内容简介",slug:"内容简介",normalizedTitle:"内容简介",charIndex:16},{level:2,title:"序言 为什么人人都该学点心理学",slug:"序言-为什么人人都该学点心理学",normalizedTitle:"序言 为什么人人都该学点心理学",charIndex:222},{level:2,title:"暗时间",slug:"暗时间",normalizedTitle:"暗时间",charIndex:3},{level:2,title:"设计你自己的进度条",slug:"设计你自己的进度条",normalizedTitle:"设计你自己的进度条",charIndex:1615},{level:3,title:"设计你自己的进度条",slug:"设计你自己的进度条-2",normalizedTitle:"设计你自己的进度条",charIndex:1615},{level:3,title:"不要过早退出循环",slug:"不要过早退出循环",normalizedTitle:"不要过早退出循环",charIndex:2007},{level:3,title:"兴趣遍地都是，专注和持之以恒才是真正稀缺的",slug:"兴趣遍地都是-专注和持之以恒才是真正稀缺的",normalizedTitle:"兴趣遍地都是，专注和持之以恒才是真正稀缺的",charIndex:2394},{level:3,title:"生活中的选择远比我们想象的要多，细微的选择差异造就不同的人生",slug:"生活中的选择远比我们想象的要多-细微的选择差异造就不同的人生",normalizedTitle:"生活中的选择远比我们想象的要多，细微的选择差异造就不同的人生",charIndex:2582},{level:3,title:"靠专业技能的成功是最具可复制的",slug:"靠专业技能的成功是最具可复制的",normalizedTitle:"靠专业技能的成功是最具可复制的",charIndex:2896},{level:3,title:"反思是让你得以改进自己的最重要的思维品质",slug:"反思是让你得以改进自己的最重要的思维品质",normalizedTitle:"反思是让你得以改进自己的最重要的思维品质",charIndex:3048},{level:3,title:"饿死在干草堆之间的驴子",slug:"饿死在干草堆之间的驴子",normalizedTitle:"饿死在干草堆之间的驴子",charIndex:3324}],headersStr:"内容简介 序言 为什么人人都该学点心理学 暗时间 设计你自己的进度条 设计你自己的进度条 不要过早退出循环 兴趣遍地都是，专注和持之以恒才是真正稀缺的 生活中的选择远比我们想象的要多，细微的选择差异造就不同的人生 靠专业技能的成功是最具可复制的 反思是让你得以改进自己的最重要的思维品质 饿死在干草堆之间的驴子",content:'# 《暗时间》读书笔记\n\n\n# 内容简介\n\n 1. 刘未鹏说：“写博客这件事情给我最大的体会就是，一件事情如果你能够坚持做8年，那么不管效率和频率多低，最终总能够取得一些很可观的收益。而另一个体会就是，一件事情只要你坚持得足够久，"坚持"就会慢慢变得"习惯"。原本需要费力去驱动的事情便成了家常便饭，云淡风轻。”\n 2. 这本书便是从刘未鹏8年的博客文章中精选出来的，主要关于心智模式、学习方法和时间利用，《暗时间》的书名便来自于此。\n\n\n# 序言 为什么人人都该学点心理学\n\n 1. 提到心理学，很多人脑海中的第一印象就是"心理问题"、"心理咨询"、"弗洛伊德"、"抑郁症"、"读心术"这些字眼，总觉得关心心理学的都是一些心理阴暗或有问题的家伙，这是对现代心理学典型的误解。\n 2. 实际上心理学，已经在结合现代科学技术手段和研究方法，跨学科研究人脑思维的特点以及和现实生活的关系。其中经济学和心理学的联姻形成的交叉领域最是硕果累累，催生出了一大批优秀的研究者和著作。研究如何针对人们思维的固有弊端来创造性地制定一些经济策略，从而为大众的健康、经济和幸福谋福利。\n 3. 心理学与日常生活息息相关的另一方面就是日常判断与决策。波普尔曾经说过：人生不过是解决问题。而判断与决策有时其中最常见的一类问题解决。糟糕的判断与决策令我们的生活变得糟糕，然而这还不是最关键的，最关键的是我们很难学会质疑自己的判断，而总是能"找到"其他为自己辩护的理由。\n 4. 大脑是我们最重要的工具，要正确利用这个工具，唯一的途径就是去了解它，尤其是了解它的弱点。与很多人的直觉相反，我们的思维有着各种各样的弱点和陷阱，我们解决日常问题的思维方式也并不总是最优的，我们感觉正确的事情有很多是错的，我们习以为常、天经地义的行为也未必就是合乎效益最大化原则的。\n\n\n# 暗时间\n\n 1. 善于利用思维时间的人，可以无形中比别人多出很多时间，从而实际意义上能比别人多活很多年。我们经常听说“心理年龄”这个词，思考得多的人，往往心理年龄更大。\n 2. 善于利用思维时间的人则能够在重要的事情上时时主动提醒自己，将临时的记忆变成硬编码的行为习惯。\n 3. 每个人的手表都走的一样快，但每个人的生命却不是。衡量一个人生活了多少年，应该用思维时间来计算。\n 4. 认为时间对每个人是均等的是一个错觉，认为别人有一天，我也有一天，其实根本不是这样。如果你正在学习一门专业，你使用自己所投入的天数来衡量，很容易会产生一种错觉，认为投入了不少时间，然而其实，“投入时间”这个说法本身就是荒唐的，实际投入的是时间和效率的乘积。\n 5. 我们每个人的生命就像一个个沙漏，每个人的沙漏里装的沙子总量是相当的(大家都活得差不多长)，不同的是，有的人的沙漏颈部较细，有些人的沙漏颈部较粗。那些颈部较细的沙漏能够抓住每一粒时间之沙，虽然沙子总量一样，但相对却拥有更长的生命。\n 6. 因为看书并记住书中的东西只是记忆，这部分推理的过程就是你的思维时间，也是人一生中占据一个显著比例的"暗时间"，你走路、买菜、洗脸洗手、坐公车、逛街、出游、吃饭、睡觉，所有这些时间都可以成为"暗时间"，你可以充分利用这些时间进行思考，反刍和消化平时看和读的东西，让你的认识能够脱离照本宣科的层面。这段时间看起来微不足道，但日积月累将会产生庞大的效应。\n 7. 能够迅速进入专注状态，以及能够长期保持专注状态，是高效学习的两个最重要的习惯。\n 8. 这里就涉及到最后一个高效的习惯：抗干扰。只有具备超强的抗干扰能力，才能有效地利用起前面提到的种种暗时间。抗干扰能力也是可以练习出来的，上本科那会经常坐车，所以我就常常拿着本大部头在车上看，坐着看或站着看都可，事实证明在有干扰的环境中看书是非常锻炼专注能力的一个办法。另外，经常利用各种碎片时间阅读和思考，对迅速集中注意力和保持注意力都非常有帮助。\n\n\n# 设计你自己的进度条\n\n\n# 设计你自己的进度条\n\n 1. 没有进度提示的话，我们无法判断这个等待什么是偶才是个尽头。如果有不断增长的进度条，那么我们对于什么时候会达到100% 就会有一个粗略的估计，这个估计是一剂定心丸，让我们知道这事情总会并且会在不久的将来完成。\n 2. 善于规划的人，会将目标分隔为一个个的里程碑，再将里程碑分隔为ToDo列表。没有分而治之，你就不知道未来还需要付出多少努力才能达到目的，这就会让你心生怯意，不敢进一步投入时间，免得血本无归。在这样的心理下，不少人就会选择保守策略 ---- 退出，以免到头来花了时间还一事无成。\n 3. 所谓的规划其实就是针对这种心理弱点的做事方法。如果你对整个目标的几个重大步骤有清晰的界定，能够对每个步骤的耗时作出靠谱的上界估计，你就不会对不确定的未来，不确定的时间投入感到恐惧，就不会被这种不确定感压迫到过早退出。\n\n\n# 不要过早退出循环\n\n 1. 过早退出的原因往往在于对于未来的不确定性，对于投资时间最终无法收到回报的恐惧，感受到的困难越大，这种恐惧越大，因为越大的困难往往暗示着这个任务需要投资的时间越多。所以其实我们都是直觉经济学家，当我们说"畏难"的时候，其实我们畏惧的不是困难本身，而是困难所暗示的时间经济学意义。\n 2. 我们的情绪大脑比较原始，仅根据碰壁的次数或硬度来判断事情的难易并不一定靠谱，如果遇到困难，不妨用一用互联网，用一用群体的智慧，看看别人当时是怎么想怎么办的，绝大多数情况下你并不孤单，你遇到的问题早就有人遇到过，你踩过的坑里尽是前人的脚印，不要仅仅因为一时摸不着头绪，找不到出路就退出，这不是informed decision，问一问自己做出退出的决策是否基于足够的信息，是否进行了足够的调查。\n 3. 模仿高德纳先生的名言：过早退出是一切失败的根源。\n\n\n# 兴趣遍地都是，专注和持之以恒才是真正稀缺的\n\n 1. 我觉得区别他们和其他人，并不是他们拥有超过常人的兴趣，而是他们拥有超过常人的毅力。\n 2. 区别他们的并不是兴趣的有无，而是他们的性格里面有没有维持兴趣的火种一直燃烧下去的燃料。\n 3. 一个人有专注和持之以恒的性格，即便在一个没有多大兴趣的领域也能成为专家(更何况，兴趣的很大一类来源就是"我擅长做这件事情")\n\n\n# 生活中的选择远比我们想象的要多，细微的选择差异造就不同的人生\n\n 1. 我相信不是所有人都有勇气上去拦住名人问普通问题的，我会给自己找很多很多的理由和借口，我想最常见的应该是两个原因：\n    \n    * 如果被批评了自尊心会受到打击。\n    * 认为问了也问不出特别的信息。\n    \n    然而事实却是相反：\n    \n    * 自尊心受到打击算不上实质性的损失。\n    * 你想不出能问出什么特别的信息并不代表就真的问不到重要的信息。别把不知道当成没有\n\n 2. 信心，是这样一种奇怪的东西，就算你没有确切地证明未来会更好，你也会坚持下去，你不会过早退出循环；而来源于过来人的信息则是信心最靠谱的保障。\n\n\n# 靠专业技能的成功是最具可复制的\n\n 1. 它需要的只是你在一个领域坚持不懈地专注下去，只需要选择一个不算太不靠谱的方向，然后专心致志地钻下去，最后必然能成为高手或绝顶高手。\n 2. 世上有很多成功带有偶然因素和运气成分或出身环境，但至少这一样，被无数人复制了无数遍，否则就不会存在学校和教育了。\n\n\n# 反思是让你得以改进自己的最重要的思维品质\n\n 1. 性格是这样一种自我实现和强化的陷进：如果你是不容易专注的人，你会发现生活中处处都是分散你注意力的东西。\n 2. 你的思维难以在一个事情上停留半小时，于是你的时间变得琐碎，你很难在一个领域有长久的积累和深入的思考，这样的现实可能会让你感到沮丧，后者让你更加无法专心，这样的现实可能会让你感到焦虑，为了避开焦虑你又会去寻求其他的刺激，结果是恶性循环。\n 3. 反思是改变自己的第一步，我们常常容易发现别人的问题，别人的错误，却难以发现自己思维中的问题，因为我们很少会把自己的思维当成目标去思考。\n\n\n# 饿死在干草堆之间的驴子\n\n 1. 有人会因为无法作出决定就推迟决定，然而实际上推迟决定是最差的决定，在推迟决定期间，时间悄悄流逝，你却没有任何一条路上的积累，白白浪费了施加。\n 2. 如果有一些时间，不知道花在A上还是B上，不行，因为过了这段时间，这段时间就不是你的了。\n 3. 所以，不管有多纠结，也不要从纠结中逃离，视图推延决定，既然终究是个痛苦的决定，就痛一回，好好思考和调查之后作出一个决定并坚持下去，只要不是太不靠谱的行业，经过你的积累总会成为高手。',normalizedContent:'# 《暗时间》读书笔记\n\n\n# 内容简介\n\n 1. 刘未鹏说：“写博客这件事情给我最大的体会就是，一件事情如果你能够坚持做8年，那么不管效率和频率多低，最终总能够取得一些很可观的收益。而另一个体会就是，一件事情只要你坚持得足够久，"坚持"就会慢慢变得"习惯"。原本需要费力去驱动的事情便成了家常便饭，云淡风轻。”\n 2. 这本书便是从刘未鹏8年的博客文章中精选出来的，主要关于心智模式、学习方法和时间利用，《暗时间》的书名便来自于此。\n\n\n# 序言 为什么人人都该学点心理学\n\n 1. 提到心理学，很多人脑海中的第一印象就是"心理问题"、"心理咨询"、"弗洛伊德"、"抑郁症"、"读心术"这些字眼，总觉得关心心理学的都是一些心理阴暗或有问题的家伙，这是对现代心理学典型的误解。\n 2. 实际上心理学，已经在结合现代科学技术手段和研究方法，跨学科研究人脑思维的特点以及和现实生活的关系。其中经济学和心理学的联姻形成的交叉领域最是硕果累累，催生出了一大批优秀的研究者和著作。研究如何针对人们思维的固有弊端来创造性地制定一些经济策略，从而为大众的健康、经济和幸福谋福利。\n 3. 心理学与日常生活息息相关的另一方面就是日常判断与决策。波普尔曾经说过：人生不过是解决问题。而判断与决策有时其中最常见的一类问题解决。糟糕的判断与决策令我们的生活变得糟糕，然而这还不是最关键的，最关键的是我们很难学会质疑自己的判断，而总是能"找到"其他为自己辩护的理由。\n 4. 大脑是我们最重要的工具，要正确利用这个工具，唯一的途径就是去了解它，尤其是了解它的弱点。与很多人的直觉相反，我们的思维有着各种各样的弱点和陷阱，我们解决日常问题的思维方式也并不总是最优的，我们感觉正确的事情有很多是错的，我们习以为常、天经地义的行为也未必就是合乎效益最大化原则的。\n\n\n# 暗时间\n\n 1. 善于利用思维时间的人，可以无形中比别人多出很多时间，从而实际意义上能比别人多活很多年。我们经常听说“心理年龄”这个词，思考得多的人，往往心理年龄更大。\n 2. 善于利用思维时间的人则能够在重要的事情上时时主动提醒自己，将临时的记忆变成硬编码的行为习惯。\n 3. 每个人的手表都走的一样快，但每个人的生命却不是。衡量一个人生活了多少年，应该用思维时间来计算。\n 4. 认为时间对每个人是均等的是一个错觉，认为别人有一天，我也有一天，其实根本不是这样。如果你正在学习一门专业，你使用自己所投入的天数来衡量，很容易会产生一种错觉，认为投入了不少时间，然而其实，“投入时间”这个说法本身就是荒唐的，实际投入的是时间和效率的乘积。\n 5. 我们每个人的生命就像一个个沙漏，每个人的沙漏里装的沙子总量是相当的(大家都活得差不多长)，不同的是，有的人的沙漏颈部较细，有些人的沙漏颈部较粗。那些颈部较细的沙漏能够抓住每一粒时间之沙，虽然沙子总量一样，但相对却拥有更长的生命。\n 6. 因为看书并记住书中的东西只是记忆，这部分推理的过程就是你的思维时间，也是人一生中占据一个显著比例的"暗时间"，你走路、买菜、洗脸洗手、坐公车、逛街、出游、吃饭、睡觉，所有这些时间都可以成为"暗时间"，你可以充分利用这些时间进行思考，反刍和消化平时看和读的东西，让你的认识能够脱离照本宣科的层面。这段时间看起来微不足道，但日积月累将会产生庞大的效应。\n 7. 能够迅速进入专注状态，以及能够长期保持专注状态，是高效学习的两个最重要的习惯。\n 8. 这里就涉及到最后一个高效的习惯：抗干扰。只有具备超强的抗干扰能力，才能有效地利用起前面提到的种种暗时间。抗干扰能力也是可以练习出来的，上本科那会经常坐车，所以我就常常拿着本大部头在车上看，坐着看或站着看都可，事实证明在有干扰的环境中看书是非常锻炼专注能力的一个办法。另外，经常利用各种碎片时间阅读和思考，对迅速集中注意力和保持注意力都非常有帮助。\n\n\n# 设计你自己的进度条\n\n\n# 设计你自己的进度条\n\n 1. 没有进度提示的话，我们无法判断这个等待什么是偶才是个尽头。如果有不断增长的进度条，那么我们对于什么时候会达到100% 就会有一个粗略的估计，这个估计是一剂定心丸，让我们知道这事情总会并且会在不久的将来完成。\n 2. 善于规划的人，会将目标分隔为一个个的里程碑，再将里程碑分隔为todo列表。没有分而治之，你就不知道未来还需要付出多少努力才能达到目的，这就会让你心生怯意，不敢进一步投入时间，免得血本无归。在这样的心理下，不少人就会选择保守策略 ---- 退出，以免到头来花了时间还一事无成。\n 3. 所谓的规划其实就是针对这种心理弱点的做事方法。如果你对整个目标的几个重大步骤有清晰的界定，能够对每个步骤的耗时作出靠谱的上界估计，你就不会对不确定的未来，不确定的时间投入感到恐惧，就不会被这种不确定感压迫到过早退出。\n\n\n# 不要过早退出循环\n\n 1. 过早退出的原因往往在于对于未来的不确定性，对于投资时间最终无法收到回报的恐惧，感受到的困难越大，这种恐惧越大，因为越大的困难往往暗示着这个任务需要投资的时间越多。所以其实我们都是直觉经济学家，当我们说"畏难"的时候，其实我们畏惧的不是困难本身，而是困难所暗示的时间经济学意义。\n 2. 我们的情绪大脑比较原始，仅根据碰壁的次数或硬度来判断事情的难易并不一定靠谱，如果遇到困难，不妨用一用互联网，用一用群体的智慧，看看别人当时是怎么想怎么办的，绝大多数情况下你并不孤单，你遇到的问题早就有人遇到过，你踩过的坑里尽是前人的脚印，不要仅仅因为一时摸不着头绪，找不到出路就退出，这不是informed decision，问一问自己做出退出的决策是否基于足够的信息，是否进行了足够的调查。\n 3. 模仿高德纳先生的名言：过早退出是一切失败的根源。\n\n\n# 兴趣遍地都是，专注和持之以恒才是真正稀缺的\n\n 1. 我觉得区别他们和其他人，并不是他们拥有超过常人的兴趣，而是他们拥有超过常人的毅力。\n 2. 区别他们的并不是兴趣的有无，而是他们的性格里面有没有维持兴趣的火种一直燃烧下去的燃料。\n 3. 一个人有专注和持之以恒的性格，即便在一个没有多大兴趣的领域也能成为专家(更何况，兴趣的很大一类来源就是"我擅长做这件事情")\n\n\n# 生活中的选择远比我们想象的要多，细微的选择差异造就不同的人生\n\n 1. 我相信不是所有人都有勇气上去拦住名人问普通问题的，我会给自己找很多很多的理由和借口，我想最常见的应该是两个原因：\n    \n    * 如果被批评了自尊心会受到打击。\n    * 认为问了也问不出特别的信息。\n    \n    然而事实却是相反：\n    \n    * 自尊心受到打击算不上实质性的损失。\n    * 你想不出能问出什么特别的信息并不代表就真的问不到重要的信息。别把不知道当成没有\n\n 2. 信心，是这样一种奇怪的东西，就算你没有确切地证明未来会更好，你也会坚持下去，你不会过早退出循环；而来源于过来人的信息则是信心最靠谱的保障。\n\n\n# 靠专业技能的成功是最具可复制的\n\n 1. 它需要的只是你在一个领域坚持不懈地专注下去，只需要选择一个不算太不靠谱的方向，然后专心致志地钻下去，最后必然能成为高手或绝顶高手。\n 2. 世上有很多成功带有偶然因素和运气成分或出身环境，但至少这一样，被无数人复制了无数遍，否则就不会存在学校和教育了。\n\n\n# 反思是让你得以改进自己的最重要的思维品质\n\n 1. 性格是这样一种自我实现和强化的陷进：如果你是不容易专注的人，你会发现生活中处处都是分散你注意力的东西。\n 2. 你的思维难以在一个事情上停留半小时，于是你的时间变得琐碎，你很难在一个领域有长久的积累和深入的思考，这样的现实可能会让你感到沮丧，后者让你更加无法专心，这样的现实可能会让你感到焦虑，为了避开焦虑你又会去寻求其他的刺激，结果是恶性循环。\n 3. 反思是改变自己的第一步，我们常常容易发现别人的问题，别人的错误，却难以发现自己思维中的问题，因为我们很少会把自己的思维当成目标去思考。\n\n\n# 饿死在干草堆之间的驴子\n\n 1. 有人会因为无法作出决定就推迟决定，然而实际上推迟决定是最差的决定，在推迟决定期间，时间悄悄流逝，你却没有任何一条路上的积累，白白浪费了施加。\n 2. 如果有一些时间，不知道花在a上还是b上，不行，因为过了这段时间，这段时间就不是你的了。\n 3. 所以，不管有多纠结，也不要从纠结中逃离，视图推延决定，既然终究是个痛苦的决定，就痛一回，好好思考和调查之后作出一个决定并坚持下去，只要不是太不靠谱的行业，经过你的积累总会成为高手。',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"《大脑强人》读书笔记",frontmatter:{title:"《大脑强人》读书笔记",date:"2022-01-20T11:50:09.000Z",permalink:"/pages/3a1429/",categories:["生活","学习方法"],tags:[null]},regularPath:"/04.%E7%94%9F%E6%B4%BB/01.%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/03.%E3%80%8A%E5%A4%A7%E8%84%91%E5%BC%BA%E4%BA%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html",relativePath:"04.生活/01.学习方法/03.《大脑强人》读书笔记.md",key:"v-6fd0f028",path:"/pages/3a1429/",headers:[{level:2,title:"第1章 质疑法：学从问处始，问倒整个世界",slug:"第1章-质疑法-学从问处始-问倒整个世界",normalizedTitle:"第1章 质疑法：学从问处始，问倒整个世界",charIndex:31},{level:2,title:"第2章 专注法：水滴石穿的求知韧性",slug:"第2章-专注法-水滴石穿的求知韧性",normalizedTitle:"第2章 专注法：水滴石穿的求知韧性",charIndex:56},{level:2,title:"第3章 思考法：开启知识宝库的金钥匙",slug:"第3章-思考法-开启知识宝库的金钥匙",normalizedTitle:"第3章 思考法：开启知识宝库的金钥匙",charIndex:78},{level:2,title:"第4章 观察法：修炼求知的慧眼",slug:"第4章-观察法-修炼求知的慧眼",normalizedTitle:"第4章 观察法：修炼求知的慧眼",charIndex:101},{level:2,title:"第5章 坚持法：恒心是征服知识高峰的法宝",slug:"第5章-坚持法-恒心是征服知识高峰的法宝",normalizedTitle:"第5章 坚持法：恒心是征服知识高峰的法宝",charIndex:121},{level:2,title:"第6章 记忆法：掌握记忆魔法，记得快记得牢",slug:"第6章-记忆法-掌握记忆魔法-记得快记得牢",normalizedTitle:"第6章 记忆法：掌握记忆魔法，记得快记得牢",charIndex:146},{level:2,title:"第7章 兴趣法：兴趣催生学习的魔力",slug:"第7章-兴趣法-兴趣催生学习的魔力",normalizedTitle:"第7章 兴趣法：兴趣催生学习的魔力",charIndex:172},{level:2,title:"第8章 统筹时间法：掌控时间就能掌控学习",slug:"第8章-统筹时间法-掌控时间就能掌控学习",normalizedTitle:"第8章 统筹时间法：掌控时间就能掌控学习",charIndex:194},{level:2,title:"第9章 计划法：学习预则立，不预则废",slug:"第9章-计划法-学习预则立-不预则废",normalizedTitle:"第9章 计划法：学习预则立，不预则废",charIndex:219},{level:2,title:"第10章 合作学习法：他人是学习的一面镜子",slug:"第10章-合作学习法-他人是学习的一面镜子",normalizedTitle:"第10章 合作学习法：他人是学习的一面镜子",charIndex:242},{level:2,title:"第11章 积极休息法：7+1>8的学习魔法",slug:"第11章-积极休息法-7-1-8的学习魔法",normalizedTitle:"第11章 积极休息法：7+1&gt;8的学习魔法",charIndex:null},{level:2,title:"第12章 预习法：赢在学习起跑线上",slug:"第12章-预习法-赢在学习起跑线上",normalizedTitle:"第12章 预习法：赢在学习起跑线上",charIndex:294},{level:2,title:"第13章 复习法：学而时习之，温故而知新",slug:"第13章-复习法-学而时习之-温故而知新",normalizedTitle:"第13章 复习法：学而时习之，温故而知新",charIndex:316},{level:2,title:"第14章 举一反三法：触类旁通，知识喷如泉涌",slug:"第14章-举一反三法-触类旁通-知识喷如泉涌",normalizedTitle:"第14章 举一反三法：触类旁通，知识喷如泉涌",charIndex:341},{level:2,title:"第15章 笔记法：不动笔墨不读书",slug:"第15章-笔记法-不动笔墨不读书",normalizedTitle:"第15章 笔记法：不动笔墨不读书",charIndex:368},{level:2,title:"第16章 读书札记：读读，写写，想想",slug:"第16章-读书札记-读读-写写-想想",normalizedTitle:"第16章 读书札记：读读，写写，想想",charIndex:389},{level:2,title:"第17章 实践法：实践出真知，功夫在书外",slug:"第17章-实践法-实践出真知-功夫在书外",normalizedTitle:"第17章 实践法：实践出真知，功夫在书外",charIndex:412},{level:2,title:"第18章 想象法：想象力让学习腾飞",slug:"第18章-想象法-想象力让学习腾飞",normalizedTitle:"第18章 想象法：想象力让学习腾飞",charIndex:437},{level:2,title:"第19章 比较法：衡量学习差距的天平",slug:"第19章-比较法-衡量学习差距的天平",normalizedTitle:"第19章 比较法：衡量学习差距的天平",charIndex:459},{level:2,title:"第20章 循序渐进法：学习之道在于细嚼慢咽",slug:"第20章-循序渐进法-学习之道在于细嚼慢咽",normalizedTitle:"第20章 循序渐进法：学习之道在于细嚼慢咽",charIndex:482},{level:3,title:"定义",slug:"定义",normalizedTitle:"定义",charIndex:508},{level:3,title:"重要性(为什么)",slug:"重要性-为什么",normalizedTitle:"重要性(为什么)",charIndex:548},{level:3,title:"如何做到循序渐进",slug:"如何做到循序渐进",normalizedTitle:"如何做到循序渐进",charIndex:698},{level:2,title:"第21章 名人学习法：以名人为师，学习少走弯路",slug:"第21章-名人学习法-以名人为师-学习少走弯路",normalizedTitle:"第21章 名人学习法：以名人为师，学习少走弯路",charIndex:1131},{level:2,title:"第22章 读书先读序文法：书山有路“序”为径",slug:"第22章-读书先读序文法-书山有路-序-为径",normalizedTitle:"第22章 读书先读序文法：书山有路“序”为径",charIndex:1159},{level:2,title:"第23章 SQ3R五步读书法：读懂一本书，一步不能少",slug:"第23章-sq3r五步读书法-读懂一本书-一步不能少",normalizedTitle:"第23章 sq3r五步读书法：读懂一本书，一步不能少",charIndex:1281},{level:3,title:"概览材料，获得大的印象",slug:"概览材料-获得大的印象",normalizedTitle:"概览材料，获得大的印象",charIndex:1389},{level:3,title:"提出问题，引发思考",slug:"提出问题-引发思考",normalizedTitle:"提出问题，引发思考",charIndex:1767},{level:3,title:"带着审判的眼光阅读材料",slug:"带着审判的眼光阅读材料",normalizedTitle:"带着审判的眼光阅读材料",charIndex:1860},{level:3,title:"复述材料，检阅阅读的效果",slug:"复述材料-检阅阅读的效果",normalizedTitle:"复述材料，检阅阅读的效果",charIndex:2094},{level:3,title:"适时温习材料，达到长期记忆",slug:"适时温习材料-达到长期记忆",normalizedTitle:"适时温习材料，达到长期记忆",charIndex:2491},{level:2,title:"第24章 筛选择优读书法：读一流的书，做一流的人",slug:"第24章-筛选择优读书法-读一流的书-做一流的人",normalizedTitle:"第24章 筛选择优读书法：读一流的书，做一流的人",charIndex:2746},{level:2,title:"第25章 薄厚互返法：厚从薄中来，薄到厚中去",slug:"第25章-薄厚互返法-厚从薄中来-薄到厚中去",normalizedTitle:"第25章 薄厚互返法：厚从薄中来，薄到厚中去",charIndex:2775},{level:2,title:"第26章 区别对待读书法：读书也要量体裁衣",slug:"第26章-区别对待读书法-读书也要量体裁衣",normalizedTitle:"第26章 区别对待读书法：读书也要量体裁衣",charIndex:2802},{level:2,title:"第27章 精读法：“全盘细化”一本书",slug:"第27章-精读法-全盘细化-一本书",normalizedTitle:"第27章 精读法：“全盘细化”一本书",charIndex:2828}],headersStr:"第1章 质疑法：学从问处始，问倒整个世界 第2章 专注法：水滴石穿的求知韧性 第3章 思考法：开启知识宝库的金钥匙 第4章 观察法：修炼求知的慧眼 第5章 坚持法：恒心是征服知识高峰的法宝 第6章 记忆法：掌握记忆魔法，记得快记得牢 第7章 兴趣法：兴趣催生学习的魔力 第8章 统筹时间法：掌控时间就能掌控学习 第9章 计划法：学习预则立，不预则废 第10章 合作学习法：他人是学习的一面镜子 第11章 积极休息法：7+1>8的学习魔法 第12章 预习法：赢在学习起跑线上 第13章 复习法：学而时习之，温故而知新 第14章 举一反三法：触类旁通，知识喷如泉涌 第15章 笔记法：不动笔墨不读书 第16章 读书札记：读读，写写，想想 第17章 实践法：实践出真知，功夫在书外 第18章 想象法：想象力让学习腾飞 第19章 比较法：衡量学习差距的天平 第20章 循序渐进法：学习之道在于细嚼慢咽 定义 重要性(为什么) 如何做到循序渐进 第21章 名人学习法：以名人为师，学习少走弯路 第22章 读书先读序文法：书山有路“序”为径 第23章 SQ3R五步读书法：读懂一本书，一步不能少 概览材料，获得大的印象 提出问题，引发思考 带着审判的眼光阅读材料 复述材料，检阅阅读的效果 适时温习材料，达到长期记忆 第24章 筛选择优读书法：读一流的书，做一流的人 第25章 薄厚互返法：厚从薄中来，薄到厚中去 第26章 区别对待读书法：读书也要量体裁衣 第27章 精读法：“全盘细化”一本书",content:"# 《学一点学习的魔法，你也可以是大脑强人》读书笔记\n\n\n# 第1章 质疑法：学从问处始，问倒整个世界\n\n\n# 第2章 专注法：水滴石穿的求知韧性\n\n\n# 第3章 思考法：开启知识宝库的金钥匙\n\n\n# 第4章 观察法：修炼求知的慧眼\n\n\n# 第5章 坚持法：恒心是征服知识高峰的法宝\n\n\n# 第6章 记忆法：掌握记忆魔法，记得快记得牢\n\n\n# 第7章 兴趣法：兴趣催生学习的魔力\n\n\n# 第8章 统筹时间法：掌控时间就能掌控学习\n\n\n# 第9章 计划法：学习预则立，不预则废\n\n\n# 第10章 合作学习法：他人是学习的一面镜子\n\n\n# 第11章 积极休息法：7+1>8的学习魔法\n\n\n# 第12章 预习法：赢在学习起跑线上\n\n\n# 第13章 复习法：学而时习之，温故而知新\n\n\n# 第14章 举一反三法：触类旁通，知识喷如泉涌\n\n\n# 第15章 笔记法：不动笔墨不读书\n\n\n# 第16章 读书札记：读读，写写，想想\n\n\n# 第17章 实践法：实践出真知，功夫在书外\n\n\n# 第18章 想象法：想象力让学习腾飞\n\n\n# 第19章 比较法：衡量学习差距的天平\n\n\n# 第20章 循序渐进法：学习之道在于细嚼慢咽\n\n\n# 定义\n\n所谓循序渐进法，就是指按照一定的顺序，有计划有步骤地进行学习。\n\n\n# 重要性(为什么)\n\n * 循序渐进也是学习文化知识的必由之路（片面求快不符合读书的辩证法）。\n\n * 循序渐进符合认识规律。\n   \n   符合知识体系的形成和发展规律，也符合人的认知的发展规律。\n   \n   人的认知从感性逐步上升到理性，由现象上升到本质，由具体上升到抽象，具有顺序性。\n\n\n# 如何做到循序渐进\n\n * 循序渐进就是先易后难。\n   \n   易就是基础知识，容易记住的内容；难就是易忘记的东西，复杂难懂的内容。\n   \n   难和易是相对的，每门知识的前后构成都遵循先易后难的规律。\n   \n   破易是攻难的台阶和武器，易是攻难的阶梯和突破口。\n\n * 先安排简单内容进行学习，可以逐步扫除畏难情绪，避免学习中的自我干扰，减少精神压力，容易取得完全成功，调动积极性。\n\n * 有时还需要频频回顾，以暂时的退步以求得扎实的学问。\n\n * 积累知识方面要养成严格循序渐进的习惯。\n\n注意事项：\n\n * 有的人读书性子急，一打开书就匆忙朝前赶。\n * 片面求快不符合读书的辩证法。\n * 前面的东西没有弄明白，切不要急于学后面的。\n * 切不要掩盖自己知识上的缺陷。\n * 注意一：不要隔断知识的连续性。\n * 注意二：紧紧把握住从“易”到难的过渡关。\n * 循序渐进缺点是趣味性差和学习见成效慢，特别是学习初期往往要靠毅力才能坚持下去。\n\n\n# 第21章 名人学习法：以名人为师，学习少走弯路\n\n\n# 第22章 读书先读序文法：书山有路“序”为径\n\n序文，凡例主要介绍该书的读者对象、主要内容及作者写书的缘由、意图、经过、体例等内容的文字。\n\n先读序文法实际上是一种探测性阅读，通常是指为了搜寻某种资料或确定读物是否具有阅读价值的阅读。\n\n\n# 第23章 SQ3R五步读书法：读懂一本书，一步不能少\n\nSQ3R即为：S—Q—R—R—R，代表为”概览—提问—阅读—复述—温习”。这种读书法符合人们读书中的一般思维规律，有助于理解书本内容和增加个人记忆力。\n\n\n# 概览材料，获得大的印象\n\n第一步：Surery，概览。可以用在整本书或一章中的一节，目的在于获得对整个材料的总体的把握。\n\n# 概览的内容如下\n\n * 开始浏览的这章或这节是怎样与全书的整个主题相配合的。（跟其他几章是同等关系，或者还是为后面的章节提供一个背景知识）\n * 翻看下这一章有多少页码，估计需要花多少时间。\n * 思考一下这章或这节分为几个主要部分或几个论题，是先把握整个主要观点呢？还是先完整地读完第一节。\n * 研究一下章名，转换为问题：这个题目谈的是什么意思；这方面我已经知道了哪些。\n * 观察思考主要标题，副标题，关键词，重要观点，思考材料是如何组织安排的。\n * 阅读下引言和小结，也可以是思考题。\n * 注意黑体字、斜体字的句子、短语或词汇。\n * 使用了哪些直观的呈现方式。\n * 可再次对所需花费时间做个进一步估计。\n\n\n# 提出问题，引发思考\n\n在开始正式阅读前，要有明确而简洁的问题，最好是写下来。\n\n明确三类问题：\n\n * 我已经知道了什么？\n\n * 作者想告诉我什么？\n\n * 我想要得到什么？\n\n\n# 带着审判的眼光阅读材料\n\n第三步，Read，阅读材料。认真、积极而带着批判性的眼光阅读。\n\n带着之前准备的问题，来认真阅读寻找答案。可以细想所阅读材料的含义，思考可能的例外和矛盾之处，检验书中的假定等。\n\n方式方法如下：\n\n * 进行必要的快速阅读训练。\n * 再回过头来开始阅读一章或一节？？\n * 在阅读中要设法回答那些在浏览和提问阶段提出的最重要的问题。\n * 在第一次阅读的时候，不要停下来重读那些难懂的段落，要直接读下去，相当快地读，直到结束为止。\n\n\n# 复述材料，检阅阅读的效果\n\n第四步，Recite，复述。可以给自己或学习伙伴重述或解释一下你所阅读的材料，也可以回答下自己早些时候提出的各类问题，最好大声地说出。（建议同伴交流材料）\n\n复述的方法：\n\n * 合拢书本。\n * 努力用自己的语言来举例说明回答最主要的问题。\n * 尽量运用图表、曲线或框图来直观表现手法。画图\n * 对自己要记住的要点抓住更多的细节。\n * 记下自己仍然要作出回答的问题或没有完全理解的概念。\n * 再次翻书，浏览检查我的答案及直观表现形式的理解程度和精确性，进一步来补充我需要作出回答的问题表。\n * 有必要的时候可以再次进行提问、阅读和复述。\n * 复述整章的要点和主要细节，准备两页纸。一张记录整章的内容分解成关键词模型。另一页纸收集一些不容易放在模型之中的具体材料。如公式、定义、统计数据、图表和问题等。\n * 寻找机会来复述或运用已学到的东西。\n\n\n# 适时温习材料，达到长期记忆\n\n方法如下：\n\n * 第一次复习应该在学习后立即就进行。通常应该花费几分钟。（在复述阶段之后休息5~10分钟之后进行，利用不断改进的关键字模型，能够对整章的内容作出概括，能够复述每一个要点和次要点，搜索与关键词模型相联系的细节）\n * 第二次复习应该在第二天就进行，也应该只花2~5分钟。\n * 第三次复习应该在一周之后进行，第四次复习应该在一个月之后。\n\n除了注意吸收他人经验，学习那些被公认的优秀方法外，还必须强调在学习实践中，根据个人的具体情况对它们进行改造与创新。\n\n\n# 第24章 筛选择优读书法：读一流的书，做一流的人\n\n\n# 第25章 薄厚互返法：厚从薄中来，薄到厚中去\n\n\n# 第26章 区别对待读书法：读书也要量体裁衣\n\n\n# 第27章 精读法：“全盘细化”一本书",normalizedContent:"# 《学一点学习的魔法，你也可以是大脑强人》读书笔记\n\n\n# 第1章 质疑法：学从问处始，问倒整个世界\n\n\n# 第2章 专注法：水滴石穿的求知韧性\n\n\n# 第3章 思考法：开启知识宝库的金钥匙\n\n\n# 第4章 观察法：修炼求知的慧眼\n\n\n# 第5章 坚持法：恒心是征服知识高峰的法宝\n\n\n# 第6章 记忆法：掌握记忆魔法，记得快记得牢\n\n\n# 第7章 兴趣法：兴趣催生学习的魔力\n\n\n# 第8章 统筹时间法：掌控时间就能掌控学习\n\n\n# 第9章 计划法：学习预则立，不预则废\n\n\n# 第10章 合作学习法：他人是学习的一面镜子\n\n\n# 第11章 积极休息法：7+1>8的学习魔法\n\n\n# 第12章 预习法：赢在学习起跑线上\n\n\n# 第13章 复习法：学而时习之，温故而知新\n\n\n# 第14章 举一反三法：触类旁通，知识喷如泉涌\n\n\n# 第15章 笔记法：不动笔墨不读书\n\n\n# 第16章 读书札记：读读，写写，想想\n\n\n# 第17章 实践法：实践出真知，功夫在书外\n\n\n# 第18章 想象法：想象力让学习腾飞\n\n\n# 第19章 比较法：衡量学习差距的天平\n\n\n# 第20章 循序渐进法：学习之道在于细嚼慢咽\n\n\n# 定义\n\n所谓循序渐进法，就是指按照一定的顺序，有计划有步骤地进行学习。\n\n\n# 重要性(为什么)\n\n * 循序渐进也是学习文化知识的必由之路（片面求快不符合读书的辩证法）。\n\n * 循序渐进符合认识规律。\n   \n   符合知识体系的形成和发展规律，也符合人的认知的发展规律。\n   \n   人的认知从感性逐步上升到理性，由现象上升到本质，由具体上升到抽象，具有顺序性。\n\n\n# 如何做到循序渐进\n\n * 循序渐进就是先易后难。\n   \n   易就是基础知识，容易记住的内容；难就是易忘记的东西，复杂难懂的内容。\n   \n   难和易是相对的，每门知识的前后构成都遵循先易后难的规律。\n   \n   破易是攻难的台阶和武器，易是攻难的阶梯和突破口。\n\n * 先安排简单内容进行学习，可以逐步扫除畏难情绪，避免学习中的自我干扰，减少精神压力，容易取得完全成功，调动积极性。\n\n * 有时还需要频频回顾，以暂时的退步以求得扎实的学问。\n\n * 积累知识方面要养成严格循序渐进的习惯。\n\n注意事项：\n\n * 有的人读书性子急，一打开书就匆忙朝前赶。\n * 片面求快不符合读书的辩证法。\n * 前面的东西没有弄明白，切不要急于学后面的。\n * 切不要掩盖自己知识上的缺陷。\n * 注意一：不要隔断知识的连续性。\n * 注意二：紧紧把握住从“易”到难的过渡关。\n * 循序渐进缺点是趣味性差和学习见成效慢，特别是学习初期往往要靠毅力才能坚持下去。\n\n\n# 第21章 名人学习法：以名人为师，学习少走弯路\n\n\n# 第22章 读书先读序文法：书山有路“序”为径\n\n序文，凡例主要介绍该书的读者对象、主要内容及作者写书的缘由、意图、经过、体例等内容的文字。\n\n先读序文法实际上是一种探测性阅读，通常是指为了搜寻某种资料或确定读物是否具有阅读价值的阅读。\n\n\n# 第23章 sq3r五步读书法：读懂一本书，一步不能少\n\nsq3r即为：s—q—r—r—r，代表为”概览—提问—阅读—复述—温习”。这种读书法符合人们读书中的一般思维规律，有助于理解书本内容和增加个人记忆力。\n\n\n# 概览材料，获得大的印象\n\n第一步：surery，概览。可以用在整本书或一章中的一节，目的在于获得对整个材料的总体的把握。\n\n# 概览的内容如下\n\n * 开始浏览的这章或这节是怎样与全书的整个主题相配合的。（跟其他几章是同等关系，或者还是为后面的章节提供一个背景知识）\n * 翻看下这一章有多少页码，估计需要花多少时间。\n * 思考一下这章或这节分为几个主要部分或几个论题，是先把握整个主要观点呢？还是先完整地读完第一节。\n * 研究一下章名，转换为问题：这个题目谈的是什么意思；这方面我已经知道了哪些。\n * 观察思考主要标题，副标题，关键词，重要观点，思考材料是如何组织安排的。\n * 阅读下引言和小结，也可以是思考题。\n * 注意黑体字、斜体字的句子、短语或词汇。\n * 使用了哪些直观的呈现方式。\n * 可再次对所需花费时间做个进一步估计。\n\n\n# 提出问题，引发思考\n\n在开始正式阅读前，要有明确而简洁的问题，最好是写下来。\n\n明确三类问题：\n\n * 我已经知道了什么？\n\n * 作者想告诉我什么？\n\n * 我想要得到什么？\n\n\n# 带着审判的眼光阅读材料\n\n第三步，read，阅读材料。认真、积极而带着批判性的眼光阅读。\n\n带着之前准备的问题，来认真阅读寻找答案。可以细想所阅读材料的含义，思考可能的例外和矛盾之处，检验书中的假定等。\n\n方式方法如下：\n\n * 进行必要的快速阅读训练。\n * 再回过头来开始阅读一章或一节？？\n * 在阅读中要设法回答那些在浏览和提问阶段提出的最重要的问题。\n * 在第一次阅读的时候，不要停下来重读那些难懂的段落，要直接读下去，相当快地读，直到结束为止。\n\n\n# 复述材料，检阅阅读的效果\n\n第四步，recite，复述。可以给自己或学习伙伴重述或解释一下你所阅读的材料，也可以回答下自己早些时候提出的各类问题，最好大声地说出。（建议同伴交流材料）\n\n复述的方法：\n\n * 合拢书本。\n * 努力用自己的语言来举例说明回答最主要的问题。\n * 尽量运用图表、曲线或框图来直观表现手法。画图\n * 对自己要记住的要点抓住更多的细节。\n * 记下自己仍然要作出回答的问题或没有完全理解的概念。\n * 再次翻书，浏览检查我的答案及直观表现形式的理解程度和精确性，进一步来补充我需要作出回答的问题表。\n * 有必要的时候可以再次进行提问、阅读和复述。\n * 复述整章的要点和主要细节，准备两页纸。一张记录整章的内容分解成关键词模型。另一页纸收集一些不容易放在模型之中的具体材料。如公式、定义、统计数据、图表和问题等。\n * 寻找机会来复述或运用已学到的东西。\n\n\n# 适时温习材料，达到长期记忆\n\n方法如下：\n\n * 第一次复习应该在学习后立即就进行。通常应该花费几分钟。（在复述阶段之后休息5~10分钟之后进行，利用不断改进的关键字模型，能够对整章的内容作出概括，能够复述每一个要点和次要点，搜索与关键词模型相联系的细节）\n * 第二次复习应该在第二天就进行，也应该只花2~5分钟。\n * 第三次复习应该在一周之后进行，第四次复习应该在一个月之后。\n\n除了注意吸收他人经验，学习那些被公认的优秀方法外，还必须强调在学习实践中，根据个人的具体情况对它们进行改造与创新。\n\n\n# 第24章 筛选择优读书法：读一流的书，做一流的人\n\n\n# 第25章 薄厚互返法：厚从薄中来，薄到厚中去\n\n\n# 第26章 区别对待读书法：读书也要量体裁衣\n\n\n# 第27章 精读法：“全盘细化”一本书",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"《原子习惯》读书笔记",frontmatter:{title:"《原子习惯》读书笔记",date:"2022-01-20T11:48:45.000Z",permalink:"/pages/8a0e8c/",categories:["生活","学习方法"],tags:[null]},regularPath:"/04.%E7%94%9F%E6%B4%BB/01.%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/01.%E3%80%8A%E5%8E%9F%E5%AD%90%E4%B9%A0%E6%83%AF%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html",relativePath:"04.生活/01.学习方法/01.《原子习惯》读书笔记.md",key:"v-5e595d06",path:"/pages/8a0e8c/",headers:[{level:2,title:"微习惯的惊人力量",slug:"微习惯的惊人力量",normalizedTitle:"微习惯的惊人力量",charIndex:17},{level:2,title:"你的习惯如何塑造你的身份",slug:"你的习惯如何塑造你的身份",normalizedTitle:"你的习惯如何塑造你的身份",charIndex:1407},{level:2,title:"培养良好习惯的四步法",slug:"培养良好习惯的四步法",normalizedTitle:"培养良好习惯的四步法",charIndex:2260},{level:2,title:"看着不对劲的那个人",slug:"看着不对劲的那个人",normalizedTitle:"看着不对劲的那个人",charIndex:3720},{level:2,title:"培养新习惯的最佳方式",slug:"培养新习惯的最佳方式",normalizedTitle:"培养新习惯的最佳方式",charIndex:5096},{level:2,title:"原动力被高估，环境往往更重要",slug:"原动力被高估-环境往往更重要",normalizedTitle:"原动力被高估，环境往往更重要",charIndex:6312},{level:2,title:"自我控制的秘密",slug:"自我控制的秘密",normalizedTitle:"自我控制的秘密",charIndex:7580},{level:2,title:"怎样使习惯不可抗拒",slug:"怎样使习惯不可抗拒",normalizedTitle:"怎样使习惯不可抗拒",charIndex:8350},{level:2,title:"在习惯形成中亲友所起的作用",slug:"在习惯形成中亲友所起的作用",normalizedTitle:"在习惯形成中亲友所起的作用",charIndex:9414},{level:2,title:"如何找到并消除你坏习惯的根源",slug:"如何找到并消除你坏习惯的根源",normalizedTitle:"如何找到并消除你坏习惯的根源",charIndex:10936},{level:2,title:"慢步前行，但绝不后退",slug:"慢步前行-但绝不后退",normalizedTitle:"慢步前行，但绝不后退",charIndex:12642},{level:2,title:"最省力法则",slug:"最省力法则",normalizedTitle:"最省力法则",charIndex:14131},{level:2,title:"怎样利用两分钟规则停止拖延",slug:"怎样利用两分钟规则停止拖延",normalizedTitle:"怎样利用两分钟规则停止拖延",charIndex:15856},{level:2,title:"怎样让好习惯不可避免，坏习惯难以养成",slug:"怎样让好习惯不可避免-坏习惯难以养成",normalizedTitle:"怎样让好习惯不可避免，坏习惯难以养成",charIndex:18575},{level:2,title:"行为转变的基本准则",slug:"行为转变的基本准则",normalizedTitle:"行为转变的基本准则",charIndex:20080},{level:2,title:"怎样天天保持好习惯",slug:"怎样天天保持好习惯",normalizedTitle:"怎样天天保持好习惯",charIndex:22553},{level:2,title:"问责伙伴何以能改变一切",slug:"问责伙伴何以能改变一切",normalizedTitle:"问责伙伴何以能改变一切",charIndex:26155},{level:2,title:"揭秘天才(当基因重要和无关紧要时)",slug:"揭秘天才-当基因重要和无关紧要时",normalizedTitle:"揭秘天才(当基因重要和无关紧要时)",charIndex:27296},{level:2,title:"金发女孩准则：如何在生活和工作中保持充沛动力",slug:"金发女孩准则-如何在生活和工作中保持充沛动力",normalizedTitle:"金发女孩准则：如何在生活和工作中保持充沛动力",charIndex:30550},{level:2,title:"培养好习惯的负面影响",slug:"培养好习惯的负面影响",normalizedTitle:"培养好习惯的负面影响",charIndex:32906},{level:2,title:"获得持久成果的秘诀",slug:"获得持久成果的秘诀",normalizedTitle:"获得持久成果的秘诀",charIndex:35743},{level:2,title:"从四大定律中吸取的教训",slug:"从四大定律中吸取的教训",normalizedTitle:"从四大定律中吸取的教训",charIndex:36902}],headersStr:"微习惯的惊人力量 你的习惯如何塑造你的身份 培养良好习惯的四步法 看着不对劲的那个人 培养新习惯的最佳方式 原动力被高估，环境往往更重要 自我控制的秘密 怎样使习惯不可抗拒 在习惯形成中亲友所起的作用 如何找到并消除你坏习惯的根源 慢步前行，但绝不后退 最省力法则 怎样利用两分钟规则停止拖延 怎样让好习惯不可避免，坏习惯难以养成 行为转变的基本准则 怎样天天保持好习惯 问责伙伴何以能改变一切 揭秘天才(当基因重要和无关紧要时) 金发女孩准则：如何在生活和工作中保持充沛动力 培养好习惯的负面影响 获得持久成果的秘诀 从四大定律中吸取的教训",content:'# 《原子习惯》读书笔记\n\n\n# 微习惯的惊人力量\n\n 1.  人们总是容易高估某个决定性时刻的重要性，也很容易低估每天进行微小改进的价值。我们常常说服自己，大规模的成功需要大规模的行动。给自己定下目标，给自己施加压力，让自己努力做出一些人人都会谈论的惊天动地的改进。\n 2.  习惯是自我提高的复利。\n 3.  只要我们日复一日地重复1%的错误，亦即反复做出不良决策、重复微小的错误，以及为自己的小失误寻找借口，久而久之，我们的小选择会叠加成有害的结果。\n 4.  你的那些选择决定了你是谁和你可能是谁之间的不同。\n 5.  成功是日常习惯累积的产物，而不是一生仅有一次的重大转变的结果。\n 6.  你此时此刻是成就辉煌还是一事无成并不重要，重要的是你当前的习惯是否让你走上了通往成功的道路。你应该更关心你在当下前行的轨迹，而不是你已经取得了什么样的结果。\n 7.  正是像这样的小拼搏定义着你未来的自我。\n 8.  成功与失败之间的差距会随着时间的延续而不断扩大。\n 9.  你在培养习惯的过程中，有相当长时间是感受不到它的影响的，直到某一天，你突破了临界点，跨入新境界。\n 10. 在任何探索的早期和中期，通常都会有一个不如意的低谷区。你期望日新月异，收到立竿见影的效果，但让你感到沮丧的是，在最初的几天、几周甚至是几个月里，几乎看不到任何明显的变化。你觉得一切都是在白费功夫。这是任何复利过程的共同特征：最有力的结果总是姗姗来迟。\n 11. 熟练掌握某种技能需要足够的耐心。\n 12. 不积跬步无以至千里，每颗习惯的种子都来自单一的、微小的决定。\n 13. 戒除坏习惯犹如连根拔起我们内心枝繁叶茂的橡树，而培养良好习惯则像是每天不忘浇水，悉心培育一只娇嫩的鲜花。\n 14. 忘记目标，专注于体系。\n 15. 如果我们完全忽略了我们的目标，只关注我们的体系，我们还会成功吗？ 我想你会的。\n 16. 任何一项运动的终极目标都是争取获得最好的成绩，但是在整场比赛中都死盯着记分牌则荒谬无比。\n 17. 争取每天都有进步才是我们走向成功的唯一方法。\n 18. "比分会自理的"。\n 19. 如果我们想要更好的结果，那就别再紧盯着目标不放，而要把精力集中到我们的体系建设上。\n 20. "目标完全无用吗？"当然不是。目标的意义在于确定大方向，但体系会促进我们的进步。\n 21. 只有在实施了一点一滴、循序渐进地改进体系之后，我们才取得了不同寻常的结果。\n 22. 真正需要改变的是导致这些结果的体系。\n 23. 假如我们只是围绕这结果动脑筋想办法，我们只能取得一时的改进。为了取得一劳永逸的成效，我们需要解决体系层面上的问题。修正输入端，输出端就会自行修正。\n 24. 来日方长，要把快乐留待未来再享受。\n 25. 目标会导致"非此即彼"的冲突；你要么实现了预定目标，最终取得了成功，那么你失败了并令人大失所望。你在精神上把自己禁锢在一种狭隘的幸福观之中，这属于自我误导。\n 26. 成功之路不止一条，我们毫无必要认定只有某个特定场景的出现，才能让你对自己的人生感到满意。\n 27. 当我们爱上过程而不是结果时，我们不必等待容许自己享受快乐的那一刻的到来。只要我们创建的体系正在运行，我们就会在整个过程中感受到快乐。\n 28. 不求拔高你的目标，但求落实你的体系。\n 29. 关注整个体系，而非单一目标。\n\n\n# 你的习惯如何塑造你的身份\n\n 1.  改变习惯之举颇具挑战性，原因有两个：1) 我们没有找对试图改变的东西；2) 我们试图以错误的方式改变我们的习惯。\n\n 2.  最深入的层次是改变你的身份。这个层次是有关于改变你的信仰：你的世界观、你的自我形象，以及你对自己和他人所做的判断。你持有的大多数信念、假设和偏见都与这个层次相关。\n\n 3.  结果意味着你得到了什么，过程意味着你做什么，身份则关系到你的信仰。\n\n 4.  很多人开始改变他们的习惯时，把注意力集中在他们想要达到的目标上。这会导致我们养成基于最终结果的习惯。正确的做法是培养基于身份的习惯。借助于这种方式，我们的着眼点是我们希望成为什么样的人。\n\n 5.  内在激励的终极形式是习惯与你的身份融为一体。说我是想要这样的那种人是一回事，而说我本身就是这种人则是另外一回事。\n\n 6.  你越是以自己身份的某一方面为傲，你就越有动力保持与之相关的习惯。\n\n 7.  目标不是阅读一本书，而是成为读者。目标不是跑马拉松，而是成为跑步者。目标不是学习一种乐器，而是成为音乐家。\n\n 8.  在任何层面--- 个人、团队、社会 ---- 积极变革的最大障碍是身份冲突。理智上，你当然认为应该培养良好习惯，可当它们与你的身份冲突时，你将无法付诸行动。\n\n 9.  进步需要你不断地修饰你的信仰，提升和扩展你的身份。\n\n 10. 如果你投了几票给不良行为或沾染了毫无建树的习惯，这没多大关系。你的目标是赢得大多数时间。\n\n 11. 新的身份需要新的证据。\n\n 12. 这是一个简单的两步过程：\n     \n     决定你想成为哪种人。\n     \n     用小赢证明给自己看。\n\n 13. 重要的是让你的价值观、原型和身份驱动这个循环回路，而不是你的结果。重点应该始终是成为那种类型的人，而不是获得某种特定的结果。\n\n 14. 习惯至关重要的真正原因不是因为它们能带给你更好的结果(尽管它们能做到这一点)，而是因为它们能改变你对自己抱有的信念。\n\n\n# 培养良好习惯的四步法\n\n 1.  习惯是重复了足够多的次数后而变得自动化的行为。习惯形成的过程始于反复尝试。\n\n 2.  每当我们在生活中遇到新的情况，我们的大脑就要做出决定。会和实验的猫一样，大脑正忙于学习最有效的行动路径。在偶然发现一个意想不到的奖励后，我们便对今后的策略做出调整。\n\n 3.  这是人类全部行为背后的反馈回路：尝试、失败、学习，然后进行不同的尝试。经过一番练习，那些无用的动作逐渐消失，而有用的动作将得到加强。这就是正在形成的习惯。\n\n 4.  我们的习惯只是解决我们经常面临的问题和压力的一系列自动解决方案。\n\n 5.  随着习惯的形成，大脑的活跃程度逐渐降低。我们会学会锁定预示成功的线索，并忽略其他一切。当未来类似的情况出现时，我们就知道该寻找什么了。\n\n 6.  习惯形成后，我们的大脑会直接跳过试错环节，并创立一条心理规则：如果是这种情形，就用那种方式应对。\n\n 7.  习惯是从经验中学到的心理捷径。从某种意义上来说，习惯只是我们过去为解决问题而采取的步骤的记忆。大脑记忆过去的主要原因是为了预测如何更好地应对未来。\n\n 8.  因此，大脑总是努力确保我们集中注意力，关注当下最根本的问题。只要有可能，头脑会有意识地把一些任务交给无意识去自动完成。这正是习惯形成时会发生的情况。习惯减轻了认知负荷，释放了心智能力，从而让我们可以将注意力分配给其他任务。\n\n 9.  习惯不会限制自由，它们会创造自由。事实上，没有建立习惯的人往往享有最少的自由。只有让生活的基本要素变得更容易，我们才能创造自由思考和创造力所需的精神空间。\n\n 10. 当我们固有的习惯适时发挥作用，生活的基本状况都得到了妥善解决，我们的头脑可以自由地专注于新的挑战并掌握接下来的问题的解决方案。在当前养成习惯会让我们在未来做更多我们想做的事情。\n\n 11. 养成习惯的过程可以分为四个简单的步骤：提示、渴求、反应和奖励。\n\n 12. 总的来说，提示触发渴求，渴求激发反应，而反应则提供满足渴求的奖励，并最终与提示相关联。这四个步骤一起形成了一个神经反馈回路---- 提示、渴求、反应、奖励、提示、渴求、反应、奖励 ----并最终让我们养成自然而然的习惯，由此构成完整的习惯循环。\n\n 13. 我们可以将这四个步骤分为两个阶段：问题阶段和解决阶段。问题阶段包括提示和渴求，也就是当我们意识到有些事情需要改变的时候。解决方案阶段包括反应和奖励，也就是当我们采取行动并实现我们想要的改变的时候。\n\n 14. 如何培养好习惯\n     \n                如何培养好习惯\n     第一定律(提示)   让它显而易见\n     第二定律(渴求)   让它有吸引力\n     第三定律(反应)   让它简便易行\n     第四定律(奖励)   让它令人愉悦\n\n 15. 如何戒除坏习惯\n     \n                  如何戒除坏习惯\n     第一定律反用(提示)   使其无从显现\n     第二定律反用(渴求)   使其缺乏吸引力\n     第三定律反用(反应)   使其难以施行\n     第四定律反用(奖励)   使其令人厌烦\n\n 16. 每当我们想要改变自己的行为时，可以问自己几个简单的问题：\n     \n     如何才能让它变得明显？\n     \n     如何才能让它更有吸引力？\n     \n     如何才能让它变得容易？\n     \n     如何才能让它令人愉悦？\n\n\n# 看着不对劲的那个人\n\n 1.  人脑是一台预测机器。它不断地感知你的周边环境，并分析和处理它所遇到的所有信息。\n 2.  只要有足够的练习，我们就可以不假思索地拾起预测特定结果的提示。\n 3.  我们的大脑会自动编码经验教训。\n 4.  我们并不能解释清楚我们正在学什么，但是学习的过程一直没有停歇，而我们在特定情况下注意到相关线索的能力是我们每个习惯的基础。\n 5.  随着习惯的形成，我们的行为会受到我们的自发和下意识的头脑的支配。我们会身不由己地陷入旧的模式而不自知。\n 6.  由于触发我们习惯的提示实在是太普遍了。所以，我们必须有意识地开启行为转变的进程。\n 7.  一旦习惯在我们的生活中牢牢扎根，它多半是下意识的和自然而然的。\n 8.  心理学家卡尔.荣格说："除非我们让下意识意识化，否则它将支配我们的生活，而我们就会称之为命运"。\n 9.  这一过程被称为指差确认，是一套安全系统，旨在减少人为失误。它看起来有些傻，但是它的效果极佳。指差确认减少的错误高达85%，并让事故发生率降低了30%。\n 10. 指差确认之所有如此有效，是因为它把下意识的习惯提升到了有意识加以确认的水平。因为列车司机必须做到眼、手、嘴和耳朵并用，这样可以确保他们提前注意到事故隐患。\n 11. 一种行为的自动化程度越高，我们就越不可能有意识地去想它。我们一遍遍地做某些事，久而久之，我们就习以为常了，只是机械地重复着，根本不会对我们所做的是否正确提出任何质疑。\n 12. 我们表现失败的缘由大多归因于缺乏自我意识。\n 13. 我们在改变习惯方面面临的最大挑战之一，就是一直能保持警觉，知道我们实际上在做什么。这有助于解释为什么坏习惯的后果会如影随行，暗暗地影响着我们。\n 14. 我们的个人生活需要一个"指差确认"系统。这就是习惯记分卡的由来，这是一个简单的练习，我们可以用它来更好理解我们的行为。\n 15. "习惯记分卡"，我们先要列出我们的日常习惯，好习惯就旁边标注"+"；坏习惯就标注"-"；中性习惯就标注"="。\n 16. 我们对习惯的标注，取决于我们的处境和目标。时好时坏完全取决我们当时努力的方向。其实世上并没有好习惯和坏习惯的之分，只有有效的习惯。解决问题上很有效。\n 17. 在练习中，我们可以依照长远来看会带给我们哪些好处的标准给自己的习惯分门别类。总的来说，好习惯会带给积极效果；坏习惯则会带来负面结果。\n 18. 我们可以尝试常问自己的问题："这种行为是否有助于我们成为我们希望成为的那种人？这个习惯是支持还是反对我想要的身份？"\n 19. 记分卡的目标，在于提醒自己注意实际发生的事情，就这么简单。观察我们的思想和行为，不要急于做出判断或自我批评。也不要因为自己有缺点而责怪自己，也不要因为自己有所成就而自我褒奖。\n 20. 改变不良习惯的第一步就是对它们保持警觉。也可以在生活中进行指差确认，大声地说出自己想要采取的行动和我们预期的结果。听到自己大声说出的坏习惯的后果显得更加触手可及。这会增加坏习惯行动的难度。即使是只想提醒自己该办哪些事，这种方法也很有用。\n 21. 行为转变的过程总是始于自觉。像指差确认以及习惯评分卡这类做法会专门帮你认清我们的习惯并认识到触发它们的提示，这使得我们有可能对自己有益的方式做出回应。\n\n\n# 培养新习惯的最佳方式\n\n 1.  他们填写的句子被研究人员称为执行意图，亦即我们事先就何时何地行动制定的计划。\n 2.  总的来说，创立执行意图的格式是："当X情况出现时，我将执行Y反应。"\n 3.  执行意图都是确保我们不改初心的有效方法。他们提高了人们坚持物品回收、学习、早睡和戒烟等习惯的可能性。\n 4.  人们就何时、何地、具体做什么指定出具体计划后，就会更有可能贯彻执行。\n 5.  许多人认为他们缺乏做事的动力，但实际上他们真正缺乏的是明确的计划。\n 6.  何时何地采取行动并不总是显而易见的。有些人耗费一生都等不到自我提高的成熟时机。\n 7.  一旦设定了执行意图，我们就不必等待灵机一动的那一刻。当行动的时刻到来时，根本就不需要再做决定。简单地按照我们的预定计划去做即可。\n 8.  我们可以尝试在某周、某月或某年的头一天，这类时间段开始新计划，因为此时人们通常会满怀期望。只要我们有希望，我们就有足够的理由采取行动。新的开始总是让人感到欢欣鼓舞。\n 9.  设立执行意图，可以明确我们想要什么并且具体化我们实现的路径，将有助于我们摒弃妨碍我们进步、分散注意力，或让我们偏离正轨的事情。\n 10. 我们需要给我们的习惯在这个世界上存在的时间和空间。这样做的目的在于让时间和地点变得如此显而易见，以至于只要反复去做，积累到一定次数后，我们就会具备在恰当的时间做该做的事的冲劲，就连自己都不能解释为什么会这样。\n 11. 习惯叠加：颠覆我们的习惯的简单计划\n 12. 事实上，一次购买行为导致的连锁反应还有个名称：狄德罗效应。狄德罗效应指出，人们买入新用品后，往往会导致螺旋上升的消费行为，最终买入更多的东西。\n 13. 许多人类行为遵循这个循环。我们会根据刚刚完成的工作来决定下一步该做什么。每个动作都成为触发下一个行为的提示。\n 14. 当谈到培养新习惯时，我们可以充分利用行为的关联性。建立新习惯的最佳方法之一就是确定我们已有的习惯，然后把我们的新行为叠加在上面。这叫做习惯叠加。\n 15. 习惯叠加是执行意图的一种特殊形式。与其在特定的时间和地点培养新习惯，不如将它与当前的习惯整合。\n 16. 一旦我们适应了这种方式，就可以开发出通用的习惯叠加，一旦遇有恰当的时机就会发挥指导作用。健身：当我们看到楼梯时，会走楼梯而不是电梯。财务状况：当想买超过100美元的东西时，我会等24小时后再买。\n 17. 创建成功习惯叠加的秘诀是选择正确的提示来启动整个过程。与具体说明给定行为的时间和位置的执行意图不同，习惯叠加隐含着相应行动的时间和地点。\n 18. 在行动前先想想定在哪个时段最有可能成功。当我们忙于其他事情时，不要强求自己同时培养另一种习惯。\n 19. 行为转变的第一定律是让它显而易见。像执行意图和习惯叠加这样的策略，是为我们的习惯创造鲜明的提示，并为何时何地采取行动设计清晰计划的最实用的方法。\n\n\n# 原动力被高估，环境往往更重要\n\n 1.  你的习惯会根据我们所在的房间以及我们面前的提示而改变。\n 2.  环境是塑造人类行为的无形之手。\n 3.  尽管我们有独特的个性，但是在特定的环境条件下，某些行为往往会重复出现。在教堂里，人们倾向于低声说话。在黑暗的街道上，人们的警惕性会比较高，行事谨慎。\n 4.  最常见的变化形式并非源自内部，而是来自外部：我们被周围的世界所改造。\n 5.  每个习惯都以特定的环境为依托。\n 6.  行为是环境中人的函数，或者B(行为)=f(函数)[P(人)，E(环境)]。\n 7.  产品或服务越是触手可及，我们就越有可能去尝试。\n 8.  我们喜欢认为一切尽在自己掌握之中。\n 9.  我们每天采取的许多行动并不是由有目的的驱动和选择决定的，而是因为最得心应手。\n 10. 在人类中，感知是由感觉神经系统引导的。我们通过视觉、听觉、嗅觉、触觉和味觉来感知世界。\n 11. 视觉提示是我们行为的最大催化剂也是不足为奇了。\n 12. 我们可以想象一下在充满了富有成效的提示，无效提示一扫而光的环境中生活和工作是多么重要。\n 13. 每个习惯都是由提示引发的，我们更有可能注意到显眼的提示。\n 14. 如果我们想要让习惯成为我们生活中的重要组成部分，就让提示成为我们生活环境中的重要组成部分。\n 15. 确保我们的最佳选择匹配最鲜明的提示。当好习惯的提示一直在我们眼前晃，我们就会自然而然地做出正确的决定。\n 16. 环境设计的效用之所以强大，不仅是因为它影响了我们与世界的交往方式，也因为我们很少这样做。\n 17. 我们可以更改我们生活和工作的空间，以增加我们接触到积极提示的机遇，同时减少接触到消极提示的机会。\n 18. 环境设计让我们重新掌控自己，成为自身生活的建筑师。\n 19. 我们要争取成为自己的世界的设计师，而不仅仅是它的消费者。\n 20. 支配我们行为的不是我们的环境中的各类物品，而是我们与它们之间的关系。\n 21. 我们可以训练自己把特定的习惯和特定的环境联系起来。\n 22. 在全新的环境中习惯更容易改变。它有助于我们远离原有微妙的、促使我们恢复旧习惯的触发因素和提示。\n 23. 当我们走出平常的环境后，我们就会把我们的行为习惯遗留在原地。我们不再与旧环境中的提示做斗争，从而使得新习惯的形成过程不受干扰。\n 24. 如果想要创造性思维，就搬到一个更大的房间，去屋顶露台上待着，或者内部宽敞的大建筑物里。离开我们日常生活和工作，也就是与我们固有的思维模式联系密切的空间，换个环境放松一下。\n 25. 假如我们无法换个全新的环境的话，重新布置或重新安排我们现有的空间。为工作、学习、锻炼、娱乐和烹饪分别创造单独的空间。\n 26. 当工作和生活之间有了明确的分界线之后，我们就能很容易放下工作，转入身心放松的模式。\n 27. 尽可能避免将一种习惯的情景与另一种习惯的混在一起。一旦我们开始混合不同的情境，我们会把各种习惯混为一谈 ---- 那些比较容易实现的习惯通常会占上风。\n\n\n# 自我控制的秘密\n\n 1.  我们总是会这样认为，增强纪律性是解决我们所有问题的灵丹妙药，这种观念已经深深根值于我们的文化之中。\n 2.  科学家们对那些看起来有强大自控能力的人详细分析之后，发现他们和那些深陷泥潭的人没有什么不同。相反，"纪律性强"的人能更好地控制自己的生活，无须时常考验自己是否有坚强的意志力和自我控制的能力。\n 3.  换句话说，他们很少置身于充满诱惑的环境中。\n 4.  自我控制能力强的人通常最不需要使用它。\n 5.  假如你不需要经常自我克制的话，做起来就会更容易。\n 6.  毅力、勇气和意志力是取得成功的要素，但是增强这些品质的途径不是期望你自己成为一个自律的人，而是创造一个有纪律的环境。\n 7.  习惯一旦被编码，每当环境提示再次出现的时候，相关行动的冲动就会随之而来。\n 8.  坏习惯具有自身催化的能力：这个过程会自我滋养。它们一边激发人们的某些感觉，一边麻痹他们。这会成为恶性循环，坏习惯失去控制，接踵而来。\n 9.  我们可以改掉一个习惯，但是我们不太可能忘记它。\n 10. 说白了，我还未见过有人长期置身于消极环境中而能坚守积极的习惯。\n 11. 更可靠的方法是从源头上改掉坏习惯。消除坏习惯的最实用的方法之一就是避免接触引起它的提示。\n 12. 自我控制只是权宜之计，而非长远良策。\n 13. 我们能抵抗一两次诱惑，但是我们不可能每次都能铆足劲，克服强烈的愿望。\n 14. 与其每当我们想正确行事时都要诉诸于新的意志力，不如把精力花在优化你所处的环境上。这就是自我控制的奥秘。\n 15. 让良好习惯的提示显而易见，让不良习惯的提示脱离视线。\n 16. 自控能力强的人尽量远离充满诱惑的环境。逃避诱惑比抗拒诱惑容易。\n 17. 戒除坏习惯的最实用的方法之一就是减少接触导致坏习惯的提示。\n\n\n# 怎样使习惯不可抗拒\n\n 1.  在野外狩猎和觅食了数十万年之后，人类的大脑逐渐进化到高度重视盐、糖和脂肪的程度。当我们过着不知道何时能吃上下一顿饭的生活时，有机会就尽可能多吃当然是最佳生存策略。\n 2.  把盐、糖和脂肪放在第一位对我们的健康不再有利，但是这种渴望会持续下去，因为大脑的奖励中心已经有大约5万年没有改变了。\n 3.  具有动态对比特性的食物会一直让我们体验到新奇的有趣，鼓励我们多吃。\n 4.  面前的机会越有吸引力，养成的习惯的可能性就越大。\n 5.  这些就是我们所在的现代世界中的超常刺激。它们极度夸大了我们天然就有吸引力的那些特征，结果导致我们的本能痴迷癫狂，促使我们养成了过度消费的习惯、沉溺于社交媒体乃至色情、饮食等林林总总的习惯。\n 6.  如果想要提高某种行为发生的概率，那么我需要让它具备吸引力。\n 7.  习惯是多巴胺驱动的反馈回路。习惯带来的奖励会驱动分泌多巴胺，后续我们看到类似的提示就会分泌多巴胺，产生强烈的渴望，随着再次奖励的到来。反复就会加强我们行为才生习惯的原动力了，因为能产生多巴胺。\n 8.  多巴胺在许多神经过程中起着核心作用，其中包括行为动力、学习和记忆、惩罚和逃避以及随意运动。\n 9.  至于习惯，关键是：不仅发生在我们体验快乐的时候，而且在我们期待快乐的时候，都会分泌多巴胺。\n 10. 每当我们预测一个机会会有回报的时候，我们的体内的多巴胺浓度就会随着这种预期飚升。每当多巴胺浓度上升，我们采取行动的动机也会随之增强。\n 11. 激发我们采取行动的原动力来自于对建立的期待之时，而非这种期待得以满足的那一刻。\n 12. 对一种体验的期待往往比体验本身，更令人感到愉悦的原因之一。\n 13. 大脑将如此多的宝贵空间分配给负责渴求和欲望的区域，这一事实进一步证明了这些过程所发挥的关键作用。\n 14. 欲望是驱动行为的引擎，每一个行动都源于此前的预期，是渴望引发了回应。\n 15. 我们需要使我们的习惯变得有吸引力，因为最初促使我们采取行动的，正是我们对有奖励的经历的期待之心。这就是所谓绑定喜好战略开始发挥作用的地方。\n 16. 假如我们在做一件事的同时得以做另一件我们喜爱的事情，那么前面一件事很可能会对我们产生一定的吸引力。\n 17. 高频行为将会强化低频行为。\n 18. 喜好绑定其实是创建任何习惯的强化版本的方法之一，具体做法是将它与我们本想要的东西相关联。\n 19. 正是对奖励的期待，而不是奖励本身，促使我们采取行动。预期越高，多巴胺峰值越大。\n\n\n# 在习惯形成中亲友所起的作用\n\n 1.  "天才不是天生的，而是教育和训练出来的。"\n 2.  如果我们的文化崇尚什么样的习惯，那些习惯就会成为最有吸引力的行为。\n 3.  人类最深层的愿望之一就是有所归属。这种源远流长的嗜好对我们的现代行为是有着巨大的影响。\n 4.  我们早期的习惯不是选择而是模仿的产物。\n 5.  每一种文化和群体都有各自独特的期望和标准。\n 6.  "社会生活的习俗和实践裹挟着我们前行"。\n 7.  大多数时候，与群体共进共推并不会让人觉得是一种负担，因为每个人都想有所归属。\n 8.  但某种行为有助于我们融入团体或社会的时候，它就具备了吸引力。\n 9.  我们从自己身边的人那里学习习惯。\n 10. 一般来说，我们与他人越亲近，就越有可能模仿他们的一些习惯。\n 11. 培养好习惯的最有效方式之一就是加入一种文化，在这种文化中，我们偏爱的行为被认定为是正常行为。\n 12. 当我们看到别人每天都这样做的时候，会觉得培养新习惯似乎并不难。\n 13. 我们的文化设定了我们对"正常"事物的期望。尽量和那些具备我们想要拥有的习惯的人在一起，我们会相互促进。\n 14. 加入一种文化，其间我们喜好的行为是正常的行为；我们已经和这个群体有一些共同之处。\n 15. 没有什么比群体归属感更能维持一个人做事的动力了。它将个体的追求转变成了群体的追求。\n 16. 但我们加入了书友会、乐队或自行车爱好者团队时，我们的身份就会与周围的人建立了关联，成长和改变不再是个体的追求。\n 17. 集体身份开始强化我们的个人身份，这就是为什么在达成目标后还要保持团队一员身份对保持我们的习惯至关重要。\n 18. 友情和社区赋予人特定的身份并帮助一种行为长期持续。\n 19. 每个群体都对其成员施加巨大压力，要求他们服从集体规范。\n 20. 大多数时候，我们宁愿跟着众人一起犯错，也不愿特立独行坚持真理。\n 21. 人类的头脑知道如何与他人和平相处。它想和别人和平共处。这是我们生活在这个世界上的天然模式。\n 22. 你可以忽略它 ---- 你可以选择忽略这个群体或者不再关注他人的想法 ---- 但是这需要付出努力。\n 23. 逆主流而上需要付出额外的努力。\n 24. 当改变习惯意味着挑战部落规矩时，改变就没有吸引力的。\n 25. 当改变我们的习惯意味着顺应这个部落的要求时，改变是非常有吸引力的。\n 26. 人人都追求权力、声望和地位。我们渴望在外衣上别着奖章。我们期望自己有总裁或合伙人的头衔。我们希望得到认可、称赞和表彰。\n 27. 从历史上来看，一个人拥有更大权力和更高的地位意味着可以获得更多资源，不再过多担忧能否生存下去，并且能够更容易找到性伴侣。\n 28. 我们被那些能赢得尊重、认可、钦佩和地位的行为深深吸引着。\n 29. 这是我们如此关心高效人士的习惯的一大原因。我们视图模仿成功人士的行为，因为我们自己渴望成功。我们的许多日常习惯都来自于我们模仿崇拜的对象。\n 30. 身居高位的人尽情享受他人的认可、尊重和赞扬。这意味着，如果一种行为能为我们赢得认可、尊重和赞扬，我们就会认为它很有吸引力。\n 31. 我们也会尽可能避免会降低我们地位的习惯行为。我们一直想知道"别人会怎么看我"并根据答案相应地改变我们的行为。\n 32. 我们倾向于培养被我们的文化推崇的习惯，因为我们强烈地渴望融入并属于这个部落。\n 33. 我们倾向于模仿三个社会群体的习惯：亲近的人(家人和朋友)、所在群体(我们所归属的部落)和有权势的人(有地位和威望的)。\n 34. 如果一个行为能为我们赢得认可、尊重和赞扬，我们就会认为它很有吸引力。\n\n\n# 如何找到并消除你坏习惯的根源\n\n 1.  每个行为都有表层的渴望和深层的动机。\n 2.  渴望是深层动机的具体表现。\n 3.  在更深层次上，我们只是想减少不确定性和缓解焦虑，赢得社会认可和接纳，或者获得一定的社会地位。\n 4.  我们的习惯其实是用以满足古老欲望的现代方法，如果这是坏习惯，那么也就是旧恶习的新形式。\n 5.  你目前的习惯不一定是解决你面临的问题的最佳方式；它们只是我们掌握的方法。一旦我们把一个解决方案和我们需要解决的问题联系起来，我们就会不断地反复加以应用。\n 6.  习惯就是关联。这些关联决定了我们是否值得不断重复某种习惯。\n 7.  生活让人感觉是在被动应对，但实际上都是可预见的。你整体都在根据你刚刚看到的和以往的经验，预测出下一步最佳的应对行动，我们没完没了地预测下一刻会发生什么。\n 8.  我们的行为在很大程度上取决于我们如何解释与我们相关的事件，而未必是事件本身的客观事实。\n 9.  感觉和情绪将我们察觉的提示以及我们的预测，转化为我们可以加以应用的信号。\n 10. 渴望是一种缺乏某些东西的感觉，是改变我们内在状态的愿望。\n 11. 欲望就是我们的现状与我们设想中的未来状况之间的差别。\n 12. 我们的感觉和情绪告诉我们是安于现状还是改变现状。\n 13. 总而言之，我们感受到的特定欲望和我们展现出的高频动作，其实都体现我们深藏的、蠢蠢欲动的根本动机。\n 14. 每当一个习惯成功地满足了一个动机，我们就会产生一种再次尝试的渴望。\n 15. 我们将习惯与积极的情感联系起来时，习惯就有了吸引力，我们有了这种认识就可以为己所用，不断寻找乐趣，同时避免烦心事。\n 16. 如果我们能学会将高难度的习惯与积极的内心体验联系起来，我们就能使它们具备吸引力。\n 17. 只需换一个词，我们就可以改变看待每个事件的方式，从而将这些行为视为负担转变为视它们为机遇。\n 18. 我没有被困在轮椅上 ---- 我被它解放了。要不是因为我的轮椅，我就只能躺在床上，根本不能到户外活动。\n 19. 重建我们的习惯，突出它们的益处而非不足，这种短平快的方式可以改变我们的思维方式，并让一个习惯更有吸引力。\n 20. 我们可以将锻炼看作是培养技能和增强体质的途径。别再对自己说"我需要一早去跑步"，而要说"是时候增强我的耐力、加快跑步速度了"。\n 21. 我们可以将省钱与自由而不是限制相联系：生活在我们目前的收入水平之下会让我们未来的生活宽裕。我们在这个月省下的钱会提高我们下个月的购买力。\n 22. 每一次冥想的中断，都可以给我们一个练习呼吸的机会，我们可以将沮丧转化为喜悦。\n 23. 我们可以将"很紧张"定义为"很兴奋"，肾上腺素的增加会帮助我们集中注意力。\n 24. 我们可以创建一种激励的仪式。只需练习把我们的习惯和我们喜欢的东西练习起来，然后无论何时我们需要多一点动力，就可以启用这个提示。\n 25. 找到让我们真正开心的事，如抚摸我们的狗或洗个泡泡浴，然后在做我们喜欢的事情之前，创建一个我们每次都要做的简短的例行程序。也许我们可以做三次深呼吸，然后微笑一下。\n 26. 最后，我们会把这种深呼吸+微笑的例行模式与心情愉快联系起来。它成为一个提示，意味着我们感觉到快乐。一旦确立了这种关联，我们可以在任何需要改变情绪状态时加以应用。\n 27. 工作压力太大？做三次深呼吸，然后微笑。生活不如意？做三次深呼吸，然后微笑。一旦养成了习惯，相关的提示会引发渴望，即使它与最初的情形毫无关系。\n 28. 找到坏习惯形成的源头并予以根除的关键是重新构建我们对坏习惯的关联。这并不容易，但是如果我们能重新编程我们的预测，我们就能把令人望而却步的习惯变为有吸引力的习惯。\n 29. 好习惯与积极的情感联系起来，而让坏习惯与讨厌的情感联系起来。这样就能为己所用，释放心智。\n 30. 强调坏习惯所带来的坏处，让坏习惯不再有吸引力。\n 31. 当我们将习惯与积极的感受相联系，习惯就有了吸引力；反之，则没有吸引力。在开始培养难度较大的习惯之前，先做些我们喜欢的事情来创造一种激励仪式。\n\n\n# 慢步前行，但绝不后退\n\n 1.  所有的优秀作品都出自数量组的学生之手。\n 2.  试图找到最佳转变方案的努力，比如试图寻找减肥捷径、强身健体的最优方案，以及开展副业的好点子等，很容易陷入困境。我们这是在一门心思地要找到做事的最佳方式，却从来不付诸于行动。\n 3.  结果是"因追求最佳而丢掉了足够好"。\n 4.  酝酿与行动是有区别的。酝酿意味着我们只是在计划、策划和学习。这是都是好东西，但是它们不会产生结果。\n 5.  采取行动才是会产生结果的行为类型。有时候酝酿也是有益的，但是它本身永远不会产生结果。\n 6.  我们为何还要去酝酿呢？有时候我们这样做是因为我们确实需要计划或了解更多情况。但通常，我们这样做的理由是它可以让我们感觉自己在取得进展，同时又不必承担失败的风险。\n 7.  我们大多数人都是回避批评的专家。遭遇失败，或被公开批评令人感觉不好，所以我们倾向于避免落入那种境地。这就是我们总是在酝酿却不采取行动的最大原因：我们是想让可能遭遇的失败来得晚一些。\n 8.  酝酿让你感觉自己正在做事。但实际上，我们只是在准备做事。\n 9.  当准备工作变成某种形式的拖延时，我们需要有所改变。我们不想只是一味地做计划，要真刀真枪地操作起来。\n 10. 如果我们想掌握一门习惯，关键是从重复开始，无须力求完美。我们也不必描画出新习惯的每一个特征。我只需要的是不断练习。\n 11. 这是第三定律的第一条要点：我们需要关注的是次数。\n 12. 习惯的形成是一种行为通过重复变得越来越自动化的过程。\n 13. 我们重复活动得越多，我们的大脑结构变化得也就越多，从而能更高效地进行那项活动。\n 14. 随着每一次重复，细胞间的信号传递得到改善，神经连接变得更加紧密。\n 15. 重复一个动作会导致大脑明显的生理变化。\n 16. 人们在学习一门外语、演奏一种乐器或演练尚不熟悉的动作时，会感动难度极大，因为每种感觉必经的通道尚未建立起来；但是，连续不断地重复打通了沟通途径之后，这种困难顿时烟消云散；这些动作变得如此连贯自然，即便心不在焉也能一气呵成。\n 17. 常识和科学证据都认同这一点：重复是一种变化形式。\n 18. 我们每次重复一个动作，也就激活了一个与这个习惯相关的特定神经回路。\n 19. 这意味着，我们养成新习惯的最关键步骤之一就是不断地重复。\n 20. 所有习惯都遵循类似的演变轨迹，从刻苦练习到行动自如，这一过程被称为自动性。\n 21. 自动性是指无须考虑每一个步骤而实施一种行为的能力，这种能力发生在下意识的时候起作用。\n 22. 科学家称之为学习曲线的这些图表上的形状，揭示了行为转变的一个重要事实：习惯是基于频率而不是时长形成的。\n 23. 就习惯的培养而言，不在于时间长短。重要的是我们这种行为的频率。\n 24. 我们的习惯已经在重复了数百次(如果不是数千次)之后被内化了。\n 25. 要养成新习惯需要同样的频率。我们需要把足够多的成功尝试串联起来，直到这种行为牢牢地嵌入我们的头脑中，使得我们超越了那条习惯线。\n 26. 在现实生活中，需要多久才能实现习惯成自然并不重要。重要的是我们要采取我们需要采取的行动以取得进步。一个动作是否完全自动并不重要。\n 27. 行为转变的第三定律是让它简便易行，也就是为了增加频率。\n 28. 最有效的学习形式是付诸实践，而不是纸上谈兵。\n 29. 专注于采取行动，而不只是酝酿行动。\n 30. 习惯的形成是一个行为通过重复逐渐变得更加自动化的过程。\n 31. 习惯的培养不在于时间长短，而在于重复的次数。\n\n\n# 最省力法则\n\n 1.  传统智慧认为动机是习惯转变的关键。也许真是这样，就是说假如你真的想要，你就真的会去做。但事实是，我们真正的动机是贪图安逸，怎么省事怎么做。\n 2.  不管最新出炉的提高生产率方面的畅销书怎么说，图省事才是一个聪明而非愚蠢的策略。\n 3.  精力是宝贵的，而大脑的设定就是尽一切可能保存精力。\n 4.  人类的天性就是遵循最省力法则：当在两种相似的选项之间做决定时，人们自然会倾向于需要最小工作量的那一个。\n 5.  在我们可能采取的所有行动中，最终被选择的行动一定是能以最小的努力获得最大价值的那一个。\n 6.  我们被激励着避重就轻，只做容易的事。\n 7.  每个动作都需要消耗一定的能量。所需能量越多，发生的可能性就越小。习惯需要的能力越少，它发生的可能性就越大。\n 8.  看看任何占据你生活大部分时间的行为，你会发现他们都简单易行，不需要有多大的激励。\n 9.  从某种意义上来说，每个习惯都妨碍着你获得真正想要的东西。坚持节食的目的能带来健身的效果，坚持冥想能感到平静，坚持写日记能得到思路清晰。\n 10. 实际上习惯本身并不是你想要的。你真正想要的是习惯带来的结果。\n 11. 障碍越大 ---- 也就是说，习惯坚持起来的越难 ---- 你和你想要的最终状态之间的阻力就越大。\n 12. 这就是为什么要让你的习惯变得简单至极，只有这样才能让你即使不喜欢它，也会坚持做。如果你能让好习惯简便易行，你就越有可能坚持下去。\n 13. 有些时候我们会逆流而上，另一些时间我们只想急流勇退。\n 14. 在那些艰难的日子里，让尽可能多的事情对你有利是至关重要的，这样你就能克服生活中遇到的不可避免的难处。\n 15. 你面对的阻力越小，你坚强的一面就越有可能浮现出来。\n 16. 让它简便易行的说法不仅仅是做容易的事，其主旨是尽可能确保你可以毫不费力地去做具有长期回报的事。\n 17. 与其劳神费力地克服生活中的阻力，不如设法减小阻力。\n 18. 在你设法减小由你的习惯产生的阻力时，最有效的方法之一就是进行环境设计。\n 19. 利用环境设计让提示更显而易见，我们也可以优化我们的环境使得我们更容易行动。\n 20. 或许更有效的方法是减少家里或办公室内部的呈现的阻力。我们完全可以清除妨碍我们办正事的阻力点。\n 21. 日本公司强调为人所知的"精益生产"理念，坚持不懈地努力寻求从生产流程中去除各种浪费，直至重新设计工作环境，使得工人们的身体不必转来转去，从而避免为拿工具而浪费时间。\n 22. 当我们消除我们时间和精力的阻力点时，我们就能够取得事半功倍的效果。\n 23. 这是整理房间让人感觉非常好的一个原因：我们减轻了环境施予我们的认知负荷，从此可以轻装前进了。\n 24. 商业上的追求永无止境，总是以更简便的方式提供同样的结果。\n 25. 归根结底，为了养成更好的习惯，我们不得不耗费很大精力，设法克服与我们已有的好习惯相关的惰性，同时加大与不良习惯相关的阻力。\n 26. 每当你整理一个空间以满足其预期用途时，你都是再启动该动机，使得接下来的动作简单易行。\n 27. 一件事做起来越麻烦，你就越不可能想要继续做。 利用到坏习惯上。\n 28. 实际上我们只需要稍微增加一些难度，人们就会停止不必要的行为。\n 29. 假如把啤酒藏到冰箱最里面很难一眼就看到的地方，我们就喝得就少了。当我们从手机上删除社交媒体应用程序后，可能需要几周才会再次下载并登陆。这些小手段不太可能遏制真正的上瘾，但对我们中的许多人来说，增加一点点坏习惯的难度，可能就意味着更容易养成好习惯。\n 30. 我们都应该问自己同样的问题："我们该怎么设计一个让人们的行为易于端正的世界?"重新设计你的生活，让对你来说最重要的事成为最容易做的事。\n 31. 人类行为遵循最省力的法则。我们天然地倾向于付出最少的工作量的选择。\n 32. 创造一个环境，尽可能让人们便于做正确的事。\n 33. 降低与良好行为相关的阻力。阻力小，习惯就容易养成。\n 34. 增加与不良行为相关的阻力。阻力大，习惯就难以养成。\n 35. 预备好你的环境，使未来的行动更容易。\n\n\n# 怎样利用两分钟规则停止拖延\n\n 1.  "这是个极其简单的动作，但是每天早上都以同样的方式去做，就成了习惯性动作 --使它可以重复，易于做到。它减少了我偷懒或以不同方式做它的机会。它不过是在我的日常行为库里有加了一项，同时减少了一件需要想起来才会做的事。"\n\n 2.  研究人员估计，我们每天的行动有40%至50%都出自习惯。这已经占比很高了，但是你的习惯施加的真正影响远大于这些数字所显示的。习惯属于自动选择，会影响到随后经深思熟虑做出的决定。\n\n 3.  习惯就像高速公路的入口匝道。它们引导你走上一条路，使你在不知不觉中加速前进，直到走上正路。\n\n 4.  持续做你已经在做的事似乎比重新开始做不同的事要容易得多。\n\n 5.  你会坚持看完长达两小时的烂电影。即使你已经吃饱了，你依旧不停地吃零食。你本想着就玩"一小会儿"手机，结果是20分钟很快就过去了，你仍然盯着手机屏幕。就这样，你不假思索的习惯往往左右着你有意做出的选择。\n\n 6.  每天都有几个时刻产生巨大的影响。我把这些小选择称为决定性时刻。你决定叫外卖或者在家自己做晚餐的那一刻，你决定开车或骑自行车的那一刻，你决定开始做家庭作业或者拿起电子游戏控制器的那一刻：这些选择就是生活之路上的岔路口。\n\n 7.  决定性时刻为你未来的设定了选择。例如，走进餐馆是一个决定性的时刻，因为它决定了你午餐吃什么。\n\n 8.  我们受到自身特有的习惯带来的限制。这就是掌握一天中决定性时刻如此重要的原因。每一天都由许多时刻组成，但真正决定你一天行为的是你的一些习惯性选择。这些小选择累积起来，每一个都为你如何度过下一段时间设定了轨迹。\n\n 9.  习惯是切入点，而不是终点。它们是出租车，而不是健身房。\n\n 10. 即使你知道应该从小处着眼，第一步也很容易迈得太大。当你梦想做出改变时，你会抑制不住地异常兴奋，一时头脑发热就容易贪多嚼不烂。我所知道的对抗这种趋势的最有效的方法是使用两分钟规则，也就是："当你开始培养一种新习惯的时候，它所用时间不应超过两分钟"。\n\n 11. 你会发现几乎任何习惯都可以缩减为两分钟的版本：\n     \n     * "每晚睡前阅读"变为"读一页"。\n     * "做30分钟瑜伽"变成"拿出我的瑜伽垫"。\n     * "复习功课"变成"打开我的笔记"。\n     * "整理衣物"变成"折叠一双袜子"。\n     * "跑3英里"变成"系好我的跑步鞋带"。\n\n 12. 这样做的思路是让你的习惯尽可能容易开始。\n\n 13. 任何人都可以沉思一分钟，读一页书，或者收好一件衣服。正如我们刚刚讨论的，这是一个强大的战略，因为一旦你开始做正确的事情，继续做下去会容易得多。\n\n 14. 一个新习惯不应该让人觉得是一个挑战。接下来的行动可能具有挑战性，但是最初两分钟应该不难。你想要的是一种"门户习惯"，它自然会引导你走上更有成效的道路。\n\n 15. 人们经常认为，读一页书、冥想一分钟或打个销售电话都是小事一桩，没什么可大惊小怪的。\n\n 16. 但此处的重点不是做一件事，而是把握住萌芽的习惯。事实上，你首先要确立一种习惯，然后才能不断改进它。\n\n 17. 如果你掌控不好养护习惯幼苗的基本技能，那么你就不大可能把握好与之相关的细节。\n\n 18. 不要指望从一开始就培养一种完美的习惯，要脚踏实地，连续不断地做些简单的事。你必须先标准化，然后才能优化。\n\n 19. 一旦你学会了呵护习惯的幼苗，前两分钟只是启动正式程序的仪式而已。\n\n 20. 这并非是为了更容易培养习惯而删繁就简的举动，而是掌握一项困难技能的理想方式。\n\n 21. 一种程序的开始阶段越是有仪式化，你就越有可能实现注意力高度集中，即做大事必需的状态。\n\n 22. 通过在每次锻炼前做同样的热身，你会更容易进入最佳状态。通过遵循同样的创造性仪式，你可以更容易地投入到艰难的创造性工作中。\n\n 23. 通过养成定时关灯的习惯，你可以更容易地在每晚合理的时点上床睡觉。你可能无法使整个过程自动化，但你可以让第一动作变成下意识动作。万事开头难，但要是把开始变得简便易行，接下来的事也就水到渠成了。\n\n 24. 我的一位读者用这种策略减肥，最终成功减去了100多磅。一开始，他每天都去健身房，但是他告诉自己，他健身的时间不能超过5分钟。他回去健身房，锻炼5分钟，时间一到就立刻离开。就这样过了几个星期后，他环顾四周，心想："嗯，反正我总是来这里。我不妨多待一会儿。"几年后，他的体重正常了。\n\n 25. 写日记提供了另一个例证。几乎每个人都能因写出自己的想法而受益，但是大多数人在写了几天后就坚持不下去了，或者根本就不写日记，觉得它就是件烦心事。\n\n 26. 坚持写日记的秘诀是永远别把它变成不得不做的工作。\n\n 27. 来自英国的领导力顾问格雷戈.麦吉沃恩养成了每天写日记的习惯，他的具体做法不是写出自己的全部想法，而是适可而止。他总是在感觉写烦了之前及时收笔。\n\n 28. 像这样的策略也有另一个原因：它们强化着你想要建立的身份。如果你连续五天现身健身房，哪怕只在那里停留两分钟，你就是在为你的新身份投赞同票。\n\n 29. 你去健身房的出发点不是要有副好身材，而是专注于成为那种不会错过健身的人。你只是采取小小的行动，但它确认着你想成为的那种人。\n\n 30. 我们很少考虑以这种方式审视改变，因为每个人都心无旁骛地盯着最终目标。\n\n 31. 在某个时候，一旦你养成了习惯，并且每天都有所表现，你就可以将两分钟规则和我们所称的习惯塑造的技术结合起来，将你要培养的习惯向最终目标扩展。\n\n 32. 从掌握最小行为的前两分钟开始。然后，向中间阶段推进，并重复这个过程---- 只关注前两分钟，一定要在这个阶段做扎实，然后再继续进入下一阶段。\n\n 33. 最终，你会养成你原本希望养成的习惯，同时仍然把注意力放在应该的地方：行为的前两分钟。\n\n 34. 每当你努力要保持一个习惯时，你都可以采用两分钟规则。这是让你的习惯变得简单的方法。\n\n 35. 习惯可以在几秒钟内完成，但会持续影响你在接下来的几分钟或几个小时的行为。\n\n 36. 许多习惯发生在决定性的时刻，每时每刻的选择就像岔路口，你的选择最终会导致卓有成效，或者一事无成的一天。\n\n 37. 两分钟规则规定："当你开始培养一种新习惯时，它所用时间不应超过两分钟"。\n\n 38. 一种程序的开始阶段越是仪式化，你就越有可能进入做大事所学的注意力高度集中的状态。\n\n 39. 习惯优化前先要实现标准化。你不能改善一个不存在的习惯。\n\n\n# 怎样让好习惯不可避免，坏习惯难以养成\n\n 1.  1830年冬天期间，由于没有适合外出的衣服，他一直待在书房里奋笔疾书。《巴黎圣母院》于1831年1月14日提前两周出版。\n\n 2.  成功不是简单地让好习惯简便易行，更重要的是让坏习惯难以延续。\n\n 3.  承诺机制是指你当下的抉择左右着你未来的行动。这是一种锁定未来行为，约束你养成良好习惯、迫使你远离不良习惯的方法。\n\n 4.  承诺机制是有用的，因为它们能让你在成为诱惑的受害者之前，先让良好的意愿发挥作用。举例来说，每当我想减少卡路里的时候，我都会让服务员在给我上饭菜之前就分成两份，其中一份打包带走。如果我一直等到饭菜端上来之后，再告诫自己"只吃一半"的话，那就太晚了。\n\n 5.  承诺机制实则使得坏习惯在当前变得难以施行，从而提高了你未来做该做的事的可能性。\n\n 6.  破除坏习惯的最好方式就是让它变得不切实际。不断提高其难度，直到你心灰意冷。\n\n 7.  只需做一次的行动，锁定好习惯\n     \n     下面的内容，只需要你做一次，就能有很好的效果。能够让你更容易睡好觉，吃得健康，做事效率高，省钱，而且通常能生活得更好。\n     \n     营养                 幸福\n     购买饮用水过滤器           养只狗\n     用小盘子吃饭，减少热量摄入      搬家到待人友好的社区\n     睡眠                 一般性健康\n     买好床垫               打疫苗\n     挂上深色窗帘             买好鞋，避免背痛\n     把电视机移出卧室           买把支撑椅或站立式桌子\n     生产力                财务\n     取消邮件订阅             加入自动储蓄计划\n     关闭消息提醒并设置群聊静音      设置自动支付账单\n     把你的手机设置为静音         取消有线电视\n     使用邮件过滤器清理你的收件箱     要求服务提供商降低你的费用\n     删除你手机上的游戏和社交媒体账号   \n\n 8.  让技术为我们服务，使得好习惯召之即来并戒除坏习惯。\n\n 9.  技术可以将以前艰难、恼人和复杂的行为转变成容易、轻松和简单的行为。它是确保正确行为的最可靠和最有效的方法。\n\n 10. 尽可能利用技术让你的生活自动化之后，你就能把腾出来的一些时间和精力用在技术还帮不上的地方。\n\n 11. 我们让技术介入的每个习惯都会释放一些时间和精力，可被投入下一个发展阶段。\n\n 12. 自动化的缺点是，我们满足于一件接一件地做些不用费脑子的事，再也不想抽时间做些稍有难度但最终更有意义的事。\n\n 13. 我们在工作之余总是禁不住要浏览社交媒体上的内容。只要感觉有些无聊，就会拿起手机。人们很容易自我安慰地说这些做法"只是放松一下"而已，但是随着时间的推移，它们会累积成一个严重的问题。持续不断的"再过一分钟"会妨碍我做任何重要的事。\n\n 14. 一旦我们的坏习惯难以持续，我们就会发现自己确实更想去做有意义的事，在我们把精神糖果从我们的环境中移除之后，吃健康食品就变得容易多了。\n\n 15. 通过利用承诺机制、战略性的一次性决策，以及技术手段，我们可以创造一个使自己无法回避的环境 ---- 在这个空间里，好习惯不仅是我们期待的结果，也是几乎不可避免的结果。\n\n 16. 锁定未来行为的终极途径是自动化你的习惯。\n\n 17. 使用技术自动化你的习惯是保证正确行为的最可靠和有效的途径。\n\n\n# 行为转变的基本准则\n\n 1.  问题不在于是否有意识，而是能否一直坚持。\n 2.  "一般来说，人们更愿意使用能带来强烈感官愉悦的产品，比如散发着薄荷香型的牙膏"\n 3.  一旦我们体验到做一件事所享有的乐趣，就很可能愿意重复去做这件事，这完全合乎逻辑。\n 4.  即使是用香皂吸收这种小事，人们体验到了闻起来很香，丰富的泡沫令人赏心悦目，由此产生的快乐感觉会给大脑发送信号："这感觉很好，继续这么做。"\n 5.  反之，如果我们体验的是不愉快，就肯定不想继续做。\n 6.  类似这样的故事证明了行为转变的基本规律：重复有回报的行为；避免受惩罚的动作。\n 7.  你会根据你过去所得到的的奖励(或受到的惩罚)学习将来该怎么做。积极的情绪有益于培养习惯，消极情绪则会摧毁它们。\n 8.  行为转变的前三条定律 ----让它显而易见；让它有吸引力；让它简便易行 ---- 增加了当下这种行为发生的概率。行为转变的第四条定律 ---- 让它令人愉悦 ----提高了下次重复这种行为的可能性。它形成了完整的习惯循环。\n 9.  我不只是在寻找满足感，我们要的是即时满足感。\n 10. 我们生活在科学家称之为延迟回报的环境中，因为你要工作很多年后才能看到预期的回报。\n 11. 人类的大脑并没有一直在延迟回报的环境中进化。近年来，整个世界发生了天翻地覆的变化，但是人性的变化微乎其微。\n 12. 重视即时满足是有道理的，活在当下，何必要杞人忧天。在即时回报环境中生活了成千上万代之后，我们的大脑进化成偏爱快速回报而不是长期回报。\n 13. 就不良习惯而言，即时结果通常感觉良好，但最终结果却不好。就好习惯而言，情况正好相反：即时结果令人不愉快，但是最终结果的感觉却很好。\n 14. "几乎总是发生这样的情况，当即时后果有利时，后来的后果将是灾难性的，反之亦然.....习惯的第一个果实越甜，以后的果实就越苦。"\n 15. 我们要在当下为良好习惯付出代价；否则我们要在将来为坏习惯付出代价。\n 16. 一般来说，我们从一项行动中越快享受到乐趣，我们就越应该质疑它是否符合我们的长远利益。\n 17. 行为转变的基本规则也可以更新为：重复有即时回报的行为，避免受即时惩罚的动作。\n 18. 我们对即时满足感的偏好揭示了一个关于成功的重要事实：因为我们天性如此，大多数人整体都在寻求及时享乐的机会。人们倾向于选择即时享乐的事，回避延迟满足的事。如果你愿意等待回报的到来，我们将面临更少的竞争，通常会获得更大的回报。能坚持到取得最后胜利的人终究是少数。\n 19. 善于延迟满足的人高考分数较高，不太可能沾染毒品，肥胖的可能性更低，能更好地应对压力，社交技能也更强。\n 20. 在某个时候，几乎每个领域的成功都要求你忽略即时奖励，而代之以延迟奖励。\n 21. 延迟满足的习惯是能训练出来的，但在我们这样做时需要顺应人性，而不是与之对抗。在训练延迟满足的过程中，凡是长远地看能带给我们回报的事，我们可以给它添加一点即时快乐；凡是不能的，我们可以添加一点即时痛苦。\n 22. 保持习惯的关键是要有成就感，哪怕只是细微的感受。成就感是一个信号，它表明我们的习惯有了回报，我们为此付出的努力是值得的。\n 23. 在现实生活中，只有在好习惯让我们尝到了一些甜头后，我们才会觉得它有价值。在它的形成阶段，我们一直是在做出牺牲。\n 24. 我们去过几次健身房，但我们并没有立刻变得强壮、健康或跑得更快，至少没有任何可见的改观。只有在几个月之后，我们的体重减掉了几磅或我们的手臂肌肉突起，从此我们就有了锻炼身体的积极性，更愿意去健身。\n 25. 开始的时候，我们需要一个坚持下去的理由。这就是为什么说即时奖励是必不可少的。它们维持着我们的兴奋点，而延迟奖励则在不动声色地逐渐累积。\n 26. 我们希望对习惯的结局存在好感。对此，最佳的方式是利用增强法，也就是利用即时奖励来提高一种行为频度的过程。\n 27. 增强法将我们的习惯与即时奖励联系在一起，当我们完成时，它会让你心满意足。\n 28. 长期保持"不冲动购物"或"本月禁酒"之类的习惯极具挑战性，因为就算我们错过了喝点小酒的欢乐时光或没有买下让你心动手痒的那双鞋子，生活照旧，与以往并没有什么不同。\n 29. 假如我们起初什么都没做，想要感到满意几乎是不可能的。我们所做的知识在抗拒诱惑，而此举不可能带给人满足感。\n 30. 解决这个问题的方法之一是颠倒过来。我们要让希望回避的习惯变得可见。开立一个储蓄账户，并注明这个账户专门用于将来买我们特别想要的东西。每放弃购买一件物品时，我们就把相应数额的钱存入这个账户。\n 31. 这就是为自己创建了一个忠诚计划。看到自己省钱买皮夹克的即时回报比放弃购物的感觉好得多。如此一来，即便你什么都没有买，依然能感到很满足。\n 32. 千万要选择能够强化我们身份的短期奖励，不能让它们与我们的身份相冲突。\n 33. 同理，泡个澡或四处闲逛来享受我们的闲散时光就是自我奖励的好例证，这与我们追求更多自由和经济独立的最终目标是一致的。\n 34. 这样使得，短期回报与我们保持身体健康的长期愿景相吻合。\n 35. 最终，随着内在建立，如心情舒畅、精力旺盛和身心放松之类相继到来，我们不再一心追求次要奖励。我们的新身份本身就变成了强化者。\n 36. 我们只有这样做才符合我们的身份，而且这么做，我们感觉很好。\n 37. 习惯与我们的生活贴合得越紧密，我们就越不需要外界的鼓励而能坚持下去。奖励可以启动一种习惯的培养进程，身份则可以维持一种习惯。\n 38. 尽管如此，证据的积累和新身份的出现需要时间。在长期回报到来之前，即时强化有助于在短期内保持动力。\n 39. 只有当转变充满乐趣的时候，习惯才会变得容易。\n 40. 当体验令人愉悦时，我们更有可能去重复一种行为。\n 41. 人脑进化为优先考虑即时奖励而不是延迟奖励。\n 42. 要保持一种习惯，我们需要有即时成就感，即使它体现在细微之处。\n\n\n# 怎样天天保持好习惯\n\n 1.  这种技巧叫做"曲别针策略"。有位女士在写作时每完成一页，就把发夹从一个容器转移到另一个容器。有位男士每做一个俯卧撑，都会从一个筒里拿出弹珠放进另一个筒里。\n 2.  取得进步令人满意，借助于视觉量度，如移动曲别针、发夹或弹珠，我们可以清晰地看到自己的进步。\n 3.  这样做的结果是，它们强化着我们的行为，并为任何活动增加一些即时满足感。\n 4.  视觉量度有多种形式：食物日志、健身日志、打孔忠诚卡、软件下载进度条、甚至书籍中的页码等。\n 5.  也许衡量我们进步的最好方法是利用习惯跟踪法。\n 6.  习惯跟踪法是衡量我们是否养成习惯的简单办法。它的最简单的方式是拿一份日历，划掉我们例行公事的每一天。\n 7.  追踪记录自己习惯的人数不胜数。从20岁开始，富兰克林就随身携带一本小册子，用来追踪自己遵从13项良好品行的情形。他的列表包括了诸如"抓紧时间。永远把时间用于做有意义的事情"以及"避免闲聊"之类的目标。每天结束，富兰克林都会打开他的小册子，记录他的进步。\n 8.  美国喜剧演员Jerry Seninfeld在记录片《喜剧演员》中，他曾解释说，他的目标仅仅是"永不中断"，坚持每天都写笑话。换句话说，他关注的不是某个笑话的好坏，或者有没有灵感，而是专注于天天这么做，不断夯实自己的基础。\n 9.  “永不中断”是一句强有力的励志语录。\n 10. 你要连续不断地拨打推销电话，只有这样才能提高你的销售业绩。不要中断健身进程，坚持下去，你会发现自己的身体状况一天比一天好，远超你的预期。不要中断你每天的创作，随着时间的积累，你会收获令人惊叹的作品集。\n 11. 习惯追踪功能强大，因为它充分利用了多个行为转变定律。它使一种行为同时变得显而易见、有吸引力和令人愉悦。\n 12. 记录你的上一个动作会创建一个启动下一个动作的触发器。习惯追踪自然会建立一系列的视觉提示，比如在日历上打的叉或者进餐日志中的食物列表。当我们翻看日历时，那一连串标记无疑在提醒我们继续采取行动。\n 13. 研究表明，追踪减肥、戒烟和降血压等目标进展的人比不追踪的人更可能有所成就。\n 14. 仅仅追踪一个行为就能激发改变它的冲动。\n 15. 习惯追踪也能让我们保持诚实。我们大多数人都对自己的行为的看法都不符合实际，我们认为自己做得很好，但事实并非如此。追踪测量可以帮助我们消除自我认识的盲点，并注意到每天我们究竟都做了什么。\n 16. 只要看一眼罐子里面有多少曲别针，我们立刻就能知道自己做了(或没有做)多少事。当证据就在我们面前时，我们不太可能再自我欺骗。\n 17. 最有效的激励形式是可知的进步。\n 18. 当我们接收到取得进展的信号后，我们会更有动力按既定的路径前进。\n 19. 这样，习惯追踪会对动机带来持续增强的效果，点滴进步都会激励你想要取得更多成就。\n 20. 在当我们遇到挫折时，这会产生奇效。当我们情绪低落时，很容易忽略我们已经取得的所有进步。习惯追踪提供了我们付出的所有艰苦努力的视觉证据，默默地提醒我们已经取得了多大进步。\n 21. 此外，我们每天早上在日历上看到的空白方格，会激励我们开始努力工作，因为我们不想因为中断一次而导致前功尽弃。\n 22. 最重要的是，追踪行为本身转化成了奖励的形式，从待办事项列表中划掉一个项目，在健身日志中又记上一笔，或者在日历上打个叉，这些都令人感觉心满意足。\n 23. 看着自己的成绩，比如投资组合的规模、书稿的页数等持续增长，满足感不言而喻。当感觉不错时，我们就更有可能坚持下去。\n 24. 习惯追踪也有助于我们心无旁骛：我们关注的焦点是过程而不是结果。我们并不执着于获得六块腹肌，只是想保持这种状态，成为那种不会偷懒、努力健身的人。\n 25. 习惯追踪具有三方面的功效：其一，创建视觉提示，提醒你采取行动；其二，内在激励机制，因为你清楚地看到了你的进步轨迹，并且不想失去它，以及其三，每当你记录下又一项成功的习惯实例时，我们都会享受到满足感。\n 26. 另外，习惯追踪提供了视觉证据，证明了我们在把自己塑造成为我们特别想成为的那类人，这本身就是一种令人感觉愉快的即时、内在满足的形式。\n 27. 许多人在抵制追逐和度量的想法。它让人感觉是个负担，因为它迫使我们养成两种习惯：我们视图培养的习惯，同时还要追踪它的习惯。\n 28. 追踪并不适用于每个人，也没有必要测量一辈子。\n 29. 如何才能让追踪轻而易举呢？只要有可能，测量应该自动化。我们的信用卡账单记录了我们出去吃饭的频率。我们的智能手环记录了我们走了多少步，睡了多久。一旦我们知道从哪里获取数据，就在日历上记一下，提醒自己每周或每月查看一次，这比每天都去查看更可行。\n 30. 其次，手动追踪应仅限于我们最重要的习惯。持续追踪一个习惯比随意跟踪十个习惯要好。\n 31. 了解一下我们在实际生活中的每时每刻是怎么度过的其实是很有趣的一件事。也就是说，每个习惯都有一个周期，总会在持续一段时间后结束。我们需要做好预案，随时应对偏离正轨的习惯，这比单纯衡量更重要。\n 32. 当我们固有的生活节奏被意想不到的事扰乱之后，每当我们遇到这种情况，我们都会试着提醒自己严格遵守一条简单的规则：绝不错过两次。\n 33. 假如有一天我错过了，我会尽可能地接上。错过一次健身会发生，但我们不会连续错过。\n 34. 我不可能做的完美无缺，但我可以避免第二次失误。\n 35. 一个习惯周期结束后，我会接着开始下一个周期。\n 36. 初犯不会毁了你。真正要命的是随之而来的不断重复的错误。\n 37. 错过一次是意外，错过两次是一种新习惯的开始。赢家和输家的差别就体现在这里。\n 38. 任何人都可能有糟糕的表现、糟糕的健身安排或某一天工作没干好。但是当成功人士摔倒后，他们会迅速爬起来。\n 39. 一个习惯偶尔被打断并不可怕，只要能迅速接上即可。\n 40. 我认为这个原则实在是太重要了，因此即便我不能像我想的那样做得很完美，我也会坚持不懈。\n 41. 很多时候，我们在培养习惯时会陷入全有全无的怪圈中。问题不在于出差错，而是如果不能做完美，就干脆不做的错误想法。\n 42. 你是真的没有意识到在你情绪低落(或忙碌)的日子里继续做有多么的可贵。\n 43. 错过的日子对你的打击大于成功的日子对你的帮助。\n 44. 如果你从100美元起步，那么50%的收益率会让你达到150美金。但是接下来只需要亏损33%就能把你打回100美元的起点。换句话说，避免33%的损失和获得50%的收益具有同等的价值。\n 45. Charlie Munger就曾经说：“复利的首要规则：除非万不得已，否则永远不要打断它”。\n 46. 健身的时候做什么并非头等重要的事，关键是你想成为严格遵循健身计划的那种人。当你感觉好的时候，锻炼很容易，但是当你情绪低落时仍然坚持锻炼、哪怕做的比平常少，重要的是你坚持不懈的表现。去健身房练五分钟不太可能提高你的表现，但它会重申你的身份。\n 47. 行为转变的全有或全无怪圈只是会让你的习惯陷入脱轨的陷阱之一。\n 48. 另一个潜在的危险，尤其是当你同时在应用习惯追踪法的时候，是观测的标的有误。\n 49. 追踪某一特定的行为的做法也有不良影响，我们因为过于专注于数字的变化，从而忘记了我们这样做的本意。\n 50. 无论玩什么游戏，人的唯一念头就是"赢"，这种陷阱明显体现在生活的许多领域。\n 51. 我们注重加班加点地工作，全然不顾我们所做的是否有意义。我们更关心凑够10000步，而不是保持健康。我们教学生应付标准化考试，而不是强调学习、好奇心和批判性思维。\n 52. 我们会针对我们所测量的进行优化。当我们选择错误的测量标的时，我们的做法就会走偏，这有时被称为古德哈特定律。\n 53. "当一项措施成为目标时，它就不再是一项好措施。度量只有在引导你并辅助大局时才有积极作用，它不应成为主角并让你疲于奔命。每个数字只不过是整个系统中的一条反馈罢了。"\n 54. 让习惯追踪起到该起的作用至关重要。我们或许乐于记录一个习惯并跟踪观测我们的进步，但是衡量之举并不是唯一重要的事情。此外，衡量进展情况的方式不在少数，有时把你的注意力转移到完全不同的事情上会对你有帮助。\n 55. 如果电子秤上的数字总是让你泄气，或许我们该关注其他一些可测的指标了，也就是能让我们看到更多进展信号的指标。\n 56. 习惯追踪提供了一个简单的方法来让你的习惯更令人满意。每一次测量都给你提供一点证据，证明你前进的大方向是正确的，并以自己出色的表现为傲，享受到略嫌短暂的即时快乐。\n 57. 习惯追踪法和其他视觉度量形式可以清晰无误地证明我们取得的进步，从而让我们对自己培养习惯的进程感到满意。\n\n\n# 问责伙伴何以能改变一切\n\n 1.  我们愿意重复感觉美好的经历，同时会设法回避曾令我们倍感痛苦的经历。\n 2.  痛苦是一个有效的老师。如果失败是痛苦的，人们便会力求成功、避免失败。\n 3.  如果失败的感觉不痛不痒，人们也就不把它当回事。错误的影响越直接，代价越大，你汲取教训的速度也就越快。\n 4.  无论什么行为，它引发即时痛苦体验的时间越快，它发生的可能性越低。\n 5.  如果你想戒掉不良习惯并避免不健康行为，那就给这类习惯和行为添加即时成本，这样可以有效地降低它们发生的概率。\n 6.  我们之所以难以戒掉坏习惯，就是因为它们在某种程度上迎合了我们的需要。就我所知，加快随这种行为而来的惩戒速度是摆脱这种困境的最佳方式。有行动便有惩罚，不能有丝毫迟滞。\n 7.  客户不愿支付滞纳金，因此会按时结清账单。学生想要好成绩，而当成绩与考勤挂钩时，他们就会准时出现在课堂上。我们常常会费尽周折，就是为了避免体验到一点点即时痛苦。\n 8.  如果你要依靠惩罚来改变行为，那么惩罚的强度必须与它视图纠正的行为的相对强度相匹配。要想有成效，拖延的代价必须大于立刻行动的代价。\n 9.  一般来说，越是局部的、有形的、具体的和直接的后果，就越有可能影响个人行为。后果越是全球性、无形性、模糊性和延迟性，影响个人行为的可能性就越小。\n 10. 提高任何坏习惯的即时成本，创立习惯契约。\n 11. 正如政府利用法律来追究公民的责任一样，我们也可以创建一个习惯契约，让自己承担特定责任。\n 12. 习惯契约是一种口头或书名的协议，我们要借此声明自己对某一特定习惯的承诺，并约定假如你违背了诺言，将会接受相应的责罚。\n 13. 要使不良习惯令人厌恶，我们最好的选择是在习惯动作刚一冒出头就让它们带来痛苦。订立习惯契约绝对是实现这一目标的捷径。\n 14. 知道有人在监督会是一个强大的动力。你不太可能拖延或放弃，因为你当即就要付出代价。如果你不坚持到底，监督你的人或许会认为你不可靠或生性懒惰。\n 15. 我们总是试图向世界展示我们最好的一面。我们梳头、刷牙、并精心打扮自己，因为我们知道这些习惯可能会给人留下好印象。\n 16. 我们很在意身边的人对自己的评价，因为他人的欣赏给了我们生活的乐趣。这就是为什么找一个责任心强的问责伙伴，或者订立习惯合同能如此有效的原因。\n 17. 如果不良习惯附加这令人痛苦或不愉快的感受，我们就不太可能重复它。\n 18. 问责伙伴可以对无所事事带来即时成本。我们非常在意别人对我们的看法，极不情愿感受别人的鄙视。\n 19. 习惯契约可被用来增加任何行为的社会成本，它使得违背承诺的代价公开而痛苦。\n 20. 知道别人在看着你，可以成为一种强大的动力。\n\n\n# 揭秘天才(当基因重要和无关紧要时)\n\n 1.  最大化你的成功概率的秘诀是选择合适的竞技领域，这适用于体育和商业，同样也适用于习惯的改变。\n 2.  如果习惯与你的天性和能力相一致，那么它更容易培养，你也更乐意保持。\n 3.  接受这一战略的前提是我们要承认一个简单的事实，即人天生具有不同的能力。\n 4.  有些人不愿意直面这个事实。从表面上看，我们的基因似乎无法改变，谈论我们无法控制的事情只能让人感到灰心丧气。此外，像生物决定论这种词语听起来似乎是有些人注定会成功，有些人注定会失败。但是这是一个关于基因影响行为的短视观点。\n 5.  遗传基因的力量无疑很强大，但它也有弱点。基因不容易改变，这意味着它们在有利的环境中提供了强大的优势，但在不利的环境中又暴露出严重的劣势。\n 6.  我们的环境决定了我们基因的适应性和我们天然禀赋的效用。当我们的环境改变时，决定成功的品质也会改变。\n 7.  身体特征如此，精神上也一样。某方面的能力与其所处的环境高度相关。任何竞争领域的顶尖人才不仅训练有素，而且天生就适合所做的事。这就是为什么，假如你想成为真正伟大的人，选对发展方向至关重要。\n 8.  简而言之：基因并不决定你的命运，而是决定这我们在哪些领域存在发挥特长的机会。\n 9.  在你先天具备了成功潜质的领域，我们更有可能养成令人满意的习惯。关键是我们选定的努力方向不仅令你生机勃勃，还能与你的天赋相匹配，从而使你的雄心与你的能力达成一致。\n 10. "我怎么才能辨别出朝哪方面努力胜算更大？我怎么才能确定适合我的机会和习惯？"我们首先要从了解我们的个性入手去寻找答案。\n 11. 我们的所有遗传特质组合在一起，赋予了我们独特的个性。我们的个性是指在各种各样的情境中表现出一致性的性格特征集合。\n 12. 经过科学分析验证的"五大"性格特征是目前得到公认的性格类别图谱。\n     * 开放性：从好奇和创造性的一端到谨慎和一丝不苟的另一个端。\n     * 自觉性：从有条理和效率高到随意性和自发性。\n     * 外向性：从活泼开朗、活力十足到孤独和保守(也就是外向型人格和内向型人格的区别)。\n     * 神经质：从焦虑不安和敏感多疑到自信、冷静和心态平和。\n 13. 亲和性的人一般表现出善良、体贴、热情的特征。他们体内的催产素含量往往较高，这种激素在增进社交活动、提高信任感等方面起着重要作用，同时还是一种天然抗抑郁剂。\n 14. 神经质特征突出的人往往比其他人更容易焦虑，总是忧心忡忡的。这一特征与杏仁核的超敏反应有关，杏仁核是大脑中负责识别威胁的部分。\n 15. 我们的习惯不仅仅是由我们的个性决定的，但毫无疑问，基因在将我们推向一个特定的发展方向。\n 16. 我们根深蒂固的偏好使得一些人不经意表现出的言谈举止，却难以在另一些人身上再现。\n 17. 我们不必为这些差异感到不安或内疚，我们必须去直面它们。例如，如果一个人的自觉性较差，他就不太可能天生爱整洁，因而可能需要更多地依靠环境设计来保持良好的习惯。\n 18. 我们应该养成适合我们个性的习惯。我们随着自己的心愿去做时就会有更充足的动力。\n 19. 我们不用培养他人告诉我们要养成的习惯。选择最适合我们的而不是最流行的习惯。\n 20. 每种习惯都有一个特定版本，能够带给我们快乐和满足，设法找到它。\n 21. 只有给人带来快乐的习惯才能长期坚持下去，这是第四定律的核心思想。\n 22. 让你培养的习惯适合你的个性是良好的开端，但这并不是故事的结尾。接下来，我们要把注意力转向寻找和设计迎合我们天性的情境。\n 23. 在实践中，我们更可能享受那些对于我们来说轻而易举的事。\n 24. 在某一特定领域有专长的人往往更胜任相关的工作，并会因表现出色而受到表扬。他们之所以精力充沛，是应为他们的成功之处，正是他人失败的地方，也因为他们获得了更高的报酬和更多的机会，这不仅让他们更快乐，还推动他们完成更高质量的工作。这是一个良性循环。\n 25. 选择正确的习惯，进步轻而易举。挑错了习惯，生活就是无休止的挣扎。\n 26. 我们该如何确保自己所掌握的一套技能能适用于我们所要做的事？最常见的方法就是不断试错。有一个有效的办法来对付这个难题，它被称为探索/利用权衡。\n 27. 在一项新活动开始时，应该有一段摸索的时间。其目标是尝试诸多可能性，研究各种想法，并撒下一张大网。在经历了初步探索之后，把注意力转移到我们找到的最佳方案，但与此同时还要偶尔再尝试一下。\n 28. 如何在两者之间保持适当的平衡取决于我们是赢还是输。如果我们正在赢，我们就利用，利用，再利用。如果我们正在输，我们要继续探索，探索，再探索。\n 29. 从长远来看，或许最有效的做法，就是抓住在绝大多数时间里提供最佳结果的战略不放，同时就其余情况进一步探索。谷歌员工把每周80%的工作时间花在正式工作上，20%花在他们自己选择的项目上。\n 30. 影响最佳做法选择还取决于我们有多少可供支配的时间。如果时间不成问题，那么我们值得把更多时间用于探索，因为一旦我们找到了正确的选项，我们将仍然有很长时间去利用它。\n 31. 如果时间紧迫的话，比如说，某个项目的最后期限即将到来，我们应该实施迄今为止找到的最佳方案，争取尽快取得一些成果。\n 32. 当我们在探求不同的选项时，我们可以问自己一系列的问题，以便逐渐接近最令我们满意的习惯和领域。\n 33. 什么对我来说充满乐趣，但对其他人来说却只是乏味的工作？我们是否适合一项任务的标志不在于我们是否喜欢它，而在于我们是否能比大多数人更容易承受这项任务带来的痛苦。当别人觉得苦不堪言时，我们却能自得其乐？伤害别人多余伤害自己的事，就是我们生来就适合做的事。\n 34. 是什么让我们忘记了时间的流逝？"心流"指的是我们因为全神贯注地投入手头的工作，从而忘记了周边世界存在的一种精神状态。\n 35. 我们在哪里能获得比普通人更高的回报？我们不断地与自己周边的人相比。如果我们做得比别人好，我们会觉得心满意足。\n 36. 我们的天性是什么？此刻，我们可以暂时忘记我们接受的教育，忽略主流社会告诉我们的事情，忽略别人对我们的期望。扪心叩问："我觉得什么很自然？我们何时感觉充满活力？我们何时看到了自己的真面目？"不要急于自我批评，也不要刻意讨人欢心。不要犹豫不定或自我批评。只注重乐在其中的感觉。无论何时，只要我们感觉真实可信，我们前进的方向就是正确的。\n 37. 我们都是地球上的冲冲过客，我们中真正伟大的人不仅付出卓越的努力而且还有幸享有天赐良机。\n 38. 假如我们无法确认什么事能让我们做到风生水起、好运连连，那就另辟蹊径，开创一番新事业。\n 39. 当我们不能比别人做的更出色，我们可以借助于与众不同而胜出。通过调用我们各方面的技能，我们降低了竞争水平，这使得我们更容易脱颖而出。我们可以修改游戏规则，简化先天条件(或资历年限)上的要求。一名好手努力战胜同一领域的众多对手，一名高手则自成一体，尽其所能扬长避短。\n 40. 沸水可以使土豆变软，但会使鸡蛋变硬。我们不能自主选择称为土豆还是鸡蛋，但在变硬还是变软之间我们可以选择对我们最有利的。如果我们能找到一个更有利的环境，我们就可以把不利于我们的情形转变为有利的情形。\n 41. 生理差异不容忽略。即便如此，关注自己能否充分发挥自己的潜力，要比与他人攀比收效更显著。人的能力是有限的这一事实，与我们是否已达到能力的上限无关。人们常常过于纠结自己能力的极限，以至于放弃了充分调动自己潜能的努力。\n 42. 假如我们不进行刻苦训练，我们根本无法获知自己的极限在哪里，或者说自己是否具备了优秀的基因。\n 43. 如果我们不能像我们所敬佩的人那样付出辛勤的汗水，就不要把他们的成功解释为运气好。\n 44. 基因并不能排除艰苦努力的需要。它们只会帮着甄别，告诉我们该努力做什么事。\n\n\n# 金发女孩准则：如何在生活和工作中保持充沛动力\n\n 1.  为什么有些人，比如马丁，能长期坚持他们的习惯，无论是讲笑话、画漫画还是弹吉他，从不懈怠，而我们大多数人却坚持不了几天就打退堂鼓。我们该怎样让习惯一直保持新鲜感，而不是过段时间就逐渐消失。\n 2.  大家公认保持动力和达到最大欲望的途径之一，就是去做"难易程度刚刚好"的事。\n 3.  人脑喜欢挑战，但前提是它面对的挑战难度适中。\n 4.  现在考虑和一个和我们水平相当的人一起打网球。随着比赛的进行，我们有输有赢。我们再努把力就有可能最终赢得比赛。我们开始集中注意力，专心打球，并进入了浑然忘我的境界。这是一个难度适中的挑战，也是金发女孩准则的一个重要实例。\n 5.  金发女孩准则指出，人们在处理其能力可及的事务时积极性最高。难易程度适中，刚刚好。\n 6.  当我们开始养成新习惯的时候，保持尽可能简单的动作是很重要的，这样即使各方面条件不完善，我们也可以坚持下去。\n 7.  然而，一旦习惯形成之后，还需要不断地添砖加瓦，持续跟进，这很重要。\n 8.  这些后续的小改进和新的挑战可以保持你的参与度。如果你刚好碰到金发女孩区，你就能达到心流状态。\n 9.  心流状态是"身在其中"并完全沉浸于一项活动中的体验。\n 10. 他们发现，要达到心流状态，我们要完成的任何难度必须比我们目前的能力高出大约4%。\n 11. 金发女孩准则的核心思想仍然存在：做力所能及、难易适中的事，似乎是保持激励水平居高不下的关键所在。\n 12. 改善需要微妙的平衡。我们需要时常处理一些具有挑战性的事务，在令我们殚精竭虑的同时，让我们能够取得足够的进步来保持激励水平。\n 13. 行为新奇才能有吸引力并带给人满足感。千篇一律，我们就会感到厌倦。厌倦或许是追求自我完善之路上的最大障碍。\n 14. "有些时候，归根结底这取决于谁能应付每天枯燥乏味的训练，一遍又一遍地反复做同样的举重动作"。\n 15. 他的回答让我吃惊，因为这是对工作伦理的颠覆性认识。人们平常说的都是如何"满怀热情"去努力实现他们的目标。\n 16. 不管是在商业、体育还是艺术界，我们听到的都是"一切都归结于激情"或者"你必须真的渴望得到它"之类的说法。\n 17. 因此，当我们失去了注意力或动力的时候，我们中的许多人会变得沮丧，因为我们认为成功的人会有无限的激情。\n 18. 但是这位教练说真正的成功人士也会和其他人一样感到激情消退。唯一不同的是，尽管感到枯燥乏味，他们仍然想办法坚持下去。\n 19. 成功的最大威胁不是失败，而是倦怠。我们厌倦了习惯，因为它们不再让我们开心，这个结果是意料之中的。\n 20. 随着我们的习惯变成日常举动，我们开始脱离固有的轨迹，转而去追求新奇的事务。也许这就是为什么我们会陷入一个永无止境的周期性循环，无论是健身方式、饮食习惯，或是创业的想法，总是换来换去的。\n 21. 激情稍有消退，我们就开始寻找新的做法，哪怕老做法依然在起作用。\n 22. 也许这就是为什么许多不停地花样百出的产品总是让人欲罢不能。电子游戏提供给人们视觉上的新奇感，色情作品提供性体验上的新奇感，速食产品则不断变换口味，上述种种经历都鞥呢让人体验到连续不断的惊喜。在心理学上，这被称为可变奖励。这种变幻不定导致多巴胺的浓度达到最大峰值。\n 23. 可变奖励不会创造渴望，也就是说，你时不时地奖给别人原本不喜欢的东西并不能使他们回心转意，喜欢上它，但是可变奖励的确会显著放大我们曾体验过的渴望，因为它们会缓解我们的倦怠感。\n 24. 在成功和失败各占一半的情形下，人们会体验到恰到好处的渴望的快感。得与失就发生在一瞬间，你需要刚刚够的"赢"来体验满足感，以及刚刚够的"渴望"来体验欲望。\n 25. 如果你本来就对某个习惯感兴趣，那么应对难易程度适中的挑战是保持事物趣味不减的好方法。\n 26. 当然，并不是所有的习惯都含有可变奖励的成分。不管有没有可变奖励，没有一种习惯会有无穷无尽的乐趣。\n 27. 在某个时刻，每个人在自我提升的旅程中都面临这同样的挑战：你必须与厌倦结缘。\n 28. 我们都有人生目标并心怀梦想，但是，假如你只在心悦来潮或一时兴起才做出一些努力，那么无论你的目标或梦想是什么，你都不可能取得显著的成果。\n 29. 我敢说，假如你下决心培养一种习惯并坚持了一段时间之后，你总有一天会想要放弃。但是，当你感到心烦意乱、苦不堪言或精疲力尽时，是鼓足干劲还是萌生退意，这是专业人士和业余人士的分水岭。\n 30. 专业人员依照既定计划行事，毫不动摇；业务爱好者则随波逐流，任性而为。专业人士指导对他们来说什么最重要，并有目的地去做；业务爱好者则随生活中的突发情况而变。\n 31. 当一个习惯对你真正重要时，你必须愿意在任何心情下坚持下去。专业人士不会因自己心情不好而改变行动的时间表。他们可能享受不到乐趣，但是他们仍能做到坚持不懈。\n 32. 我懒得做的组合训练有很多，但我从未后悔健身的选择。我懒得写的文章也不少，但我从未后悔按时发表。有很多天我都想放松一下，但我从未后悔准时到场，努力去做对我来说很重要的事。\n 33. 称为出色的人的必经之路，是无休止地反复做同样的事，且痴心不改。你必须爱上厌倦。\n 34. 金发女孩准则指出，人们在处理其能力可及的事务时积极性最高。\n 35. 成功的最大威胁不是失败，而是倦怠。\n 36. 随着习惯成为常规，它们变得不那么有趣，也不那么令人满意。我们开始感到无聊。\n 37. 每个人受到激励时都能努力工作。但当工作不那么令人兴奋时，仍能继续奋进的则是人中佼佼者。\n 38. 专业人员依照既定计划行事，毫不动摇；业务爱好者则随波逐流，任性而为。\n\n\n# 培养好习惯的负面影响\n\n 1.  习惯是精通的铺路石。\n 2.  我们对一些简单动作熟练到不假思索就能完成的地步之后，我们就可以自由地关注更高层次的内容。\n 3.  这样看来，在任何追求卓越的努力中，习惯都是不可或缺的支柱。\n 4.  然后，习惯的好处是有代价的。起初，每一次重复都让你的动作更加流畅、做得更快、更娴熟。但到了后来，随着我们的习惯动作越来越自如，我们对反馈的敏感度也会下降。\n 5.  我们会心不在焉，机械地重复着熟悉的动作。就算是犯了错也不再上心，就随它去了。既然自动驾驶已经能做到"足够好"，我们就不再考虑如何做得更好。\n 6.  习惯的好处是我们能够不假思索做任何事，缺点是我们习惯了以某种方式做事，不再介意其间暴露出的小纰漏。\n 7.  我们认为做得越来越好是因为我们的经验越来越丰富。但是实际上，我们只是在强化，而不是在改善我们当前的习惯。\n 8.  事实上，一些研究表明，当一个人熟练掌握了一项技能之后，他随后表现出的水准会有所下降。\n 9.  如果我们想最大限度地发挥我们的潜力，并达到出类拔萃的水准，我们的做法就要有所不同。我们不能漫不经心地反复做同样的事，同时期望自己能有非同凡响的表现。\n 10. 习惯是必要的，但还不足以称得上精通。我们需要的是习惯动作与刻意练习相结合。\n 11. 习惯动作 + 刻意练习 = 精通\n 12. 掌握的过程要求你在一个又一个的基础上不断进步，每个习惯都是建立在最后一个基础上的，直到达到新的表现水平，更高范围的技能被内化。\n 13. 掌握的过程，就是你把注意力集中到成功的一个小元素的过程。重复这一过程，直到技能内化，然后以这个新习惯为跳板，继续扩展我们的发展空间。\n 14. 任何事做第二遍时都会变得容易一些，但不是总体上变得更容易，因为现在你把精力投入到了下一个挑战中。\n 15. 每个习惯都会开启下一个级别的表现。这是一个无止境的循环。\n 16. 正是在我们开始觉得自己已经熟练掌握一项技能的时候，也就是在所有动作都无比娴熟和轻而易举的那一刻，我们必须避免陷入自满的陷阱。\n 17. 如何避免呢？答案是建立一个反思和审视的体系。\n 18. 赖利后来说："坚持不懈地努力对任何企业来说都极其重要。成功之策就是学会正确地做事，然后每次都以同样的方式去做"。\n 19. 反思和回顾有助于长期改善所有习惯，因为它对我们认清自己的不足，并帮助我们考虑可能的改善途径。\n 20. 没有反思，我们会为自己的行为寻找理由、找借口，并自我欺骗。我们会因缺乏这样一种程序而无法确定我们与以往相比表现得更好还是更差。\n 21. 我听说过一些高管和投资者有写"决策日志"的习惯，记录他们每周做出的重大决策，决策的理由，以及预期的结果。他们会在每个月或年底回顾他们的选择，看看哪些是对的，哪些出了问题。\n 22. 习惯不仅需要改善，也需要微调。反思和回顾可以确保我们的时间用在了正事上，并在必要的时候修正方法。\n 23. 就我个人而言，我主要通过两种方式进行反思和回顾。每年12月，我都会进行年终总结，回顾一下往年的都有哪些行为，比如发表了多少篇文档，进行了多少次健身，浏览过多少个新地方。\n 24. 我通过回答三个问题来反思我的进步(或不足)：\n     * 今年什么事做得比较好？\n     * 今年什么事做得不太好？\n     * 我学到了什么？\n 25. 半年后，当夏天来临之际，我们会做一份诚信报告。像大家一样，我犯过很多错误，我的诚信报告有助于我认清哪里出了问题，并激励我回到正轨。\n 26. 我以此为契机，重新审视我的核心价值观，并审视自己是否一直在践行自己的价值观。\n 27. 我也借此机会反思我的身份以及怎样努力成为我心目中的那类人。\n 28. 我的年度诚信报告回答了三个问题：\n     * 推动我生活和工作的核心价值观是什么？\n     * 我现在生活和工作的正直程度如何？\n     * 我怎样为将来设定更高的标准？\n 29. 这两份报告，不需要很长时间就能完成，每年不过几个小时，但它们是自我完善的关键时期。\n 30. 这几个小时会帮我制止不经意间的懈怠和疏忽，会一年一度提醒我重新审视自己想要的身份，并考虑我的习惯是在怎样帮助我成为我崇拜的那种人。\n 31. 它们表明我何时应该提升我的习惯，迎接新的挑战，何时应该回归初心，练好基本功。\n 32. 反思也能扩展视野。日常习惯之所以强大，是因为它们具有复利特性，但天天为自己的每个选择忧心忡忡就像用放大镜看自己，过于短视了。\n 33. 我们专注于局部的瑕疵，忽略了更大的画面。这属于反馈过多。\n 34. 定期的反思和回顾就像是从正常距离照镜子，我们既不失整体画面，也能看到应该做出的重大决定。我们想观赏整个山脉，而不是局限于山峰和山谷。\n 35. 反思和回顾也是个良机，可用来重新审视行为转变最重要的方面之一：身份。\n 36. 当我们具备了这种新的身份后，这些相同的信念会阻碍我们进入下一个发展阶段。我们的身份与自己作对的时候，会发出某种"傲慢"，怂恿我们否认自己的缺点，阻止我们真正成长。这就是养成习惯的最大负面影响之一。\n 37. 我们越是执念于某个想法，也就是说，它与我们的身份越紧密，我们就越坚决地捍卫它不受质疑。\n 38. 学校老师无视创新的教学方法，固守其久经实践检验的教案；资深经理执意要自行其是；外科医生拒绝年轻同事提出的建议；一支乐队在发行首个震撼人心的专辑之后便故步自封，再无创新。我们越是执着于一个身份，就越难超越它。\n 39. 解决这种问题的办法之一，就是避免让我们的身份的任何单一属性主导我们的为人处世。\n 40. 假如我们固守一种身份，我们会变得不堪一击。失去那个身份，我们就失去了自己的全部。\n 41. 当我们一生都在用一种方式定义自己，而这种定义消失了，我们现在究竟是谁？\n 42. 要想减轻随身份丧失而来的负面影响，关键是必须重新定义自己，这样即使我们的特定角色发生了变化，我们也可以保留身份的重要方面。\n 43. "我是运动员"变成"我是那种精神坚强、喜欢身体上的挑战的人"。\n 44. "我是优秀的士兵"转变成"我是那种纪律严明、诚实可靠、富于团队合作精神的人"。\n 45. "我是首席执行官"转变为"我是那种制作和创造东西的人"。\n 46. 如选择恰当，身份可以是灵活的，而非不堪一击的。就像水在障碍物周围流动一样，我们的身份会随着环境的变化而变化，而不是与环境对抗。\n 47. 习惯带来了很多好处，但缺点是它们也会让我们陷入以前的思维和行为模式，不能跟上时代前进的步伐。\n 48. 一切都是无常的。生活在不断变化，所以我们需要定期检查一下，看看我们固有的习惯和信仰是否还在为我们服务。\n 49. 缺乏自我意识是毒药，反思和回顾是解药。\n 50. 反思和回顾是一个过程，使我们能够时刻关注自己的表现。\n\n\n# 获得持久成果的秘诀\n\n 1.  一个小小的改变能改变你的人生吗？你不太可能说是的。但是如果你又做了一个呢？又做了另一个呢？接着又做了一个呢？\n 2.  在某个时刻，我们会不得不承认我们的人生被一个个小小的变化改变了。\n 3.  习惯改变的"圣杯"不是单个1%的改进，而是成千个。它是无数微习惯堆积起来的结果，其中每个微习惯都是构成整个系统的基本单元。\n 4.  一开始，小改进往往微不足道，因为它面对的整个系统体量太大了，无法撼动。\n 5.  然而，随着我们继续将微小的变化层层叠加，人生的天平开始偏移。\n 6.  每次改进就像在有利于你的天平的一侧添加一粒沙，使它慢慢地偏向你。\n 7.  假如你能坚持下去，最终我们会达到产生重大偏移的临界点。\n 8.  突然间，坚持好习惯变得轻而易举了。整个系统开始偏重你，不再与你作对。\n 9.  我们在此提及的每个人、团队和公司处境各有不同，但最终都以同样的方式取得进步：致力于微小、可持续、不懈的改进。\n 10. 成功不是要达到的目标，也不是要跨越的终点线。它是一个让你得以进步的体系、精益求精的过程。\n 11. 正如在第1章所描述的："如果我们很难改变自己的习惯，问题的根源不是我们本身，而是我们的体系。坏习惯循环往复，不是因为我们不想改变，而是因为我们用来改变的体系存在问题"。\n 12. 掌握了行为转变的四大定律，我们就拥有了一套工具和策略，可以用来建立更好的系统和养成更好的习惯。\n 13. 有时一个习惯很难记住，我们需要让它显而易见。其他时候，我们不想开始培养习惯，我们需要让它有吸引力。在许多情况下，我们可能会发现太难养成习惯，我们需要让它简便易行。有时候，我们不想坚持下去，我们需要让它令人愉悦。\n 14. 让好习惯显而易见，让坏习惯脱离视线；让好习惯有吸引力，让坏习惯缺乏吸引力；让好习惯简便易行，让坏习惯难以施行；让好习惯令人愉悦，让坏习惯令人厌恶。\n 15. 这是一个连续不断的过程，没有终点线，也没有永久的解决方案。每当我们想要自我提高时，我们都可以围绕行为转变四定律循序渐进地发展，直到我们发现下一个瓶颈。\n 16. 让它显而易见，让它有吸引力，让它简便易行，让它令人愉悦。一圈又一圈地循环发展。不停地寻求用来获得1%的进步的新方法。\n 17. 获得持久成果的秘诀是不断进步，永不停歇。\n 18. 只要我们一刻不停，坚持下去，我们难以想象自己能取得多么伟大的成就。\n 19. 假如我们不停止工作，我们的公司业务发展将蒸蒸日上。假如我们不停止健身，我们将拥有一副强健体魄。假如我们不停止学习，我们能汇聚起知识的宝库。假如我们不停止储蓄，我们将积少成多，得到一笔巨款。假如我们不停止关爱，我们的朋友将会遍及天下。小习惯不会简单相加，它们会复合。\n\n\n# 从四大定律中吸取的教训\n\n 1.  意识先于欲望。当你赋予提示一定意义之后，就会产生渴求。你的大脑会构造一种情绪后感觉来描述你的现状，这意味着渴求只会产生于你发现了机会之后。\n 2.  幸福转瞬即逝，因为我们总是会升腾起新的欲望。\n 3.  卡德.布德里斯所说: "幸福是已得到满足的欲望与酝酿中的欲望之间的空档"。同样，痛苦则是渴望改变现状与改变得以实现那一刻之间的空档。\n 4.  我们追求的是快乐的理念。我们寻求我们脑海中产生的快乐影像。在采取行动时，我们并不知道获得这个影响会给我们带来什么(甚至不确定它是否会令我们满意)。满足感只有在事发之后才会出现。\n 5.  奥地利神经学家维克多.弗兰克说幸福是追求不到的，只能尾随而来。我们追求的是欲望。快乐来自行动。\n 6.  有了充足的理由，我们可以克服任何困难。\n 7.  尼采有一句名言:"有足够理由活着的人几乎可以忍受任何生存方式。"这一说法包含了一个关于人类行为的重要事实。如果我们的动机和欲望足够强大(也就是说，我们为什么要行动)，即使困难重重，我们也会采取行动。强烈的渴望可以推动伟大的行动 ---- 即使阻力巨大。\n 8.  好奇心总比头脑灵活好。积极性和好奇心比头脑灵活更重要，因为前者会导致行动。头脑灵活永远不会独自产生结果，因为它不会让你采取行动。\n 9.  促使行为的是欲望，而不是智力。\n 10. 纳瓦尔.拉维康特说:"做任何事情的诀窍是首先是培养对它的渴望"。\n 11. 情绪驱动行动。在某种程度上，每个决定都是一个情绪上的决定。\n 12. 不管我们采取行动的逻辑原因是什么，我们只会因为情绪而感受到采取行动的必要性。事实上，大脑情感中心受损的人可以列出许多采取行动的理由，但始终不会行动，因为他们缺乏情感驱动。这就是为什么渴望先于回应。先有感觉，然后才有行动。\n 13. 我们的情绪先于理性和逻辑。大脑的主模式是感觉；此模式是思考。我们的第一反应---- 大脑中快速、下意识的部分 ---- 是针对感觉和预期而优化的。我们的次一级反应 ---- 大脑中缓慢、有意识的部分 --- 是"思考"的部分。\n 14. 心理学家将此区分为系统1(感觉和快速判断)与系统2(理性分析)。感觉在先(系统1)；理性只在随后介入(系统2)。当两者协同一致时，会发挥极佳作用，但当两者不一致的时候，就会产生不合逻辑、感情用事的后果。\n 15. 我们的反应倾向于跟随我们的情绪。我们的思想和行动根植于我们认为有吸引力的东西，而不一定是符合逻辑的东西。如果一个话题让某人感到情绪激动，他们很少会对数据感兴趣。这就是为什么情绪会对明智的决策造成较大的威胁。\n 16. 痛苦推动进步。所有痛苦的根源是对改变现状的渴望。这也是所有进步的源泉。对改变现状的渴求激励着我们采取行动。\n 17. 内心涌动这渴望，意味着我们不满意，但动力十足。没有渴望，我们就心满意足，不思进取。\n 18. 回报是牺牲的另一面。回应(牺牲能量)总是先于回报(收集资源)。"跑步者的愉悦感"是在运动量超过一定程度后的体验，只有在消耗掉一定能量后，回报才会到来。\n 19. 自我控制很难做到，因为它不令人满意。奖励是让你的渴望得到满足的结果。这使得自我控制难以起效，因为抑制我们的欲望通常不会根除它们。抵制诱惑并不能满足你的渴望；它只是忽略了它，打通了让渴望穿过的通道。自我控制要求你释放而不是满足它。\n 20. 如果期望和结果之间的不匹配是积极的(惊喜)，那么我们将来重复一种行动的可能性就很大。如果是负面的(失望和沮丧)，那么我们就不太可能再去做。\n 21. 在行动之前，有一种感觉在激励你行动，那就是渴望。行动之后，有一种感觉教导你在未来重复这个动作，那就是奖励。\n 22. 快乐和满足给予一种行为源源不断的动力，感觉有动力才会让你行动起来，成就感则促使我们不断重复那种行为。\n 23. 新方式带来了希望，因为我们不曾经历过，可以敞开了想象。新策略似乎比旧策略更有吸引力，因为它们可以有无限的希望。',normalizedContent:'# 《原子习惯》读书笔记\n\n\n# 微习惯的惊人力量\n\n 1.  人们总是容易高估某个决定性时刻的重要性，也很容易低估每天进行微小改进的价值。我们常常说服自己，大规模的成功需要大规模的行动。给自己定下目标，给自己施加压力，让自己努力做出一些人人都会谈论的惊天动地的改进。\n 2.  习惯是自我提高的复利。\n 3.  只要我们日复一日地重复1%的错误，亦即反复做出不良决策、重复微小的错误，以及为自己的小失误寻找借口，久而久之，我们的小选择会叠加成有害的结果。\n 4.  你的那些选择决定了你是谁和你可能是谁之间的不同。\n 5.  成功是日常习惯累积的产物，而不是一生仅有一次的重大转变的结果。\n 6.  你此时此刻是成就辉煌还是一事无成并不重要，重要的是你当前的习惯是否让你走上了通往成功的道路。你应该更关心你在当下前行的轨迹，而不是你已经取得了什么样的结果。\n 7.  正是像这样的小拼搏定义着你未来的自我。\n 8.  成功与失败之间的差距会随着时间的延续而不断扩大。\n 9.  你在培养习惯的过程中，有相当长时间是感受不到它的影响的，直到某一天，你突破了临界点，跨入新境界。\n 10. 在任何探索的早期和中期，通常都会有一个不如意的低谷区。你期望日新月异，收到立竿见影的效果，但让你感到沮丧的是，在最初的几天、几周甚至是几个月里，几乎看不到任何明显的变化。你觉得一切都是在白费功夫。这是任何复利过程的共同特征：最有力的结果总是姗姗来迟。\n 11. 熟练掌握某种技能需要足够的耐心。\n 12. 不积跬步无以至千里，每颗习惯的种子都来自单一的、微小的决定。\n 13. 戒除坏习惯犹如连根拔起我们内心枝繁叶茂的橡树，而培养良好习惯则像是每天不忘浇水，悉心培育一只娇嫩的鲜花。\n 14. 忘记目标，专注于体系。\n 15. 如果我们完全忽略了我们的目标，只关注我们的体系，我们还会成功吗？ 我想你会的。\n 16. 任何一项运动的终极目标都是争取获得最好的成绩，但是在整场比赛中都死盯着记分牌则荒谬无比。\n 17. 争取每天都有进步才是我们走向成功的唯一方法。\n 18. "比分会自理的"。\n 19. 如果我们想要更好的结果，那就别再紧盯着目标不放，而要把精力集中到我们的体系建设上。\n 20. "目标完全无用吗？"当然不是。目标的意义在于确定大方向，但体系会促进我们的进步。\n 21. 只有在实施了一点一滴、循序渐进地改进体系之后，我们才取得了不同寻常的结果。\n 22. 真正需要改变的是导致这些结果的体系。\n 23. 假如我们只是围绕这结果动脑筋想办法，我们只能取得一时的改进。为了取得一劳永逸的成效，我们需要解决体系层面上的问题。修正输入端，输出端就会自行修正。\n 24. 来日方长，要把快乐留待未来再享受。\n 25. 目标会导致"非此即彼"的冲突；你要么实现了预定目标，最终取得了成功，那么你失败了并令人大失所望。你在精神上把自己禁锢在一种狭隘的幸福观之中，这属于自我误导。\n 26. 成功之路不止一条，我们毫无必要认定只有某个特定场景的出现，才能让你对自己的人生感到满意。\n 27. 当我们爱上过程而不是结果时，我们不必等待容许自己享受快乐的那一刻的到来。只要我们创建的体系正在运行，我们就会在整个过程中感受到快乐。\n 28. 不求拔高你的目标，但求落实你的体系。\n 29. 关注整个体系，而非单一目标。\n\n\n# 你的习惯如何塑造你的身份\n\n 1.  改变习惯之举颇具挑战性，原因有两个：1) 我们没有找对试图改变的东西；2) 我们试图以错误的方式改变我们的习惯。\n\n 2.  最深入的层次是改变你的身份。这个层次是有关于改变你的信仰：你的世界观、你的自我形象，以及你对自己和他人所做的判断。你持有的大多数信念、假设和偏见都与这个层次相关。\n\n 3.  结果意味着你得到了什么，过程意味着你做什么，身份则关系到你的信仰。\n\n 4.  很多人开始改变他们的习惯时，把注意力集中在他们想要达到的目标上。这会导致我们养成基于最终结果的习惯。正确的做法是培养基于身份的习惯。借助于这种方式，我们的着眼点是我们希望成为什么样的人。\n\n 5.  内在激励的终极形式是习惯与你的身份融为一体。说我是想要这样的那种人是一回事，而说我本身就是这种人则是另外一回事。\n\n 6.  你越是以自己身份的某一方面为傲，你就越有动力保持与之相关的习惯。\n\n 7.  目标不是阅读一本书，而是成为读者。目标不是跑马拉松，而是成为跑步者。目标不是学习一种乐器，而是成为音乐家。\n\n 8.  在任何层面--- 个人、团队、社会 ---- 积极变革的最大障碍是身份冲突。理智上，你当然认为应该培养良好习惯，可当它们与你的身份冲突时，你将无法付诸行动。\n\n 9.  进步需要你不断地修饰你的信仰，提升和扩展你的身份。\n\n 10. 如果你投了几票给不良行为或沾染了毫无建树的习惯，这没多大关系。你的目标是赢得大多数时间。\n\n 11. 新的身份需要新的证据。\n\n 12. 这是一个简单的两步过程：\n     \n     决定你想成为哪种人。\n     \n     用小赢证明给自己看。\n\n 13. 重要的是让你的价值观、原型和身份驱动这个循环回路，而不是你的结果。重点应该始终是成为那种类型的人，而不是获得某种特定的结果。\n\n 14. 习惯至关重要的真正原因不是因为它们能带给你更好的结果(尽管它们能做到这一点)，而是因为它们能改变你对自己抱有的信念。\n\n\n# 培养良好习惯的四步法\n\n 1.  习惯是重复了足够多的次数后而变得自动化的行为。习惯形成的过程始于反复尝试。\n\n 2.  每当我们在生活中遇到新的情况，我们的大脑就要做出决定。会和实验的猫一样，大脑正忙于学习最有效的行动路径。在偶然发现一个意想不到的奖励后，我们便对今后的策略做出调整。\n\n 3.  这是人类全部行为背后的反馈回路：尝试、失败、学习，然后进行不同的尝试。经过一番练习，那些无用的动作逐渐消失，而有用的动作将得到加强。这就是正在形成的习惯。\n\n 4.  我们的习惯只是解决我们经常面临的问题和压力的一系列自动解决方案。\n\n 5.  随着习惯的形成，大脑的活跃程度逐渐降低。我们会学会锁定预示成功的线索，并忽略其他一切。当未来类似的情况出现时，我们就知道该寻找什么了。\n\n 6.  习惯形成后，我们的大脑会直接跳过试错环节，并创立一条心理规则：如果是这种情形，就用那种方式应对。\n\n 7.  习惯是从经验中学到的心理捷径。从某种意义上来说，习惯只是我们过去为解决问题而采取的步骤的记忆。大脑记忆过去的主要原因是为了预测如何更好地应对未来。\n\n 8.  因此，大脑总是努力确保我们集中注意力，关注当下最根本的问题。只要有可能，头脑会有意识地把一些任务交给无意识去自动完成。这正是习惯形成时会发生的情况。习惯减轻了认知负荷，释放了心智能力，从而让我们可以将注意力分配给其他任务。\n\n 9.  习惯不会限制自由，它们会创造自由。事实上，没有建立习惯的人往往享有最少的自由。只有让生活的基本要素变得更容易，我们才能创造自由思考和创造力所需的精神空间。\n\n 10. 当我们固有的习惯适时发挥作用，生活的基本状况都得到了妥善解决，我们的头脑可以自由地专注于新的挑战并掌握接下来的问题的解决方案。在当前养成习惯会让我们在未来做更多我们想做的事情。\n\n 11. 养成习惯的过程可以分为四个简单的步骤：提示、渴求、反应和奖励。\n\n 12. 总的来说，提示触发渴求，渴求激发反应，而反应则提供满足渴求的奖励，并最终与提示相关联。这四个步骤一起形成了一个神经反馈回路---- 提示、渴求、反应、奖励、提示、渴求、反应、奖励 ----并最终让我们养成自然而然的习惯，由此构成完整的习惯循环。\n\n 13. 我们可以将这四个步骤分为两个阶段：问题阶段和解决阶段。问题阶段包括提示和渴求，也就是当我们意识到有些事情需要改变的时候。解决方案阶段包括反应和奖励，也就是当我们采取行动并实现我们想要的改变的时候。\n\n 14. 如何培养好习惯\n     \n                如何培养好习惯\n     第一定律(提示)   让它显而易见\n     第二定律(渴求)   让它有吸引力\n     第三定律(反应)   让它简便易行\n     第四定律(奖励)   让它令人愉悦\n\n 15. 如何戒除坏习惯\n     \n                  如何戒除坏习惯\n     第一定律反用(提示)   使其无从显现\n     第二定律反用(渴求)   使其缺乏吸引力\n     第三定律反用(反应)   使其难以施行\n     第四定律反用(奖励)   使其令人厌烦\n\n 16. 每当我们想要改变自己的行为时，可以问自己几个简单的问题：\n     \n     如何才能让它变得明显？\n     \n     如何才能让它更有吸引力？\n     \n     如何才能让它变得容易？\n     \n     如何才能让它令人愉悦？\n\n\n# 看着不对劲的那个人\n\n 1.  人脑是一台预测机器。它不断地感知你的周边环境，并分析和处理它所遇到的所有信息。\n 2.  只要有足够的练习，我们就可以不假思索地拾起预测特定结果的提示。\n 3.  我们的大脑会自动编码经验教训。\n 4.  我们并不能解释清楚我们正在学什么，但是学习的过程一直没有停歇，而我们在特定情况下注意到相关线索的能力是我们每个习惯的基础。\n 5.  随着习惯的形成，我们的行为会受到我们的自发和下意识的头脑的支配。我们会身不由己地陷入旧的模式而不自知。\n 6.  由于触发我们习惯的提示实在是太普遍了。所以，我们必须有意识地开启行为转变的进程。\n 7.  一旦习惯在我们的生活中牢牢扎根，它多半是下意识的和自然而然的。\n 8.  心理学家卡尔.荣格说："除非我们让下意识意识化，否则它将支配我们的生活，而我们就会称之为命运"。\n 9.  这一过程被称为指差确认，是一套安全系统，旨在减少人为失误。它看起来有些傻，但是它的效果极佳。指差确认减少的错误高达85%，并让事故发生率降低了30%。\n 10. 指差确认之所有如此有效，是因为它把下意识的习惯提升到了有意识加以确认的水平。因为列车司机必须做到眼、手、嘴和耳朵并用，这样可以确保他们提前注意到事故隐患。\n 11. 一种行为的自动化程度越高，我们就越不可能有意识地去想它。我们一遍遍地做某些事，久而久之，我们就习以为常了，只是机械地重复着，根本不会对我们所做的是否正确提出任何质疑。\n 12. 我们表现失败的缘由大多归因于缺乏自我意识。\n 13. 我们在改变习惯方面面临的最大挑战之一，就是一直能保持警觉，知道我们实际上在做什么。这有助于解释为什么坏习惯的后果会如影随行，暗暗地影响着我们。\n 14. 我们的个人生活需要一个"指差确认"系统。这就是习惯记分卡的由来，这是一个简单的练习，我们可以用它来更好理解我们的行为。\n 15. "习惯记分卡"，我们先要列出我们的日常习惯，好习惯就旁边标注"+"；坏习惯就标注"-"；中性习惯就标注"="。\n 16. 我们对习惯的标注，取决于我们的处境和目标。时好时坏完全取决我们当时努力的方向。其实世上并没有好习惯和坏习惯的之分，只有有效的习惯。解决问题上很有效。\n 17. 在练习中，我们可以依照长远来看会带给我们哪些好处的标准给自己的习惯分门别类。总的来说，好习惯会带给积极效果；坏习惯则会带来负面结果。\n 18. 我们可以尝试常问自己的问题："这种行为是否有助于我们成为我们希望成为的那种人？这个习惯是支持还是反对我想要的身份？"\n 19. 记分卡的目标，在于提醒自己注意实际发生的事情，就这么简单。观察我们的思想和行为，不要急于做出判断或自我批评。也不要因为自己有缺点而责怪自己，也不要因为自己有所成就而自我褒奖。\n 20. 改变不良习惯的第一步就是对它们保持警觉。也可以在生活中进行指差确认，大声地说出自己想要采取的行动和我们预期的结果。听到自己大声说出的坏习惯的后果显得更加触手可及。这会增加坏习惯行动的难度。即使是只想提醒自己该办哪些事，这种方法也很有用。\n 21. 行为转变的过程总是始于自觉。像指差确认以及习惯评分卡这类做法会专门帮你认清我们的习惯并认识到触发它们的提示，这使得我们有可能对自己有益的方式做出回应。\n\n\n# 培养新习惯的最佳方式\n\n 1.  他们填写的句子被研究人员称为执行意图，亦即我们事先就何时何地行动制定的计划。\n 2.  总的来说，创立执行意图的格式是："当x情况出现时，我将执行y反应。"\n 3.  执行意图都是确保我们不改初心的有效方法。他们提高了人们坚持物品回收、学习、早睡和戒烟等习惯的可能性。\n 4.  人们就何时、何地、具体做什么指定出具体计划后，就会更有可能贯彻执行。\n 5.  许多人认为他们缺乏做事的动力，但实际上他们真正缺乏的是明确的计划。\n 6.  何时何地采取行动并不总是显而易见的。有些人耗费一生都等不到自我提高的成熟时机。\n 7.  一旦设定了执行意图，我们就不必等待灵机一动的那一刻。当行动的时刻到来时，根本就不需要再做决定。简单地按照我们的预定计划去做即可。\n 8.  我们可以尝试在某周、某月或某年的头一天，这类时间段开始新计划，因为此时人们通常会满怀期望。只要我们有希望，我们就有足够的理由采取行动。新的开始总是让人感到欢欣鼓舞。\n 9.  设立执行意图，可以明确我们想要什么并且具体化我们实现的路径，将有助于我们摒弃妨碍我们进步、分散注意力，或让我们偏离正轨的事情。\n 10. 我们需要给我们的习惯在这个世界上存在的时间和空间。这样做的目的在于让时间和地点变得如此显而易见，以至于只要反复去做，积累到一定次数后，我们就会具备在恰当的时间做该做的事的冲劲，就连自己都不能解释为什么会这样。\n 11. 习惯叠加：颠覆我们的习惯的简单计划\n 12. 事实上，一次购买行为导致的连锁反应还有个名称：狄德罗效应。狄德罗效应指出，人们买入新用品后，往往会导致螺旋上升的消费行为，最终买入更多的东西。\n 13. 许多人类行为遵循这个循环。我们会根据刚刚完成的工作来决定下一步该做什么。每个动作都成为触发下一个行为的提示。\n 14. 当谈到培养新习惯时，我们可以充分利用行为的关联性。建立新习惯的最佳方法之一就是确定我们已有的习惯，然后把我们的新行为叠加在上面。这叫做习惯叠加。\n 15. 习惯叠加是执行意图的一种特殊形式。与其在特定的时间和地点培养新习惯，不如将它与当前的习惯整合。\n 16. 一旦我们适应了这种方式，就可以开发出通用的习惯叠加，一旦遇有恰当的时机就会发挥指导作用。健身：当我们看到楼梯时，会走楼梯而不是电梯。财务状况：当想买超过100美元的东西时，我会等24小时后再买。\n 17. 创建成功习惯叠加的秘诀是选择正确的提示来启动整个过程。与具体说明给定行为的时间和位置的执行意图不同，习惯叠加隐含着相应行动的时间和地点。\n 18. 在行动前先想想定在哪个时段最有可能成功。当我们忙于其他事情时，不要强求自己同时培养另一种习惯。\n 19. 行为转变的第一定律是让它显而易见。像执行意图和习惯叠加这样的策略，是为我们的习惯创造鲜明的提示，并为何时何地采取行动设计清晰计划的最实用的方法。\n\n\n# 原动力被高估，环境往往更重要\n\n 1.  你的习惯会根据我们所在的房间以及我们面前的提示而改变。\n 2.  环境是塑造人类行为的无形之手。\n 3.  尽管我们有独特的个性，但是在特定的环境条件下，某些行为往往会重复出现。在教堂里，人们倾向于低声说话。在黑暗的街道上，人们的警惕性会比较高，行事谨慎。\n 4.  最常见的变化形式并非源自内部，而是来自外部：我们被周围的世界所改造。\n 5.  每个习惯都以特定的环境为依托。\n 6.  行为是环境中人的函数，或者b(行为)=f(函数)[p(人)，e(环境)]。\n 7.  产品或服务越是触手可及，我们就越有可能去尝试。\n 8.  我们喜欢认为一切尽在自己掌握之中。\n 9.  我们每天采取的许多行动并不是由有目的的驱动和选择决定的，而是因为最得心应手。\n 10. 在人类中，感知是由感觉神经系统引导的。我们通过视觉、听觉、嗅觉、触觉和味觉来感知世界。\n 11. 视觉提示是我们行为的最大催化剂也是不足为奇了。\n 12. 我们可以想象一下在充满了富有成效的提示，无效提示一扫而光的环境中生活和工作是多么重要。\n 13. 每个习惯都是由提示引发的，我们更有可能注意到显眼的提示。\n 14. 如果我们想要让习惯成为我们生活中的重要组成部分，就让提示成为我们生活环境中的重要组成部分。\n 15. 确保我们的最佳选择匹配最鲜明的提示。当好习惯的提示一直在我们眼前晃，我们就会自然而然地做出正确的决定。\n 16. 环境设计的效用之所以强大，不仅是因为它影响了我们与世界的交往方式，也因为我们很少这样做。\n 17. 我们可以更改我们生活和工作的空间，以增加我们接触到积极提示的机遇，同时减少接触到消极提示的机会。\n 18. 环境设计让我们重新掌控自己，成为自身生活的建筑师。\n 19. 我们要争取成为自己的世界的设计师，而不仅仅是它的消费者。\n 20. 支配我们行为的不是我们的环境中的各类物品，而是我们与它们之间的关系。\n 21. 我们可以训练自己把特定的习惯和特定的环境联系起来。\n 22. 在全新的环境中习惯更容易改变。它有助于我们远离原有微妙的、促使我们恢复旧习惯的触发因素和提示。\n 23. 当我们走出平常的环境后，我们就会把我们的行为习惯遗留在原地。我们不再与旧环境中的提示做斗争，从而使得新习惯的形成过程不受干扰。\n 24. 如果想要创造性思维，就搬到一个更大的房间，去屋顶露台上待着，或者内部宽敞的大建筑物里。离开我们日常生活和工作，也就是与我们固有的思维模式联系密切的空间，换个环境放松一下。\n 25. 假如我们无法换个全新的环境的话，重新布置或重新安排我们现有的空间。为工作、学习、锻炼、娱乐和烹饪分别创造单独的空间。\n 26. 当工作和生活之间有了明确的分界线之后，我们就能很容易放下工作，转入身心放松的模式。\n 27. 尽可能避免将一种习惯的情景与另一种习惯的混在一起。一旦我们开始混合不同的情境，我们会把各种习惯混为一谈 ---- 那些比较容易实现的习惯通常会占上风。\n\n\n# 自我控制的秘密\n\n 1.  我们总是会这样认为，增强纪律性是解决我们所有问题的灵丹妙药，这种观念已经深深根值于我们的文化之中。\n 2.  科学家们对那些看起来有强大自控能力的人详细分析之后，发现他们和那些深陷泥潭的人没有什么不同。相反，"纪律性强"的人能更好地控制自己的生活，无须时常考验自己是否有坚强的意志力和自我控制的能力。\n 3.  换句话说，他们很少置身于充满诱惑的环境中。\n 4.  自我控制能力强的人通常最不需要使用它。\n 5.  假如你不需要经常自我克制的话，做起来就会更容易。\n 6.  毅力、勇气和意志力是取得成功的要素，但是增强这些品质的途径不是期望你自己成为一个自律的人，而是创造一个有纪律的环境。\n 7.  习惯一旦被编码，每当环境提示再次出现的时候，相关行动的冲动就会随之而来。\n 8.  坏习惯具有自身催化的能力：这个过程会自我滋养。它们一边激发人们的某些感觉，一边麻痹他们。这会成为恶性循环，坏习惯失去控制，接踵而来。\n 9.  我们可以改掉一个习惯，但是我们不太可能忘记它。\n 10. 说白了，我还未见过有人长期置身于消极环境中而能坚守积极的习惯。\n 11. 更可靠的方法是从源头上改掉坏习惯。消除坏习惯的最实用的方法之一就是避免接触引起它的提示。\n 12. 自我控制只是权宜之计，而非长远良策。\n 13. 我们能抵抗一两次诱惑，但是我们不可能每次都能铆足劲，克服强烈的愿望。\n 14. 与其每当我们想正确行事时都要诉诸于新的意志力，不如把精力花在优化你所处的环境上。这就是自我控制的奥秘。\n 15. 让良好习惯的提示显而易见，让不良习惯的提示脱离视线。\n 16. 自控能力强的人尽量远离充满诱惑的环境。逃避诱惑比抗拒诱惑容易。\n 17. 戒除坏习惯的最实用的方法之一就是减少接触导致坏习惯的提示。\n\n\n# 怎样使习惯不可抗拒\n\n 1.  在野外狩猎和觅食了数十万年之后，人类的大脑逐渐进化到高度重视盐、糖和脂肪的程度。当我们过着不知道何时能吃上下一顿饭的生活时，有机会就尽可能多吃当然是最佳生存策略。\n 2.  把盐、糖和脂肪放在第一位对我们的健康不再有利，但是这种渴望会持续下去，因为大脑的奖励中心已经有大约5万年没有改变了。\n 3.  具有动态对比特性的食物会一直让我们体验到新奇的有趣，鼓励我们多吃。\n 4.  面前的机会越有吸引力，养成的习惯的可能性就越大。\n 5.  这些就是我们所在的现代世界中的超常刺激。它们极度夸大了我们天然就有吸引力的那些特征，结果导致我们的本能痴迷癫狂，促使我们养成了过度消费的习惯、沉溺于社交媒体乃至色情、饮食等林林总总的习惯。\n 6.  如果想要提高某种行为发生的概率，那么我需要让它具备吸引力。\n 7.  习惯是多巴胺驱动的反馈回路。习惯带来的奖励会驱动分泌多巴胺，后续我们看到类似的提示就会分泌多巴胺，产生强烈的渴望，随着再次奖励的到来。反复就会加强我们行为才生习惯的原动力了，因为能产生多巴胺。\n 8.  多巴胺在许多神经过程中起着核心作用，其中包括行为动力、学习和记忆、惩罚和逃避以及随意运动。\n 9.  至于习惯，关键是：不仅发生在我们体验快乐的时候，而且在我们期待快乐的时候，都会分泌多巴胺。\n 10. 每当我们预测一个机会会有回报的时候，我们的体内的多巴胺浓度就会随着这种预期飚升。每当多巴胺浓度上升，我们采取行动的动机也会随之增强。\n 11. 激发我们采取行动的原动力来自于对建立的期待之时，而非这种期待得以满足的那一刻。\n 12. 对一种体验的期待往往比体验本身，更令人感到愉悦的原因之一。\n 13. 大脑将如此多的宝贵空间分配给负责渴求和欲望的区域，这一事实进一步证明了这些过程所发挥的关键作用。\n 14. 欲望是驱动行为的引擎，每一个行动都源于此前的预期，是渴望引发了回应。\n 15. 我们需要使我们的习惯变得有吸引力，因为最初促使我们采取行动的，正是我们对有奖励的经历的期待之心。这就是所谓绑定喜好战略开始发挥作用的地方。\n 16. 假如我们在做一件事的同时得以做另一件我们喜爱的事情，那么前面一件事很可能会对我们产生一定的吸引力。\n 17. 高频行为将会强化低频行为。\n 18. 喜好绑定其实是创建任何习惯的强化版本的方法之一，具体做法是将它与我们本想要的东西相关联。\n 19. 正是对奖励的期待，而不是奖励本身，促使我们采取行动。预期越高，多巴胺峰值越大。\n\n\n# 在习惯形成中亲友所起的作用\n\n 1.  "天才不是天生的，而是教育和训练出来的。"\n 2.  如果我们的文化崇尚什么样的习惯，那些习惯就会成为最有吸引力的行为。\n 3.  人类最深层的愿望之一就是有所归属。这种源远流长的嗜好对我们的现代行为是有着巨大的影响。\n 4.  我们早期的习惯不是选择而是模仿的产物。\n 5.  每一种文化和群体都有各自独特的期望和标准。\n 6.  "社会生活的习俗和实践裹挟着我们前行"。\n 7.  大多数时候，与群体共进共推并不会让人觉得是一种负担，因为每个人都想有所归属。\n 8.  但某种行为有助于我们融入团体或社会的时候，它就具备了吸引力。\n 9.  我们从自己身边的人那里学习习惯。\n 10. 一般来说，我们与他人越亲近，就越有可能模仿他们的一些习惯。\n 11. 培养好习惯的最有效方式之一就是加入一种文化，在这种文化中，我们偏爱的行为被认定为是正常行为。\n 12. 当我们看到别人每天都这样做的时候，会觉得培养新习惯似乎并不难。\n 13. 我们的文化设定了我们对"正常"事物的期望。尽量和那些具备我们想要拥有的习惯的人在一起，我们会相互促进。\n 14. 加入一种文化，其间我们喜好的行为是正常的行为；我们已经和这个群体有一些共同之处。\n 15. 没有什么比群体归属感更能维持一个人做事的动力了。它将个体的追求转变成了群体的追求。\n 16. 但我们加入了书友会、乐队或自行车爱好者团队时，我们的身份就会与周围的人建立了关联，成长和改变不再是个体的追求。\n 17. 集体身份开始强化我们的个人身份，这就是为什么在达成目标后还要保持团队一员身份对保持我们的习惯至关重要。\n 18. 友情和社区赋予人特定的身份并帮助一种行为长期持续。\n 19. 每个群体都对其成员施加巨大压力，要求他们服从集体规范。\n 20. 大多数时候，我们宁愿跟着众人一起犯错，也不愿特立独行坚持真理。\n 21. 人类的头脑知道如何与他人和平相处。它想和别人和平共处。这是我们生活在这个世界上的天然模式。\n 22. 你可以忽略它 ---- 你可以选择忽略这个群体或者不再关注他人的想法 ---- 但是这需要付出努力。\n 23. 逆主流而上需要付出额外的努力。\n 24. 当改变习惯意味着挑战部落规矩时，改变就没有吸引力的。\n 25. 当改变我们的习惯意味着顺应这个部落的要求时，改变是非常有吸引力的。\n 26. 人人都追求权力、声望和地位。我们渴望在外衣上别着奖章。我们期望自己有总裁或合伙人的头衔。我们希望得到认可、称赞和表彰。\n 27. 从历史上来看，一个人拥有更大权力和更高的地位意味着可以获得更多资源，不再过多担忧能否生存下去，并且能够更容易找到性伴侣。\n 28. 我们被那些能赢得尊重、认可、钦佩和地位的行为深深吸引着。\n 29. 这是我们如此关心高效人士的习惯的一大原因。我们视图模仿成功人士的行为，因为我们自己渴望成功。我们的许多日常习惯都来自于我们模仿崇拜的对象。\n 30. 身居高位的人尽情享受他人的认可、尊重和赞扬。这意味着，如果一种行为能为我们赢得认可、尊重和赞扬，我们就会认为它很有吸引力。\n 31. 我们也会尽可能避免会降低我们地位的习惯行为。我们一直想知道"别人会怎么看我"并根据答案相应地改变我们的行为。\n 32. 我们倾向于培养被我们的文化推崇的习惯，因为我们强烈地渴望融入并属于这个部落。\n 33. 我们倾向于模仿三个社会群体的习惯：亲近的人(家人和朋友)、所在群体(我们所归属的部落)和有权势的人(有地位和威望的)。\n 34. 如果一个行为能为我们赢得认可、尊重和赞扬，我们就会认为它很有吸引力。\n\n\n# 如何找到并消除你坏习惯的根源\n\n 1.  每个行为都有表层的渴望和深层的动机。\n 2.  渴望是深层动机的具体表现。\n 3.  在更深层次上，我们只是想减少不确定性和缓解焦虑，赢得社会认可和接纳，或者获得一定的社会地位。\n 4.  我们的习惯其实是用以满足古老欲望的现代方法，如果这是坏习惯，那么也就是旧恶习的新形式。\n 5.  你目前的习惯不一定是解决你面临的问题的最佳方式；它们只是我们掌握的方法。一旦我们把一个解决方案和我们需要解决的问题联系起来，我们就会不断地反复加以应用。\n 6.  习惯就是关联。这些关联决定了我们是否值得不断重复某种习惯。\n 7.  生活让人感觉是在被动应对，但实际上都是可预见的。你整体都在根据你刚刚看到的和以往的经验，预测出下一步最佳的应对行动，我们没完没了地预测下一刻会发生什么。\n 8.  我们的行为在很大程度上取决于我们如何解释与我们相关的事件，而未必是事件本身的客观事实。\n 9.  感觉和情绪将我们察觉的提示以及我们的预测，转化为我们可以加以应用的信号。\n 10. 渴望是一种缺乏某些东西的感觉，是改变我们内在状态的愿望。\n 11. 欲望就是我们的现状与我们设想中的未来状况之间的差别。\n 12. 我们的感觉和情绪告诉我们是安于现状还是改变现状。\n 13. 总而言之，我们感受到的特定欲望和我们展现出的高频动作，其实都体现我们深藏的、蠢蠢欲动的根本动机。\n 14. 每当一个习惯成功地满足了一个动机，我们就会产生一种再次尝试的渴望。\n 15. 我们将习惯与积极的情感联系起来时，习惯就有了吸引力，我们有了这种认识就可以为己所用，不断寻找乐趣，同时避免烦心事。\n 16. 如果我们能学会将高难度的习惯与积极的内心体验联系起来，我们就能使它们具备吸引力。\n 17. 只需换一个词，我们就可以改变看待每个事件的方式，从而将这些行为视为负担转变为视它们为机遇。\n 18. 我没有被困在轮椅上 ---- 我被它解放了。要不是因为我的轮椅，我就只能躺在床上，根本不能到户外活动。\n 19. 重建我们的习惯，突出它们的益处而非不足，这种短平快的方式可以改变我们的思维方式，并让一个习惯更有吸引力。\n 20. 我们可以将锻炼看作是培养技能和增强体质的途径。别再对自己说"我需要一早去跑步"，而要说"是时候增强我的耐力、加快跑步速度了"。\n 21. 我们可以将省钱与自由而不是限制相联系：生活在我们目前的收入水平之下会让我们未来的生活宽裕。我们在这个月省下的钱会提高我们下个月的购买力。\n 22. 每一次冥想的中断，都可以给我们一个练习呼吸的机会，我们可以将沮丧转化为喜悦。\n 23. 我们可以将"很紧张"定义为"很兴奋"，肾上腺素的增加会帮助我们集中注意力。\n 24. 我们可以创建一种激励的仪式。只需练习把我们的习惯和我们喜欢的东西练习起来，然后无论何时我们需要多一点动力，就可以启用这个提示。\n 25. 找到让我们真正开心的事，如抚摸我们的狗或洗个泡泡浴，然后在做我们喜欢的事情之前，创建一个我们每次都要做的简短的例行程序。也许我们可以做三次深呼吸，然后微笑一下。\n 26. 最后，我们会把这种深呼吸+微笑的例行模式与心情愉快联系起来。它成为一个提示，意味着我们感觉到快乐。一旦确立了这种关联，我们可以在任何需要改变情绪状态时加以应用。\n 27. 工作压力太大？做三次深呼吸，然后微笑。生活不如意？做三次深呼吸，然后微笑。一旦养成了习惯，相关的提示会引发渴望，即使它与最初的情形毫无关系。\n 28. 找到坏习惯形成的源头并予以根除的关键是重新构建我们对坏习惯的关联。这并不容易，但是如果我们能重新编程我们的预测，我们就能把令人望而却步的习惯变为有吸引力的习惯。\n 29. 好习惯与积极的情感联系起来，而让坏习惯与讨厌的情感联系起来。这样就能为己所用，释放心智。\n 30. 强调坏习惯所带来的坏处，让坏习惯不再有吸引力。\n 31. 当我们将习惯与积极的感受相联系，习惯就有了吸引力；反之，则没有吸引力。在开始培养难度较大的习惯之前，先做些我们喜欢的事情来创造一种激励仪式。\n\n\n# 慢步前行，但绝不后退\n\n 1.  所有的优秀作品都出自数量组的学生之手。\n 2.  试图找到最佳转变方案的努力，比如试图寻找减肥捷径、强身健体的最优方案，以及开展副业的好点子等，很容易陷入困境。我们这是在一门心思地要找到做事的最佳方式，却从来不付诸于行动。\n 3.  结果是"因追求最佳而丢掉了足够好"。\n 4.  酝酿与行动是有区别的。酝酿意味着我们只是在计划、策划和学习。这是都是好东西，但是它们不会产生结果。\n 5.  采取行动才是会产生结果的行为类型。有时候酝酿也是有益的，但是它本身永远不会产生结果。\n 6.  我们为何还要去酝酿呢？有时候我们这样做是因为我们确实需要计划或了解更多情况。但通常，我们这样做的理由是它可以让我们感觉自己在取得进展，同时又不必承担失败的风险。\n 7.  我们大多数人都是回避批评的专家。遭遇失败，或被公开批评令人感觉不好，所以我们倾向于避免落入那种境地。这就是我们总是在酝酿却不采取行动的最大原因：我们是想让可能遭遇的失败来得晚一些。\n 8.  酝酿让你感觉自己正在做事。但实际上，我们只是在准备做事。\n 9.  当准备工作变成某种形式的拖延时，我们需要有所改变。我们不想只是一味地做计划，要真刀真枪地操作起来。\n 10. 如果我们想掌握一门习惯，关键是从重复开始，无须力求完美。我们也不必描画出新习惯的每一个特征。我只需要的是不断练习。\n 11. 这是第三定律的第一条要点：我们需要关注的是次数。\n 12. 习惯的形成是一种行为通过重复变得越来越自动化的过程。\n 13. 我们重复活动得越多，我们的大脑结构变化得也就越多，从而能更高效地进行那项活动。\n 14. 随着每一次重复，细胞间的信号传递得到改善，神经连接变得更加紧密。\n 15. 重复一个动作会导致大脑明显的生理变化。\n 16. 人们在学习一门外语、演奏一种乐器或演练尚不熟悉的动作时，会感动难度极大，因为每种感觉必经的通道尚未建立起来；但是，连续不断地重复打通了沟通途径之后，这种困难顿时烟消云散；这些动作变得如此连贯自然，即便心不在焉也能一气呵成。\n 17. 常识和科学证据都认同这一点：重复是一种变化形式。\n 18. 我们每次重复一个动作，也就激活了一个与这个习惯相关的特定神经回路。\n 19. 这意味着，我们养成新习惯的最关键步骤之一就是不断地重复。\n 20. 所有习惯都遵循类似的演变轨迹，从刻苦练习到行动自如，这一过程被称为自动性。\n 21. 自动性是指无须考虑每一个步骤而实施一种行为的能力，这种能力发生在下意识的时候起作用。\n 22. 科学家称之为学习曲线的这些图表上的形状，揭示了行为转变的一个重要事实：习惯是基于频率而不是时长形成的。\n 23. 就习惯的培养而言，不在于时间长短。重要的是我们这种行为的频率。\n 24. 我们的习惯已经在重复了数百次(如果不是数千次)之后被内化了。\n 25. 要养成新习惯需要同样的频率。我们需要把足够多的成功尝试串联起来，直到这种行为牢牢地嵌入我们的头脑中，使得我们超越了那条习惯线。\n 26. 在现实生活中，需要多久才能实现习惯成自然并不重要。重要的是我们要采取我们需要采取的行动以取得进步。一个动作是否完全自动并不重要。\n 27. 行为转变的第三定律是让它简便易行，也就是为了增加频率。\n 28. 最有效的学习形式是付诸实践，而不是纸上谈兵。\n 29. 专注于采取行动，而不只是酝酿行动。\n 30. 习惯的形成是一个行为通过重复逐渐变得更加自动化的过程。\n 31. 习惯的培养不在于时间长短，而在于重复的次数。\n\n\n# 最省力法则\n\n 1.  传统智慧认为动机是习惯转变的关键。也许真是这样，就是说假如你真的想要，你就真的会去做。但事实是，我们真正的动机是贪图安逸，怎么省事怎么做。\n 2.  不管最新出炉的提高生产率方面的畅销书怎么说，图省事才是一个聪明而非愚蠢的策略。\n 3.  精力是宝贵的，而大脑的设定就是尽一切可能保存精力。\n 4.  人类的天性就是遵循最省力法则：当在两种相似的选项之间做决定时，人们自然会倾向于需要最小工作量的那一个。\n 5.  在我们可能采取的所有行动中，最终被选择的行动一定是能以最小的努力获得最大价值的那一个。\n 6.  我们被激励着避重就轻，只做容易的事。\n 7.  每个动作都需要消耗一定的能量。所需能量越多，发生的可能性就越小。习惯需要的能力越少，它发生的可能性就越大。\n 8.  看看任何占据你生活大部分时间的行为，你会发现他们都简单易行，不需要有多大的激励。\n 9.  从某种意义上来说，每个习惯都妨碍着你获得真正想要的东西。坚持节食的目的能带来健身的效果，坚持冥想能感到平静，坚持写日记能得到思路清晰。\n 10. 实际上习惯本身并不是你想要的。你真正想要的是习惯带来的结果。\n 11. 障碍越大 ---- 也就是说，习惯坚持起来的越难 ---- 你和你想要的最终状态之间的阻力就越大。\n 12. 这就是为什么要让你的习惯变得简单至极，只有这样才能让你即使不喜欢它，也会坚持做。如果你能让好习惯简便易行，你就越有可能坚持下去。\n 13. 有些时候我们会逆流而上，另一些时间我们只想急流勇退。\n 14. 在那些艰难的日子里，让尽可能多的事情对你有利是至关重要的，这样你就能克服生活中遇到的不可避免的难处。\n 15. 你面对的阻力越小，你坚强的一面就越有可能浮现出来。\n 16. 让它简便易行的说法不仅仅是做容易的事，其主旨是尽可能确保你可以毫不费力地去做具有长期回报的事。\n 17. 与其劳神费力地克服生活中的阻力，不如设法减小阻力。\n 18. 在你设法减小由你的习惯产生的阻力时，最有效的方法之一就是进行环境设计。\n 19. 利用环境设计让提示更显而易见，我们也可以优化我们的环境使得我们更容易行动。\n 20. 或许更有效的方法是减少家里或办公室内部的呈现的阻力。我们完全可以清除妨碍我们办正事的阻力点。\n 21. 日本公司强调为人所知的"精益生产"理念，坚持不懈地努力寻求从生产流程中去除各种浪费，直至重新设计工作环境，使得工人们的身体不必转来转去，从而避免为拿工具而浪费时间。\n 22. 当我们消除我们时间和精力的阻力点时，我们就能够取得事半功倍的效果。\n 23. 这是整理房间让人感觉非常好的一个原因：我们减轻了环境施予我们的认知负荷，从此可以轻装前进了。\n 24. 商业上的追求永无止境，总是以更简便的方式提供同样的结果。\n 25. 归根结底，为了养成更好的习惯，我们不得不耗费很大精力，设法克服与我们已有的好习惯相关的惰性，同时加大与不良习惯相关的阻力。\n 26. 每当你整理一个空间以满足其预期用途时，你都是再启动该动机，使得接下来的动作简单易行。\n 27. 一件事做起来越麻烦，你就越不可能想要继续做。 利用到坏习惯上。\n 28. 实际上我们只需要稍微增加一些难度，人们就会停止不必要的行为。\n 29. 假如把啤酒藏到冰箱最里面很难一眼就看到的地方，我们就喝得就少了。当我们从手机上删除社交媒体应用程序后，可能需要几周才会再次下载并登陆。这些小手段不太可能遏制真正的上瘾，但对我们中的许多人来说，增加一点点坏习惯的难度，可能就意味着更容易养成好习惯。\n 30. 我们都应该问自己同样的问题："我们该怎么设计一个让人们的行为易于端正的世界?"重新设计你的生活，让对你来说最重要的事成为最容易做的事。\n 31. 人类行为遵循最省力的法则。我们天然地倾向于付出最少的工作量的选择。\n 32. 创造一个环境，尽可能让人们便于做正确的事。\n 33. 降低与良好行为相关的阻力。阻力小，习惯就容易养成。\n 34. 增加与不良行为相关的阻力。阻力大，习惯就难以养成。\n 35. 预备好你的环境，使未来的行动更容易。\n\n\n# 怎样利用两分钟规则停止拖延\n\n 1.  "这是个极其简单的动作，但是每天早上都以同样的方式去做，就成了习惯性动作 --使它可以重复，易于做到。它减少了我偷懒或以不同方式做它的机会。它不过是在我的日常行为库里有加了一项，同时减少了一件需要想起来才会做的事。"\n\n 2.  研究人员估计，我们每天的行动有40%至50%都出自习惯。这已经占比很高了，但是你的习惯施加的真正影响远大于这些数字所显示的。习惯属于自动选择，会影响到随后经深思熟虑做出的决定。\n\n 3.  习惯就像高速公路的入口匝道。它们引导你走上一条路，使你在不知不觉中加速前进，直到走上正路。\n\n 4.  持续做你已经在做的事似乎比重新开始做不同的事要容易得多。\n\n 5.  你会坚持看完长达两小时的烂电影。即使你已经吃饱了，你依旧不停地吃零食。你本想着就玩"一小会儿"手机，结果是20分钟很快就过去了，你仍然盯着手机屏幕。就这样，你不假思索的习惯往往左右着你有意做出的选择。\n\n 6.  每天都有几个时刻产生巨大的影响。我把这些小选择称为决定性时刻。你决定叫外卖或者在家自己做晚餐的那一刻，你决定开车或骑自行车的那一刻，你决定开始做家庭作业或者拿起电子游戏控制器的那一刻：这些选择就是生活之路上的岔路口。\n\n 7.  决定性时刻为你未来的设定了选择。例如，走进餐馆是一个决定性的时刻，因为它决定了你午餐吃什么。\n\n 8.  我们受到自身特有的习惯带来的限制。这就是掌握一天中决定性时刻如此重要的原因。每一天都由许多时刻组成，但真正决定你一天行为的是你的一些习惯性选择。这些小选择累积起来，每一个都为你如何度过下一段时间设定了轨迹。\n\n 9.  习惯是切入点，而不是终点。它们是出租车，而不是健身房。\n\n 10. 即使你知道应该从小处着眼，第一步也很容易迈得太大。当你梦想做出改变时，你会抑制不住地异常兴奋，一时头脑发热就容易贪多嚼不烂。我所知道的对抗这种趋势的最有效的方法是使用两分钟规则，也就是："当你开始培养一种新习惯的时候，它所用时间不应超过两分钟"。\n\n 11. 你会发现几乎任何习惯都可以缩减为两分钟的版本：\n     \n     * "每晚睡前阅读"变为"读一页"。\n     * "做30分钟瑜伽"变成"拿出我的瑜伽垫"。\n     * "复习功课"变成"打开我的笔记"。\n     * "整理衣物"变成"折叠一双袜子"。\n     * "跑3英里"变成"系好我的跑步鞋带"。\n\n 12. 这样做的思路是让你的习惯尽可能容易开始。\n\n 13. 任何人都可以沉思一分钟，读一页书，或者收好一件衣服。正如我们刚刚讨论的，这是一个强大的战略，因为一旦你开始做正确的事情，继续做下去会容易得多。\n\n 14. 一个新习惯不应该让人觉得是一个挑战。接下来的行动可能具有挑战性，但是最初两分钟应该不难。你想要的是一种"门户习惯"，它自然会引导你走上更有成效的道路。\n\n 15. 人们经常认为，读一页书、冥想一分钟或打个销售电话都是小事一桩，没什么可大惊小怪的。\n\n 16. 但此处的重点不是做一件事，而是把握住萌芽的习惯。事实上，你首先要确立一种习惯，然后才能不断改进它。\n\n 17. 如果你掌控不好养护习惯幼苗的基本技能，那么你就不大可能把握好与之相关的细节。\n\n 18. 不要指望从一开始就培养一种完美的习惯，要脚踏实地，连续不断地做些简单的事。你必须先标准化，然后才能优化。\n\n 19. 一旦你学会了呵护习惯的幼苗，前两分钟只是启动正式程序的仪式而已。\n\n 20. 这并非是为了更容易培养习惯而删繁就简的举动，而是掌握一项困难技能的理想方式。\n\n 21. 一种程序的开始阶段越是有仪式化，你就越有可能实现注意力高度集中，即做大事必需的状态。\n\n 22. 通过在每次锻炼前做同样的热身，你会更容易进入最佳状态。通过遵循同样的创造性仪式，你可以更容易地投入到艰难的创造性工作中。\n\n 23. 通过养成定时关灯的习惯，你可以更容易地在每晚合理的时点上床睡觉。你可能无法使整个过程自动化，但你可以让第一动作变成下意识动作。万事开头难，但要是把开始变得简便易行，接下来的事也就水到渠成了。\n\n 24. 我的一位读者用这种策略减肥，最终成功减去了100多磅。一开始，他每天都去健身房，但是他告诉自己，他健身的时间不能超过5分钟。他回去健身房，锻炼5分钟，时间一到就立刻离开。就这样过了几个星期后，他环顾四周，心想："嗯，反正我总是来这里。我不妨多待一会儿。"几年后，他的体重正常了。\n\n 25. 写日记提供了另一个例证。几乎每个人都能因写出自己的想法而受益，但是大多数人在写了几天后就坚持不下去了，或者根本就不写日记，觉得它就是件烦心事。\n\n 26. 坚持写日记的秘诀是永远别把它变成不得不做的工作。\n\n 27. 来自英国的领导力顾问格雷戈.麦吉沃恩养成了每天写日记的习惯，他的具体做法不是写出自己的全部想法，而是适可而止。他总是在感觉写烦了之前及时收笔。\n\n 28. 像这样的策略也有另一个原因：它们强化着你想要建立的身份。如果你连续五天现身健身房，哪怕只在那里停留两分钟，你就是在为你的新身份投赞同票。\n\n 29. 你去健身房的出发点不是要有副好身材，而是专注于成为那种不会错过健身的人。你只是采取小小的行动，但它确认着你想成为的那种人。\n\n 30. 我们很少考虑以这种方式审视改变，因为每个人都心无旁骛地盯着最终目标。\n\n 31. 在某个时候，一旦你养成了习惯，并且每天都有所表现，你就可以将两分钟规则和我们所称的习惯塑造的技术结合起来，将你要培养的习惯向最终目标扩展。\n\n 32. 从掌握最小行为的前两分钟开始。然后，向中间阶段推进，并重复这个过程---- 只关注前两分钟，一定要在这个阶段做扎实，然后再继续进入下一阶段。\n\n 33. 最终，你会养成你原本希望养成的习惯，同时仍然把注意力放在应该的地方：行为的前两分钟。\n\n 34. 每当你努力要保持一个习惯时，你都可以采用两分钟规则。这是让你的习惯变得简单的方法。\n\n 35. 习惯可以在几秒钟内完成，但会持续影响你在接下来的几分钟或几个小时的行为。\n\n 36. 许多习惯发生在决定性的时刻，每时每刻的选择就像岔路口，你的选择最终会导致卓有成效，或者一事无成的一天。\n\n 37. 两分钟规则规定："当你开始培养一种新习惯时，它所用时间不应超过两分钟"。\n\n 38. 一种程序的开始阶段越是仪式化，你就越有可能进入做大事所学的注意力高度集中的状态。\n\n 39. 习惯优化前先要实现标准化。你不能改善一个不存在的习惯。\n\n\n# 怎样让好习惯不可避免，坏习惯难以养成\n\n 1.  1830年冬天期间，由于没有适合外出的衣服，他一直待在书房里奋笔疾书。《巴黎圣母院》于1831年1月14日提前两周出版。\n\n 2.  成功不是简单地让好习惯简便易行，更重要的是让坏习惯难以延续。\n\n 3.  承诺机制是指你当下的抉择左右着你未来的行动。这是一种锁定未来行为，约束你养成良好习惯、迫使你远离不良习惯的方法。\n\n 4.  承诺机制是有用的，因为它们能让你在成为诱惑的受害者之前，先让良好的意愿发挥作用。举例来说，每当我想减少卡路里的时候，我都会让服务员在给我上饭菜之前就分成两份，其中一份打包带走。如果我一直等到饭菜端上来之后，再告诫自己"只吃一半"的话，那就太晚了。\n\n 5.  承诺机制实则使得坏习惯在当前变得难以施行，从而提高了你未来做该做的事的可能性。\n\n 6.  破除坏习惯的最好方式就是让它变得不切实际。不断提高其难度，直到你心灰意冷。\n\n 7.  只需做一次的行动，锁定好习惯\n     \n     下面的内容，只需要你做一次，就能有很好的效果。能够让你更容易睡好觉，吃得健康，做事效率高，省钱，而且通常能生活得更好。\n     \n     营养                 幸福\n     购买饮用水过滤器           养只狗\n     用小盘子吃饭，减少热量摄入      搬家到待人友好的社区\n     睡眠                 一般性健康\n     买好床垫               打疫苗\n     挂上深色窗帘             买好鞋，避免背痛\n     把电视机移出卧室           买把支撑椅或站立式桌子\n     生产力                财务\n     取消邮件订阅             加入自动储蓄计划\n     关闭消息提醒并设置群聊静音      设置自动支付账单\n     把你的手机设置为静音         取消有线电视\n     使用邮件过滤器清理你的收件箱     要求服务提供商降低你的费用\n     删除你手机上的游戏和社交媒体账号   \n\n 8.  让技术为我们服务，使得好习惯召之即来并戒除坏习惯。\n\n 9.  技术可以将以前艰难、恼人和复杂的行为转变成容易、轻松和简单的行为。它是确保正确行为的最可靠和最有效的方法。\n\n 10. 尽可能利用技术让你的生活自动化之后，你就能把腾出来的一些时间和精力用在技术还帮不上的地方。\n\n 11. 我们让技术介入的每个习惯都会释放一些时间和精力，可被投入下一个发展阶段。\n\n 12. 自动化的缺点是，我们满足于一件接一件地做些不用费脑子的事，再也不想抽时间做些稍有难度但最终更有意义的事。\n\n 13. 我们在工作之余总是禁不住要浏览社交媒体上的内容。只要感觉有些无聊，就会拿起手机。人们很容易自我安慰地说这些做法"只是放松一下"而已，但是随着时间的推移，它们会累积成一个严重的问题。持续不断的"再过一分钟"会妨碍我做任何重要的事。\n\n 14. 一旦我们的坏习惯难以持续，我们就会发现自己确实更想去做有意义的事，在我们把精神糖果从我们的环境中移除之后，吃健康食品就变得容易多了。\n\n 15. 通过利用承诺机制、战略性的一次性决策，以及技术手段，我们可以创造一个使自己无法回避的环境 ---- 在这个空间里，好习惯不仅是我们期待的结果，也是几乎不可避免的结果。\n\n 16. 锁定未来行为的终极途径是自动化你的习惯。\n\n 17. 使用技术自动化你的习惯是保证正确行为的最可靠和有效的途径。\n\n\n# 行为转变的基本准则\n\n 1.  问题不在于是否有意识，而是能否一直坚持。\n 2.  "一般来说，人们更愿意使用能带来强烈感官愉悦的产品，比如散发着薄荷香型的牙膏"\n 3.  一旦我们体验到做一件事所享有的乐趣，就很可能愿意重复去做这件事，这完全合乎逻辑。\n 4.  即使是用香皂吸收这种小事，人们体验到了闻起来很香，丰富的泡沫令人赏心悦目，由此产生的快乐感觉会给大脑发送信号："这感觉很好，继续这么做。"\n 5.  反之，如果我们体验的是不愉快，就肯定不想继续做。\n 6.  类似这样的故事证明了行为转变的基本规律：重复有回报的行为；避免受惩罚的动作。\n 7.  你会根据你过去所得到的的奖励(或受到的惩罚)学习将来该怎么做。积极的情绪有益于培养习惯，消极情绪则会摧毁它们。\n 8.  行为转变的前三条定律 ----让它显而易见；让它有吸引力；让它简便易行 ---- 增加了当下这种行为发生的概率。行为转变的第四条定律 ---- 让它令人愉悦 ----提高了下次重复这种行为的可能性。它形成了完整的习惯循环。\n 9.  我不只是在寻找满足感，我们要的是即时满足感。\n 10. 我们生活在科学家称之为延迟回报的环境中，因为你要工作很多年后才能看到预期的回报。\n 11. 人类的大脑并没有一直在延迟回报的环境中进化。近年来，整个世界发生了天翻地覆的变化，但是人性的变化微乎其微。\n 12. 重视即时满足是有道理的，活在当下，何必要杞人忧天。在即时回报环境中生活了成千上万代之后，我们的大脑进化成偏爱快速回报而不是长期回报。\n 13. 就不良习惯而言，即时结果通常感觉良好，但最终结果却不好。就好习惯而言，情况正好相反：即时结果令人不愉快，但是最终结果的感觉却很好。\n 14. "几乎总是发生这样的情况，当即时后果有利时，后来的后果将是灾难性的，反之亦然.....习惯的第一个果实越甜，以后的果实就越苦。"\n 15. 我们要在当下为良好习惯付出代价；否则我们要在将来为坏习惯付出代价。\n 16. 一般来说，我们从一项行动中越快享受到乐趣，我们就越应该质疑它是否符合我们的长远利益。\n 17. 行为转变的基本规则也可以更新为：重复有即时回报的行为，避免受即时惩罚的动作。\n 18. 我们对即时满足感的偏好揭示了一个关于成功的重要事实：因为我们天性如此，大多数人整体都在寻求及时享乐的机会。人们倾向于选择即时享乐的事，回避延迟满足的事。如果你愿意等待回报的到来，我们将面临更少的竞争，通常会获得更大的回报。能坚持到取得最后胜利的人终究是少数。\n 19. 善于延迟满足的人高考分数较高，不太可能沾染毒品，肥胖的可能性更低，能更好地应对压力，社交技能也更强。\n 20. 在某个时候，几乎每个领域的成功都要求你忽略即时奖励，而代之以延迟奖励。\n 21. 延迟满足的习惯是能训练出来的，但在我们这样做时需要顺应人性，而不是与之对抗。在训练延迟满足的过程中，凡是长远地看能带给我们回报的事，我们可以给它添加一点即时快乐；凡是不能的，我们可以添加一点即时痛苦。\n 22. 保持习惯的关键是要有成就感，哪怕只是细微的感受。成就感是一个信号，它表明我们的习惯有了回报，我们为此付出的努力是值得的。\n 23. 在现实生活中，只有在好习惯让我们尝到了一些甜头后，我们才会觉得它有价值。在它的形成阶段，我们一直是在做出牺牲。\n 24. 我们去过几次健身房，但我们并没有立刻变得强壮、健康或跑得更快，至少没有任何可见的改观。只有在几个月之后，我们的体重减掉了几磅或我们的手臂肌肉突起，从此我们就有了锻炼身体的积极性，更愿意去健身。\n 25. 开始的时候，我们需要一个坚持下去的理由。这就是为什么说即时奖励是必不可少的。它们维持着我们的兴奋点，而延迟奖励则在不动声色地逐渐累积。\n 26. 我们希望对习惯的结局存在好感。对此，最佳的方式是利用增强法，也就是利用即时奖励来提高一种行为频度的过程。\n 27. 增强法将我们的习惯与即时奖励联系在一起，当我们完成时，它会让你心满意足。\n 28. 长期保持"不冲动购物"或"本月禁酒"之类的习惯极具挑战性，因为就算我们错过了喝点小酒的欢乐时光或没有买下让你心动手痒的那双鞋子，生活照旧，与以往并没有什么不同。\n 29. 假如我们起初什么都没做，想要感到满意几乎是不可能的。我们所做的知识在抗拒诱惑，而此举不可能带给人满足感。\n 30. 解决这个问题的方法之一是颠倒过来。我们要让希望回避的习惯变得可见。开立一个储蓄账户，并注明这个账户专门用于将来买我们特别想要的东西。每放弃购买一件物品时，我们就把相应数额的钱存入这个账户。\n 31. 这就是为自己创建了一个忠诚计划。看到自己省钱买皮夹克的即时回报比放弃购物的感觉好得多。如此一来，即便你什么都没有买，依然能感到很满足。\n 32. 千万要选择能够强化我们身份的短期奖励，不能让它们与我们的身份相冲突。\n 33. 同理，泡个澡或四处闲逛来享受我们的闲散时光就是自我奖励的好例证，这与我们追求更多自由和经济独立的最终目标是一致的。\n 34. 这样使得，短期回报与我们保持身体健康的长期愿景相吻合。\n 35. 最终，随着内在建立，如心情舒畅、精力旺盛和身心放松之类相继到来，我们不再一心追求次要奖励。我们的新身份本身就变成了强化者。\n 36. 我们只有这样做才符合我们的身份，而且这么做，我们感觉很好。\n 37. 习惯与我们的生活贴合得越紧密，我们就越不需要外界的鼓励而能坚持下去。奖励可以启动一种习惯的培养进程，身份则可以维持一种习惯。\n 38. 尽管如此，证据的积累和新身份的出现需要时间。在长期回报到来之前，即时强化有助于在短期内保持动力。\n 39. 只有当转变充满乐趣的时候，习惯才会变得容易。\n 40. 当体验令人愉悦时，我们更有可能去重复一种行为。\n 41. 人脑进化为优先考虑即时奖励而不是延迟奖励。\n 42. 要保持一种习惯，我们需要有即时成就感，即使它体现在细微之处。\n\n\n# 怎样天天保持好习惯\n\n 1.  这种技巧叫做"曲别针策略"。有位女士在写作时每完成一页，就把发夹从一个容器转移到另一个容器。有位男士每做一个俯卧撑，都会从一个筒里拿出弹珠放进另一个筒里。\n 2.  取得进步令人满意，借助于视觉量度，如移动曲别针、发夹或弹珠，我们可以清晰地看到自己的进步。\n 3.  这样做的结果是，它们强化着我们的行为，并为任何活动增加一些即时满足感。\n 4.  视觉量度有多种形式：食物日志、健身日志、打孔忠诚卡、软件下载进度条、甚至书籍中的页码等。\n 5.  也许衡量我们进步的最好方法是利用习惯跟踪法。\n 6.  习惯跟踪法是衡量我们是否养成习惯的简单办法。它的最简单的方式是拿一份日历，划掉我们例行公事的每一天。\n 7.  追踪记录自己习惯的人数不胜数。从20岁开始，富兰克林就随身携带一本小册子，用来追踪自己遵从13项良好品行的情形。他的列表包括了诸如"抓紧时间。永远把时间用于做有意义的事情"以及"避免闲聊"之类的目标。每天结束，富兰克林都会打开他的小册子，记录他的进步。\n 8.  美国喜剧演员jerry seninfeld在记录片《喜剧演员》中，他曾解释说，他的目标仅仅是"永不中断"，坚持每天都写笑话。换句话说，他关注的不是某个笑话的好坏，或者有没有灵感，而是专注于天天这么做，不断夯实自己的基础。\n 9.  “永不中断”是一句强有力的励志语录。\n 10. 你要连续不断地拨打推销电话，只有这样才能提高你的销售业绩。不要中断健身进程，坚持下去，你会发现自己的身体状况一天比一天好，远超你的预期。不要中断你每天的创作，随着时间的积累，你会收获令人惊叹的作品集。\n 11. 习惯追踪功能强大，因为它充分利用了多个行为转变定律。它使一种行为同时变得显而易见、有吸引力和令人愉悦。\n 12. 记录你的上一个动作会创建一个启动下一个动作的触发器。习惯追踪自然会建立一系列的视觉提示，比如在日历上打的叉或者进餐日志中的食物列表。当我们翻看日历时，那一连串标记无疑在提醒我们继续采取行动。\n 13. 研究表明，追踪减肥、戒烟和降血压等目标进展的人比不追踪的人更可能有所成就。\n 14. 仅仅追踪一个行为就能激发改变它的冲动。\n 15. 习惯追踪也能让我们保持诚实。我们大多数人都对自己的行为的看法都不符合实际，我们认为自己做得很好，但事实并非如此。追踪测量可以帮助我们消除自我认识的盲点，并注意到每天我们究竟都做了什么。\n 16. 只要看一眼罐子里面有多少曲别针，我们立刻就能知道自己做了(或没有做)多少事。当证据就在我们面前时，我们不太可能再自我欺骗。\n 17. 最有效的激励形式是可知的进步。\n 18. 当我们接收到取得进展的信号后，我们会更有动力按既定的路径前进。\n 19. 这样，习惯追踪会对动机带来持续增强的效果，点滴进步都会激励你想要取得更多成就。\n 20. 在当我们遇到挫折时，这会产生奇效。当我们情绪低落时，很容易忽略我们已经取得的所有进步。习惯追踪提供了我们付出的所有艰苦努力的视觉证据，默默地提醒我们已经取得了多大进步。\n 21. 此外，我们每天早上在日历上看到的空白方格，会激励我们开始努力工作，因为我们不想因为中断一次而导致前功尽弃。\n 22. 最重要的是，追踪行为本身转化成了奖励的形式，从待办事项列表中划掉一个项目，在健身日志中又记上一笔，或者在日历上打个叉，这些都令人感觉心满意足。\n 23. 看着自己的成绩，比如投资组合的规模、书稿的页数等持续增长，满足感不言而喻。当感觉不错时，我们就更有可能坚持下去。\n 24. 习惯追踪也有助于我们心无旁骛：我们关注的焦点是过程而不是结果。我们并不执着于获得六块腹肌，只是想保持这种状态，成为那种不会偷懒、努力健身的人。\n 25. 习惯追踪具有三方面的功效：其一，创建视觉提示，提醒你采取行动；其二，内在激励机制，因为你清楚地看到了你的进步轨迹，并且不想失去它，以及其三，每当你记录下又一项成功的习惯实例时，我们都会享受到满足感。\n 26. 另外，习惯追踪提供了视觉证据，证明了我们在把自己塑造成为我们特别想成为的那类人，这本身就是一种令人感觉愉快的即时、内在满足的形式。\n 27. 许多人在抵制追逐和度量的想法。它让人感觉是个负担，因为它迫使我们养成两种习惯：我们视图培养的习惯，同时还要追踪它的习惯。\n 28. 追踪并不适用于每个人，也没有必要测量一辈子。\n 29. 如何才能让追踪轻而易举呢？只要有可能，测量应该自动化。我们的信用卡账单记录了我们出去吃饭的频率。我们的智能手环记录了我们走了多少步，睡了多久。一旦我们知道从哪里获取数据，就在日历上记一下，提醒自己每周或每月查看一次，这比每天都去查看更可行。\n 30. 其次，手动追踪应仅限于我们最重要的习惯。持续追踪一个习惯比随意跟踪十个习惯要好。\n 31. 了解一下我们在实际生活中的每时每刻是怎么度过的其实是很有趣的一件事。也就是说，每个习惯都有一个周期，总会在持续一段时间后结束。我们需要做好预案，随时应对偏离正轨的习惯，这比单纯衡量更重要。\n 32. 当我们固有的生活节奏被意想不到的事扰乱之后，每当我们遇到这种情况，我们都会试着提醒自己严格遵守一条简单的规则：绝不错过两次。\n 33. 假如有一天我错过了，我会尽可能地接上。错过一次健身会发生，但我们不会连续错过。\n 34. 我不可能做的完美无缺，但我可以避免第二次失误。\n 35. 一个习惯周期结束后，我会接着开始下一个周期。\n 36. 初犯不会毁了你。真正要命的是随之而来的不断重复的错误。\n 37. 错过一次是意外，错过两次是一种新习惯的开始。赢家和输家的差别就体现在这里。\n 38. 任何人都可能有糟糕的表现、糟糕的健身安排或某一天工作没干好。但是当成功人士摔倒后，他们会迅速爬起来。\n 39. 一个习惯偶尔被打断并不可怕，只要能迅速接上即可。\n 40. 我认为这个原则实在是太重要了，因此即便我不能像我想的那样做得很完美，我也会坚持不懈。\n 41. 很多时候，我们在培养习惯时会陷入全有全无的怪圈中。问题不在于出差错，而是如果不能做完美，就干脆不做的错误想法。\n 42. 你是真的没有意识到在你情绪低落(或忙碌)的日子里继续做有多么的可贵。\n 43. 错过的日子对你的打击大于成功的日子对你的帮助。\n 44. 如果你从100美元起步，那么50%的收益率会让你达到150美金。但是接下来只需要亏损33%就能把你打回100美元的起点。换句话说，避免33%的损失和获得50%的收益具有同等的价值。\n 45. charlie munger就曾经说：“复利的首要规则：除非万不得已，否则永远不要打断它”。\n 46. 健身的时候做什么并非头等重要的事，关键是你想成为严格遵循健身计划的那种人。当你感觉好的时候，锻炼很容易，但是当你情绪低落时仍然坚持锻炼、哪怕做的比平常少，重要的是你坚持不懈的表现。去健身房练五分钟不太可能提高你的表现，但它会重申你的身份。\n 47. 行为转变的全有或全无怪圈只是会让你的习惯陷入脱轨的陷阱之一。\n 48. 另一个潜在的危险，尤其是当你同时在应用习惯追踪法的时候，是观测的标的有误。\n 49. 追踪某一特定的行为的做法也有不良影响，我们因为过于专注于数字的变化，从而忘记了我们这样做的本意。\n 50. 无论玩什么游戏，人的唯一念头就是"赢"，这种陷阱明显体现在生活的许多领域。\n 51. 我们注重加班加点地工作，全然不顾我们所做的是否有意义。我们更关心凑够10000步，而不是保持健康。我们教学生应付标准化考试，而不是强调学习、好奇心和批判性思维。\n 52. 我们会针对我们所测量的进行优化。当我们选择错误的测量标的时，我们的做法就会走偏，这有时被称为古德哈特定律。\n 53. "当一项措施成为目标时，它就不再是一项好措施。度量只有在引导你并辅助大局时才有积极作用，它不应成为主角并让你疲于奔命。每个数字只不过是整个系统中的一条反馈罢了。"\n 54. 让习惯追踪起到该起的作用至关重要。我们或许乐于记录一个习惯并跟踪观测我们的进步，但是衡量之举并不是唯一重要的事情。此外，衡量进展情况的方式不在少数，有时把你的注意力转移到完全不同的事情上会对你有帮助。\n 55. 如果电子秤上的数字总是让你泄气，或许我们该关注其他一些可测的指标了，也就是能让我们看到更多进展信号的指标。\n 56. 习惯追踪提供了一个简单的方法来让你的习惯更令人满意。每一次测量都给你提供一点证据，证明你前进的大方向是正确的，并以自己出色的表现为傲，享受到略嫌短暂的即时快乐。\n 57. 习惯追踪法和其他视觉度量形式可以清晰无误地证明我们取得的进步，从而让我们对自己培养习惯的进程感到满意。\n\n\n# 问责伙伴何以能改变一切\n\n 1.  我们愿意重复感觉美好的经历，同时会设法回避曾令我们倍感痛苦的经历。\n 2.  痛苦是一个有效的老师。如果失败是痛苦的，人们便会力求成功、避免失败。\n 3.  如果失败的感觉不痛不痒，人们也就不把它当回事。错误的影响越直接，代价越大，你汲取教训的速度也就越快。\n 4.  无论什么行为，它引发即时痛苦体验的时间越快，它发生的可能性越低。\n 5.  如果你想戒掉不良习惯并避免不健康行为，那就给这类习惯和行为添加即时成本，这样可以有效地降低它们发生的概率。\n 6.  我们之所以难以戒掉坏习惯，就是因为它们在某种程度上迎合了我们的需要。就我所知，加快随这种行为而来的惩戒速度是摆脱这种困境的最佳方式。有行动便有惩罚，不能有丝毫迟滞。\n 7.  客户不愿支付滞纳金，因此会按时结清账单。学生想要好成绩，而当成绩与考勤挂钩时，他们就会准时出现在课堂上。我们常常会费尽周折，就是为了避免体验到一点点即时痛苦。\n 8.  如果你要依靠惩罚来改变行为，那么惩罚的强度必须与它视图纠正的行为的相对强度相匹配。要想有成效，拖延的代价必须大于立刻行动的代价。\n 9.  一般来说，越是局部的、有形的、具体的和直接的后果，就越有可能影响个人行为。后果越是全球性、无形性、模糊性和延迟性，影响个人行为的可能性就越小。\n 10. 提高任何坏习惯的即时成本，创立习惯契约。\n 11. 正如政府利用法律来追究公民的责任一样，我们也可以创建一个习惯契约，让自己承担特定责任。\n 12. 习惯契约是一种口头或书名的协议，我们要借此声明自己对某一特定习惯的承诺，并约定假如你违背了诺言，将会接受相应的责罚。\n 13. 要使不良习惯令人厌恶，我们最好的选择是在习惯动作刚一冒出头就让它们带来痛苦。订立习惯契约绝对是实现这一目标的捷径。\n 14. 知道有人在监督会是一个强大的动力。你不太可能拖延或放弃，因为你当即就要付出代价。如果你不坚持到底，监督你的人或许会认为你不可靠或生性懒惰。\n 15. 我们总是试图向世界展示我们最好的一面。我们梳头、刷牙、并精心打扮自己，因为我们知道这些习惯可能会给人留下好印象。\n 16. 我们很在意身边的人对自己的评价，因为他人的欣赏给了我们生活的乐趣。这就是为什么找一个责任心强的问责伙伴，或者订立习惯合同能如此有效的原因。\n 17. 如果不良习惯附加这令人痛苦或不愉快的感受，我们就不太可能重复它。\n 18. 问责伙伴可以对无所事事带来即时成本。我们非常在意别人对我们的看法，极不情愿感受别人的鄙视。\n 19. 习惯契约可被用来增加任何行为的社会成本，它使得违背承诺的代价公开而痛苦。\n 20. 知道别人在看着你，可以成为一种强大的动力。\n\n\n# 揭秘天才(当基因重要和无关紧要时)\n\n 1.  最大化你的成功概率的秘诀是选择合适的竞技领域，这适用于体育和商业，同样也适用于习惯的改变。\n 2.  如果习惯与你的天性和能力相一致，那么它更容易培养，你也更乐意保持。\n 3.  接受这一战略的前提是我们要承认一个简单的事实，即人天生具有不同的能力。\n 4.  有些人不愿意直面这个事实。从表面上看，我们的基因似乎无法改变，谈论我们无法控制的事情只能让人感到灰心丧气。此外，像生物决定论这种词语听起来似乎是有些人注定会成功，有些人注定会失败。但是这是一个关于基因影响行为的短视观点。\n 5.  遗传基因的力量无疑很强大，但它也有弱点。基因不容易改变，这意味着它们在有利的环境中提供了强大的优势，但在不利的环境中又暴露出严重的劣势。\n 6.  我们的环境决定了我们基因的适应性和我们天然禀赋的效用。当我们的环境改变时，决定成功的品质也会改变。\n 7.  身体特征如此，精神上也一样。某方面的能力与其所处的环境高度相关。任何竞争领域的顶尖人才不仅训练有素，而且天生就适合所做的事。这就是为什么，假如你想成为真正伟大的人，选对发展方向至关重要。\n 8.  简而言之：基因并不决定你的命运，而是决定这我们在哪些领域存在发挥特长的机会。\n 9.  在你先天具备了成功潜质的领域，我们更有可能养成令人满意的习惯。关键是我们选定的努力方向不仅令你生机勃勃，还能与你的天赋相匹配，从而使你的雄心与你的能力达成一致。\n 10. "我怎么才能辨别出朝哪方面努力胜算更大？我怎么才能确定适合我的机会和习惯？"我们首先要从了解我们的个性入手去寻找答案。\n 11. 我们的所有遗传特质组合在一起，赋予了我们独特的个性。我们的个性是指在各种各样的情境中表现出一致性的性格特征集合。\n 12. 经过科学分析验证的"五大"性格特征是目前得到公认的性格类别图谱。\n     * 开放性：从好奇和创造性的一端到谨慎和一丝不苟的另一个端。\n     * 自觉性：从有条理和效率高到随意性和自发性。\n     * 外向性：从活泼开朗、活力十足到孤独和保守(也就是外向型人格和内向型人格的区别)。\n     * 神经质：从焦虑不安和敏感多疑到自信、冷静和心态平和。\n 13. 亲和性的人一般表现出善良、体贴、热情的特征。他们体内的催产素含量往往较高，这种激素在增进社交活动、提高信任感等方面起着重要作用，同时还是一种天然抗抑郁剂。\n 14. 神经质特征突出的人往往比其他人更容易焦虑，总是忧心忡忡的。这一特征与杏仁核的超敏反应有关，杏仁核是大脑中负责识别威胁的部分。\n 15. 我们的习惯不仅仅是由我们的个性决定的，但毫无疑问，基因在将我们推向一个特定的发展方向。\n 16. 我们根深蒂固的偏好使得一些人不经意表现出的言谈举止，却难以在另一些人身上再现。\n 17. 我们不必为这些差异感到不安或内疚，我们必须去直面它们。例如，如果一个人的自觉性较差，他就不太可能天生爱整洁，因而可能需要更多地依靠环境设计来保持良好的习惯。\n 18. 我们应该养成适合我们个性的习惯。我们随着自己的心愿去做时就会有更充足的动力。\n 19. 我们不用培养他人告诉我们要养成的习惯。选择最适合我们的而不是最流行的习惯。\n 20. 每种习惯都有一个特定版本，能够带给我们快乐和满足，设法找到它。\n 21. 只有给人带来快乐的习惯才能长期坚持下去，这是第四定律的核心思想。\n 22. 让你培养的习惯适合你的个性是良好的开端，但这并不是故事的结尾。接下来，我们要把注意力转向寻找和设计迎合我们天性的情境。\n 23. 在实践中，我们更可能享受那些对于我们来说轻而易举的事。\n 24. 在某一特定领域有专长的人往往更胜任相关的工作，并会因表现出色而受到表扬。他们之所以精力充沛，是应为他们的成功之处，正是他人失败的地方，也因为他们获得了更高的报酬和更多的机会，这不仅让他们更快乐，还推动他们完成更高质量的工作。这是一个良性循环。\n 25. 选择正确的习惯，进步轻而易举。挑错了习惯，生活就是无休止的挣扎。\n 26. 我们该如何确保自己所掌握的一套技能能适用于我们所要做的事？最常见的方法就是不断试错。有一个有效的办法来对付这个难题，它被称为探索/利用权衡。\n 27. 在一项新活动开始时，应该有一段摸索的时间。其目标是尝试诸多可能性，研究各种想法，并撒下一张大网。在经历了初步探索之后，把注意力转移到我们找到的最佳方案，但与此同时还要偶尔再尝试一下。\n 28. 如何在两者之间保持适当的平衡取决于我们是赢还是输。如果我们正在赢，我们就利用，利用，再利用。如果我们正在输，我们要继续探索，探索，再探索。\n 29. 从长远来看，或许最有效的做法，就是抓住在绝大多数时间里提供最佳结果的战略不放，同时就其余情况进一步探索。谷歌员工把每周80%的工作时间花在正式工作上，20%花在他们自己选择的项目上。\n 30. 影响最佳做法选择还取决于我们有多少可供支配的时间。如果时间不成问题，那么我们值得把更多时间用于探索，因为一旦我们找到了正确的选项，我们将仍然有很长时间去利用它。\n 31. 如果时间紧迫的话，比如说，某个项目的最后期限即将到来，我们应该实施迄今为止找到的最佳方案，争取尽快取得一些成果。\n 32. 当我们在探求不同的选项时，我们可以问自己一系列的问题，以便逐渐接近最令我们满意的习惯和领域。\n 33. 什么对我来说充满乐趣，但对其他人来说却只是乏味的工作？我们是否适合一项任务的标志不在于我们是否喜欢它，而在于我们是否能比大多数人更容易承受这项任务带来的痛苦。当别人觉得苦不堪言时，我们却能自得其乐？伤害别人多余伤害自己的事，就是我们生来就适合做的事。\n 34. 是什么让我们忘记了时间的流逝？"心流"指的是我们因为全神贯注地投入手头的工作，从而忘记了周边世界存在的一种精神状态。\n 35. 我们在哪里能获得比普通人更高的回报？我们不断地与自己周边的人相比。如果我们做得比别人好，我们会觉得心满意足。\n 36. 我们的天性是什么？此刻，我们可以暂时忘记我们接受的教育，忽略主流社会告诉我们的事情，忽略别人对我们的期望。扪心叩问："我觉得什么很自然？我们何时感觉充满活力？我们何时看到了自己的真面目？"不要急于自我批评，也不要刻意讨人欢心。不要犹豫不定或自我批评。只注重乐在其中的感觉。无论何时，只要我们感觉真实可信，我们前进的方向就是正确的。\n 37. 我们都是地球上的冲冲过客，我们中真正伟大的人不仅付出卓越的努力而且还有幸享有天赐良机。\n 38. 假如我们无法确认什么事能让我们做到风生水起、好运连连，那就另辟蹊径，开创一番新事业。\n 39. 当我们不能比别人做的更出色，我们可以借助于与众不同而胜出。通过调用我们各方面的技能，我们降低了竞争水平，这使得我们更容易脱颖而出。我们可以修改游戏规则，简化先天条件(或资历年限)上的要求。一名好手努力战胜同一领域的众多对手，一名高手则自成一体，尽其所能扬长避短。\n 40. 沸水可以使土豆变软，但会使鸡蛋变硬。我们不能自主选择称为土豆还是鸡蛋，但在变硬还是变软之间我们可以选择对我们最有利的。如果我们能找到一个更有利的环境，我们就可以把不利于我们的情形转变为有利的情形。\n 41. 生理差异不容忽略。即便如此，关注自己能否充分发挥自己的潜力，要比与他人攀比收效更显著。人的能力是有限的这一事实，与我们是否已达到能力的上限无关。人们常常过于纠结自己能力的极限，以至于放弃了充分调动自己潜能的努力。\n 42. 假如我们不进行刻苦训练，我们根本无法获知自己的极限在哪里，或者说自己是否具备了优秀的基因。\n 43. 如果我们不能像我们所敬佩的人那样付出辛勤的汗水，就不要把他们的成功解释为运气好。\n 44. 基因并不能排除艰苦努力的需要。它们只会帮着甄别，告诉我们该努力做什么事。\n\n\n# 金发女孩准则：如何在生活和工作中保持充沛动力\n\n 1.  为什么有些人，比如马丁，能长期坚持他们的习惯，无论是讲笑话、画漫画还是弹吉他，从不懈怠，而我们大多数人却坚持不了几天就打退堂鼓。我们该怎样让习惯一直保持新鲜感，而不是过段时间就逐渐消失。\n 2.  大家公认保持动力和达到最大欲望的途径之一，就是去做"难易程度刚刚好"的事。\n 3.  人脑喜欢挑战，但前提是它面对的挑战难度适中。\n 4.  现在考虑和一个和我们水平相当的人一起打网球。随着比赛的进行，我们有输有赢。我们再努把力就有可能最终赢得比赛。我们开始集中注意力，专心打球，并进入了浑然忘我的境界。这是一个难度适中的挑战，也是金发女孩准则的一个重要实例。\n 5.  金发女孩准则指出，人们在处理其能力可及的事务时积极性最高。难易程度适中，刚刚好。\n 6.  当我们开始养成新习惯的时候，保持尽可能简单的动作是很重要的，这样即使各方面条件不完善，我们也可以坚持下去。\n 7.  然而，一旦习惯形成之后，还需要不断地添砖加瓦，持续跟进，这很重要。\n 8.  这些后续的小改进和新的挑战可以保持你的参与度。如果你刚好碰到金发女孩区，你就能达到心流状态。\n 9.  心流状态是"身在其中"并完全沉浸于一项活动中的体验。\n 10. 他们发现，要达到心流状态，我们要完成的任何难度必须比我们目前的能力高出大约4%。\n 11. 金发女孩准则的核心思想仍然存在：做力所能及、难易适中的事，似乎是保持激励水平居高不下的关键所在。\n 12. 改善需要微妙的平衡。我们需要时常处理一些具有挑战性的事务，在令我们殚精竭虑的同时，让我们能够取得足够的进步来保持激励水平。\n 13. 行为新奇才能有吸引力并带给人满足感。千篇一律，我们就会感到厌倦。厌倦或许是追求自我完善之路上的最大障碍。\n 14. "有些时候，归根结底这取决于谁能应付每天枯燥乏味的训练，一遍又一遍地反复做同样的举重动作"。\n 15. 他的回答让我吃惊，因为这是对工作伦理的颠覆性认识。人们平常说的都是如何"满怀热情"去努力实现他们的目标。\n 16. 不管是在商业、体育还是艺术界，我们听到的都是"一切都归结于激情"或者"你必须真的渴望得到它"之类的说法。\n 17. 因此，当我们失去了注意力或动力的时候，我们中的许多人会变得沮丧，因为我们认为成功的人会有无限的激情。\n 18. 但是这位教练说真正的成功人士也会和其他人一样感到激情消退。唯一不同的是，尽管感到枯燥乏味，他们仍然想办法坚持下去。\n 19. 成功的最大威胁不是失败，而是倦怠。我们厌倦了习惯，因为它们不再让我们开心，这个结果是意料之中的。\n 20. 随着我们的习惯变成日常举动，我们开始脱离固有的轨迹，转而去追求新奇的事务。也许这就是为什么我们会陷入一个永无止境的周期性循环，无论是健身方式、饮食习惯，或是创业的想法，总是换来换去的。\n 21. 激情稍有消退，我们就开始寻找新的做法，哪怕老做法依然在起作用。\n 22. 也许这就是为什么许多不停地花样百出的产品总是让人欲罢不能。电子游戏提供给人们视觉上的新奇感，色情作品提供性体验上的新奇感，速食产品则不断变换口味，上述种种经历都鞥呢让人体验到连续不断的惊喜。在心理学上，这被称为可变奖励。这种变幻不定导致多巴胺的浓度达到最大峰值。\n 23. 可变奖励不会创造渴望，也就是说，你时不时地奖给别人原本不喜欢的东西并不能使他们回心转意，喜欢上它，但是可变奖励的确会显著放大我们曾体验过的渴望，因为它们会缓解我们的倦怠感。\n 24. 在成功和失败各占一半的情形下，人们会体验到恰到好处的渴望的快感。得与失就发生在一瞬间，你需要刚刚够的"赢"来体验满足感，以及刚刚够的"渴望"来体验欲望。\n 25. 如果你本来就对某个习惯感兴趣，那么应对难易程度适中的挑战是保持事物趣味不减的好方法。\n 26. 当然，并不是所有的习惯都含有可变奖励的成分。不管有没有可变奖励，没有一种习惯会有无穷无尽的乐趣。\n 27. 在某个时刻，每个人在自我提升的旅程中都面临这同样的挑战：你必须与厌倦结缘。\n 28. 我们都有人生目标并心怀梦想，但是，假如你只在心悦来潮或一时兴起才做出一些努力，那么无论你的目标或梦想是什么，你都不可能取得显著的成果。\n 29. 我敢说，假如你下决心培养一种习惯并坚持了一段时间之后，你总有一天会想要放弃。但是，当你感到心烦意乱、苦不堪言或精疲力尽时，是鼓足干劲还是萌生退意，这是专业人士和业余人士的分水岭。\n 30. 专业人员依照既定计划行事，毫不动摇；业务爱好者则随波逐流，任性而为。专业人士指导对他们来说什么最重要，并有目的地去做；业务爱好者则随生活中的突发情况而变。\n 31. 当一个习惯对你真正重要时，你必须愿意在任何心情下坚持下去。专业人士不会因自己心情不好而改变行动的时间表。他们可能享受不到乐趣，但是他们仍能做到坚持不懈。\n 32. 我懒得做的组合训练有很多，但我从未后悔健身的选择。我懒得写的文章也不少，但我从未后悔按时发表。有很多天我都想放松一下，但我从未后悔准时到场，努力去做对我来说很重要的事。\n 33. 称为出色的人的必经之路，是无休止地反复做同样的事，且痴心不改。你必须爱上厌倦。\n 34. 金发女孩准则指出，人们在处理其能力可及的事务时积极性最高。\n 35. 成功的最大威胁不是失败，而是倦怠。\n 36. 随着习惯成为常规，它们变得不那么有趣，也不那么令人满意。我们开始感到无聊。\n 37. 每个人受到激励时都能努力工作。但当工作不那么令人兴奋时，仍能继续奋进的则是人中佼佼者。\n 38. 专业人员依照既定计划行事，毫不动摇；业务爱好者则随波逐流，任性而为。\n\n\n# 培养好习惯的负面影响\n\n 1.  习惯是精通的铺路石。\n 2.  我们对一些简单动作熟练到不假思索就能完成的地步之后，我们就可以自由地关注更高层次的内容。\n 3.  这样看来，在任何追求卓越的努力中，习惯都是不可或缺的支柱。\n 4.  然后，习惯的好处是有代价的。起初，每一次重复都让你的动作更加流畅、做得更快、更娴熟。但到了后来，随着我们的习惯动作越来越自如，我们对反馈的敏感度也会下降。\n 5.  我们会心不在焉，机械地重复着熟悉的动作。就算是犯了错也不再上心，就随它去了。既然自动驾驶已经能做到"足够好"，我们就不再考虑如何做得更好。\n 6.  习惯的好处是我们能够不假思索做任何事，缺点是我们习惯了以某种方式做事，不再介意其间暴露出的小纰漏。\n 7.  我们认为做得越来越好是因为我们的经验越来越丰富。但是实际上，我们只是在强化，而不是在改善我们当前的习惯。\n 8.  事实上，一些研究表明，当一个人熟练掌握了一项技能之后，他随后表现出的水准会有所下降。\n 9.  如果我们想最大限度地发挥我们的潜力，并达到出类拔萃的水准，我们的做法就要有所不同。我们不能漫不经心地反复做同样的事，同时期望自己能有非同凡响的表现。\n 10. 习惯是必要的，但还不足以称得上精通。我们需要的是习惯动作与刻意练习相结合。\n 11. 习惯动作 + 刻意练习 = 精通\n 12. 掌握的过程要求你在一个又一个的基础上不断进步，每个习惯都是建立在最后一个基础上的，直到达到新的表现水平，更高范围的技能被内化。\n 13. 掌握的过程，就是你把注意力集中到成功的一个小元素的过程。重复这一过程，直到技能内化，然后以这个新习惯为跳板，继续扩展我们的发展空间。\n 14. 任何事做第二遍时都会变得容易一些，但不是总体上变得更容易，因为现在你把精力投入到了下一个挑战中。\n 15. 每个习惯都会开启下一个级别的表现。这是一个无止境的循环。\n 16. 正是在我们开始觉得自己已经熟练掌握一项技能的时候，也就是在所有动作都无比娴熟和轻而易举的那一刻，我们必须避免陷入自满的陷阱。\n 17. 如何避免呢？答案是建立一个反思和审视的体系。\n 18. 赖利后来说："坚持不懈地努力对任何企业来说都极其重要。成功之策就是学会正确地做事，然后每次都以同样的方式去做"。\n 19. 反思和回顾有助于长期改善所有习惯，因为它对我们认清自己的不足，并帮助我们考虑可能的改善途径。\n 20. 没有反思，我们会为自己的行为寻找理由、找借口，并自我欺骗。我们会因缺乏这样一种程序而无法确定我们与以往相比表现得更好还是更差。\n 21. 我听说过一些高管和投资者有写"决策日志"的习惯，记录他们每周做出的重大决策，决策的理由，以及预期的结果。他们会在每个月或年底回顾他们的选择，看看哪些是对的，哪些出了问题。\n 22. 习惯不仅需要改善，也需要微调。反思和回顾可以确保我们的时间用在了正事上，并在必要的时候修正方法。\n 23. 就我个人而言，我主要通过两种方式进行反思和回顾。每年12月，我都会进行年终总结，回顾一下往年的都有哪些行为，比如发表了多少篇文档，进行了多少次健身，浏览过多少个新地方。\n 24. 我通过回答三个问题来反思我的进步(或不足)：\n     * 今年什么事做得比较好？\n     * 今年什么事做得不太好？\n     * 我学到了什么？\n 25. 半年后，当夏天来临之际，我们会做一份诚信报告。像大家一样，我犯过很多错误，我的诚信报告有助于我认清哪里出了问题，并激励我回到正轨。\n 26. 我以此为契机，重新审视我的核心价值观，并审视自己是否一直在践行自己的价值观。\n 27. 我也借此机会反思我的身份以及怎样努力成为我心目中的那类人。\n 28. 我的年度诚信报告回答了三个问题：\n     * 推动我生活和工作的核心价值观是什么？\n     * 我现在生活和工作的正直程度如何？\n     * 我怎样为将来设定更高的标准？\n 29. 这两份报告，不需要很长时间就能完成，每年不过几个小时，但它们是自我完善的关键时期。\n 30. 这几个小时会帮我制止不经意间的懈怠和疏忽，会一年一度提醒我重新审视自己想要的身份，并考虑我的习惯是在怎样帮助我成为我崇拜的那种人。\n 31. 它们表明我何时应该提升我的习惯，迎接新的挑战，何时应该回归初心，练好基本功。\n 32. 反思也能扩展视野。日常习惯之所以强大，是因为它们具有复利特性，但天天为自己的每个选择忧心忡忡就像用放大镜看自己，过于短视了。\n 33. 我们专注于局部的瑕疵，忽略了更大的画面。这属于反馈过多。\n 34. 定期的反思和回顾就像是从正常距离照镜子，我们既不失整体画面，也能看到应该做出的重大决定。我们想观赏整个山脉，而不是局限于山峰和山谷。\n 35. 反思和回顾也是个良机，可用来重新审视行为转变最重要的方面之一：身份。\n 36. 当我们具备了这种新的身份后，这些相同的信念会阻碍我们进入下一个发展阶段。我们的身份与自己作对的时候，会发出某种"傲慢"，怂恿我们否认自己的缺点，阻止我们真正成长。这就是养成习惯的最大负面影响之一。\n 37. 我们越是执念于某个想法，也就是说，它与我们的身份越紧密，我们就越坚决地捍卫它不受质疑。\n 38. 学校老师无视创新的教学方法，固守其久经实践检验的教案；资深经理执意要自行其是；外科医生拒绝年轻同事提出的建议；一支乐队在发行首个震撼人心的专辑之后便故步自封，再无创新。我们越是执着于一个身份，就越难超越它。\n 39. 解决这种问题的办法之一，就是避免让我们的身份的任何单一属性主导我们的为人处世。\n 40. 假如我们固守一种身份，我们会变得不堪一击。失去那个身份，我们就失去了自己的全部。\n 41. 当我们一生都在用一种方式定义自己，而这种定义消失了，我们现在究竟是谁？\n 42. 要想减轻随身份丧失而来的负面影响，关键是必须重新定义自己，这样即使我们的特定角色发生了变化，我们也可以保留身份的重要方面。\n 43. "我是运动员"变成"我是那种精神坚强、喜欢身体上的挑战的人"。\n 44. "我是优秀的士兵"转变成"我是那种纪律严明、诚实可靠、富于团队合作精神的人"。\n 45. "我是首席执行官"转变为"我是那种制作和创造东西的人"。\n 46. 如选择恰当，身份可以是灵活的，而非不堪一击的。就像水在障碍物周围流动一样，我们的身份会随着环境的变化而变化，而不是与环境对抗。\n 47. 习惯带来了很多好处，但缺点是它们也会让我们陷入以前的思维和行为模式，不能跟上时代前进的步伐。\n 48. 一切都是无常的。生活在不断变化，所以我们需要定期检查一下，看看我们固有的习惯和信仰是否还在为我们服务。\n 49. 缺乏自我意识是毒药，反思和回顾是解药。\n 50. 反思和回顾是一个过程，使我们能够时刻关注自己的表现。\n\n\n# 获得持久成果的秘诀\n\n 1.  一个小小的改变能改变你的人生吗？你不太可能说是的。但是如果你又做了一个呢？又做了另一个呢？接着又做了一个呢？\n 2.  在某个时刻，我们会不得不承认我们的人生被一个个小小的变化改变了。\n 3.  习惯改变的"圣杯"不是单个1%的改进，而是成千个。它是无数微习惯堆积起来的结果，其中每个微习惯都是构成整个系统的基本单元。\n 4.  一开始，小改进往往微不足道，因为它面对的整个系统体量太大了，无法撼动。\n 5.  然而，随着我们继续将微小的变化层层叠加，人生的天平开始偏移。\n 6.  每次改进就像在有利于你的天平的一侧添加一粒沙，使它慢慢地偏向你。\n 7.  假如你能坚持下去，最终我们会达到产生重大偏移的临界点。\n 8.  突然间，坚持好习惯变得轻而易举了。整个系统开始偏重你，不再与你作对。\n 9.  我们在此提及的每个人、团队和公司处境各有不同，但最终都以同样的方式取得进步：致力于微小、可持续、不懈的改进。\n 10. 成功不是要达到的目标，也不是要跨越的终点线。它是一个让你得以进步的体系、精益求精的过程。\n 11. 正如在第1章所描述的："如果我们很难改变自己的习惯，问题的根源不是我们本身，而是我们的体系。坏习惯循环往复，不是因为我们不想改变，而是因为我们用来改变的体系存在问题"。\n 12. 掌握了行为转变的四大定律，我们就拥有了一套工具和策略，可以用来建立更好的系统和养成更好的习惯。\n 13. 有时一个习惯很难记住，我们需要让它显而易见。其他时候，我们不想开始培养习惯，我们需要让它有吸引力。在许多情况下，我们可能会发现太难养成习惯，我们需要让它简便易行。有时候，我们不想坚持下去，我们需要让它令人愉悦。\n 14. 让好习惯显而易见，让坏习惯脱离视线；让好习惯有吸引力，让坏习惯缺乏吸引力；让好习惯简便易行，让坏习惯难以施行；让好习惯令人愉悦，让坏习惯令人厌恶。\n 15. 这是一个连续不断的过程，没有终点线，也没有永久的解决方案。每当我们想要自我提高时，我们都可以围绕行为转变四定律循序渐进地发展，直到我们发现下一个瓶颈。\n 16. 让它显而易见，让它有吸引力，让它简便易行，让它令人愉悦。一圈又一圈地循环发展。不停地寻求用来获得1%的进步的新方法。\n 17. 获得持久成果的秘诀是不断进步，永不停歇。\n 18. 只要我们一刻不停，坚持下去，我们难以想象自己能取得多么伟大的成就。\n 19. 假如我们不停止工作，我们的公司业务发展将蒸蒸日上。假如我们不停止健身，我们将拥有一副强健体魄。假如我们不停止学习，我们能汇聚起知识的宝库。假如我们不停止储蓄，我们将积少成多，得到一笔巨款。假如我们不停止关爱，我们的朋友将会遍及天下。小习惯不会简单相加，它们会复合。\n\n\n# 从四大定律中吸取的教训\n\n 1.  意识先于欲望。当你赋予提示一定意义之后，就会产生渴求。你的大脑会构造一种情绪后感觉来描述你的现状，这意味着渴求只会产生于你发现了机会之后。\n 2.  幸福转瞬即逝，因为我们总是会升腾起新的欲望。\n 3.  卡德.布德里斯所说: "幸福是已得到满足的欲望与酝酿中的欲望之间的空档"。同样，痛苦则是渴望改变现状与改变得以实现那一刻之间的空档。\n 4.  我们追求的是快乐的理念。我们寻求我们脑海中产生的快乐影像。在采取行动时，我们并不知道获得这个影响会给我们带来什么(甚至不确定它是否会令我们满意)。满足感只有在事发之后才会出现。\n 5.  奥地利神经学家维克多.弗兰克说幸福是追求不到的，只能尾随而来。我们追求的是欲望。快乐来自行动。\n 6.  有了充足的理由，我们可以克服任何困难。\n 7.  尼采有一句名言:"有足够理由活着的人几乎可以忍受任何生存方式。"这一说法包含了一个关于人类行为的重要事实。如果我们的动机和欲望足够强大(也就是说，我们为什么要行动)，即使困难重重，我们也会采取行动。强烈的渴望可以推动伟大的行动 ---- 即使阻力巨大。\n 8.  好奇心总比头脑灵活好。积极性和好奇心比头脑灵活更重要，因为前者会导致行动。头脑灵活永远不会独自产生结果，因为它不会让你采取行动。\n 9.  促使行为的是欲望，而不是智力。\n 10. 纳瓦尔.拉维康特说:"做任何事情的诀窍是首先是培养对它的渴望"。\n 11. 情绪驱动行动。在某种程度上，每个决定都是一个情绪上的决定。\n 12. 不管我们采取行动的逻辑原因是什么，我们只会因为情绪而感受到采取行动的必要性。事实上，大脑情感中心受损的人可以列出许多采取行动的理由，但始终不会行动，因为他们缺乏情感驱动。这就是为什么渴望先于回应。先有感觉，然后才有行动。\n 13. 我们的情绪先于理性和逻辑。大脑的主模式是感觉；此模式是思考。我们的第一反应---- 大脑中快速、下意识的部分 ---- 是针对感觉和预期而优化的。我们的次一级反应 ---- 大脑中缓慢、有意识的部分 --- 是"思考"的部分。\n 14. 心理学家将此区分为系统1(感觉和快速判断)与系统2(理性分析)。感觉在先(系统1)；理性只在随后介入(系统2)。当两者协同一致时，会发挥极佳作用，但当两者不一致的时候，就会产生不合逻辑、感情用事的后果。\n 15. 我们的反应倾向于跟随我们的情绪。我们的思想和行动根植于我们认为有吸引力的东西，而不一定是符合逻辑的东西。如果一个话题让某人感到情绪激动，他们很少会对数据感兴趣。这就是为什么情绪会对明智的决策造成较大的威胁。\n 16. 痛苦推动进步。所有痛苦的根源是对改变现状的渴望。这也是所有进步的源泉。对改变现状的渴求激励着我们采取行动。\n 17. 内心涌动这渴望，意味着我们不满意，但动力十足。没有渴望，我们就心满意足，不思进取。\n 18. 回报是牺牲的另一面。回应(牺牲能量)总是先于回报(收集资源)。"跑步者的愉悦感"是在运动量超过一定程度后的体验，只有在消耗掉一定能量后，回报才会到来。\n 19. 自我控制很难做到，因为它不令人满意。奖励是让你的渴望得到满足的结果。这使得自我控制难以起效，因为抑制我们的欲望通常不会根除它们。抵制诱惑并不能满足你的渴望；它只是忽略了它，打通了让渴望穿过的通道。自我控制要求你释放而不是满足它。\n 20. 如果期望和结果之间的不匹配是积极的(惊喜)，那么我们将来重复一种行动的可能性就很大。如果是负面的(失望和沮丧)，那么我们就不太可能再去做。\n 21. 在行动之前，有一种感觉在激励你行动，那就是渴望。行动之后，有一种感觉教导你在未来重复这个动作，那就是奖励。\n 22. 快乐和满足给予一种行为源源不断的动力，感觉有动力才会让你行动起来，成就感则促使我们不断重复那种行为。\n 23. 新方式带来了希望，因为我们不曾经历过，可以敞开了想象。新策略似乎比旧策略更有吸引力，因为它们可以有无限的希望。',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"友情链接",frontmatter:{title:"友情链接",date:"2019-12-25T14:27:01.000Z",permalink:"/friends",article:!1,sidebar:!1},regularPath:"/04.%E7%94%9F%E6%B4%BB/99.%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5.html",relativePath:"04.生活/99.友情链接.md",key:"v-5a5a4a6b",path:"/friends/",headers:[{level:3,title:"友链申请",slug:"友链申请",normalizedTitle:"友链申请",charIndex:5360}],headersStr:"友链申请",content:"麋鹿鲁哟\n\n大道至简，知易行难\n\nXAOXUU\n\n#IOS #Volantis主题作者\n\nlookroot的个人空间\n\n寻求理想和显示的平衡\n\n平凡的你我\n\n理想成为大牛的\n小陈同学\n\nznote\n\n荷尽已无擎雨盖，\n菊残犹有傲霜枝。\n\n全栈软件开发直通车\n\n全栈软件开发技术博客，\n从小白到大神!\n\n易良同学的博客\n\n正在努力！\n\n永远的救赎者\n\n知者减半，省者全无。\n\n辰旭博客\n\n凤鸣初阳，百鸟朝凰\n\nJokerM's Palace\n\nTake your heart\n\nSaul.J.Wu\n\n立身之本，不在高低。\n\nLake's blog\n\n不积跬步，无以至千里；不积小流，无以成江海。\n\nCubik的小站\n\nRECOMMENDED BY DR.CREATIVE\n\nx·π\n\n为开发者量身制作的技术博客和知识库管理平台。\n\n眼里有光\n\n道阻且长，行则将至\n\nHeo\n\n爱折腾的设计师\n\nChuyuxuan\n\n临渊羡鱼，不如退而结网\n\n全栈杂货站\n\n千里万里杂货站里，天青色等烟雨，而我在等你。\n\n小鱼博客\n\n总是半途而废的废柴\n\n大胡子\n\n记录你我，分享精彩。\n\n嘟先生学WebGL\n\n流水不争先，争的是滔滔不绝。\n\n嶋屿麋鹿\n\n小鹿的知识库\n\n@小右_\n\n学而不厌 不耻下问\n\n途中的树\n\n走出自己的傲慢，承认自己的局限。\n\nHaobo's Blog\n\n半只脚跨入炼丹师的大门的新人\n\n小胖墩er\n\n迟到总比不到的好，所以好好加油吧。\n\nCloudNative Operations\n\n专注于云原生运维,致敬每个爱学习的你。\n\nJoseph Z.\n\nJoseph Z.的小站\n\n- name: 麋鹿鲁哟\n  desc: 大道至简，知易行难\n  avatar: https://cdn.jsdelivr.net/gh/xugaoyi/image_store/blog/20200122153807.jpg # 可选\n  link: https://www.cnblogs.com/miluluyo/ # 可选\n  bgColor: '#CBEAFA' # 可选，默认var(--bodyBg)。颜色值有#号时请添加单引号\n  textColor: '#6854A1' # 可选，默认var(--textColor)\n- name: XAOXUU\n  desc: '#IOS #Volantis主题作者'\n  avatar: https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/avatar/avatar.png\n  link: https://xaoxuu.com\n  bgColor: '#B9D59C'\n  textColor: '#3B551F'\n- name: lookroot的个人空间\n  desc: 寻求理想和显示的平衡\n  avatar: https://www.lookroot.cn/logo.png\n  link: https://www.lookroot.cn/\n  bgColor: '#B7DBFF'\n  textColor: '#294D71'\n- name: 平凡的你我\n  desc: 理想成为大牛的<br/>小陈同学\n  avatar: https://reinness.com/static/avatar.png\n  link: https://reinness.com\n  bgColor: '#FFE5B4'\n  textColor: '#A05F2C'\n- name: znote\n  desc: 荷尽已无擎雨盖，<br/>菊残犹有傲霜枝。\n  avatar: https://zpj80231.gitee.io/znote/vuepress/head-fish.jpg\n  link: https://zpj80231.gitee.io/znote/\n  bgColor: '#FCE5BF'\n  textColor: '#7B2532'\n- name: 全栈软件开发直通车\n  desc: 全栈软件开发技术博客，<br/>从小白到大神!\n  avatar: https://gitee.com/wangshibiao/blog_picBed2/raw/master/images/20200806151030.png\n  link: https://sofineday.com\n  bgColor: '#FBEBEC'\n  textColor: '#603420'\n- name: 易良同学的博客\n  desc: 正在努力！\n  avatar: https://yiliang.site/assets/images/avatar.jpeg\n  link: https://yiliang.site\n  bgColor: '#FFEFE2'\n  textColor: '#A05F2C'\n- name: 永远的救赎者\n  desc: 知者减半，省者全无。\n  avatar: https://i.loli.net/2020/08/10/PkQMGL6pATW1vBg.jpg\n  link: http://www.yuanchengcheng.vip/\n  bgColor: '#FBEBEC'\n  textColor: '#603420'\n- name: 辰旭博客\n  desc: 凤鸣初阳，百鸟朝凰\n  avatar: https://s1.ax1x.com/2020/08/09/aoLTDx.png\n  link: https://kareny.cn\n  bgColor: '#FFCEDE'\n  textColor: '#621529'\n- name: JokerM's Palace\n  desc: Take your heart\n  avatar: https://jokerm.com/wp-content/uploads/2020/09/jmflogo.png\n  link: https://jokerm.com/\n- name: Saul.J.Wu\n  desc: 立身之本，不在高低。\n  avatar: https://gitee.com/SaulJWu/blog-images/raw/master/images/20210627222322.jpg\n  link: https://sauljwu.github.io/\n- name: Lake's blog\n  desc: 不积跬步，无以至千里；不积小流，无以成江海。\n  avatar: https://cdn.jsdelivr.net/gh/taixingyiji/image_store@main/blog/logo/img.png\n  link: https://taixingyiji.com/\n- name: Cubik的小站\n  desc: RECOMMENDED BY DR.CREATIVE\n  avatar: https://cdn.jsdelivr.net/gh/Cubik65536/cubik-favicons@main/CubikLogo.png\n  link: https://www.cubik65536.top/\n- name: x·π\n  desc: 为开发者量身制作的技术博客和知识库管理平台。\n  avatar: https://cdn.jsdelivr.net/gh/Ezuy-Lee/RainzeDrawingBed/media/logo.png\n  link: https://ezuy-lee.github.io/xpai/\n- name: 眼里有光\n  desc: 道阻且长，行则将至\n  avatar: https://icooloop.gitee.io/img/logo.jpg\n  link: https://icooloop.gitee.io/\n- name: Heo\n  desc: 爱折腾的设计师\n  link: https://blog.zhheo.com/\n  avatar: https://blog.zhheo.com/img/avatar.png\n- name: Chuyuxuan\n  desc: 临渊羡鱼，不如退而结网\n  link: https://blog.chuyuxuan.top/\n  avatar: http://blog.chuyuxuan.top/img/avatar1.jpg\n- name: 全栈杂货站\n  desc: 千里万里杂货站里，天青色等烟雨，而我在等你。\n  avatar: http://cdn.tea-culture.top/tech/images/avatar/3.jpg\n  link: http://tech.tea-culture.top/\n- name: 小鱼博客\n  desc: 总是半途而废的废柴\n  avatar: https://cdn.jsdelivr.net/gh/xiaoyu-666/image_store/blog/minion.png\n  link: https://xiaoyu-666.github.io/\n- name: 大胡子\n  desc: 记录你我，分享精彩。\n  avatar: https://photo.jakehu.cn/favicon.png\n  link: https://www.jakehu.cn\n- name: 嘟先生学WebGL\n  desc: 流水不争先，争的是滔滔不绝。\n  avatar: https://joy1412.cn/img/dudu.jpeg\n  link: https://joy1412.cn\n- name: 嶋屿麋鹿\n  desc: 小鹿的知识库\n  avatar: https://www.fongloo.com/img/logo.png\n  link: https://www.fongloo.com/\n- name: '@小右_'\n  desc: 学而不厌 不耻下问\n  avatar: https://lordblog.cn/upload/2021/05/logo%20(4)-742f1f7e15db44a1b3140035104ea239.png\n  link: https://lordblog.cn/\n- name: 途中的树\n  desc: 走出自己的傲慢，承认自己的局限。\n  avatar: https://zkpeace.com/blog/img/avatar.jpg\n  link: https://zkpeace.com/\n- name: Haobo's Blog\n  link: https://discover304.top/\n  avatar: https://discover304.top/img/head.png\n  desc: 半只脚跨入炼丹师的大门的新人\n- name: 小胖墩er\n  desc: 迟到总比不到的好，所以好好加油吧。\n  avatar: https://cdn.jsdelivr.net/gh/Chubby-Duner/image-hosting@master/blog/logo.jpeg\n  link: https://chubbyduner.top\n- name: CloudNative Operations\n  desc: 专注于云原生运维,致敬每个爱学习的你。\n  avatar: https://kubesre.com/img/logo.png\n  link: https://kubesre.com/\n- name: Joseph Z.\n  desc: Joseph Z.的小站\n  avatar: https://josephz.top/res/joseph.jpg\n  link: https://josephz.top/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n\n\n\n# 友链申请\n\n与我 联系 或者 在本页面评论区留言您的友链信息，格式：(点击代码块右上角一键复制)\n\n- name: Evan's blog # 昵称\n  desc: 积跬步以至千里，喜欢学习喜欢你。 # 介绍\n  avatar: https://cdn.jsdelivr.net/gh/xugaoyi/image_store/blog/20200103123203.jpg # 头像\n  link: https://xugaoyi.com/  # 链接\n\n\n1\n2\n3\n4\n\n\n申请前记得先添加本站哦~",normalizedContent:"麋鹿鲁哟\n\n大道至简，知易行难\n\nxaoxuu\n\n#ios #volantis主题作者\n\nlookroot的个人空间\n\n寻求理想和显示的平衡\n\n平凡的你我\n\n理想成为大牛的\n小陈同学\n\nznote\n\n荷尽已无擎雨盖，\n菊残犹有傲霜枝。\n\n全栈软件开发直通车\n\n全栈软件开发技术博客，\n从小白到大神!\n\n易良同学的博客\n\n正在努力！\n\n永远的救赎者\n\n知者减半，省者全无。\n\n辰旭博客\n\n凤鸣初阳，百鸟朝凰\n\njokerm's palace\n\ntake your heart\n\nsaul.j.wu\n\n立身之本，不在高低。\n\nlake's blog\n\n不积跬步，无以至千里；不积小流，无以成江海。\n\ncubik的小站\n\nrecommended by dr.creative\n\nx·π\n\n为开发者量身制作的技术博客和知识库管理平台。\n\n眼里有光\n\n道阻且长，行则将至\n\nheo\n\n爱折腾的设计师\n\nchuyuxuan\n\n临渊羡鱼，不如退而结网\n\n全栈杂货站\n\n千里万里杂货站里，天青色等烟雨，而我在等你。\n\n小鱼博客\n\n总是半途而废的废柴\n\n大胡子\n\n记录你我，分享精彩。\n\n嘟先生学webgl\n\n流水不争先，争的是滔滔不绝。\n\n嶋屿麋鹿\n\n小鹿的知识库\n\n@小右_\n\n学而不厌 不耻下问\n\n途中的树\n\n走出自己的傲慢，承认自己的局限。\n\nhaobo's blog\n\n半只脚跨入炼丹师的大门的新人\n\n小胖墩er\n\n迟到总比不到的好，所以好好加油吧。\n\ncloudnative operations\n\n专注于云原生运维,致敬每个爱学习的你。\n\njoseph z.\n\njoseph z.的小站\n\n- name: 麋鹿鲁哟\n  desc: 大道至简，知易行难\n  avatar: https://cdn.jsdelivr.net/gh/xugaoyi/image_store/blog/20200122153807.jpg # 可选\n  link: https://www.cnblogs.com/miluluyo/ # 可选\n  bgcolor: '#cbeafa' # 可选，默认var(--bodybg)。颜色值有#号时请添加单引号\n  textcolor: '#6854a1' # 可选，默认var(--textcolor)\n- name: xaoxuu\n  desc: '#ios #volantis主题作者'\n  avatar: https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/avatar/avatar.png\n  link: https://xaoxuu.com\n  bgcolor: '#b9d59c'\n  textcolor: '#3b551f'\n- name: lookroot的个人空间\n  desc: 寻求理想和显示的平衡\n  avatar: https://www.lookroot.cn/logo.png\n  link: https://www.lookroot.cn/\n  bgcolor: '#b7dbff'\n  textcolor: '#294d71'\n- name: 平凡的你我\n  desc: 理想成为大牛的<br/>小陈同学\n  avatar: https://reinness.com/static/avatar.png\n  link: https://reinness.com\n  bgcolor: '#ffe5b4'\n  textcolor: '#a05f2c'\n- name: znote\n  desc: 荷尽已无擎雨盖，<br/>菊残犹有傲霜枝。\n  avatar: https://zpj80231.gitee.io/znote/vuepress/head-fish.jpg\n  link: https://zpj80231.gitee.io/znote/\n  bgcolor: '#fce5bf'\n  textcolor: '#7b2532'\n- name: 全栈软件开发直通车\n  desc: 全栈软件开发技术博客，<br/>从小白到大神!\n  avatar: https://gitee.com/wangshibiao/blog_picbed2/raw/master/images/20200806151030.png\n  link: https://sofineday.com\n  bgcolor: '#fbebec'\n  textcolor: '#603420'\n- name: 易良同学的博客\n  desc: 正在努力！\n  avatar: https://yiliang.site/assets/images/avatar.jpeg\n  link: https://yiliang.site\n  bgcolor: '#ffefe2'\n  textcolor: '#a05f2c'\n- name: 永远的救赎者\n  desc: 知者减半，省者全无。\n  avatar: https://i.loli.net/2020/08/10/pkqmgl6patw1vbg.jpg\n  link: http://www.yuanchengcheng.vip/\n  bgcolor: '#fbebec'\n  textcolor: '#603420'\n- name: 辰旭博客\n  desc: 凤鸣初阳，百鸟朝凰\n  avatar: https://s1.ax1x.com/2020/08/09/aoltdx.png\n  link: https://kareny.cn\n  bgcolor: '#ffcede'\n  textcolor: '#621529'\n- name: jokerm's palace\n  desc: take your heart\n  avatar: https://jokerm.com/wp-content/uploads/2020/09/jmflogo.png\n  link: https://jokerm.com/\n- name: saul.j.wu\n  desc: 立身之本，不在高低。\n  avatar: https://gitee.com/sauljwu/blog-images/raw/master/images/20210627222322.jpg\n  link: https://sauljwu.github.io/\n- name: lake's blog\n  desc: 不积跬步，无以至千里；不积小流，无以成江海。\n  avatar: https://cdn.jsdelivr.net/gh/taixingyiji/image_store@main/blog/logo/img.png\n  link: https://taixingyiji.com/\n- name: cubik的小站\n  desc: recommended by dr.creative\n  avatar: https://cdn.jsdelivr.net/gh/cubik65536/cubik-favicons@main/cubiklogo.png\n  link: https://www.cubik65536.top/\n- name: x·π\n  desc: 为开发者量身制作的技术博客和知识库管理平台。\n  avatar: https://cdn.jsdelivr.net/gh/ezuy-lee/rainzedrawingbed/media/logo.png\n  link: https://ezuy-lee.github.io/xpai/\n- name: 眼里有光\n  desc: 道阻且长，行则将至\n  avatar: https://icooloop.gitee.io/img/logo.jpg\n  link: https://icooloop.gitee.io/\n- name: heo\n  desc: 爱折腾的设计师\n  link: https://blog.zhheo.com/\n  avatar: https://blog.zhheo.com/img/avatar.png\n- name: chuyuxuan\n  desc: 临渊羡鱼，不如退而结网\n  link: https://blog.chuyuxuan.top/\n  avatar: http://blog.chuyuxuan.top/img/avatar1.jpg\n- name: 全栈杂货站\n  desc: 千里万里杂货站里，天青色等烟雨，而我在等你。\n  avatar: http://cdn.tea-culture.top/tech/images/avatar/3.jpg\n  link: http://tech.tea-culture.top/\n- name: 小鱼博客\n  desc: 总是半途而废的废柴\n  avatar: https://cdn.jsdelivr.net/gh/xiaoyu-666/image_store/blog/minion.png\n  link: https://xiaoyu-666.github.io/\n- name: 大胡子\n  desc: 记录你我，分享精彩。\n  avatar: https://photo.jakehu.cn/favicon.png\n  link: https://www.jakehu.cn\n- name: 嘟先生学webgl\n  desc: 流水不争先，争的是滔滔不绝。\n  avatar: https://joy1412.cn/img/dudu.jpeg\n  link: https://joy1412.cn\n- name: 嶋屿麋鹿\n  desc: 小鹿的知识库\n  avatar: https://www.fongloo.com/img/logo.png\n  link: https://www.fongloo.com/\n- name: '@小右_'\n  desc: 学而不厌 不耻下问\n  avatar: https://lordblog.cn/upload/2021/05/logo%20(4)-742f1f7e15db44a1b3140035104ea239.png\n  link: https://lordblog.cn/\n- name: 途中的树\n  desc: 走出自己的傲慢，承认自己的局限。\n  avatar: https://zkpeace.com/blog/img/avatar.jpg\n  link: https://zkpeace.com/\n- name: haobo's blog\n  link: https://discover304.top/\n  avatar: https://discover304.top/img/head.png\n  desc: 半只脚跨入炼丹师的大门的新人\n- name: 小胖墩er\n  desc: 迟到总比不到的好，所以好好加油吧。\n  avatar: https://cdn.jsdelivr.net/gh/chubby-duner/image-hosting@master/blog/logo.jpeg\n  link: https://chubbyduner.top\n- name: cloudnative operations\n  desc: 专注于云原生运维,致敬每个爱学习的你。\n  avatar: https://kubesre.com/img/logo.png\n  link: https://kubesre.com/\n- name: joseph z.\n  desc: joseph z.的小站\n  avatar: https://josephz.top/res/joseph.jpg\n  link: https://josephz.top/\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n\n\n\n# 友链申请\n\n与我 联系 或者 在本页面评论区留言您的友链信息，格式：(点击代码块右上角一键复制)\n\n- name: evan's blog # 昵称\n  desc: 积跬步以至千里，喜欢学习喜欢你。 # 介绍\n  avatar: https://cdn.jsdelivr.net/gh/xugaoyi/image_store/blog/20200103123203.jpg # 头像\n  link: https://xugaoyi.com/  # 链接\n\n\n1\n2\n3\n4\n\n\n申请前记得先添加本站哦~",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"《直线学习法》读书笔记",frontmatter:{title:"《直线学习法》读书笔记",date:"2022-01-20T11:50:30.000Z",permalink:"/pages/485aeb/",categories:["生活","学习方法"],tags:[null]},regularPath:"/04.%E7%94%9F%E6%B4%BB/01.%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/04.%E3%80%8A%E7%9B%B4%E7%BA%BF%E5%AD%A6%E4%B9%A0%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html",relativePath:"04.生活/01.学习方法/04.《直线学习法》读书笔记.md",key:"v-088adbe6",path:"/pages/485aeb/",headers:[{level:2,title:"备考必须弄清的10件事",slug:"备考必须弄清的10件事",normalizedTitle:"备考必须弄清的10件事",charIndex:91},{level:3,title:'"追求学习量"是最烂的学习法',slug:"追求学习量-是最烂的学习法",normalizedTitle:"&quot;追求学习量&quot;是最烂的学习法",charIndex:null},{level:3,title:"复印目录帮你把握整体内容",slug:"复印目录帮你把握整体内容",normalizedTitle:"复印目录帮你把握整体内容",charIndex:594},{level:3,title:"学习从最终目标回溯到现在的思考方法",slug:"学习从最终目标回溯到现在的思考方法",normalizedTitle:"学习从最终目标回溯到现在的思考方法",charIndex:948},{level:3,title:"根据最终结果来调整自己",slug:"根据最终结果来调整自己",normalizedTitle:"根据最终结果来调整自己",charIndex:1374},{level:3,title:"找出失败的原因才能更好地学习",slug:"找出失败的原因才能更好地学习",normalizedTitle:"找出失败的原因才能更好地学习",charIndex:1891},{level:3,title:"只有两类问题会出现在考试里",slug:"只有两类问题会出现在考试里",normalizedTitle:"只有两类问题会出现在考试里",charIndex:2173},{level:3,title:"极限之后再走一步",slug:"极限之后再走一步",normalizedTitle:"极限之后再走一步",charIndex:2799},{level:3,title:"学习也需要“有效的徒劳”",slug:"学习也需要-有效的徒劳",normalizedTitle:"学习也需要“有效的徒劳”",charIndex:2946},{level:3,title:"大处着眼，微小处着手",slug:"大处着眼-微小处着手",normalizedTitle:"大处着眼，微小处着手",charIndex:3179},{level:3,title:"休息时间定为三小时一次",slug:"休息时间定为三小时一次",normalizedTitle:"休息时间定为三小时一次",charIndex:3333},{level:2,title:"掌握学习的技巧",slug:"掌握学习的技巧",normalizedTitle:"掌握学习的技巧",charIndex:109},{level:3,title:"为什么铃木一郎一直做手法练习",slug:"为什么铃木一郎一直做手法练习",normalizedTitle:"为什么铃木一郎一直做手法练习",charIndex:3772},{level:3,title:"自己给自己讲课的妙用",slug:"自己给自己讲课的妙用",normalizedTitle:"自己给自己讲课的妙用",charIndex:4127},{level:3,title:"能明确表达喜怒哀乐的人往往记忆力很好",slug:"能明确表达喜怒哀乐的人往往记忆力很好",normalizedTitle:"能明确表达喜怒哀乐的人往往记忆力很好",charIndex:4562},{level:3,title:"反复练习直到大脑产生错觉",slug:"反复练习直到大脑产生错觉",normalizedTitle:"反复练习直到大脑产生错觉",charIndex:5053},{level:3,title:"每一张过去的试卷都有意义",slug:"每一张过去的试卷都有意义",normalizedTitle:"每一张过去的试卷都有意义",charIndex:5739},{level:3,title:"做标记的方法会决定你的成绩",slug:"做标记的方法会决定你的成绩",normalizedTitle:"做标记的方法会决定你的成绩",charIndex:6209},{level:3,title:"让大脑快速运转的方法",slug:"让大脑快速运转的方法",normalizedTitle:"让大脑快速运转的方法",charIndex:6615},{level:3,title:'调整精神状态的"腹式呼吸法"',slug:"调整精神状态的-腹式呼吸法",normalizedTitle:"调整精神状态的&quot;腹式呼吸法&quot;",charIndex:null},{level:3,title:"预习、复习其实只需10分钟",slug:"预习、复习其实只需10分钟",normalizedTitle:"预习、复习其实只需10分钟",charIndex:7265},{level:3,title:"实现梦想的时间管理方法",slug:"实现梦想的时间管理方法",normalizedTitle:"实现梦想的时间管理方法",charIndex:7721},{level:3,title:"结束时再拖延五分钟",slug:"结束时再拖延五分钟",normalizedTitle:"结束时再拖延五分钟",charIndex:8362},{level:3,title:"当日达到最佳状态的方法",slug:"当日达到最佳状态的方法",normalizedTitle:"当日达到最佳状态的方法",charIndex:8522},{level:3,title:"模拟考试时对自己严苛一点",slug:"模拟考试时对自己严苛一点",normalizedTitle:"模拟考试时对自己严苛一点",charIndex:8927},{level:3,title:'"能力\\勇气学习方法"的乘法',slug:"能力-勇气-学习方法-的乘法",normalizedTitle:"&quot;能力\\勇气学习方法&quot;的乘法",charIndex:null},{level:2,title:"教你怎样面对挫折",slug:"教你怎样面对挫折",normalizedTitle:"教你怎样面对挫折",charIndex:123},{level:3,title:"如果你现在陷入低谷，那么恭喜你了",slug:"如果你现在陷入低谷-那么恭喜你了",normalizedTitle:"如果你现在陷入低谷，那么恭喜你了",charIndex:9791},{level:3,title:"把焦虑不安的原因全部写在纸上",slug:"把焦虑不安的原因全部写在纸上",normalizedTitle:"把焦虑不安的原因全部写在纸上",charIndex:10517},{level:3,title:'"精神笔记"和"梦想笔记"',slug:"精神笔记-和-梦想笔记",normalizedTitle:"&quot;精神笔记&quot;和&quot;梦想笔记&quot;",charIndex:null},{level:3,title:"找出你最迫切要实现的梦想",slug:"找出你最迫切要实现的梦想",normalizedTitle:"找出你最迫切要实现的梦想",charIndex:11916},{level:3,title:"在人生的岔路口做出明智选择",slug:"在人生的岔路口做出明智选择",normalizedTitle:"在人生的岔路口做出明智选择",charIndex:12402},{level:3,title:"让悲观情绪释放正能量",slug:"让悲观情绪释放正能量",normalizedTitle:"让悲观情绪释放正能量",charIndex:12965},{level:3,title:"创造多面性人生为自己减压",slug:"创造多面性人生为自己减压",normalizedTitle:"创造多面性人生为自己减压",charIndex:13498},{level:3,title:'善于炫耀自己的"成功报酬"',slug:"善于炫耀自己的-成功报酬",normalizedTitle:"善于炫耀自己的&quot;成功报酬&quot;",charIndex:null},{level:3,title:'打造自己专属的"变身魔咒"',slug:"打造自己专属的-变身魔咒",normalizedTitle:"打造自己专属的&quot;变身魔咒&quot;",charIndex:null},{level:3,title:'注重调节"睡眠、饮食、压力"的平衡',slug:"注重调节-睡眠、饮食、压力-的平衡",normalizedTitle:"注重调节&quot;睡眠、饮食、压力&quot;的平衡",charIndex:null},{level:3,title:"身处低谷，心在高处",slug:"身处低谷-心在高处",normalizedTitle:"身处低谷，心在高处",charIndex:15273},{level:3,title:"糟糕的事会成为人生最重要的经历",slug:"糟糕的事会成为人生最重要的经历",normalizedTitle:"糟糕的事会成为人生最重要的经历",charIndex:16011},{level:2,title:"学习能够磨炼你的人生",slug:"学习能够磨炼你的人生",normalizedTitle:"学习能够磨炼你的人生",charIndex:138},{level:3,title:'被告人教会我们的"三个问题"',slug:"被告人教会我们的-三个问题",normalizedTitle:"被告人教会我们的&quot;三个问题&quot;",charIndex:null},{level:2,title:"助你美梦成真的思考法",slug:"助你美梦成真的思考法",normalizedTitle:"助你美梦成真的思考法",charIndex:154},{level:3,title:'大声说"一定能成功"',slug:"大声说-一定能成功",normalizedTitle:"大声说&quot;一定能成功&quot;",charIndex:null},{level:2,title:"不要想象十年后的自己",slug:"不要想象十年后的自己",normalizedTitle:"不要想象十年后的自己",charIndex:16488},{level:3,title:"学习的本质意义",slug:"学习的本质意义",normalizedTitle:"学习的本质意义",charIndex:16703},{level:3,title:'"自我满足"是助人成长的动力',slug:"自我满足-是助人成长的动力",normalizedTitle:"&quot;自我满足&quot;是助人成长的动力",charIndex:null},{level:3,title:"学习就是人生的播种",slug:"学习就是人生的播种",normalizedTitle:"学习就是人生的播种",charIndex:17471}],headersStr:'备考必须弄清的10件事 "追求学习量"是最烂的学习法 复印目录帮你把握整体内容 学习从最终目标回溯到现在的思考方法 根据最终结果来调整自己 找出失败的原因才能更好地学习 只有两类问题会出现在考试里 极限之后再走一步 学习也需要“有效的徒劳” 大处着眼，微小处着手 休息时间定为三小时一次 掌握学习的技巧 为什么铃木一郎一直做手法练习 自己给自己讲课的妙用 能明确表达喜怒哀乐的人往往记忆力很好 反复练习直到大脑产生错觉 每一张过去的试卷都有意义 做标记的方法会决定你的成绩 让大脑快速运转的方法 调整精神状态的"腹式呼吸法" 预习、复习其实只需10分钟 实现梦想的时间管理方法 结束时再拖延五分钟 当日达到最佳状态的方法 模拟考试时对自己严苛一点 "能力\\勇气学习方法"的乘法 教你怎样面对挫折 如果你现在陷入低谷，那么恭喜你了 把焦虑不安的原因全部写在纸上 "精神笔记"和"梦想笔记" 找出你最迫切要实现的梦想 在人生的岔路口做出明智选择 让悲观情绪释放正能量 创造多面性人生为自己减压 善于炫耀自己的"成功报酬" 打造自己专属的"变身魔咒" 注重调节"睡眠、饮食、压力"的平衡 身处低谷，心在高处 糟糕的事会成为人生最重要的经历 学习能够磨炼你的人生 被告人教会我们的"三个问题" 助你美梦成真的思考法 大声说"一定能成功" 不要想象十年后的自己 学习的本质意义 "自我满足"是助人成长的动力 学习就是人生的播种',content:'# 《考生们最需要的直线学习法》读书笔记\n\n这书取名直线学习法的目的很明确了，就要想要考生从中汲取学习方法的知识，少走弯路，提高学习效率的目的。\n\n这本书共分为5个章节.\n\n第1章：备考必须弄清的10件事；\n\n第2章：掌握学习的技巧；\n\n第3章：教你怎样面对挫折；\n\n第4章：学习能够磨炼你的人生；\n\n第5章助你美梦成真的思考法。\n\n总体来说分为4个部分，第一部分概述了备考前要做的准备工作，也就是第一章的内容；\n\n第二部分是备考过程中学习的注意技巧，也就是第二章的内容；\n\n第三部分是遇到挫折或失败要有信念和毅力，也就是第三章的内容；\n\n第四部分是升华部分，学习是人生的一部分。\n\n\n# 备考必须弄清的10件事\n\n\n# "追求学习量"是最烂的学习法\n\n花更多的时间来追求学习量，“多学习总会有一定的效果”，这种方法只是适合于一部分死记硬背为主的考试，但是对于像司法考试这种来说，就学习量来说所用的时间是完全不够的。\n\n对于这种考试，胜败不在于"量"的问题，而在于“质”的问题。\n\n但是真正高质量的学习方法，不是每个人都能做到的。书中给出的答案是：“把握整体” 。\n\n我的理解是，学习的过程中要把握整体，有的放矢，什么是重点，什么是难点，不能平均用力。另外在学习过程中，要重基础，重基本概念，重基本公式的理解，循序渐进的学习。学习后要能总体上复述书中的内容，把握书中知识的脉络。\n\n\n# 复印目录帮你把握整体内容\n\n在学习中考虑到“体系”很重要，书中推荐的方法是，作为掌握整体内容的学习法，我想大家推荐的是复印目录法。\n\n读书的过程中，时不时的看一下复印的目录，这样可以确认自己现在读的是哪个部分，在整本书中占什么位置。\n\n反复把书翻到前面去看目录，这样会把思考中断。若是复印的目录就放在眼前，则相当于是意识到整体内容的同时再读书。如此一来，就可以知道现在正在读的地方和整体内容的位置关系。复制出来的目录就像是地图一样，可以帮你确认现在所处的位置。\n\n“理解”是什么意思？就是“具体”和“抽象”的反复。从实践中来，到实践中去。\n\n而且，最好能够把目录当成书签夹在书里。可以随时取出来看，下次开始读书之前，可以先确认自己读到了什么位置。\n\n先读“开始”和“结尾”也是“把握整体”的好方法。\n\n\n# 学习从最终目标回溯到现在的思考方法\n\n从目标出发来决定现在应该做的事情，这是合理的学习法中不可或缺的基本方法。\n\n学习不应该是漫无目的的胡乱学。认识到自己的目标并朝着目标不断努力，才是合理的。\n\n如果当前的目的是通过考试，那么，就应该考虑只能在考试当天做的事是什么，然后再考虑考试前一周应该做的事情、前一个月应该做的事情，由此向前，一直推断出一年前的现在应该做什么。\n\n为了让大家认识自己的目标，我们学校在开学之初，就要求大家写出合格体验谈。合格体验谈，就是假象一下自己通过考试后的感受，要写的真实一点，好像是真的通过了考试一样，要写的尽量具体、写实。可以在日记中写上那一天的身体状况、衣服、空气状况等等，总之写得越真实越好。\n\n这就是从自己通过考试的时间点开始学习的学习法。于是，现在只要把录影带倒回去并加以临摹即可，即使中间出现了困难挫折也无需担心。因为一切都在“按预定轨道前行”，最后的结果就是通过考试，所以无论有什么障碍，都能够跨越。\n\n\n# 根据最终结果来调整自己\n\n调整也就是说是要对准目标后调整聚焦点。\n\n调整的目的是什么呢？或者说如何有节奏感的去努力呢？调整或对频率的目的在于是使得实现目标的过程更为顺畅。\n\n如果发现进展不顺利的时候，一定要停下来思考思考。思考什么呢？进展是什么？是利用调整的方式方法去实现目标的过程，要反思是目标不够明确呢？还是调整的方式不对？用摄影来思考，如果要拍摄的对象不清楚很模糊，这样的话，很难进行聚焦的；还有是否在对对象进行聚焦的时候，方式方法不对呢？\n\n问题一：没有明确和具体的目标，最好这个目标可以度量，可以量化。过于笼统的目标，很难具体把握。\n\n问题二：面对目标，不知道自己所处的位置，不了解自己的状况和弱点。不知道自己要如何努力，或者努力到何种程度才能缩短自己与目标之间的差距。自己与目标的距离，决定了自己的弱点（我的理解为方式方法或自身的基础）在哪里，而那也是自己应该加强的部分，所以，不明白这一点，就不知道自己应该学习什么。\n\n从目标出发找到自己的位置和弱点，一遍弥补自己的缺点，进而缩短自己和目标之间的距离，这种调整非常重要。\n\n这种调整时学习时重要的关键词。\n\n认清目标，牢记调整，才能不断接近目标，最终实现目标。\n\n\n# 找出失败的原因才能更好地学习\n\n"有人会说，根本不用这么拼命！不用这么拼命也能考上的"。\n\n但是如果不踏踏实实地学习，那么，失败到底是懒还是因为学习方法本身有问题造成的，就很难找到真正的原因了。所以，为了获得和下一次努力紧密相连的结果，就有必要尽全力、彻彻底底地学习。\n\n很多人在这方面的认识都是错误的。因为拼尽全力学习结果却失败了是件很难接受的事情，所以他们就保留力量以求分散风险，但是这样是大错特错的。\n\n保留力量以求分散风险，结果只会什么都得不到。一定要踏踏实实地学习，即使现在的学习很痛苦，也一定要拼尽全力坚持，这才是不浪费每一次经验的最好捷径。\n\n\n# 只有两类问题会出现在考试里\n\n考试中只会出现两类问题，“自己知道的问题”和“自己不知道的问题”。\n\n永远会出现自己未知的问题，出现未知的问题也没有必要去害怕。\n\n> 柏拉图的《苏格拉底的辩白》中，阐述了对“死是不是幸福”的思考。苏格拉底说，死也是一种幸福。这是因为，人死了之后自己的存在和意识都会消失不见，就和熟睡的夜晚一样令人心情舒畅。\n\n只要假设这两类问题都存在，并相应地采取一定的措施就可以了。我们一直会执着于努力增加自己知道的问题的数量，我们更应该去考虑是对不知道的问题的处理方式。\n\n不断出现意料以外的事情才是真正的人生。如何解决自己不知道的未知问题，才是更重要的\n\n遇到想都没想过的障碍时，如何才能跨越它？遇到失败或挫折时，如何克服它？如何应对意料之外的未知问题，掌握这种方法是通向梦想道路上的重要一环。\n\n因此，不要单看事务的表面，更重要的是养成从事务深层考虑问题的习惯。\n\n为了不失败，要做好所有的准备，但另一方面，也要考虑好万一失败后应该采取的对策；分析、整理意料之外的危机，考虑相应的处理手段。\n\n首先，彻底分析假设的情况并进行模拟实验。\n\n对未知问题的处理方法也是一样的，对大量的问题进行分析，将假设的未知问题进行分类并制定解答模板，如果遇到了某类未知的问题，只需要套用相应的模板即可。只要制作了解答模板，剩下的就简单多了。\n\n要制作模板就要搜集各种信息，并在自己的大脑中进行整理，然后再思考解决方法，最后才能制成模板。\n\n\n# 极限之后再走一步\n\n出现未知的问题，直面意料之外的困难时，人们都会采用各种方法加以应对。其中最好的方法就是之前说的，在日常生活中就考虑好应对方法和处理顺序。\n\n但有的时候，无论怎么努力依然会失败，尝试了所有手段依旧啃不动这块硬骨头时，这会让我们感到束手无策。这个时候要咬紧牙继续坚持！\n\n\n# 学习也需要“有效的徒劳”\n\n学习过程中也需要“有效的徒劳”，“有效的徒劳”是指：和考试没有直接关系，但了解其本质非常重要，它是实现真正目标的过程中不可或缺的知识。\n\n在遇到困难时，心情难过时，不妨看看别人的过往，他人的人生，再者分析下我们现在学习和努力的意义时，就可以知道，目前的考试也只是人生道路上的一个跨栏障碍而已，只要朝着大目标不断前行，肯定会到达终点的。\n\n有效的徒劳，会让自己心胸开阔，思路扩展，打开心扉，对于确立真正的目标，有着十分重要的意义。\n\n\n# 大处着眼，微小处着手\n\n人必须更加谦虚，只要谦虚就一定会有所收获，特别是自己尚未成熟的时候，这种谦虚非常重要。\n\n有很多事情，是我们现在还不懂得的，所以我们需要保持谦虚的态度。随着年龄的增长，保持谦虚的态度就更难了。只有保持谦虚，在读书或是听别人说话的时候才能产生新鲜感、惊讶感，才能吸收新的知识。\n\n\n# 休息时间定为三小时一次\n\n学习中需要“有效的徒劳”，如何判断徒劳是否有效，如果不加区分的去做，很有可能本末倒置，那么选择的标准是什么？\n\n简单来说，就是自己的直觉和好奇心，自己的直觉和好奇心不同于理性和大脑的思考，它是来自内心的真实感受，也可以说是具有更接近真正目标的指向性。只有自己内心真正需要的事务才最有可能对真正目标起作用。\n\n作者认为，只要是自己感兴趣的、能激发自己好奇心的或是无论如何都想做的事，就不应该抑制自己的情感。\n\n“有效的徒劳”，也是需要注意的，不能放下眼前必须要做的事。如果荒废了达成假定目标的学习，那么“有效的徒劳”就是在本末倒置了。\n\n人类是容易随波逐流的生物，所以在做某种和假定目标相关的事情的时候，一定要分清楚这是不是自己逃避的借口。\n\n怎么才能分清楚了。\n\n作者给出的答案是限定时间，三个小时为一个阶段，这也是分给“有效的徒劳”的时间。\n\n这样一来就能做到有急有缓，不会随波逐流也不会让有效的徒劳变成懒惰的借口。\n\n\n# 掌握学习的技巧\n\n\n# 为什么铃木一郎一直做手法练习\n\n在学习过程中，不断地重复进行基础练习是很重要的。这是因为，通过基础的积累就会习惯问题的类型。\n\n"学习"这一词汇来自"模仿"。彻彻底底地模仿基础问题并反复练习，就能明白问题的"型"。\n\n掌握了出题类型、问法还有回答问题的"型"，任何应用方面的问题就都能迎刃而解了。\n\n但是有时候我们也会对不断钻研重复简单的问题感到难以忍受，因为人总是在追求新的知识。而如果不能忍受单纯的反复模仿的学习，就会落后。\n\n一定要通过单纯的重复把"型"记到骨子里面，可事实上有人对"型"只是看似记住了一样的略过，并没有真正的掌握它。\n\n所以，学习也要不厌其烦地反复练习基础性知识，要将这种知识融化到骨子里面。融化到骨子里的瞬间就能达到目的地的高速列车般的线路越多，就越能快乐地享受学习的过程。\n\n\n# 自己给自己讲课的妙用\n\n# 我的记忆法就是"运用五感"\n\n动员"看"、"听"、"触"等所有身体感觉来记忆知识的方法。\n\n大家上课的时候的注意事项，首先要讲的就是，上课的时候光用耳朵听是不够的。一定要看教材，用手记笔记，还要自己说出来，充分刺激自己的视觉和触觉。\n\n所以，就需要使用眼、手、口，动员所有感觉来记忆。\n\n# "自我授课法"\n\n另外，推荐的是自己给自己讲课，即"自我授课"法。\n\n要想给别人上课，自己就必须理解这一事项。如果自己都弄不明白，就不能条理清晰地讲给别人听。\n\n在这一过程中，最重要的就是要"发出声音"。读出的声音会激活大脑，自己说的话传到自己的耳朵里，再一次的询问、理解就化为了对记忆的强调。\n\n用眼睛看，发出声音，听到的自己的声音就是对记忆的再次确认。经过几次重复，记忆会变得越来越稳固，也只有这种不断的强调才能巩固记忆。\n\n有个同学，她每天上完课回家时都会提前一站下车，然后在这一站的距离上边走边自言自语地自我授课。她每天坚持这样做。\n\n\n# 能明确表达喜怒哀乐的人往往记忆力很好\n\n记忆分为知识记忆和经验记忆。\n\n死记硬背、记忆单词和数字都属于知识记忆。而经验记忆是过去的经验和感情相结合的记忆。记得年轻的时候曾做过这样的事情，记着自己曾做过的某件事，这些都属于经验记忆。知识记忆会随着年龄的增长慢慢减弱，而经验记忆则与之完全想法。\n\n所以，如果学习能和经验联系到一起，就不会那么容易被忘记了。特别是随着感情的深入记忆也会更深刻。与开心、快乐、悲伤的感情经验固定在一起的记忆，无论何时都不会消失。\n\n这和大脑的扁桃体有关。脑内负责记忆的海马和负责感情的扁桃体关系密切，因此可以通过扁桃体将感情和记忆结合起来，所以，喜怒哀乐感情丰富的人往往记忆很好。\n\n即使上了年纪，也不该失去自己灵敏的感性，如果一个人心态年轻，那么通过考试也更容易。\n\n"嗯，是吗？"嗯，太好了！" "太厉害了！"能表现出感动或惊讶的人更容易通过考试。\n\n这也就是说，如果想提高记忆力，就要放开自己喜怒哀乐的感情。不要事事冷静，也不要装出什么都懂的样子，多少要表现的夸张点，要毫不害羞地表现出自己的感情，只要做到这点，记忆力应该就会有很大的提高。\n\n\n# 反复练习直到大脑产生错觉\n\n除了充分活用自己的"五感"并与喜怒哀乐的感情相结合之外，还能强化记忆的就只有反复了。要反复到令自己讨厌的程度。\n\n在反复的过程中，超过了一定的程度大脑就会产生错觉："这就是生存必需的记忆"。生存必需的记忆因反复地刺激大脑而被输入大脑之外。因为这些记忆是以接近本能和潜在意识的程度刻在大脑之中的，所以绝对不会再忘掉。\n\n反复练习是痛苦的，很容易让人感到厌倦的；学习中最痛可的就是这种反复练习。\n\n人的大脑明明应该记10个，可第二天却只记住了2个，其实，即便如此也没什么问题。这是因为我们已经记住了2个，那么只要把剩下的再重复4次就能全部记住了，仅此而已。\n\n而我们之所以感到痛苦，是因为我们在意的不是记住了2个，而是忘掉了8个，而事实上这种痛苦完全没有必要的，很多人也正是因为这个才越来越退步。\n\n不过，我们可以超越痛苦的办法。是什么呢？\n\n那就是把自己做过的事情可视化。\n\n记住一个，就贴上一个标签，或者在教材上画圆圈，随着记住内容的增加不断添加圆圈的数量即可。这种办法非常简单，但却是一种很棒的鼓励，据此我们就能越过因不断反复而带来的痛苦。\n\n伊藤学习开创了网上答题系统，答题结果分数化。有人从这种方法中体验到了意料之外的快乐，以游戏的感觉记忆知识，既心情愉快又有成就感。\n\n学习本来就应该是快乐的，因为既可以记忆新知识又可以探知未知世界能刺激大家的好奇心，令人涌现出无限热情。\n\n反复记忆的学习也应该是有趣的，但有时也会让人感到痛苦，不过这种痛苦并不是学习的全部。\n\n所以，如何才能把反复学习变得快乐呢？游戏的手法，就是方法之一。\n\n\n# 每一张过去的试卷都有意义\n\n# 选择参考书\n\n参考书的选择方法是非常重要的，假如因为选择了一本比较难的教材而学不会，最后只会增加挫折感。\n\n选择方法的重点就是，先要选择一本能大体知道难易程度的教材。一开始，尽量选择薄的、简单的，而且只买一本，先把握考试科目的整体情况抓住大概。\n\n然后买第二本，这次要选择自己觉得有趣的教材，这样就能和上一本对比着读。因为已经通过第一本了解了前提下的知识，所以就能知道第二本是否详细，是否有漏掉的内容。一边对比，一边整理。\n\n选书的时候，"现在自己想做什么？" "自己是为了什么而学习的?", "自己的目的是什么"，也就是"了解自己"。从自己的目的、想法、弱点、需求等多个角度分析自己，考虑自己需要的是什么，就是要了解自己。\n\n# 问题集\n\n通过分析过去的问题，能了解出题的领域和倾向以及要求。通过做过去的问题，能看到自己不擅长的领域和弱点。\n\n此外，还能预测下次出题的内容。如果能轻而易举地解答过去的问题，并且自己也能出题，通过考试将不再存在任何问题。\n\n过去的问题是能更好地了解自己目标的手段。\n\n\n# 做标记的方法会决定你的成绩\n\n我会建议学生先把教材全部复印下来。教材用过之后以后就不能再用了，所以要在复印件上做笔记。\n\n然后在复习的时候，再从笔记中摘录重要的内容写到教材原件中加以整理，总之就是要制作迷你笔记本。\n\n这里需要注意两点：\n\n第一，疑问点一定要做笔记。做笔记的时候，记下老师说的话是基础，但是最重要的是要一边做笔记一边写下自己有疑问的地方。\n\n第二个注意点是关于记笔记的重点。虽然说要记下老师说的话，但特别要记下的是重要的具体事例。事实上这些具体事例才是重点，只有具体事例才是理解抽象事项的线索。\n\n书中写的大多数是抽象的话、原理和原则。按照老师讲的顺序，讲到的具体事例的地方才是讲义的真正价值所在。\n\n一般人都会记下抽象的原理原则，这就大错特错了；这些内容即使不做笔记书上也会有的。老师的讲课的价值就在于老师讲到的具体事例，所以一定要将好不容易听到的具体事例记到笔记中，这样才不会忘记。\n\n\n# 让大脑快速运转的方法\n\n所谓大脑快速运转是指激活大脑。大脑运转快，理解也应该更快。\n\n可以尝试加速播放讲义的录影带，加速播放新闻和电视剧的录像带等方法。如果以两倍的速度停录影带，大脑就必须以两倍的速度运转，按道理来说，大脑的运转也会更快。在伊藤学校，我们推荐快速听讲义录影带复习的方法。\n\n即使现在，我也会在学习中犯困的时候站起来，或者一边走一遍快速朗读。\n\n说到考试学习，很久以前流行过包头巾的学习姿势。\n\n这种感觉和自我暗示会收到意想不到的效果。学习中也会有宽心丸效果。\n\n最近，我们都会随身带着添加了氨基酸和柠檬酸的矿泉水。\n\n像这样，"这样我的大脑转得更快"，"这么做我的头脑更清晰"，拥有属于自己的工具和仪式非常重要。\n\n用来激励自己的工具越多越好，工具越多，对自己的学习越有利。\n\n\n# 调整精神状态的"腹式呼吸法"\n\n腹式呼吸类似于自我暗示。腹式呼吸可以保持心情平静，放松情绪，建议大家在考场中使用这个方法。\n\n腹式呼吸能让呼吸更加缓慢，于是，大脑的开关就会从交感神经切换到副交感神经，情绪也会平稳下来。\n\n自律神经分为交感神经和副交感神经。焦躁不安、紧张时交感神经占上风，心情平稳、放松的状态下副交感神经占上风。\n\n腹式呼吸能完成二者的交换，让整个身体进入放松的状态。大家试试看：深呼吸，呼气，感觉一下横膈膜，吸气到肚子膨胀起来，吸到最大程度之后，再缓慢地呼气。\n\n精神状态是呼吸的外在表现，呼吸是基础。心情烦躁的时候先做深呼吸，如果能有意识地做腹式呼吸，心情就能恢复平静。\n\n\n# 预习、复习其实只需10分钟\n\n不预习、不复习，只考虑结果等自顾自的做法是不可取的。预习和复习可以使学习达到以往两三倍的效果。\n\n预习的目的是什么？是为了更好地理解讲义吗？\n\n更重要的目的，在于找到自己的疑问点。通过预习，能够找到以自己之前的知识和经验就能明白的内容，还能找到自己没有经历的、不能推测和理解的未知事项。\n\n哪怕只有5分钟也一定要预习。只要在上课开始前的5分钟对各条目做出标记既可，哪怕只是这样一个小动作，也会使自己对讲义的理解方式变得大为不同，如此一来，就能理解自己难以理解的内容。\n\n那么，复习是为了什么呢？\n\n是要整理学过的内容，要把知识转换成属于自己的东西。特别是上课结束后的五分钟，简直就是黄金时间。在这个时间里，课堂上的一切还都是记忆犹新，所以更容易巩固大脑中的知识，而且效果会非常好。\n\n很多人在上完课后，坐在原地。有的整理笔记，有的重读教材，有的把重点写到卡片上以备回家的电车上背诵用，尽管用的只是上课结束之后短短的5分钟或十分钟，但是每天积累下来，最后就会形成巨大的差异。\n\n\n# 实现梦想的时间管理方法\n\n增加学习时间固然重要，但是还有更重要的，那就是下功夫学会"时间管理方法"。能合理管理时间的人学习效率就高。\n\n因为时间是有限的，所以，如果有想做的事情，就必须放弃另外一些事情。如果还和之前一样地玩，用大量的时间去做自己感兴趣的事情，或者长时间睡觉，那就不可能学习。\n\n作为合理的时间管理方法，最重要的就是要决定优先顺序。要想好在一天的二十四小时中应该做什么以决定优先顺序。我一般用消除法来砍掉自己不必做的事情。\n\n从自己需要做的实现开始考虑，砍掉离目标最遥远的事情；在通过考试之前，减少和朋友会面，减少做自己感兴趣的时间，将多余的事情全部砍掉。确定优先顺序是合理管理时间的第一步。\n\n建议大家在合理管理时间时进行时间分配。备战考试时需要做的事情太多，所以容易焦躁不安。而越是焦躁不安，越做无用功，效率就越低。\n\n"这个也必须做，那个也必须做，时间就来不及"，容易出现恐慌。这是，如果能看一下时间分配就会感到安心；这个明天做，那个后天做也可以，等等，这些已经提前做出了安排，所以只要集中精力做现在该做的事情即可。时间分配能起到一种精神安定剂的作用。\n\n每天人能真正集中精力的时间大概只有6个小时。超过这个时间就会磨磨蹭蹭，也是对时间的浪费。\n\n因此，经常为自己准备一些成功报酬：学习结束之后，给自己些小福利，读喜欢的书、看想看的电视节目等，这样一来，就能专心地学习自己想学的内容了。\n\n为了有效地管理时间，指定截止时间效果会更好，而这就是截止效果。\n\n\n# 结束时再拖延五分钟\n\n学习中非常重要的就是最后的坚持和忍耐，在自认为不行之后还能否再坚持是决定能否通过考试的关键。\n\n为了这最后的一搏，最好能养成再努力五分钟的习惯。\n\n"今天就到这里吧"，从想去休息的那一刻开始，再努力五分钟；从觉得今天不行的那一刻开始，再看一页，再回答一个问题，这些坚持终将化为最后的一搏。\n\n\n# 当日达到最佳状态的方法\n\n为了能在正式考试中发挥出平时积蓄的力量，相比平常的学习，能在正式考试的当天达到最佳状态 ----"达到顶峰"的技术更为重要。\n\n在我们学习，从正式考试一周前就开始进行模拟考试。从一周之前就采取和考试当天的休息时间，在相同的时间起床、相同的时间段答题、相同的时间里吃午饭、上厕所。\n\n已和考试当天完全相同的姿态接受训练，比如午饭吃什么、吃多少，对实际状况也进行最真实的模拟。\n\n提前去看考场也是必不可少的。提前去感受下桌子，椅子的舒适度，毕竟几个小时下来也挺累的，如果凳子不舒服，可以带个坐垫去，提前做好准备。看看厕所、开水房的位置，也很重要。\n\n另外，为了配合考试当天的考试形式，我还用铅笔做了答题卡的涂抹练习。持续练习了大约一个月，使用时间就缩短了三分钟。三分钟，足够回答一道题了。\n\n像这样，朝着自己的目标调整自己的身体能力和精神能力，是考试当天达到最佳状态的有效方法。\n\n\n# 模拟考试时对自己严苛一点\n\n# 考试当天情绪调整\n\n进入考场之后，不要想自己不会的内容而是回忆自己擅长的内容。\n\n考试当天，带着自己做过的或已经记住的单词卡片还有能顺利完成的习题集，"做到这里了"，"这部分是这样的"，这样就能让自己保持平和心态。提高"我明白了"的情绪，保持好心情非常重要。\n\n# 模拟恶劣考试环境\n\n考场上也会发生意料之外的事情。比如，发错试卷，发生火灾，等等。但即使在这种不利的条件下，决胜因素也与安静会场中的没有什么不同。考试，无论在什么环境下都要从容应对。\n\n所以在模拟考试中，尽量选择条件恶劣的座位，或者坐在奇怪的人旁边。考试当天，不知道自己会坐在什么座位上，即使能提前看到考场，也无法看到自己所坐的位置以及旁边的人。\n\n正因为如此，才要在模拟考试的时候特意坐到出入口旁边等位置的座位上，因为出入的人比较多总会影响自己的心情，再不然就坐到看上去喜欢乱晃的人旁边。\n\n即使发烧的时候也要去参加模拟考试，这样就能明白自己在发烧的状态下能正确回答几道题了。\n\n总之，如果能提前经历恶劣条件，在正式考试的时候，无论什么样的环境都不会让你感到惊慌。\n\n\n# "能力*勇气*学习方法"的乘法\n\n能够通过考试取决于"能力*勇气*学习方法"的平衡。\n\n如果能力一般，勇气和学习方法相乘的结果也足以挽回一定的局面。相反，无论能力高出别人多少倍，如果勇气是零，或者负数，那么相乘的结果也会是零或者负数，所以最后的结果也会是零或者负数。\n\n必须注意，能力、勇气、学习方法，哪一个都不能是零或者负数。\n\n关于学习方法，如果大家能从本书中学到从目标出发、调整、对未知的问题的处理等各种方法就太好了。\n\n能力因人而异，只要大于一就足够了。\n\n勇气是关键。如果自己不是从内心渴望着某件事情，就无法保持勇气，就会很容易陷入负数状态。\n\n充满勇气是因为前面有令自己心动的梦想；但是不服气，是因为开始学习之后就不能半途而废才产生的，所以并没有梦想的支撑。\n\n如果不是发自内心的渴望，就一定不能梦想成真。\n\n\n# 教你怎样面对挫折\n\n\n# 如果你现在陷入低谷，那么恭喜你了\n\n# 心态不畏惧\n\n陷入低谷，每个人都会遇到。\n\n之所以会陷入低谷，正是因为不满于现状。也就是说，打破现状、实现下一个目标之前的过程中就会陷入低谷。\n\n所以，低谷是值得骄傲的事情。\n\n# 克服困难，改变现状\n\n陷入低谷的时候，不要害怕，重要的是一定要认为这是合格迈进了一步，一定能克服。在此基础上，为了阻止低谷进一步恶化，想办法恢复即可。\n\n首先会在纸上写出低谷的原因。不知道原因，肯定也不知道怎么办。\n\n比如"为什么会陷入低谷？" "因为分数比预想的低?" "身体感到疲惫?" "是谁说了让自己情绪低落的话? " ....\n\n自己为什么会感到处于低谷状态的原因，把能想到的全部写在纸上，然后再一个一个地检查这些是不是真正的原因。\n\n如果是偶尔的分数不高，也不应该对自己失去信心；如果是别有用心的人故意陷害自己，就会发现"什么啊，这些根本不足以让我心情低落"，情绪也就自然恢复了。\n\n另外，如果写出来的内容真的非常严重，那就再一一地想对策即可。如果是客观上的分数下降，就要考虑改变该部分的学习方法，或者跟老师谈谈。\n\n如果是人际关系上有麻烦，要毫不犹豫地给对方打电话、写信，坦率地说出自己的想法。总之，能解决的问题就要及时解决。\n\n当然，有的问题也不能马上得到解决。但是能做的事情，一定要去做，这一点非常重要。想尽办法却无法解决的问题，就算花费太多时间去想，也没有意义。\n\n这样的事情，也要写在纸上。"即使现在想也没有结果，一周后再想吧"，"学完这个再想把"，经过这样的调整，恐慌情绪就会减弱了。\n\n总之，以恢复状态为目的采取具体措施，是走出低谷的第一步。什么都不做、放任这种状态持续下去，并不是最好的办法。\n\n\n# 把焦虑不安的原因全部写在纸上\n\n# 重视焦虑不安的情绪\n\n笼罩在不安和恐惧中，坐立不安地向现实低头，很难达到自己期望的效果。\n\n因不安而压抑自己的情绪，面对现实，就能创造出自己希望的未来吗？\n\n恐惧和不安会令人失去判断力、情绪低落，并且会降低挑战当前课题和现实的动力。\n\n# 如何克服焦虑不安的情绪\n\n写下来：\n\n不仅是陷入低谷，只要我感到不安，我就会在纸上写出来。\n\n自己想到的事情不要停留在大脑中，而要将其可视化，化为眼睛能看到的形状，这样一来会更容易解决。\n\n**为什么会感觉不安呢？**因为我讨厌明年的再次落榜。为什么讨厌呢？\n\n我试着把所有的原因都写在纸上。对不起父母，样子很落魄，不喜欢再做一年巡警，还要住在这个像蒸笼一样的小屋里...只要我能想到的，就全部写出来。\n\n为什么讨厌这些呢？因为这样只会让我更受伤。\n\n对不起父母，不想看到父母悲伤的脸。\n\n父母知道后会觉得很丢人....反复多想两次、三次之后，意外地发现这也没什么。这根本不足以成为讨厌的理由。\n\n什么啊，我居然在为这种无聊的事情伤脑筋，于是心情大好。百谈莫如一试，一旦觉得自己被情绪欺骗了，大家最好能把所有的不安都在纸上写出来。除这种"写出不安法"之外，我还会通过语言的力量来提高自己的士气。\n\n语言的力量：\n\n怎么做呢？我不断地和自己说，"只要做就能成功"。\n\n无论什么话，说上100万遍，就会成为事实。\n\n把这句话深深地刻在脑子里，大脑就会产生这样的感觉，自己机会深深地相信 ---"只要做就能成功"\n\n人类大脑内的海马体会筛选出重要的信息，再传导到负责记忆的大脑皮层。反复地说"只要做就能成功"，就能巩固这一记忆，形成"只要做就能成功"的思考线路。\n\n在反复说的过程中，线路会越来越粗，消极的思考线路也会变成积极地思考线路。这就是洗脑。\n\n为考试失败而痛苦的时候，我反复地对自己说积极地话，努力把自己从恐惧和不安中解放出来。\n\n无论是谁，都会有恐惧和不安的时刻。但是在重要的考试、课题、商谈之前，需要摆正自己的态度。\n\n\n# "精神笔记"和"梦想笔记"\n\n# "精神笔记"\n\n"精神笔记"就是看到某物就会精神百倍时，用来记录某物的笔记本。\n\n写出激励自己的话和自己的优点。比如朋友和老师的说的能感动自己的话、读到的书中激励自己的语句等。\n\n或者是能激发自己活力的音乐、增加勇气的DVD、喜欢的人的照片等，把能激发自己活力的小工具列成清单。\n\n另外，自己去过的令自己感动的地方的门票或落叶，都可以作为唤醒感动记忆的钥匙。看到这些，会一瞬间又穿梭回当时的场景。\n\n于是就会回想自己充满活力的时候，看到、听到这些场景，自己也会恢复活力。\n\n看到自己的优点，想着自己还有这么多好的地方，自然就找回了自行。也能够恢复前进的动力。\n\n如果能提前准备好这些让自己精力充沛的小工具和对策，在情绪低落的时候一定能成为恢复精力最好法宝，这本"精神笔记"也是处于低谷时的危机管理手册。\n\n# "梦想笔记"\n\n"梦想笔记"中，要写出通过考试之后和将来想要做的所有事情。\n\n比如想住大房子、想开法拉利...写出自己的梦想，随身带着，心情低落的时候就拿出来看看，自然提高自己的士气。\n\n写出自己的梦想，朝着梦想努力奋斗，就开始产生无穷无尽的正能量。\n\n写出来就能从客观的角度观察自己，整理大脑中的想法，更冷静地把握、分析当前的状况。\n\n\n# 找出你最迫切要实现的梦想\n\n多个梦想都只是自己假定的目标而已，一定不要忘记自己真正的目标。\n\n考试，只不过是实现真正目标过程中的假定目标而已。现在自己苦苦奋斗、不断努力学习，并不是为了通过考试，而是为了实现真正的目标。\n\n但是有时候真正的目标太过于遥远，实现的过程中有时候还会迷失方向。迫于每日学习的压力，会误认为在眼前的考试中取得好成绩就是人生所有的目标了。\n\n然后考试中出现的一喜一忧，都成为自己走向低谷的原因。\n\n"现在自己为什么要学习?" "为什么而生?" "自己内心渴望的生活方式、愿望?"经常确认梦想，这些都是防止自己陷入低谷的技巧。\n\n自己真正想做的是什么?\n\n为了确定自己真正的目标，我有时会问自己这样的问题："如果自己是全能的，会做什么呢?"\n\n如果现在，用魔法"能够实现唯一一个愿望"，自己想实现什么愿望呢?\n\n像这样冥思苦想地找出自己最想实现的愿望，非常重要。换言而之就是，在学习的间隙，偶尔也要想一想自己最根本的价值观是什么？\n\n确定自己的真正目的和根本的价值观，然后如果想离真正的目标更近一步，就算陷入低谷之中，也能立刻纠正自己的心情。\n\n\n# 在人生的岔路口做出明智选择\n\n人生，经常要面对各种不得已的选择，人生的道路上布满了岔路口。\n\n自己是放弃司法考试去就业更好呢，还是就这样继续学习更好呢？\n\n我自己迷茫的时候，通常会用非常单纯的判断标准来做决定。\n\n当为不知如何选择而烦恼的时候，人往往都会想到消极的一面：如果继续在这种状态下学习如果没通过考试怎么办？进入公司工作不能出人头地怎么办？公司倒闭了怎么办？能想到的全是消极的可能性，总是想朝着风险更小的方向去想。\n\n但是无论怎么评估风险，结果也不会因此而发生改变，不是吗？这是因为没有人知道未来怎么样。比如走了风险较小的路进入公司上班，但公司也可能会重组、裁员。无论选择哪条路，风险都是不可避免的。\n\n而且，没有足够经验和知识的自己，在预估将来的风险时，准确度又能有多高呢？\n\n如果这样，消极的考虑和积极的考虑又有什么不同呢？\n\n也就是说，如果一切都会按自己的思路进行，选择更能激发自己活力的思考方式显然更好。\n\n当我迷失方向的时候，经常以这种判断标准来选择。哪一种更能激发自己的活力、让自己的人生更快乐，就选择哪一种人生。\n\n不要想着避开未来路上的风险和障碍，我们一定会碰到某种阻碍。所以不要想着如何逃避，遇到阻碍的时候应该考虑当下选择更能激发活力的事情；迷茫的时候，应该主动判断哪一个更能提高自己士气。\n\n\n# 让悲观情绪释放正能量\n\n想考100分却只考了80分，准备用一天来完成的工作却花了两天，遇到这种情况，情绪会低落。害怕失败、无法面对失败，这是虚弱优等生的典型特征。\n\n因为我自己非常讨厌这种性格，所以经常想，怎么做才能不害怕失败、成为一个积极考虑问题的人。\n\n为了超越悲观主义，我学了以下两种方法：\n\n第一，不能将部分弱点看成全部。\n\n即使学不好现在的领域，也不代表所有科目都学不好，这一点和"被她甩了也不能说明自己本身没有存在价值"是一样的道理。必须停止将部分弱点和失败扩大化或者当成整体的思考方式。\n\n第二，一时的失败并不代表永远的失败。\n\n第一次失败了，下一次却有可能成功。即使被她甩了，也不要认为自己不能再恋爱了、不会再有人喜欢自己了。\n\n我努力让自己做到这两点，即使遇到失败感到悲观，我也会对自己说："这只是一时的失败，没事的。这不过是工作上的失败而已，我还有家人和朋友，所以不用悲观失落"。\n\n为了实现真正的目标而奋斗的过程中，即使出现进展不顺利的情况也是必然的，倒更应该去想，早点了解主题和自己不擅长的部分也是好事。\n\n即使正式考试失败，也已经回不去了。失败来得越早越好。超越这种失败，自己就能更好地成长，希望大家能经常对自己说这些鼓励的话语。\n\n\n# 创造多面性人生为自己减压\n\n# 广阔的视野\n\n和乐观的思考方式相同，不片面地看问题非常重要。\n\n即使司法考试和大学考试失败了，世界末日也不会到来。通过考试失败，自己能够成长，也能扩展自己的广度和深度。如果推迟一年通过考试，也只要以后多活一年即可。即使接下来的一年原地踏步，这一年和今后的人生相比也只是短暂的一瞬，没什么大不了的。如果不能从此种广阔的视野来看待事务，就会死死地盯着这一年，而无法前进一步。\n\n# 创造多个世界\n\n自己固执己见地认为只有这个才行，是万万不可取的。我们不应该用狭窄的视野来减少自己的选项。\n\n日常的生活中，就应该创造多个世界。朋友关系、家人关系、感兴趣的事务、工作等等，就算有一个世界遭遇失败，其他的世界也可以掩盖掉某一世界的失败。作为家庭一份子的自己、活在兴趣世界的自己、朋友关系中的自己等，多面性的自己应对危机的能力也会比较强。\n\n仅坚守一项工作，心里难免会不安和脆弱。即使没有了这个也还有其他的，如果能有多面的世界、多种思考方式，就不会因为一点儿小事儿就泄气了。\n\n退路，听起来像是消极的，但是为了分散风险、为了能全神贯注的做一件事情，积极地挑战多项工作是很好的方法。\n\n\n# 善于炫耀自己的"成功报酬"\n\n在"只有少数人会用的时间术"中也曾讲过，为了学习中的松弛有度，为了更有动力，最好能为自己准备些成功报酬，即褒奖。如果通过了考试，想做...把这种小愿望写到"梦想笔记"中，即使是每天的学习中，也要善用小小的成功报酬。\n\n因此，拥有学习之外其他兴趣的人更容易通过考试，因为为了获得更多的奖励也会努力学习的。\n\n虽然都是些小小的报酬，但还是要努力让其成为精神食粮。有时小小的报酬也会起到大作用，希望大家能善用成功报酬，控制自己的动力。\n\n\n# 打造自己专属的"变身魔咒"\n\n# 咒语式的物品\n\n身体虚弱的话，心灵也会虚弱。在挑战重要考试和工作的时候，也要注意身体的健康管理。\n\n对自己说喝了力保美达就会精神倍增，不断地自我暗示，所以喝了之后就真的感觉自己精神百倍。\n\n像喝过饮料之后就会精神百倍一样，找到一种属于自己的，能令学习进展顺利的咒语式的物品至关重要。可以是健康茶或咖啡啥的。\n\n# 心情治疗感冒法\n\n感冒是由睡眠不足和疲劳过度引起的，所以首先要保证营养和睡眠的平衡。然后，重要的是心情。以我的经验，保持心情畅快也能很快的预防感冒。\n\n我住在公寓的九楼上，但睡觉的时候我总是想象着自己的后背是接近大地的。\n\n也就是说，想象着后背穿过被子下面的地板、穿过八楼的地板、穿过七楼、六楼，一直下降到地面，并且暗示自己的身体深入地下、不断地吸收地球的能量。\n\n相信自己具有自我治愈的能力，和地球融为一体，一边吸收地球的能量一边治愈自己的身体。就这样用意念治疗，身体就会不可思议的好转。\n\n\n# 注重调节"睡眠、饮食、压力"的平衡\n\n充足的睡眠和饮食及平衡压力是避免陷入低谷的重要方法。\n\n睡眠、饮食、压力三个之中有两个处于恶劣状态下的时候，身体就会陷入非常危险的状态。\n\n所以，当我感觉到睡眠不足的时候，就会在饮食上补充营养、并减少压力。压力过大的时候，就会选择更好的饮食、并保证睡眠充足。\n\n# 提高睡眠质量\n\n想增加学习时间的时候，最先失衡的就是睡眠。所以，我会在白天分阶段浅睡一会儿，以此来消除长期的睡眠不足。\n\n大脑皮层中有老皮层和新皮层。老皮层控制喜怒哀乐等感情。新皮层用来吸收知识、记忆知识。\n\n即使没有深度睡眠，老皮层也不会感到疲劳。而学习的嘶吼，浅睡就能让疲劳的新皮层恢复活力。\n\n也就是说，学习时感到疲劳的话，只要短暂地小睡片刻就能消除疲劳。所以，为了让老皮层得到休息，我会有三个小时的深度睡眠；然后为了让新皮层得到休息，白天会用五分钟或者十分钟的时间来浅睡一下。\n\n学习两个小时后，如果感到有些疲劳，我就会趴在桌子上睡五分钟。这样一来，头脑一下子就清醒了。\n\n我在备战司法考试的过程中，常常利用一些空闲时间浅睡一会儿，在电车上抓着吊环睡觉也是一件容易的事情。在备战的过程中，我还掌握了一边走路一边睡觉的方法。\n\n像这样花时间保持睡眠、饮食、压力的平衡，学习也能顺利进行。学习生活虽然残酷，但却保持着自己的身心健康，不断朝着目标迈进。\n\n虽是全速奔跑，但偶尔也要休息一下，这种平衡非常重要。\n\n\n# 身处低谷，心在高处\n\n学习中，陷入低谷时，我有时会爬上东京塔这样的高处。从高处向下望去，东京街头尽收眼底，人变得比蚂蚁还小。\n\n原来自己的烦恼竞是小到可以忽略一般，心情也会瞬间平静下来。\n\n所谓低谷，其实是没有注意到自己周围的广阔的世界。\n\n# "空间轴"的扩展\n\n无论往哪个方向走都是自己的自由，现在却蜷缩在陶罐一样的地方筑起了堵塞四面八方的墙壁。这时，如果能打开物理空间，就会出现客观凝视自我位置的契机。\n\n登上高处，俯瞰世界，眼前的视野一下子就开阔起来。蜷缩一处的自己，会得到释放，心情也会开朗起来。\n\n如果想看到更高远的高处，建议大家看世界地图和地球仪。旋转地球仪的同时，遥想一下自己还没有去过的国家。就会发现，居于小小的日本，为一点小事就闷闷不乐的自己是多么的荒唐可笑。\n\n遥望处于宇宙之中的地球的照片也会收到很好的效果。漆黑的宇宙中，蓝色的地球静静地漂浮其中。我的房间里有一本地球照片的写真集，这本写真集对我来说就是宝贝。\n\n只要凝视奇迹般存在的美丽地球，就会想到人类的烦恼真不值一提，这种想法会产生不可思议的力量，将自己从苦闷中拯救出来。\n\n想这样，把压缩的"空间轴"从陶罐一样的尺寸渐渐扩展到地球一样大、宇宙一样大，心的尺寸也会渐渐变大。\n\n# "时间轴"的扩展\n\n同样，"时间轴"的延展也一样重要。处于低谷时，没有意识到自己时间轴的前后状况。这种情况下，只会永远处于低谷之中。\n\n如果地球的历史是365天，那么人类的历史也不过是除夕夜11点59分的简短一瞬而已。\n\n在如此漫长的时间中，我究竟在苦苦挣扎什么呢？\n\n像这样吗，将扩展的空间和时间同自己的生存相比较，就会发现自己面临的问题是多么微不足道。即便对自己来说是天大的问题，也是可以大而化小的。\n\n\n# 糟糕的事会成为人生最重要的经历\n\n也正是因为我有这样的经历，所以遇到小事情才不会觉得害怕。\n\n糟糕的经历也不是毫无用处的。人生中发生的所有事情都有其存在的意义，都会成为人的精神食粮。\n\n\n# 学习能够磨炼你的人生\n\n\n# 被告人教会我们的"三个问题"\n\n自己的人生目标是什么？\n\n为什么是这个目标？\n\n为了这个目标你现在在做什么呢?\n\n为了寻找这个答案，我们一直都在努力学习。\n\n\n# 助你美梦成真的思考法\n\n\n# 大声说"一定能成功"\n\n我的口头禅是，"只要做就能成功，一定能成功"。\n\n我认为，说出口的话都是正面的思考。有一种说法是"语言的灵魂"，这是因为语言中蕴含着眼睛看不到的能量。\n\n我经常多学生说，不要待在说否定话语的人旁边。不要靠近那些只会傻傻地说"这事，就算做也只能失败"的朋友。\n\n如果听了负面的话，不知不觉中就会积攒负面的理念。\n\n话用耳朵听，耳朵离大脑很近。语言会以声音的形式经耳朵进入大脑，抓化为潜在意识和记忆然后固定在大脑中。所以，相比与否定的语言，肯定的语言更有意义。\n\n只用耳朵听正面的语言，自己也只说正面的语言。\n\n\n# 不要想象十年后的自己\n\n向学生解释的时候，经常以登山做比喻：登山的时候，如果一直看着山顶，登山就会很痛苦。到达山顶还有那么远啊，还有那么难爬的山道在等着我们，瞬间心生厌恶。\n\n这个时候，最好能往后看一下。应该会看到我们身后绵延的山路。原来自己已经爬到这么高的地方了，不止不觉间自己也会越爬越高。\n\n我们并不知道十年后的自己是什么样子，但是今天的自己和十年前的自己相比，的的确确进步了。如果能继续往上走，山顶就会越来越近。\n\n\n# 学习的本质意义\n\n人生，多多少少都会有些不安。\n\n有些人为考试失败，工作压力，亲人离世。。。\n\n但是，最根本的不安是什么，难道不是死亡吗?\n\n自己从哪里来？要到哪里去？又为什么会在这里？自己究竟是什么人，又为了什么而生？\n\n通过学习，才能了解这些未知的事物，才能摆脱不安的束缚而获得自由。\n\n摆脱不安，心灵才能归于平静。\n\n没有不安，人才能安静、幸福地生活下去，也就是说，学习正是为了幸福地活着。\n\n并且通过学习来追求自己的幸福，人就应该更想获得幸福。\n\n学习，正是为了明白真理。掌握真理的时候，就是人类变得自由、幸福的时候。\n\n\n# "自我满足"是助人成长的动力\n\n大家在生活中都在追求成就感、最大的幸福感、满足感。\n\n学习，就是为了获得满足感而采取的手段，学习本身也同满足感和最大幸福感相关联。\n\n运动就是一个自我满足的世界。\n\n田径100米赛跑的记录缩短零点几秒会具有什么意义呢？马拉松42公里很艰难，又具有什么意义？举重项目中举起那么重的重量又起到什么作用？\n\n在研究学问的领域里，很多人一生都在研究没有任何作用的内容。\n\n这些都是在追求终极的自我满足，但是人类总是希望挑战极限，更进一步，了解未来。\n\n跑步想用再少零点一秒的欲望，想更进一步了解的欲望，或许都只不过是自我满足，但是通过对能力更进一步、半步的追求，人类才能进化，文明才能发展。\n\n人类，不仅仅只有一个人。所以通过追求自己的自我满足，可能会影响其他人、其他人再利用自己的所得在其他领域发挥重大作用。\n\n通过彻底追求单纯的自我满足，人们互相激励对方、互相学习，以此很累才得以进步。\n\n也就是说，正是因为个人对自我满足的追求，人类才能一直发展下去。\n\n通过学习获得自我满足，一定不要觉得不好意思。我认为，这正是自己个性的证明，推动人类进化发展的能量的一部分。\n\n\n# 学习就是人生的播种\n\n通过学习，我最想找到的就是自己生存的意义，也就是自己的人生使命。\n\n为什么我会活在这个世界上？为什么会在这里？找到这些原因和意义才是最重要的。\n\n我认为，所有的人生在这个时代、这个国家、这个地方，都有其生存的意义。\n\n每个人学习的内容各不相同，但是只要努力学习，就能不断接近自己的生存意义。\n\n如果你还没找到自己生存意义和使命，只要朝着自己发自内心的目标努力，就一定能找到自己的使命。\n\n这是因为，学习就是人生的播种。自己现在的人生和命运，只不过是收获自己过去播下的种子而已。\n\n大家都会受到各种环境和周围人的影响，同时，能够决定自己将来的只有自己播下的种子。\n\n自己的未来自己创造，所以，将来有什么样的梦想，现在就要播下什么样的种子。\n\n努力播下学习的种子，就一定能收获想要的果实，以此回报自己。\n\n只要努力学习，就一定能结出正能量的果实。\n\n最终，我们定将找到自己的使命所在。通过学习，我们一定能找到不同的"自己的使命"和"生存的意义"。',normalizedContent:'# 《考生们最需要的直线学习法》读书笔记\n\n这书取名直线学习法的目的很明确了，就要想要考生从中汲取学习方法的知识，少走弯路，提高学习效率的目的。\n\n这本书共分为5个章节.\n\n第1章：备考必须弄清的10件事；\n\n第2章：掌握学习的技巧；\n\n第3章：教你怎样面对挫折；\n\n第4章：学习能够磨炼你的人生；\n\n第5章助你美梦成真的思考法。\n\n总体来说分为4个部分，第一部分概述了备考前要做的准备工作，也就是第一章的内容；\n\n第二部分是备考过程中学习的注意技巧，也就是第二章的内容；\n\n第三部分是遇到挫折或失败要有信念和毅力，也就是第三章的内容；\n\n第四部分是升华部分，学习是人生的一部分。\n\n\n# 备考必须弄清的10件事\n\n\n# "追求学习量"是最烂的学习法\n\n花更多的时间来追求学习量，“多学习总会有一定的效果”，这种方法只是适合于一部分死记硬背为主的考试，但是对于像司法考试这种来说，就学习量来说所用的时间是完全不够的。\n\n对于这种考试，胜败不在于"量"的问题，而在于“质”的问题。\n\n但是真正高质量的学习方法，不是每个人都能做到的。书中给出的答案是：“把握整体” 。\n\n我的理解是，学习的过程中要把握整体，有的放矢，什么是重点，什么是难点，不能平均用力。另外在学习过程中，要重基础，重基本概念，重基本公式的理解，循序渐进的学习。学习后要能总体上复述书中的内容，把握书中知识的脉络。\n\n\n# 复印目录帮你把握整体内容\n\n在学习中考虑到“体系”很重要，书中推荐的方法是，作为掌握整体内容的学习法，我想大家推荐的是复印目录法。\n\n读书的过程中，时不时的看一下复印的目录，这样可以确认自己现在读的是哪个部分，在整本书中占什么位置。\n\n反复把书翻到前面去看目录，这样会把思考中断。若是复印的目录就放在眼前，则相当于是意识到整体内容的同时再读书。如此一来，就可以知道现在正在读的地方和整体内容的位置关系。复制出来的目录就像是地图一样，可以帮你确认现在所处的位置。\n\n“理解”是什么意思？就是“具体”和“抽象”的反复。从实践中来，到实践中去。\n\n而且，最好能够把目录当成书签夹在书里。可以随时取出来看，下次开始读书之前，可以先确认自己读到了什么位置。\n\n先读“开始”和“结尾”也是“把握整体”的好方法。\n\n\n# 学习从最终目标回溯到现在的思考方法\n\n从目标出发来决定现在应该做的事情，这是合理的学习法中不可或缺的基本方法。\n\n学习不应该是漫无目的的胡乱学。认识到自己的目标并朝着目标不断努力，才是合理的。\n\n如果当前的目的是通过考试，那么，就应该考虑只能在考试当天做的事是什么，然后再考虑考试前一周应该做的事情、前一个月应该做的事情，由此向前，一直推断出一年前的现在应该做什么。\n\n为了让大家认识自己的目标，我们学校在开学之初，就要求大家写出合格体验谈。合格体验谈，就是假象一下自己通过考试后的感受，要写的真实一点，好像是真的通过了考试一样，要写的尽量具体、写实。可以在日记中写上那一天的身体状况、衣服、空气状况等等，总之写得越真实越好。\n\n这就是从自己通过考试的时间点开始学习的学习法。于是，现在只要把录影带倒回去并加以临摹即可，即使中间出现了困难挫折也无需担心。因为一切都在“按预定轨道前行”，最后的结果就是通过考试，所以无论有什么障碍，都能够跨越。\n\n\n# 根据最终结果来调整自己\n\n调整也就是说是要对准目标后调整聚焦点。\n\n调整的目的是什么呢？或者说如何有节奏感的去努力呢？调整或对频率的目的在于是使得实现目标的过程更为顺畅。\n\n如果发现进展不顺利的时候，一定要停下来思考思考。思考什么呢？进展是什么？是利用调整的方式方法去实现目标的过程，要反思是目标不够明确呢？还是调整的方式不对？用摄影来思考，如果要拍摄的对象不清楚很模糊，这样的话，很难进行聚焦的；还有是否在对对象进行聚焦的时候，方式方法不对呢？\n\n问题一：没有明确和具体的目标，最好这个目标可以度量，可以量化。过于笼统的目标，很难具体把握。\n\n问题二：面对目标，不知道自己所处的位置，不了解自己的状况和弱点。不知道自己要如何努力，或者努力到何种程度才能缩短自己与目标之间的差距。自己与目标的距离，决定了自己的弱点（我的理解为方式方法或自身的基础）在哪里，而那也是自己应该加强的部分，所以，不明白这一点，就不知道自己应该学习什么。\n\n从目标出发找到自己的位置和弱点，一遍弥补自己的缺点，进而缩短自己和目标之间的距离，这种调整非常重要。\n\n这种调整时学习时重要的关键词。\n\n认清目标，牢记调整，才能不断接近目标，最终实现目标。\n\n\n# 找出失败的原因才能更好地学习\n\n"有人会说，根本不用这么拼命！不用这么拼命也能考上的"。\n\n但是如果不踏踏实实地学习，那么，失败到底是懒还是因为学习方法本身有问题造成的，就很难找到真正的原因了。所以，为了获得和下一次努力紧密相连的结果，就有必要尽全力、彻彻底底地学习。\n\n很多人在这方面的认识都是错误的。因为拼尽全力学习结果却失败了是件很难接受的事情，所以他们就保留力量以求分散风险，但是这样是大错特错的。\n\n保留力量以求分散风险，结果只会什么都得不到。一定要踏踏实实地学习，即使现在的学习很痛苦，也一定要拼尽全力坚持，这才是不浪费每一次经验的最好捷径。\n\n\n# 只有两类问题会出现在考试里\n\n考试中只会出现两类问题，“自己知道的问题”和“自己不知道的问题”。\n\n永远会出现自己未知的问题，出现未知的问题也没有必要去害怕。\n\n> 柏拉图的《苏格拉底的辩白》中，阐述了对“死是不是幸福”的思考。苏格拉底说，死也是一种幸福。这是因为，人死了之后自己的存在和意识都会消失不见，就和熟睡的夜晚一样令人心情舒畅。\n\n只要假设这两类问题都存在，并相应地采取一定的措施就可以了。我们一直会执着于努力增加自己知道的问题的数量，我们更应该去考虑是对不知道的问题的处理方式。\n\n不断出现意料以外的事情才是真正的人生。如何解决自己不知道的未知问题，才是更重要的\n\n遇到想都没想过的障碍时，如何才能跨越它？遇到失败或挫折时，如何克服它？如何应对意料之外的未知问题，掌握这种方法是通向梦想道路上的重要一环。\n\n因此，不要单看事务的表面，更重要的是养成从事务深层考虑问题的习惯。\n\n为了不失败，要做好所有的准备，但另一方面，也要考虑好万一失败后应该采取的对策；分析、整理意料之外的危机，考虑相应的处理手段。\n\n首先，彻底分析假设的情况并进行模拟实验。\n\n对未知问题的处理方法也是一样的，对大量的问题进行分析，将假设的未知问题进行分类并制定解答模板，如果遇到了某类未知的问题，只需要套用相应的模板即可。只要制作了解答模板，剩下的就简单多了。\n\n要制作模板就要搜集各种信息，并在自己的大脑中进行整理，然后再思考解决方法，最后才能制成模板。\n\n\n# 极限之后再走一步\n\n出现未知的问题，直面意料之外的困难时，人们都会采用各种方法加以应对。其中最好的方法就是之前说的，在日常生活中就考虑好应对方法和处理顺序。\n\n但有的时候，无论怎么努力依然会失败，尝试了所有手段依旧啃不动这块硬骨头时，这会让我们感到束手无策。这个时候要咬紧牙继续坚持！\n\n\n# 学习也需要“有效的徒劳”\n\n学习过程中也需要“有效的徒劳”，“有效的徒劳”是指：和考试没有直接关系，但了解其本质非常重要，它是实现真正目标的过程中不可或缺的知识。\n\n在遇到困难时，心情难过时，不妨看看别人的过往，他人的人生，再者分析下我们现在学习和努力的意义时，就可以知道，目前的考试也只是人生道路上的一个跨栏障碍而已，只要朝着大目标不断前行，肯定会到达终点的。\n\n有效的徒劳，会让自己心胸开阔，思路扩展，打开心扉，对于确立真正的目标，有着十分重要的意义。\n\n\n# 大处着眼，微小处着手\n\n人必须更加谦虚，只要谦虚就一定会有所收获，特别是自己尚未成熟的时候，这种谦虚非常重要。\n\n有很多事情，是我们现在还不懂得的，所以我们需要保持谦虚的态度。随着年龄的增长，保持谦虚的态度就更难了。只有保持谦虚，在读书或是听别人说话的时候才能产生新鲜感、惊讶感，才能吸收新的知识。\n\n\n# 休息时间定为三小时一次\n\n学习中需要“有效的徒劳”，如何判断徒劳是否有效，如果不加区分的去做，很有可能本末倒置，那么选择的标准是什么？\n\n简单来说，就是自己的直觉和好奇心，自己的直觉和好奇心不同于理性和大脑的思考，它是来自内心的真实感受，也可以说是具有更接近真正目标的指向性。只有自己内心真正需要的事务才最有可能对真正目标起作用。\n\n作者认为，只要是自己感兴趣的、能激发自己好奇心的或是无论如何都想做的事，就不应该抑制自己的情感。\n\n“有效的徒劳”，也是需要注意的，不能放下眼前必须要做的事。如果荒废了达成假定目标的学习，那么“有效的徒劳”就是在本末倒置了。\n\n人类是容易随波逐流的生物，所以在做某种和假定目标相关的事情的时候，一定要分清楚这是不是自己逃避的借口。\n\n怎么才能分清楚了。\n\n作者给出的答案是限定时间，三个小时为一个阶段，这也是分给“有效的徒劳”的时间。\n\n这样一来就能做到有急有缓，不会随波逐流也不会让有效的徒劳变成懒惰的借口。\n\n\n# 掌握学习的技巧\n\n\n# 为什么铃木一郎一直做手法练习\n\n在学习过程中，不断地重复进行基础练习是很重要的。这是因为，通过基础的积累就会习惯问题的类型。\n\n"学习"这一词汇来自"模仿"。彻彻底底地模仿基础问题并反复练习，就能明白问题的"型"。\n\n掌握了出题类型、问法还有回答问题的"型"，任何应用方面的问题就都能迎刃而解了。\n\n但是有时候我们也会对不断钻研重复简单的问题感到难以忍受，因为人总是在追求新的知识。而如果不能忍受单纯的反复模仿的学习，就会落后。\n\n一定要通过单纯的重复把"型"记到骨子里面，可事实上有人对"型"只是看似记住了一样的略过，并没有真正的掌握它。\n\n所以，学习也要不厌其烦地反复练习基础性知识，要将这种知识融化到骨子里面。融化到骨子里的瞬间就能达到目的地的高速列车般的线路越多，就越能快乐地享受学习的过程。\n\n\n# 自己给自己讲课的妙用\n\n# 我的记忆法就是"运用五感"\n\n动员"看"、"听"、"触"等所有身体感觉来记忆知识的方法。\n\n大家上课的时候的注意事项，首先要讲的就是，上课的时候光用耳朵听是不够的。一定要看教材，用手记笔记，还要自己说出来，充分刺激自己的视觉和触觉。\n\n所以，就需要使用眼、手、口，动员所有感觉来记忆。\n\n# "自我授课法"\n\n另外，推荐的是自己给自己讲课，即"自我授课"法。\n\n要想给别人上课，自己就必须理解这一事项。如果自己都弄不明白，就不能条理清晰地讲给别人听。\n\n在这一过程中，最重要的就是要"发出声音"。读出的声音会激活大脑，自己说的话传到自己的耳朵里，再一次的询问、理解就化为了对记忆的强调。\n\n用眼睛看，发出声音，听到的自己的声音就是对记忆的再次确认。经过几次重复，记忆会变得越来越稳固，也只有这种不断的强调才能巩固记忆。\n\n有个同学，她每天上完课回家时都会提前一站下车，然后在这一站的距离上边走边自言自语地自我授课。她每天坚持这样做。\n\n\n# 能明确表达喜怒哀乐的人往往记忆力很好\n\n记忆分为知识记忆和经验记忆。\n\n死记硬背、记忆单词和数字都属于知识记忆。而经验记忆是过去的经验和感情相结合的记忆。记得年轻的时候曾做过这样的事情，记着自己曾做过的某件事，这些都属于经验记忆。知识记忆会随着年龄的增长慢慢减弱，而经验记忆则与之完全想法。\n\n所以，如果学习能和经验联系到一起，就不会那么容易被忘记了。特别是随着感情的深入记忆也会更深刻。与开心、快乐、悲伤的感情经验固定在一起的记忆，无论何时都不会消失。\n\n这和大脑的扁桃体有关。脑内负责记忆的海马和负责感情的扁桃体关系密切，因此可以通过扁桃体将感情和记忆结合起来，所以，喜怒哀乐感情丰富的人往往记忆很好。\n\n即使上了年纪，也不该失去自己灵敏的感性，如果一个人心态年轻，那么通过考试也更容易。\n\n"嗯，是吗？"嗯，太好了！" "太厉害了！"能表现出感动或惊讶的人更容易通过考试。\n\n这也就是说，如果想提高记忆力，就要放开自己喜怒哀乐的感情。不要事事冷静，也不要装出什么都懂的样子，多少要表现的夸张点，要毫不害羞地表现出自己的感情，只要做到这点，记忆力应该就会有很大的提高。\n\n\n# 反复练习直到大脑产生错觉\n\n除了充分活用自己的"五感"并与喜怒哀乐的感情相结合之外，还能强化记忆的就只有反复了。要反复到令自己讨厌的程度。\n\n在反复的过程中，超过了一定的程度大脑就会产生错觉："这就是生存必需的记忆"。生存必需的记忆因反复地刺激大脑而被输入大脑之外。因为这些记忆是以接近本能和潜在意识的程度刻在大脑之中的，所以绝对不会再忘掉。\n\n反复练习是痛苦的，很容易让人感到厌倦的；学习中最痛可的就是这种反复练习。\n\n人的大脑明明应该记10个，可第二天却只记住了2个，其实，即便如此也没什么问题。这是因为我们已经记住了2个，那么只要把剩下的再重复4次就能全部记住了，仅此而已。\n\n而我们之所以感到痛苦，是因为我们在意的不是记住了2个，而是忘掉了8个，而事实上这种痛苦完全没有必要的，很多人也正是因为这个才越来越退步。\n\n不过，我们可以超越痛苦的办法。是什么呢？\n\n那就是把自己做过的事情可视化。\n\n记住一个，就贴上一个标签，或者在教材上画圆圈，随着记住内容的增加不断添加圆圈的数量即可。这种办法非常简单，但却是一种很棒的鼓励，据此我们就能越过因不断反复而带来的痛苦。\n\n伊藤学习开创了网上答题系统，答题结果分数化。有人从这种方法中体验到了意料之外的快乐，以游戏的感觉记忆知识，既心情愉快又有成就感。\n\n学习本来就应该是快乐的，因为既可以记忆新知识又可以探知未知世界能刺激大家的好奇心，令人涌现出无限热情。\n\n反复记忆的学习也应该是有趣的，但有时也会让人感到痛苦，不过这种痛苦并不是学习的全部。\n\n所以，如何才能把反复学习变得快乐呢？游戏的手法，就是方法之一。\n\n\n# 每一张过去的试卷都有意义\n\n# 选择参考书\n\n参考书的选择方法是非常重要的，假如因为选择了一本比较难的教材而学不会，最后只会增加挫折感。\n\n选择方法的重点就是，先要选择一本能大体知道难易程度的教材。一开始，尽量选择薄的、简单的，而且只买一本，先把握考试科目的整体情况抓住大概。\n\n然后买第二本，这次要选择自己觉得有趣的教材，这样就能和上一本对比着读。因为已经通过第一本了解了前提下的知识，所以就能知道第二本是否详细，是否有漏掉的内容。一边对比，一边整理。\n\n选书的时候，"现在自己想做什么？" "自己是为了什么而学习的?", "自己的目的是什么"，也就是"了解自己"。从自己的目的、想法、弱点、需求等多个角度分析自己，考虑自己需要的是什么，就是要了解自己。\n\n# 问题集\n\n通过分析过去的问题，能了解出题的领域和倾向以及要求。通过做过去的问题，能看到自己不擅长的领域和弱点。\n\n此外，还能预测下次出题的内容。如果能轻而易举地解答过去的问题，并且自己也能出题，通过考试将不再存在任何问题。\n\n过去的问题是能更好地了解自己目标的手段。\n\n\n# 做标记的方法会决定你的成绩\n\n我会建议学生先把教材全部复印下来。教材用过之后以后就不能再用了，所以要在复印件上做笔记。\n\n然后在复习的时候，再从笔记中摘录重要的内容写到教材原件中加以整理，总之就是要制作迷你笔记本。\n\n这里需要注意两点：\n\n第一，疑问点一定要做笔记。做笔记的时候，记下老师说的话是基础，但是最重要的是要一边做笔记一边写下自己有疑问的地方。\n\n第二个注意点是关于记笔记的重点。虽然说要记下老师说的话，但特别要记下的是重要的具体事例。事实上这些具体事例才是重点，只有具体事例才是理解抽象事项的线索。\n\n书中写的大多数是抽象的话、原理和原则。按照老师讲的顺序，讲到的具体事例的地方才是讲义的真正价值所在。\n\n一般人都会记下抽象的原理原则，这就大错特错了；这些内容即使不做笔记书上也会有的。老师的讲课的价值就在于老师讲到的具体事例，所以一定要将好不容易听到的具体事例记到笔记中，这样才不会忘记。\n\n\n# 让大脑快速运转的方法\n\n所谓大脑快速运转是指激活大脑。大脑运转快，理解也应该更快。\n\n可以尝试加速播放讲义的录影带，加速播放新闻和电视剧的录像带等方法。如果以两倍的速度停录影带，大脑就必须以两倍的速度运转，按道理来说，大脑的运转也会更快。在伊藤学校，我们推荐快速听讲义录影带复习的方法。\n\n即使现在，我也会在学习中犯困的时候站起来，或者一边走一遍快速朗读。\n\n说到考试学习，很久以前流行过包头巾的学习姿势。\n\n这种感觉和自我暗示会收到意想不到的效果。学习中也会有宽心丸效果。\n\n最近，我们都会随身带着添加了氨基酸和柠檬酸的矿泉水。\n\n像这样，"这样我的大脑转得更快"，"这么做我的头脑更清晰"，拥有属于自己的工具和仪式非常重要。\n\n用来激励自己的工具越多越好，工具越多，对自己的学习越有利。\n\n\n# 调整精神状态的"腹式呼吸法"\n\n腹式呼吸类似于自我暗示。腹式呼吸可以保持心情平静，放松情绪，建议大家在考场中使用这个方法。\n\n腹式呼吸能让呼吸更加缓慢，于是，大脑的开关就会从交感神经切换到副交感神经，情绪也会平稳下来。\n\n自律神经分为交感神经和副交感神经。焦躁不安、紧张时交感神经占上风，心情平稳、放松的状态下副交感神经占上风。\n\n腹式呼吸能完成二者的交换，让整个身体进入放松的状态。大家试试看：深呼吸，呼气，感觉一下横膈膜，吸气到肚子膨胀起来，吸到最大程度之后，再缓慢地呼气。\n\n精神状态是呼吸的外在表现，呼吸是基础。心情烦躁的时候先做深呼吸，如果能有意识地做腹式呼吸，心情就能恢复平静。\n\n\n# 预习、复习其实只需10分钟\n\n不预习、不复习，只考虑结果等自顾自的做法是不可取的。预习和复习可以使学习达到以往两三倍的效果。\n\n预习的目的是什么？是为了更好地理解讲义吗？\n\n更重要的目的，在于找到自己的疑问点。通过预习，能够找到以自己之前的知识和经验就能明白的内容，还能找到自己没有经历的、不能推测和理解的未知事项。\n\n哪怕只有5分钟也一定要预习。只要在上课开始前的5分钟对各条目做出标记既可，哪怕只是这样一个小动作，也会使自己对讲义的理解方式变得大为不同，如此一来，就能理解自己难以理解的内容。\n\n那么，复习是为了什么呢？\n\n是要整理学过的内容，要把知识转换成属于自己的东西。特别是上课结束后的五分钟，简直就是黄金时间。在这个时间里，课堂上的一切还都是记忆犹新，所以更容易巩固大脑中的知识，而且效果会非常好。\n\n很多人在上完课后，坐在原地。有的整理笔记，有的重读教材，有的把重点写到卡片上以备回家的电车上背诵用，尽管用的只是上课结束之后短短的5分钟或十分钟，但是每天积累下来，最后就会形成巨大的差异。\n\n\n# 实现梦想的时间管理方法\n\n增加学习时间固然重要，但是还有更重要的，那就是下功夫学会"时间管理方法"。能合理管理时间的人学习效率就高。\n\n因为时间是有限的，所以，如果有想做的事情，就必须放弃另外一些事情。如果还和之前一样地玩，用大量的时间去做自己感兴趣的事情，或者长时间睡觉，那就不可能学习。\n\n作为合理的时间管理方法，最重要的就是要决定优先顺序。要想好在一天的二十四小时中应该做什么以决定优先顺序。我一般用消除法来砍掉自己不必做的事情。\n\n从自己需要做的实现开始考虑，砍掉离目标最遥远的事情；在通过考试之前，减少和朋友会面，减少做自己感兴趣的时间，将多余的事情全部砍掉。确定优先顺序是合理管理时间的第一步。\n\n建议大家在合理管理时间时进行时间分配。备战考试时需要做的事情太多，所以容易焦躁不安。而越是焦躁不安，越做无用功，效率就越低。\n\n"这个也必须做，那个也必须做，时间就来不及"，容易出现恐慌。这是，如果能看一下时间分配就会感到安心；这个明天做，那个后天做也可以，等等，这些已经提前做出了安排，所以只要集中精力做现在该做的事情即可。时间分配能起到一种精神安定剂的作用。\n\n每天人能真正集中精力的时间大概只有6个小时。超过这个时间就会磨磨蹭蹭，也是对时间的浪费。\n\n因此，经常为自己准备一些成功报酬：学习结束之后，给自己些小福利，读喜欢的书、看想看的电视节目等，这样一来，就能专心地学习自己想学的内容了。\n\n为了有效地管理时间，指定截止时间效果会更好，而这就是截止效果。\n\n\n# 结束时再拖延五分钟\n\n学习中非常重要的就是最后的坚持和忍耐，在自认为不行之后还能否再坚持是决定能否通过考试的关键。\n\n为了这最后的一搏，最好能养成再努力五分钟的习惯。\n\n"今天就到这里吧"，从想去休息的那一刻开始，再努力五分钟；从觉得今天不行的那一刻开始，再看一页，再回答一个问题，这些坚持终将化为最后的一搏。\n\n\n# 当日达到最佳状态的方法\n\n为了能在正式考试中发挥出平时积蓄的力量，相比平常的学习，能在正式考试的当天达到最佳状态 ----"达到顶峰"的技术更为重要。\n\n在我们学习，从正式考试一周前就开始进行模拟考试。从一周之前就采取和考试当天的休息时间，在相同的时间起床、相同的时间段答题、相同的时间里吃午饭、上厕所。\n\n已和考试当天完全相同的姿态接受训练，比如午饭吃什么、吃多少，对实际状况也进行最真实的模拟。\n\n提前去看考场也是必不可少的。提前去感受下桌子，椅子的舒适度，毕竟几个小时下来也挺累的，如果凳子不舒服，可以带个坐垫去，提前做好准备。看看厕所、开水房的位置，也很重要。\n\n另外，为了配合考试当天的考试形式，我还用铅笔做了答题卡的涂抹练习。持续练习了大约一个月，使用时间就缩短了三分钟。三分钟，足够回答一道题了。\n\n像这样，朝着自己的目标调整自己的身体能力和精神能力，是考试当天达到最佳状态的有效方法。\n\n\n# 模拟考试时对自己严苛一点\n\n# 考试当天情绪调整\n\n进入考场之后，不要想自己不会的内容而是回忆自己擅长的内容。\n\n考试当天，带着自己做过的或已经记住的单词卡片还有能顺利完成的习题集，"做到这里了"，"这部分是这样的"，这样就能让自己保持平和心态。提高"我明白了"的情绪，保持好心情非常重要。\n\n# 模拟恶劣考试环境\n\n考场上也会发生意料之外的事情。比如，发错试卷，发生火灾，等等。但即使在这种不利的条件下，决胜因素也与安静会场中的没有什么不同。考试，无论在什么环境下都要从容应对。\n\n所以在模拟考试中，尽量选择条件恶劣的座位，或者坐在奇怪的人旁边。考试当天，不知道自己会坐在什么座位上，即使能提前看到考场，也无法看到自己所坐的位置以及旁边的人。\n\n正因为如此，才要在模拟考试的时候特意坐到出入口旁边等位置的座位上，因为出入的人比较多总会影响自己的心情，再不然就坐到看上去喜欢乱晃的人旁边。\n\n即使发烧的时候也要去参加模拟考试，这样就能明白自己在发烧的状态下能正确回答几道题了。\n\n总之，如果能提前经历恶劣条件，在正式考试的时候，无论什么样的环境都不会让你感到惊慌。\n\n\n# "能力*勇气*学习方法"的乘法\n\n能够通过考试取决于"能力*勇气*学习方法"的平衡。\n\n如果能力一般，勇气和学习方法相乘的结果也足以挽回一定的局面。相反，无论能力高出别人多少倍，如果勇气是零，或者负数，那么相乘的结果也会是零或者负数，所以最后的结果也会是零或者负数。\n\n必须注意，能力、勇气、学习方法，哪一个都不能是零或者负数。\n\n关于学习方法，如果大家能从本书中学到从目标出发、调整、对未知的问题的处理等各种方法就太好了。\n\n能力因人而异，只要大于一就足够了。\n\n勇气是关键。如果自己不是从内心渴望着某件事情，就无法保持勇气，就会很容易陷入负数状态。\n\n充满勇气是因为前面有令自己心动的梦想；但是不服气，是因为开始学习之后就不能半途而废才产生的，所以并没有梦想的支撑。\n\n如果不是发自内心的渴望，就一定不能梦想成真。\n\n\n# 教你怎样面对挫折\n\n\n# 如果你现在陷入低谷，那么恭喜你了\n\n# 心态不畏惧\n\n陷入低谷，每个人都会遇到。\n\n之所以会陷入低谷，正是因为不满于现状。也就是说，打破现状、实现下一个目标之前的过程中就会陷入低谷。\n\n所以，低谷是值得骄傲的事情。\n\n# 克服困难，改变现状\n\n陷入低谷的时候，不要害怕，重要的是一定要认为这是合格迈进了一步，一定能克服。在此基础上，为了阻止低谷进一步恶化，想办法恢复即可。\n\n首先会在纸上写出低谷的原因。不知道原因，肯定也不知道怎么办。\n\n比如"为什么会陷入低谷？" "因为分数比预想的低?" "身体感到疲惫?" "是谁说了让自己情绪低落的话? " ....\n\n自己为什么会感到处于低谷状态的原因，把能想到的全部写在纸上，然后再一个一个地检查这些是不是真正的原因。\n\n如果是偶尔的分数不高，也不应该对自己失去信心；如果是别有用心的人故意陷害自己，就会发现"什么啊，这些根本不足以让我心情低落"，情绪也就自然恢复了。\n\n另外，如果写出来的内容真的非常严重，那就再一一地想对策即可。如果是客观上的分数下降，就要考虑改变该部分的学习方法，或者跟老师谈谈。\n\n如果是人际关系上有麻烦，要毫不犹豫地给对方打电话、写信，坦率地说出自己的想法。总之，能解决的问题就要及时解决。\n\n当然，有的问题也不能马上得到解决。但是能做的事情，一定要去做，这一点非常重要。想尽办法却无法解决的问题，就算花费太多时间去想，也没有意义。\n\n这样的事情，也要写在纸上。"即使现在想也没有结果，一周后再想吧"，"学完这个再想把"，经过这样的调整，恐慌情绪就会减弱了。\n\n总之，以恢复状态为目的采取具体措施，是走出低谷的第一步。什么都不做、放任这种状态持续下去，并不是最好的办法。\n\n\n# 把焦虑不安的原因全部写在纸上\n\n# 重视焦虑不安的情绪\n\n笼罩在不安和恐惧中，坐立不安地向现实低头，很难达到自己期望的效果。\n\n因不安而压抑自己的情绪，面对现实，就能创造出自己希望的未来吗？\n\n恐惧和不安会令人失去判断力、情绪低落，并且会降低挑战当前课题和现实的动力。\n\n# 如何克服焦虑不安的情绪\n\n写下来：\n\n不仅是陷入低谷，只要我感到不安，我就会在纸上写出来。\n\n自己想到的事情不要停留在大脑中，而要将其可视化，化为眼睛能看到的形状，这样一来会更容易解决。\n\n**为什么会感觉不安呢？**因为我讨厌明年的再次落榜。为什么讨厌呢？\n\n我试着把所有的原因都写在纸上。对不起父母，样子很落魄，不喜欢再做一年巡警，还要住在这个像蒸笼一样的小屋里...只要我能想到的，就全部写出来。\n\n为什么讨厌这些呢？因为这样只会让我更受伤。\n\n对不起父母，不想看到父母悲伤的脸。\n\n父母知道后会觉得很丢人....反复多想两次、三次之后，意外地发现这也没什么。这根本不足以成为讨厌的理由。\n\n什么啊，我居然在为这种无聊的事情伤脑筋，于是心情大好。百谈莫如一试，一旦觉得自己被情绪欺骗了，大家最好能把所有的不安都在纸上写出来。除这种"写出不安法"之外，我还会通过语言的力量来提高自己的士气。\n\n语言的力量：\n\n怎么做呢？我不断地和自己说，"只要做就能成功"。\n\n无论什么话，说上100万遍，就会成为事实。\n\n把这句话深深地刻在脑子里，大脑就会产生这样的感觉，自己机会深深地相信 ---"只要做就能成功"\n\n人类大脑内的海马体会筛选出重要的信息，再传导到负责记忆的大脑皮层。反复地说"只要做就能成功"，就能巩固这一记忆，形成"只要做就能成功"的思考线路。\n\n在反复说的过程中，线路会越来越粗，消极的思考线路也会变成积极地思考线路。这就是洗脑。\n\n为考试失败而痛苦的时候，我反复地对自己说积极地话，努力把自己从恐惧和不安中解放出来。\n\n无论是谁，都会有恐惧和不安的时刻。但是在重要的考试、课题、商谈之前，需要摆正自己的态度。\n\n\n# "精神笔记"和"梦想笔记"\n\n# "精神笔记"\n\n"精神笔记"就是看到某物就会精神百倍时，用来记录某物的笔记本。\n\n写出激励自己的话和自己的优点。比如朋友和老师的说的能感动自己的话、读到的书中激励自己的语句等。\n\n或者是能激发自己活力的音乐、增加勇气的dvd、喜欢的人的照片等，把能激发自己活力的小工具列成清单。\n\n另外，自己去过的令自己感动的地方的门票或落叶，都可以作为唤醒感动记忆的钥匙。看到这些，会一瞬间又穿梭回当时的场景。\n\n于是就会回想自己充满活力的时候，看到、听到这些场景，自己也会恢复活力。\n\n看到自己的优点，想着自己还有这么多好的地方，自然就找回了自行。也能够恢复前进的动力。\n\n如果能提前准备好这些让自己精力充沛的小工具和对策，在情绪低落的时候一定能成为恢复精力最好法宝，这本"精神笔记"也是处于低谷时的危机管理手册。\n\n# "梦想笔记"\n\n"梦想笔记"中，要写出通过考试之后和将来想要做的所有事情。\n\n比如想住大房子、想开法拉利...写出自己的梦想，随身带着，心情低落的时候就拿出来看看，自然提高自己的士气。\n\n写出自己的梦想，朝着梦想努力奋斗，就开始产生无穷无尽的正能量。\n\n写出来就能从客观的角度观察自己，整理大脑中的想法，更冷静地把握、分析当前的状况。\n\n\n# 找出你最迫切要实现的梦想\n\n多个梦想都只是自己假定的目标而已，一定不要忘记自己真正的目标。\n\n考试，只不过是实现真正目标过程中的假定目标而已。现在自己苦苦奋斗、不断努力学习，并不是为了通过考试，而是为了实现真正的目标。\n\n但是有时候真正的目标太过于遥远，实现的过程中有时候还会迷失方向。迫于每日学习的压力，会误认为在眼前的考试中取得好成绩就是人生所有的目标了。\n\n然后考试中出现的一喜一忧，都成为自己走向低谷的原因。\n\n"现在自己为什么要学习?" "为什么而生?" "自己内心渴望的生活方式、愿望?"经常确认梦想，这些都是防止自己陷入低谷的技巧。\n\n自己真正想做的是什么?\n\n为了确定自己真正的目标，我有时会问自己这样的问题："如果自己是全能的，会做什么呢?"\n\n如果现在，用魔法"能够实现唯一一个愿望"，自己想实现什么愿望呢?\n\n像这样冥思苦想地找出自己最想实现的愿望，非常重要。换言而之就是，在学习的间隙，偶尔也要想一想自己最根本的价值观是什么？\n\n确定自己的真正目的和根本的价值观，然后如果想离真正的目标更近一步，就算陷入低谷之中，也能立刻纠正自己的心情。\n\n\n# 在人生的岔路口做出明智选择\n\n人生，经常要面对各种不得已的选择，人生的道路上布满了岔路口。\n\n自己是放弃司法考试去就业更好呢，还是就这样继续学习更好呢？\n\n我自己迷茫的时候，通常会用非常单纯的判断标准来做决定。\n\n当为不知如何选择而烦恼的时候，人往往都会想到消极的一面：如果继续在这种状态下学习如果没通过考试怎么办？进入公司工作不能出人头地怎么办？公司倒闭了怎么办？能想到的全是消极的可能性，总是想朝着风险更小的方向去想。\n\n但是无论怎么评估风险，结果也不会因此而发生改变，不是吗？这是因为没有人知道未来怎么样。比如走了风险较小的路进入公司上班，但公司也可能会重组、裁员。无论选择哪条路，风险都是不可避免的。\n\n而且，没有足够经验和知识的自己，在预估将来的风险时，准确度又能有多高呢？\n\n如果这样，消极的考虑和积极的考虑又有什么不同呢？\n\n也就是说，如果一切都会按自己的思路进行，选择更能激发自己活力的思考方式显然更好。\n\n当我迷失方向的时候，经常以这种判断标准来选择。哪一种更能激发自己的活力、让自己的人生更快乐，就选择哪一种人生。\n\n不要想着避开未来路上的风险和障碍，我们一定会碰到某种阻碍。所以不要想着如何逃避，遇到阻碍的时候应该考虑当下选择更能激发活力的事情；迷茫的时候，应该主动判断哪一个更能提高自己士气。\n\n\n# 让悲观情绪释放正能量\n\n想考100分却只考了80分，准备用一天来完成的工作却花了两天，遇到这种情况，情绪会低落。害怕失败、无法面对失败，这是虚弱优等生的典型特征。\n\n因为我自己非常讨厌这种性格，所以经常想，怎么做才能不害怕失败、成为一个积极考虑问题的人。\n\n为了超越悲观主义，我学了以下两种方法：\n\n第一，不能将部分弱点看成全部。\n\n即使学不好现在的领域，也不代表所有科目都学不好，这一点和"被她甩了也不能说明自己本身没有存在价值"是一样的道理。必须停止将部分弱点和失败扩大化或者当成整体的思考方式。\n\n第二，一时的失败并不代表永远的失败。\n\n第一次失败了，下一次却有可能成功。即使被她甩了，也不要认为自己不能再恋爱了、不会再有人喜欢自己了。\n\n我努力让自己做到这两点，即使遇到失败感到悲观，我也会对自己说："这只是一时的失败，没事的。这不过是工作上的失败而已，我还有家人和朋友，所以不用悲观失落"。\n\n为了实现真正的目标而奋斗的过程中，即使出现进展不顺利的情况也是必然的，倒更应该去想，早点了解主题和自己不擅长的部分也是好事。\n\n即使正式考试失败，也已经回不去了。失败来得越早越好。超越这种失败，自己就能更好地成长，希望大家能经常对自己说这些鼓励的话语。\n\n\n# 创造多面性人生为自己减压\n\n# 广阔的视野\n\n和乐观的思考方式相同，不片面地看问题非常重要。\n\n即使司法考试和大学考试失败了，世界末日也不会到来。通过考试失败，自己能够成长，也能扩展自己的广度和深度。如果推迟一年通过考试，也只要以后多活一年即可。即使接下来的一年原地踏步，这一年和今后的人生相比也只是短暂的一瞬，没什么大不了的。如果不能从此种广阔的视野来看待事务，就会死死地盯着这一年，而无法前进一步。\n\n# 创造多个世界\n\n自己固执己见地认为只有这个才行，是万万不可取的。我们不应该用狭窄的视野来减少自己的选项。\n\n日常的生活中，就应该创造多个世界。朋友关系、家人关系、感兴趣的事务、工作等等，就算有一个世界遭遇失败，其他的世界也可以掩盖掉某一世界的失败。作为家庭一份子的自己、活在兴趣世界的自己、朋友关系中的自己等，多面性的自己应对危机的能力也会比较强。\n\n仅坚守一项工作，心里难免会不安和脆弱。即使没有了这个也还有其他的，如果能有多面的世界、多种思考方式，就不会因为一点儿小事儿就泄气了。\n\n退路，听起来像是消极的，但是为了分散风险、为了能全神贯注的做一件事情，积极地挑战多项工作是很好的方法。\n\n\n# 善于炫耀自己的"成功报酬"\n\n在"只有少数人会用的时间术"中也曾讲过，为了学习中的松弛有度，为了更有动力，最好能为自己准备些成功报酬，即褒奖。如果通过了考试，想做...把这种小愿望写到"梦想笔记"中，即使是每天的学习中，也要善用小小的成功报酬。\n\n因此，拥有学习之外其他兴趣的人更容易通过考试，因为为了获得更多的奖励也会努力学习的。\n\n虽然都是些小小的报酬，但还是要努力让其成为精神食粮。有时小小的报酬也会起到大作用，希望大家能善用成功报酬，控制自己的动力。\n\n\n# 打造自己专属的"变身魔咒"\n\n# 咒语式的物品\n\n身体虚弱的话，心灵也会虚弱。在挑战重要考试和工作的时候，也要注意身体的健康管理。\n\n对自己说喝了力保美达就会精神倍增，不断地自我暗示，所以喝了之后就真的感觉自己精神百倍。\n\n像喝过饮料之后就会精神百倍一样，找到一种属于自己的，能令学习进展顺利的咒语式的物品至关重要。可以是健康茶或咖啡啥的。\n\n# 心情治疗感冒法\n\n感冒是由睡眠不足和疲劳过度引起的，所以首先要保证营养和睡眠的平衡。然后，重要的是心情。以我的经验，保持心情畅快也能很快的预防感冒。\n\n我住在公寓的九楼上，但睡觉的时候我总是想象着自己的后背是接近大地的。\n\n也就是说，想象着后背穿过被子下面的地板、穿过八楼的地板、穿过七楼、六楼，一直下降到地面，并且暗示自己的身体深入地下、不断地吸收地球的能量。\n\n相信自己具有自我治愈的能力，和地球融为一体，一边吸收地球的能量一边治愈自己的身体。就这样用意念治疗，身体就会不可思议的好转。\n\n\n# 注重调节"睡眠、饮食、压力"的平衡\n\n充足的睡眠和饮食及平衡压力是避免陷入低谷的重要方法。\n\n睡眠、饮食、压力三个之中有两个处于恶劣状态下的时候，身体就会陷入非常危险的状态。\n\n所以，当我感觉到睡眠不足的时候，就会在饮食上补充营养、并减少压力。压力过大的时候，就会选择更好的饮食、并保证睡眠充足。\n\n# 提高睡眠质量\n\n想增加学习时间的时候，最先失衡的就是睡眠。所以，我会在白天分阶段浅睡一会儿，以此来消除长期的睡眠不足。\n\n大脑皮层中有老皮层和新皮层。老皮层控制喜怒哀乐等感情。新皮层用来吸收知识、记忆知识。\n\n即使没有深度睡眠，老皮层也不会感到疲劳。而学习的嘶吼，浅睡就能让疲劳的新皮层恢复活力。\n\n也就是说，学习时感到疲劳的话，只要短暂地小睡片刻就能消除疲劳。所以，为了让老皮层得到休息，我会有三个小时的深度睡眠；然后为了让新皮层得到休息，白天会用五分钟或者十分钟的时间来浅睡一下。\n\n学习两个小时后，如果感到有些疲劳，我就会趴在桌子上睡五分钟。这样一来，头脑一下子就清醒了。\n\n我在备战司法考试的过程中，常常利用一些空闲时间浅睡一会儿，在电车上抓着吊环睡觉也是一件容易的事情。在备战的过程中，我还掌握了一边走路一边睡觉的方法。\n\n像这样花时间保持睡眠、饮食、压力的平衡，学习也能顺利进行。学习生活虽然残酷，但却保持着自己的身心健康，不断朝着目标迈进。\n\n虽是全速奔跑，但偶尔也要休息一下，这种平衡非常重要。\n\n\n# 身处低谷，心在高处\n\n学习中，陷入低谷时，我有时会爬上东京塔这样的高处。从高处向下望去，东京街头尽收眼底，人变得比蚂蚁还小。\n\n原来自己的烦恼竞是小到可以忽略一般，心情也会瞬间平静下来。\n\n所谓低谷，其实是没有注意到自己周围的广阔的世界。\n\n# "空间轴"的扩展\n\n无论往哪个方向走都是自己的自由，现在却蜷缩在陶罐一样的地方筑起了堵塞四面八方的墙壁。这时，如果能打开物理空间，就会出现客观凝视自我位置的契机。\n\n登上高处，俯瞰世界，眼前的视野一下子就开阔起来。蜷缩一处的自己，会得到释放，心情也会开朗起来。\n\n如果想看到更高远的高处，建议大家看世界地图和地球仪。旋转地球仪的同时，遥想一下自己还没有去过的国家。就会发现，居于小小的日本，为一点小事就闷闷不乐的自己是多么的荒唐可笑。\n\n遥望处于宇宙之中的地球的照片也会收到很好的效果。漆黑的宇宙中，蓝色的地球静静地漂浮其中。我的房间里有一本地球照片的写真集，这本写真集对我来说就是宝贝。\n\n只要凝视奇迹般存在的美丽地球，就会想到人类的烦恼真不值一提，这种想法会产生不可思议的力量，将自己从苦闷中拯救出来。\n\n想这样，把压缩的"空间轴"从陶罐一样的尺寸渐渐扩展到地球一样大、宇宙一样大，心的尺寸也会渐渐变大。\n\n# "时间轴"的扩展\n\n同样，"时间轴"的延展也一样重要。处于低谷时，没有意识到自己时间轴的前后状况。这种情况下，只会永远处于低谷之中。\n\n如果地球的历史是365天，那么人类的历史也不过是除夕夜11点59分的简短一瞬而已。\n\n在如此漫长的时间中，我究竟在苦苦挣扎什么呢？\n\n像这样吗，将扩展的空间和时间同自己的生存相比较，就会发现自己面临的问题是多么微不足道。即便对自己来说是天大的问题，也是可以大而化小的。\n\n\n# 糟糕的事会成为人生最重要的经历\n\n也正是因为我有这样的经历，所以遇到小事情才不会觉得害怕。\n\n糟糕的经历也不是毫无用处的。人生中发生的所有事情都有其存在的意义，都会成为人的精神食粮。\n\n\n# 学习能够磨炼你的人生\n\n\n# 被告人教会我们的"三个问题"\n\n自己的人生目标是什么？\n\n为什么是这个目标？\n\n为了这个目标你现在在做什么呢?\n\n为了寻找这个答案，我们一直都在努力学习。\n\n\n# 助你美梦成真的思考法\n\n\n# 大声说"一定能成功"\n\n我的口头禅是，"只要做就能成功，一定能成功"。\n\n我认为，说出口的话都是正面的思考。有一种说法是"语言的灵魂"，这是因为语言中蕴含着眼睛看不到的能量。\n\n我经常多学生说，不要待在说否定话语的人旁边。不要靠近那些只会傻傻地说"这事，就算做也只能失败"的朋友。\n\n如果听了负面的话，不知不觉中就会积攒负面的理念。\n\n话用耳朵听，耳朵离大脑很近。语言会以声音的形式经耳朵进入大脑，抓化为潜在意识和记忆然后固定在大脑中。所以，相比与否定的语言，肯定的语言更有意义。\n\n只用耳朵听正面的语言，自己也只说正面的语言。\n\n\n# 不要想象十年后的自己\n\n向学生解释的时候，经常以登山做比喻：登山的时候，如果一直看着山顶，登山就会很痛苦。到达山顶还有那么远啊，还有那么难爬的山道在等着我们，瞬间心生厌恶。\n\n这个时候，最好能往后看一下。应该会看到我们身后绵延的山路。原来自己已经爬到这么高的地方了，不止不觉间自己也会越爬越高。\n\n我们并不知道十年后的自己是什么样子，但是今天的自己和十年前的自己相比，的的确确进步了。如果能继续往上走，山顶就会越来越近。\n\n\n# 学习的本质意义\n\n人生，多多少少都会有些不安。\n\n有些人为考试失败，工作压力，亲人离世。。。\n\n但是，最根本的不安是什么，难道不是死亡吗?\n\n自己从哪里来？要到哪里去？又为什么会在这里？自己究竟是什么人，又为了什么而生？\n\n通过学习，才能了解这些未知的事物，才能摆脱不安的束缚而获得自由。\n\n摆脱不安，心灵才能归于平静。\n\n没有不安，人才能安静、幸福地生活下去，也就是说，学习正是为了幸福地活着。\n\n并且通过学习来追求自己的幸福，人就应该更想获得幸福。\n\n学习，正是为了明白真理。掌握真理的时候，就是人类变得自由、幸福的时候。\n\n\n# "自我满足"是助人成长的动力\n\n大家在生活中都在追求成就感、最大的幸福感、满足感。\n\n学习，就是为了获得满足感而采取的手段，学习本身也同满足感和最大幸福感相关联。\n\n运动就是一个自我满足的世界。\n\n田径100米赛跑的记录缩短零点几秒会具有什么意义呢？马拉松42公里很艰难，又具有什么意义？举重项目中举起那么重的重量又起到什么作用？\n\n在研究学问的领域里，很多人一生都在研究没有任何作用的内容。\n\n这些都是在追求终极的自我满足，但是人类总是希望挑战极限，更进一步，了解未来。\n\n跑步想用再少零点一秒的欲望，想更进一步了解的欲望，或许都只不过是自我满足，但是通过对能力更进一步、半步的追求，人类才能进化，文明才能发展。\n\n人类，不仅仅只有一个人。所以通过追求自己的自我满足，可能会影响其他人、其他人再利用自己的所得在其他领域发挥重大作用。\n\n通过彻底追求单纯的自我满足，人们互相激励对方、互相学习，以此很累才得以进步。\n\n也就是说，正是因为个人对自我满足的追求，人类才能一直发展下去。\n\n通过学习获得自我满足，一定不要觉得不好意思。我认为，这正是自己个性的证明，推动人类进化发展的能量的一部分。\n\n\n# 学习就是人生的播种\n\n通过学习，我最想找到的就是自己生存的意义，也就是自己的人生使命。\n\n为什么我会活在这个世界上？为什么会在这里？找到这些原因和意义才是最重要的。\n\n我认为，所有的人生在这个时代、这个国家、这个地方，都有其生存的意义。\n\n每个人学习的内容各不相同，但是只要努力学习，就能不断接近自己的生存意义。\n\n如果你还没找到自己生存意义和使命，只要朝着自己发自内心的目标努力，就一定能找到自己的使命。\n\n这是因为，学习就是人生的播种。自己现在的人生和命运，只不过是收获自己过去播下的种子而已。\n\n大家都会受到各种环境和周围人的影响，同时，能够决定自己将来的只有自己播下的种子。\n\n自己的未来自己创造，所以，将来有什么样的梦想，现在就要播下什么样的种子。\n\n努力播下学习的种子，就一定能收获想要的果实，以此回报自己。\n\n只要努力学习，就一定能结出正能量的果实。\n\n最终，我们定将找到自己的使命所在。通过学习，我们一定能找到不同的"自己的使命"和"生存的意义"。',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"学习杂记",frontmatter:{title:"学习杂记",date:"2022-01-20T11:47:12.000Z",permalink:"/pages/e80362/",categories:["生活","杂记"],tags:[null]},regularPath:"/04.%E7%94%9F%E6%B4%BB/80.%E6%9D%82%E8%AE%B0/01.%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0.html",relativePath:"04.生活/80.杂记/01.学习杂记.md",key:"v-b5f9e054",path:"/pages/e80362/",headers:[{level:2,title:"为什么要记笔记呢",slug:"为什么要记笔记呢",normalizedTitle:"为什么要记笔记呢",charIndex:48},{level:2,title:"怎样做读书笔记呢",slug:"怎样做读书笔记呢",normalizedTitle:"怎样做读书笔记呢",charIndex:90},{level:2,title:"如何做知识点笔记",slug:"如何做知识点笔记",normalizedTitle:"如何做知识点笔记",charIndex:489}],headersStr:"为什么要记笔记呢 怎样做读书笔记呢 如何做知识点笔记",content:'学习方法的整理\n\n\n# 如何写笔记\n\n读一本书，特别是专业性知识的书籍，一定要记笔记。\n\n\n# 为什么要记笔记呢\n\n摘抄能减缓我们阅读的速度，让我们对文字有更深度的理解。\n\n\n# 怎样做读书笔记呢\n\n笔记做什么呢？特别是读书笔记，需要记录的重点是什么呢？\n\n做读书笔记的最大的目的，在于我们对整本书有清晰的脉络理解，也就是先要抓住作者写书的灵魂和要点是什么？作者写作的思路是什么？作者花了大段的章节和废话，究竟在描述一个什么样的事情？\n\n按照书籍的目录来，重点的，难以理解的知识点，和在阐述的过程的内容，要重点摘录下来，细细品味。\n\n对于特别重要的内容，要单列出知识点来整理。知识点的整理，我的想法还是以提问/解答的方式来整理，每个知识点当然是为了解决一个小方面的问题。\n\n在保留读书笔记的完整性上来看，也可以留下，例如"如下XXXX的特性的重要知识点，已经单独到另外的知识点笔记中了！"\n\n> 需要注意的是，读书笔记是别人的东西，而"知识点笔记"才是自己的东西，读书的目的在于借鉴别人的东西来消化吸收为自己的东西。所以"知识点笔记"很重要，一定要是自己思考后的结晶。\n\n\n# 如何做知识点笔记\n\n千万不要将别人的书中的内容，不加思考的全盘记录到"知识点笔记"中。\n\n否则记录"知识点笔记"没有了任何的意义了，可以罗列，可以实操一遍，可以用自己的语言来描述一下(转述一遍)，都是非常非常有意义的。\n\n别人的知识，不动，它还躺在那里，是人家的。而只有经过自己理解和思考后的东西，才是在自己的头脑里面，是自己的东西。\n\n\n# 杂记\n\n 1.  事实上，任何有难度的知识和技巧，都不是那么容器被掌握的。我尽管已经朝着通俗易懂的方向努力，可有些数据结构，特别是经典算法，是几代科学家的智慧结晶，因此要掌握它们还是需要读者的全力投入。\n     \n     > 摘自"大话数据结构" 前言 P6\n\n 2.  阅读时，摘抄是非常好的习惯。“最淡的墨水也胜于最强的记忆！”有不少读者会认为摘抄了将来也不会再去看，有什么必要，但是其实在写字的过程就是大脑学习的过程，写字在减缓你阅读的速度，从而让你更好地消化阅读的内容。相信大家都能理解，"囫囵吞枣"和"慢慢品味"的差异，学习同样如此。\n     \n     > 摘自"大话数据结构" 前言 P6\n\n 3.  不要管理时间，去管理事件\n     \n     很多人想着在家办公就想把工作安排细致到几点几分，如10点到11点处理数据，11点到11点半做活动总结等。\n     \n     但是这样做的风险比较大，一旦我们的计划被额外的事情打乱，或者没有在规定时间内完成，就会陷入无限的烦躁循环中。\n     \n     工作还是需要管理的，不过最好不是根据时间，而是根据事件。\n     \n     做个事件list清单，做完一项，划掉一项，成就感满满。\n\n 4.  早上换一身衣服\n     \n     不要穿着睡衣开始工作！我知道大花睡衣方便又舒服的，可能只要穿着它，就有一种很放松的感觉，难以进入工作状态。\n\n 5.  在家办公精力有限，调整好事情优先级\n     \n     在家办公其实会比在办公室办公更加容易疲惫，所以列好优先级非常重要，这点是老生常谈了，按重要和紧急的来划分。\n     \n     同事找我们好办，可以晚点回复，领导找我们怎么办呢？当然是秒回！不过不代表我要秒完成，根据事项，比如说安排一个新的任务给你，你手上正有重要的事不方便被打断且deadline快到了，我们可以这样回复：\n     \n     领导，我现在正在做XX，比较紧急，要求X点完成上交，我大概还需要X小时可以做完，处理完后马上开始办你给我的事，预计X点可以给你做好，你看可以吗？\n     \n     以上重点在于：给出明确的时间节点+事情紧急性+把最终决定权交给领导+语气诚恳。\n\n 6.  尽量保持和上班一致的工作时间\n     \n     一般工作时间是9点-5点，当然，我干过早点10点起床，然后加班到晚上1，2点的事情，看着工作时间是长，但是效率极低，无法言明的困。\n     \n     而且大多数时候，一看时间晚了就对自己说，明天早起再做！结果可想而知。\n     \n     熬夜工作，远比我们想的要低效。\n\n 7.  上班时间不要做家务\n     \n     如果要洗衣服，请在开始工作之前或完成之后进行。把衣服放进洗衣机这种只花几分钟的小任务看起来微不足道，但是几分钟加起来，就会无形中占用很长的时间。\n\n 8.  设定严格的上班界限\n     \n     提前和家人说好，几点到几点是我要工作，饭做好了/狗在叫了/发现我没有洗碗洗衣服想来骂我一顿，都不要这个时间来找我。\n     \n     不做私事。上班时间不刷知乎、不看视频、不撸猫、不要去吃零食，就像在办公室一样，不然你花在这些事上的零碎时间，远远比你预期的多。\n\n 9.  提前在脑中细分任务\n     \n     早上临近起床，或者中午休息的时候，想要一会要去工作，第一感觉就是好烦，不想起不想起，不知不觉就拿起了手机，一边刷一遍翻滚着想工作还没做完，不断自责，压力变大，手上又停不下来，恶性循环的。\n     \n     打破这个循环我们可以从拆分任务入手。比如我们之前对自己将，我要去工作了。我们可以改变思路想想，一会儿我要去做事了，先做什么呢？后做什么呢？然后写下来。这样一拆解，就没有那么多抵触心理了，反而更让人跃跃欲试。\n\n 10. 解决一个问题的时候，记得把与其相关的"周边知识"也要加强认知一下。\n\n 11. 遇到问题解决问题，看似很快，但这是"低水平勤奋"的标志，很容易形成"浅尝辄止"的陋习。\n\n 12. 你认为未来五年、十年、或二十年，最重要的大知识到底是什么呢？\n\n 13. 万物生长不靠太阳、立体农业不靠土壤、掌控生老病死不是梦想。\n\n 14. 世界是为活着长的人准备的。\n\n 15. 创新者相信，抓住技术变革带来的产业和科学的双重机会，就等于抓住了未来。\n\n 16. 工具是驱动变化的最本质的一个东西，最核心的东西。\n\n 17. 有了用火的能力，就有了熟，肉食，才有了人类进化；有稍微高温度的控制，就有了陶瓷，就有了青铜器；到了上千度温度的控制，就有了铁器；到了上万度温度的控制就有了火炮；到了上百万，上亿度温度的控制，就有了核武器。\n\n 18. 偏见比无知离真理更远。\n\n 19. 我碰见的问题都是这个时代的问题，其实我们每一个人都跟着走，跟着时代的脚印在走。\n\n 20. 黑格尔，“只有方法才是世界上唯一的，至高无上的，不可战胜的力量”。所以的方法都有它的局限性和缺陷。\n\n 21. ',normalizedContent:'学习方法的整理\n\n\n# 如何写笔记\n\n读一本书，特别是专业性知识的书籍，一定要记笔记。\n\n\n# 为什么要记笔记呢\n\n摘抄能减缓我们阅读的速度，让我们对文字有更深度的理解。\n\n\n# 怎样做读书笔记呢\n\n笔记做什么呢？特别是读书笔记，需要记录的重点是什么呢？\n\n做读书笔记的最大的目的，在于我们对整本书有清晰的脉络理解，也就是先要抓住作者写书的灵魂和要点是什么？作者写作的思路是什么？作者花了大段的章节和废话，究竟在描述一个什么样的事情？\n\n按照书籍的目录来，重点的，难以理解的知识点，和在阐述的过程的内容，要重点摘录下来，细细品味。\n\n对于特别重要的内容，要单列出知识点来整理。知识点的整理，我的想法还是以提问/解答的方式来整理，每个知识点当然是为了解决一个小方面的问题。\n\n在保留读书笔记的完整性上来看，也可以留下，例如"如下xxxx的特性的重要知识点，已经单独到另外的知识点笔记中了！"\n\n> 需要注意的是，读书笔记是别人的东西，而"知识点笔记"才是自己的东西，读书的目的在于借鉴别人的东西来消化吸收为自己的东西。所以"知识点笔记"很重要，一定要是自己思考后的结晶。\n\n\n# 如何做知识点笔记\n\n千万不要将别人的书中的内容，不加思考的全盘记录到"知识点笔记"中。\n\n否则记录"知识点笔记"没有了任何的意义了，可以罗列，可以实操一遍，可以用自己的语言来描述一下(转述一遍)，都是非常非常有意义的。\n\n别人的知识，不动，它还躺在那里，是人家的。而只有经过自己理解和思考后的东西，才是在自己的头脑里面，是自己的东西。\n\n\n# 杂记\n\n 1.  事实上，任何有难度的知识和技巧，都不是那么容器被掌握的。我尽管已经朝着通俗易懂的方向努力，可有些数据结构，特别是经典算法，是几代科学家的智慧结晶，因此要掌握它们还是需要读者的全力投入。\n     \n     > 摘自"大话数据结构" 前言 p6\n\n 2.  阅读时，摘抄是非常好的习惯。“最淡的墨水也胜于最强的记忆！”有不少读者会认为摘抄了将来也不会再去看，有什么必要，但是其实在写字的过程就是大脑学习的过程，写字在减缓你阅读的速度，从而让你更好地消化阅读的内容。相信大家都能理解，"囫囵吞枣"和"慢慢品味"的差异，学习同样如此。\n     \n     > 摘自"大话数据结构" 前言 p6\n\n 3.  不要管理时间，去管理事件\n     \n     很多人想着在家办公就想把工作安排细致到几点几分，如10点到11点处理数据，11点到11点半做活动总结等。\n     \n     但是这样做的风险比较大，一旦我们的计划被额外的事情打乱，或者没有在规定时间内完成，就会陷入无限的烦躁循环中。\n     \n     工作还是需要管理的，不过最好不是根据时间，而是根据事件。\n     \n     做个事件list清单，做完一项，划掉一项，成就感满满。\n\n 4.  早上换一身衣服\n     \n     不要穿着睡衣开始工作！我知道大花睡衣方便又舒服的，可能只要穿着它，就有一种很放松的感觉，难以进入工作状态。\n\n 5.  在家办公精力有限，调整好事情优先级\n     \n     在家办公其实会比在办公室办公更加容易疲惫，所以列好优先级非常重要，这点是老生常谈了，按重要和紧急的来划分。\n     \n     同事找我们好办，可以晚点回复，领导找我们怎么办呢？当然是秒回！不过不代表我要秒完成，根据事项，比如说安排一个新的任务给你，你手上正有重要的事不方便被打断且deadline快到了，我们可以这样回复：\n     \n     领导，我现在正在做xx，比较紧急，要求x点完成上交，我大概还需要x小时可以做完，处理完后马上开始办你给我的事，预计x点可以给你做好，你看可以吗？\n     \n     以上重点在于：给出明确的时间节点+事情紧急性+把最终决定权交给领导+语气诚恳。\n\n 6.  尽量保持和上班一致的工作时间\n     \n     一般工作时间是9点-5点，当然，我干过早点10点起床，然后加班到晚上1，2点的事情，看着工作时间是长，但是效率极低，无法言明的困。\n     \n     而且大多数时候，一看时间晚了就对自己说，明天早起再做！结果可想而知。\n     \n     熬夜工作，远比我们想的要低效。\n\n 7.  上班时间不要做家务\n     \n     如果要洗衣服，请在开始工作之前或完成之后进行。把衣服放进洗衣机这种只花几分钟的小任务看起来微不足道，但是几分钟加起来，就会无形中占用很长的时间。\n\n 8.  设定严格的上班界限\n     \n     提前和家人说好，几点到几点是我要工作，饭做好了/狗在叫了/发现我没有洗碗洗衣服想来骂我一顿，都不要这个时间来找我。\n     \n     不做私事。上班时间不刷知乎、不看视频、不撸猫、不要去吃零食，就像在办公室一样，不然你花在这些事上的零碎时间，远远比你预期的多。\n\n 9.  提前在脑中细分任务\n     \n     早上临近起床，或者中午休息的时候，想要一会要去工作，第一感觉就是好烦，不想起不想起，不知不觉就拿起了手机，一边刷一遍翻滚着想工作还没做完，不断自责，压力变大，手上又停不下来，恶性循环的。\n     \n     打破这个循环我们可以从拆分任务入手。比如我们之前对自己将，我要去工作了。我们可以改变思路想想，一会儿我要去做事了，先做什么呢？后做什么呢？然后写下来。这样一拆解，就没有那么多抵触心理了，反而更让人跃跃欲试。\n\n 10. 解决一个问题的时候，记得把与其相关的"周边知识"也要加强认知一下。\n\n 11. 遇到问题解决问题，看似很快，但这是"低水平勤奋"的标志，很容易形成"浅尝辄止"的陋习。\n\n 12. 你认为未来五年、十年、或二十年，最重要的大知识到底是什么呢？\n\n 13. 万物生长不靠太阳、立体农业不靠土壤、掌控生老病死不是梦想。\n\n 14. 世界是为活着长的人准备的。\n\n 15. 创新者相信，抓住技术变革带来的产业和科学的双重机会，就等于抓住了未来。\n\n 16. 工具是驱动变化的最本质的一个东西，最核心的东西。\n\n 17. 有了用火的能力，就有了熟，肉食，才有了人类进化；有稍微高温度的控制，就有了陶瓷，就有了青铜器；到了上千度温度的控制，就有了铁器；到了上万度温度的控制就有了火炮；到了上百万，上亿度温度的控制，就有了核武器。\n\n 18. 偏见比无知离真理更远。\n\n 19. 我碰见的问题都是这个时代的问题，其实我们每一个人都跟着走，跟着时代的脚印在走。\n\n 20. 黑格尔，“只有方法才是世界上唯一的，至高无上的，不可战胜的力量”。所以的方法都有它的局限性和缺陷。\n\n 21. ',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"网站",frontmatter:{title:"网站",permalink:"/pages/beb6c0bd8a66cea6",date:"2020-04-19T11:33:04.000Z",article:!1},regularPath:"/06.%E6%94%B6%E8%97%8F%E5%A4%B9/01.%E7%BD%91%E7%AB%99.html",relativePath:"06.收藏夹/01.网站.md",key:"v-a670efd8",path:"/pages/beb6c0bd8a66cea6/",headers:[{level:2,title:"推荐",slug:"推荐",normalizedTitle:"推荐",charIndex:12},{level:2,title:"文档",slug:"文档",normalizedTitle:"文档",charIndex:167},{level:2,title:"社区",slug:"社区",normalizedTitle:"社区",charIndex:375},{level:3,title:"社区互动",slug:"社区互动",normalizedTitle:"社区互动",charIndex:596},{level:2,title:"技巧",slug:"技巧",normalizedTitle:"技巧",charIndex:623},{level:2,title:"博客",slug:"博客",normalizedTitle:"博客",charIndex:711},{level:2,title:"电子书",slug:"电子书",normalizedTitle:"电子书",charIndex:775},{level:2,title:"优秀文章",slug:"优秀文章",normalizedTitle:"优秀文章",charIndex:833},{level:2,title:"视频",slug:"视频",normalizedTitle:"视频",charIndex:897},{level:2,title:"Github",slug:"github",normalizedTitle:"github",charIndex:382},{level:2,title:"评论系统",slug:"评论系统",normalizedTitle:"评论系统",charIndex:1148},{level:2,title:"前端小工具",slug:"前端小工具",normalizedTitle:"前端小工具",charIndex:1197},{level:2,title:"代码编辑",slug:"代码编辑",normalizedTitle:"代码编辑",charIndex:1269},{level:2,title:"Emoji表情",slug:"emoji表情",normalizedTitle:"emoji表情",charIndex:1330},{level:2,title:"图片工具",slug:"图片工具",normalizedTitle:"图片工具",charIndex:1493},{level:2,title:"思维导图",slug:"思维导图",normalizedTitle:"思维导图",charIndex:1753},{level:2,title:"CSS",slug:"css",normalizedTitle:"css",charIndex:875},{level:2,title:"CDN加速",slug:"cdn加速",normalizedTitle:"cdn加速",charIndex:2090},{level:2,title:"网站托管",slug:"网站托管",normalizedTitle:"网站托管",charIndex:2150},{level:2,title:"正则",slug:"正则",normalizedTitle:"正则",charIndex:2180},{level:2,title:"其他",slug:"其他",normalizedTitle:"其他",charIndex:2241},{level:2,title:"设计",slug:"设计",normalizedTitle:"设计",charIndex:574},{level:2,title:"图库",slug:"图库",normalizedTitle:"图库",charIndex:3342},{level:2,title:"交互",slug:"交互",normalizedTitle:"交互",charIndex:2420},{level:2,title:"有趣",slug:"有趣",normalizedTitle:"有趣",charIndex:139},{level:2,title:"生成器",slug:"生成器",normalizedTitle:"生成器",charIndex:1839},{level:2,title:"元宇宙",slug:"元宇宙",normalizedTitle:"元宇宙",charIndex:4310},{level:2,title:"教程",slug:"教程",normalizedTitle:"教程",charIndex:251},{level:2,title:"产品",slug:"产品",normalizedTitle:"产品",charIndex:3329},{level:2,title:"实用",slug:"实用",normalizedTitle:"实用",charIndex:4530},{level:2,title:"Talk",slug:"talk",normalizedTitle:"talk",charIndex:4903},{level:2,title:"算法",slug:"算法",normalizedTitle:"算法",charIndex:2330},{level:2,title:"nginx",slug:"nginx",normalizedTitle:"nginx",charIndex:4983},{level:2,title:"生活",slug:"生活",normalizedTitle:"生活",charIndex:5011}],excerpt:'<h1 id="个人收藏夹"><a class="header-anchor" href="#个人收藏夹">#</a> 个人收藏夹</h1>\n<h2 id="推荐"><a class="header-anchor" href="#推荐">#</a> 推荐</h2>\n<ul>\n<li><a href="https://panjiachen.github.io/awesome-bookmarks/" target="_blank" rel="noopener noreferrer">panjiachen<OutboundLink/></a> by 花裤衩</li>\n<li><a href="https://www.code-nav.cn/" target="_blank" rel="noopener noreferrer">编程导航<OutboundLink/></a> by 程序员鱼皮</li>\n<li><a href="https://r2coding.com/" target="_blank" rel="noopener noreferrer">编程自学之路<OutboundLink/></a> by 程序羊</li>\n<li><a href="https://gitee.com/jishupang/web_atlas" target="_blank" rel="noopener noreferrer">前端知识图谱+B站资源整合<OutboundLink/></a> by 技术胖</li>\n<li><a href="https://shengxinjing.cn/" target="_blank" rel="noopener noreferrer">大圣编程自学网<OutboundLink/></a> by 大圣</li>\n<li><a href="https://devtool.tech/" target="_blank" rel="noopener noreferrer">开发者武器库<OutboundLink/></a></li>\n<li><a href="https://www.fly63.com/tool/home.html" target="_blank" rel="noopener noreferrer">工具大全<OutboundLink/></a></li>\n</ul>\n<p align="center">\n<img src="https://cdn.jsdelivr.net/gh/xugaoyi/image_store@master/blog/qrcode.zdqv9mlfc0g.jpg" width="200">\n</p>\n<div class="center-container"><p>关注公众号[有趣研究社]，回复<code>前端资源</code>，获取 <a href="https://github.com/xugaoyi/blog-gitalk-comment/wiki/Front-end-Study" target="_blank" rel="noopener noreferrer">前端学习资料<OutboundLink/></a></p>\n</div>',headersStr:"推荐 文档 社区 社区互动 技巧 博客 电子书 优秀文章 视频 Github 评论系统 前端小工具 代码编辑 Emoji表情 图片工具 思维导图 CSS CDN加速 网站托管 正则 其他 设计 图库 交互 有趣 生成器 元宇宙 教程 产品 实用 Talk 算法 nginx 生活",content:"# 个人收藏夹\n\n\n# 推荐\n\n * panjiachen by 花裤衩\n * 编程导航 by 程序员鱼皮\n * 编程自学之路 by 程序羊\n * 前端知识图谱+B站资源整合 by 技术胖\n * 大圣编程自学网 by 大圣\n * 开发者武器库\n * 工具大全\n\n\n\n关注公众号[有趣研究社]，回复前端资源，获取 前端学习资料\n\n\n# 文档\n\n * MDN | MDN-JS标准内置对象 Web技术权威文档\n * DevDocs Web 开发技术文档，非常不错的学习手册！\n * 现代JavaScript教程 以最新标准为基准的JS教程\n * ES5教程 阮一峰的JS教程\n * ES6教程 阮一峰的ES6教程\n * Bash 脚本教程 阮一峰编写\n * ECMA ECMA官网\n * 菜鸟教程 涵盖多种语言的初级教程\n * 腾讯云开发者手册\n\n\n# 社区\n\n * Github 程序员同性交友社区\n * 掘金 一个帮助开发者成长的社区\n * 简书 有很多频道的创作社区\n * 思否 解决技术问题的社区\n * stack overflow 同上，外网的\n * InfoQ 促进软件开发及相关领域知识与创新的传播\n * V2EX 创意工作者们的社区\n * 鱼塘热榜 划水网站，收集了很多网站，当天热门文章\n * 码力全开资源库 很全很强大，独立开发者/设计干货/优质利器/工具资源...\n\n\n# 社区互动\n\n * gitter\n * 兔小巢\n\n\n# 技巧\n\n * Google 趋势 查看某项技术或关键字的热度趋势，可用于分析某项技术的发展前景，或对比某两项技术的热度。\n * 百度指数 同上，但百度的数据仅限国内。\n\n\n# 博客\n\n * 阮一峰的网络日志\n * samanthaming 对前端小知识点的总结，并为每个知识点制作精美的小卡片。\n\n\n# 电子书\n\n * 高教书苑 高等教育出版社的书籍，包含多种学科。\n * SoBooks 免费的电子书资源网站\n\n\n# 优秀文章\n\n * 我做系统架构的一些原则 作者对系统架构的方法论总结\n * 灵活运用CSS开发技巧\n * 防御性CSS\n\n\n# 视频\n\n * bilibili B站，上面很多免费教学视频\n * 慕课网 实战视频教程\n * 妙味课堂 比较系统的前端入门视频教程\n * 中国大学MOOC 涵盖计算机、外语、心理学等专业免费课程\n * egghead 质量还不错的短视频教程，外网\n\n\n# Github\n\n * Repobeats 生成仓库的动态数据统计图\n\n * github 短域名服务\n * shields 徽章图标\n * followers 全球排名\n * star-history 展示一个项目 Stars 增长曲线\n\n\n# 评论系统\n\n * giscus 由 GitHub Discussions 驱动的评论系统\n\n\n# 前端小工具\n\n * Can I use 查看属性和方法的兼容性\n * 30 seconds of code 收集了许多有用的代码小片段\n\n\n# 代码编辑\n\n * codepen 在线代码编辑与演示\n * codesandbox 内嵌VSCode的在线IDE\n\n\n# Emoji表情\n\n * emoji表情\n * emoji表情备忘录\n * 根据文本匹配emoji\n * gitmoji 通过 emoji 表达 git 的操作内容\n\n> 在任意输入框快速打开emoji表情方法：\n> Windows系统下按Win + .\n> Mac系统下按Control + Command + 空格\n\n\n# 图片工具\n\n * tinypng图片压缩 压缩png很有用\n * 微图 浏览器端图片压缩，不会上传图片到服务器\n * Squoosh 谷歌出品在线免费图片压缩工具（jpg、png等,压缩效果比tinypng稍好）\n * waifu2x 通过卷积网络放大图片\n * vectormagic 转换矢量图\n * vectorizer 真正的 png 转 svg 神器\n * 在线AI图片处理 黑白修复、无损放大、动漫化、铅笔画等。\n * remove AI抠图\n * backgroundremover 又一个抠图的\n\n\n# 思维导图\n\n * processon在线作图 流程图、思维导图、原型图等\n * 百度脑图 思维导图\n * plectica 绘制知识图谱\n\n\n# CSS\n\n * 各种CSS生成器和JS代码片段\n\n * CSS Tricks CSS技巧收集与演示\n * CSS生成器\n * CSS渐变生成器\n * CSS3-Box Shadow(阴影)\n * 贝塞尔曲线生成器\n * 花纹背景生成器\n * 花纹背景-pattern.css\n * 3D字体\n * css-tricks css技巧文章\n * You-need-to-know-css CSS的各种DEMO，很全\n * animista CSS动画可视化工具，复制代码就能用\n * navnav 各种炫酷的CSS动画组件\n\n\n# CDN加速\n\n * jsDelivr 国外的一家优秀的公共 CDN 服务提供商\n * unpkg cdn 服务\n\n\n# 网站托管\n\n * vercel 好用的网站托管服务\n\n\n# 正则\n\n * 正则可视化\n * iHateRegex 正则搜索，细节做得很好\n * 正则迷你书 学习正则的小手册\n\n\n# 其他\n\n * Linux命令手册\n * carbon代码图片生成器 生成好看的代码图片\n\n\n# 设计\n\n * 创造师导航\n * 设计师网址导航\n * remove AI抠图，抠图算法很厉害\n * Manypixels 插画\n * Undraw 插画\n * storytale 插画，种类丰富，包含3D插画\n * uimovement 能从这个网站找到不少动画交互的灵感\n * awwwards是一个一个专门为设计精美的网站以及富有创意的网站颁奖的网站\n * dribbble 经常能在上面找到很多有创意好看的 gif 或者图片\n * Bēhance dribbble 是设计师的微博，Bēhance 是设计师的博客\n * Logojoy 使用 ai 做 logo 的网站，做出来的 logo 质量还不错。\n * brandmark 另一个在线制作 logo 网站\n * instant 又一个 logo 制作网站\n * namecheap又一个 logo 制作网站\n * logo-maker 又一个 logo 制作网站 这个更简单点 就是选模板之后微调\n * coolors 帮你在线配色的网站 你能找到不少配色灵感\n * colorhunt 另一个配色网站\n * uigradients 渐变色网站\n * designcap 在线海报设计\n * Flat UI 色表 Flat UI 色表\n * 0to255 颜色梯度\n * Ikonate 提供免费的图标 icons\n * remixicon 又一个提供免费图标 icons\n * feather 免费的 icons\n * nord 北欧性冷淡风主题配色\n * Unsplash 提供免费的高清图片\n * Pexels 提供免费的高清图片\n * colorkitty 从你的图片中提取配色\n * design.youzan 有赞设计原则\n * iconfont 阿里巴巴矢量图标库\n * undraw 免费的矢量插画\n * icomoon 矢量图标库\n * cssicon 所有的 icon 都是纯 css 画的 缺点：icon 不够多\n * CSS triangle generator 帮你快速用 css 做出三角形\n * clippy 在线帮你使用 css clip-path 做出各种形状的图形\n * Lorem Picsum 提供免费的占位图\n * Canva 可画 生成插画、封面、海报、头像等\n * 404页 404页素材\n * collectui 按功能组件分类的设计图\n * smartmockups 产品模板生成工具\n\n\n# 图库\n\n * uigradients 渐变色生成工具\n * freepik banner 图库\n * 觅元素一天免费下载十张 psd（免抠元素）\n * 搞定设计 可以抠图\n * vectorizer 真正的 png 转 svg 神器\n * 站酷 国内优秀的设计作品展示\n * 花瓣\n * 虎克 ps 学习教程\n * beTheme\n * UI 中国\n * wallhaven 壁纸网站-\n\n\n# 交互\n\n * 微交互 里面收集了市面上很多很好的微交互例子 值得学习\n\n * Little Big Details 同上，一个国外微交互汇集网站\n\n * cruip 登录页的各种页面设计，可以免费下载模板\n\n * Comixify 一个波兰团队做了非常好玩的工具，可以把视频自动转成漫画，上图是他们提供的 demo，效果很棒。\n\n * taiko-web 太鼓达人网页版 只能说很 6\n\n\n# 有趣\n\n * 奇趣网站收藏家 收藏了很多有趣的网站\n * FC在线模拟器(小霸王游戏机) 童年回忆\n * 帮你百度一下 可以 点我测试一下-\n * 国际版 同帮我百度一下-点我测试一下-\n * URL 地址播放 Emojis 动画 在地址栏里面播放 emoji\n * Can't Unsee 强烈建议前端、客户端、UI 开发的同学玩下，检查一下自己对设计稿的敏感度怎么样\n * ggtalk 平时一直在听的一个技术博客\n * awesome-comment 里面收集了很多有趣的代码注释\n * text-img 都将图片转化为 ascii 用来写注释\n * ascii video 使用ascii编码生成视频动画\n * weird-fonts 将普通字母转化为 特殊 unicode\n * snake 在地址栏里面玩贪吃蛇\n * zero-width-lib 利用零宽度字符实现 隐形水印、加密信息分享、逃脱词匹配，很有创意\n * abbreviations 查看一个简写是什么意思的网站\n * magi ai 搜索神器，超屌\n\n\n# 生成器\n\n * artbreeder 动漫图生成真人图像\n\n * 声音生成器 生成下雨、咖啡厅、海浪、火车等声音，可几种声音合成\n\n * 诺基亚短信图片生成器\n\n * 到账语音生成器 支付宝到账1亿元\n\n\n# 元宇宙\n\n符合元宇宙特征的几个网站，允许用户拥有虚拟土地，在上面构建自己的世界：\n\n * 沙盒\n * 梦境空间\n * Decentraland\n\n\n# 教程\n\n * npx 教你怎么合理的使用 npx\n * hacksplaining 网络安全学习网站\n * mobile-web-best-practice 移动 web 最佳实践\n\n\n# 产品\n\n * 产品大牛 什么有很多完整的产品原型可以借鉴\n * 磨刀 快速出 ui 原型\n\n\n# 实用\n\n * webden 在线网页编辑器，轻便快捷\n * browser-update 浏览器版本更新提示插件\n * typeform 一个国外的在线调查问卷网站\n * VideoFk VideoFk 视频在线解析下载\n * 全历史 历史内容聚合网站\n * UzerMe 云端办公工具\n * SoBooks 强大的电子书资源网站\n * 稿定设计 键式设计工具+智能抠图\n * 大力盘 百度网盘搜索\n * ENFI 下载器 不限速下载器\n * 来画视频 像做 PPT 一样做短视频\n * Arkie 海报制作工具\n * 优品 PPT\n * 比格 PPT\n * 高清免费图片\n * 高清免费图片 2\n * shapedivider 生成波浪分隔线\n * Notion 知识库、快速笔记、TaskList、日记、读书清单，各种类型，应有尽有\n\n\n# Talk\n\n * peerigon-talks 收集了不少有意思的 talks\n\n\n# 算法\n\n * leetcode 用 js 刷 leetcode\n\n\n# nginx\n\n * nginx 可视化配置工具\n\n\n# 生活\n\n * Ventusky 风雨气温图",normalizedContent:"# 个人收藏夹\n\n\n# 推荐\n\n * panjiachen by 花裤衩\n * 编程导航 by 程序员鱼皮\n * 编程自学之路 by 程序羊\n * 前端知识图谱+b站资源整合 by 技术胖\n * 大圣编程自学网 by 大圣\n * 开发者武器库\n * 工具大全\n\n\n\n关注公众号[有趣研究社]，回复前端资源，获取 前端学习资料\n\n\n# 文档\n\n * mdn | mdn-js标准内置对象 web技术权威文档\n * devdocs web 开发技术文档，非常不错的学习手册！\n * 现代javascript教程 以最新标准为基准的js教程\n * es5教程 阮一峰的js教程\n * es6教程 阮一峰的es6教程\n * bash 脚本教程 阮一峰编写\n * ecma ecma官网\n * 菜鸟教程 涵盖多种语言的初级教程\n * 腾讯云开发者手册\n\n\n# 社区\n\n * github 程序员同性交友社区\n * 掘金 一个帮助开发者成长的社区\n * 简书 有很多频道的创作社区\n * 思否 解决技术问题的社区\n * stack overflow 同上，外网的\n * infoq 促进软件开发及相关领域知识与创新的传播\n * v2ex 创意工作者们的社区\n * 鱼塘热榜 划水网站，收集了很多网站，当天热门文章\n * 码力全开资源库 很全很强大，独立开发者/设计干货/优质利器/工具资源...\n\n\n# 社区互动\n\n * gitter\n * 兔小巢\n\n\n# 技巧\n\n * google 趋势 查看某项技术或关键字的热度趋势，可用于分析某项技术的发展前景，或对比某两项技术的热度。\n * 百度指数 同上，但百度的数据仅限国内。\n\n\n# 博客\n\n * 阮一峰的网络日志\n * samanthaming 对前端小知识点的总结，并为每个知识点制作精美的小卡片。\n\n\n# 电子书\n\n * 高教书苑 高等教育出版社的书籍，包含多种学科。\n * sobooks 免费的电子书资源网站\n\n\n# 优秀文章\n\n * 我做系统架构的一些原则 作者对系统架构的方法论总结\n * 灵活运用css开发技巧\n * 防御性css\n\n\n# 视频\n\n * bilibili b站，上面很多免费教学视频\n * 慕课网 实战视频教程\n * 妙味课堂 比较系统的前端入门视频教程\n * 中国大学mooc 涵盖计算机、外语、心理学等专业免费课程\n * egghead 质量还不错的短视频教程，外网\n\n\n# github\n\n * repobeats 生成仓库的动态数据统计图\n\n * github 短域名服务\n * shields 徽章图标\n * followers 全球排名\n * star-history 展示一个项目 stars 增长曲线\n\n\n# 评论系统\n\n * giscus 由 github discussions 驱动的评论系统\n\n\n# 前端小工具\n\n * can i use 查看属性和方法的兼容性\n * 30 seconds of code 收集了许多有用的代码小片段\n\n\n# 代码编辑\n\n * codepen 在线代码编辑与演示\n * codesandbox 内嵌vscode的在线ide\n\n\n# emoji表情\n\n * emoji表情\n * emoji表情备忘录\n * 根据文本匹配emoji\n * gitmoji 通过 emoji 表达 git 的操作内容\n\n> 在任意输入框快速打开emoji表情方法：\n> windows系统下按win + .\n> mac系统下按control + command + 空格\n\n\n# 图片工具\n\n * tinypng图片压缩 压缩png很有用\n * 微图 浏览器端图片压缩，不会上传图片到服务器\n * squoosh 谷歌出品在线免费图片压缩工具（jpg、png等,压缩效果比tinypng稍好）\n * waifu2x 通过卷积网络放大图片\n * vectormagic 转换矢量图\n * vectorizer 真正的 png 转 svg 神器\n * 在线ai图片处理 黑白修复、无损放大、动漫化、铅笔画等。\n * remove ai抠图\n * backgroundremover 又一个抠图的\n\n\n# 思维导图\n\n * processon在线作图 流程图、思维导图、原型图等\n * 百度脑图 思维导图\n * plectica 绘制知识图谱\n\n\n# css\n\n * 各种css生成器和js代码片段\n\n * css tricks css技巧收集与演示\n * css生成器\n * css渐变生成器\n * css3-box shadow(阴影)\n * 贝塞尔曲线生成器\n * 花纹背景生成器\n * 花纹背景-pattern.css\n * 3d字体\n * css-tricks css技巧文章\n * you-need-to-know-css css的各种demo，很全\n * animista css动画可视化工具，复制代码就能用\n * navnav 各种炫酷的css动画组件\n\n\n# cdn加速\n\n * jsdelivr 国外的一家优秀的公共 cdn 服务提供商\n * unpkg cdn 服务\n\n\n# 网站托管\n\n * vercel 好用的网站托管服务\n\n\n# 正则\n\n * 正则可视化\n * ihateregex 正则搜索，细节做得很好\n * 正则迷你书 学习正则的小手册\n\n\n# 其他\n\n * linux命令手册\n * carbon代码图片生成器 生成好看的代码图片\n\n\n# 设计\n\n * 创造师导航\n * 设计师网址导航\n * remove ai抠图，抠图算法很厉害\n * manypixels 插画\n * undraw 插画\n * storytale 插画，种类丰富，包含3d插画\n * uimovement 能从这个网站找到不少动画交互的灵感\n * awwwards是一个一个专门为设计精美的网站以及富有创意的网站颁奖的网站\n * dribbble 经常能在上面找到很多有创意好看的 gif 或者图片\n * behance dribbble 是设计师的微博，behance 是设计师的博客\n * logojoy 使用 ai 做 logo 的网站，做出来的 logo 质量还不错。\n * brandmark 另一个在线制作 logo 网站\n * instant 又一个 logo 制作网站\n * namecheap又一个 logo 制作网站\n * logo-maker 又一个 logo 制作网站 这个更简单点 就是选模板之后微调\n * coolors 帮你在线配色的网站 你能找到不少配色灵感\n * colorhunt 另一个配色网站\n * uigradients 渐变色网站\n * designcap 在线海报设计\n * flat ui 色表 flat ui 色表\n * 0to255 颜色梯度\n * ikonate 提供免费的图标 icons\n * remixicon 又一个提供免费图标 icons\n * feather 免费的 icons\n * nord 北欧性冷淡风主题配色\n * unsplash 提供免费的高清图片\n * pexels 提供免费的高清图片\n * colorkitty 从你的图片中提取配色\n * design.youzan 有赞设计原则\n * iconfont 阿里巴巴矢量图标库\n * undraw 免费的矢量插画\n * icomoon 矢量图标库\n * cssicon 所有的 icon 都是纯 css 画的 缺点：icon 不够多\n * css triangle generator 帮你快速用 css 做出三角形\n * clippy 在线帮你使用 css clip-path 做出各种形状的图形\n * lorem picsum 提供免费的占位图\n * canva 可画 生成插画、封面、海报、头像等\n * 404页 404页素材\n * collectui 按功能组件分类的设计图\n * smartmockups 产品模板生成工具\n\n\n# 图库\n\n * uigradients 渐变色生成工具\n * freepik banner 图库\n * 觅元素一天免费下载十张 psd（免抠元素）\n * 搞定设计 可以抠图\n * vectorizer 真正的 png 转 svg 神器\n * 站酷 国内优秀的设计作品展示\n * 花瓣\n * 虎克 ps 学习教程\n * betheme\n * ui 中国\n * wallhaven 壁纸网站-\n\n\n# 交互\n\n * 微交互 里面收集了市面上很多很好的微交互例子 值得学习\n\n * little big details 同上，一个国外微交互汇集网站\n\n * cruip 登录页的各种页面设计，可以免费下载模板\n\n * comixify 一个波兰团队做了非常好玩的工具，可以把视频自动转成漫画，上图是他们提供的 demo，效果很棒。\n\n * taiko-web 太鼓达人网页版 只能说很 6\n\n\n# 有趣\n\n * 奇趣网站收藏家 收藏了很多有趣的网站\n * fc在线模拟器(小霸王游戏机) 童年回忆\n * 帮你百度一下 可以 点我测试一下-\n * 国际版 同帮我百度一下-点我测试一下-\n * url 地址播放 emojis 动画 在地址栏里面播放 emoji\n * can't unsee 强烈建议前端、客户端、ui 开发的同学玩下，检查一下自己对设计稿的敏感度怎么样\n * ggtalk 平时一直在听的一个技术博客\n * awesome-comment 里面收集了很多有趣的代码注释\n * text-img 都将图片转化为 ascii 用来写注释\n * ascii video 使用ascii编码生成视频动画\n * weird-fonts 将普通字母转化为 特殊 unicode\n * snake 在地址栏里面玩贪吃蛇\n * zero-width-lib 利用零宽度字符实现 隐形水印、加密信息分享、逃脱词匹配，很有创意\n * abbreviations 查看一个简写是什么意思的网站\n * magi ai 搜索神器，超屌\n\n\n# 生成器\n\n * artbreeder 动漫图生成真人图像\n\n * 声音生成器 生成下雨、咖啡厅、海浪、火车等声音，可几种声音合成\n\n * 诺基亚短信图片生成器\n\n * 到账语音生成器 支付宝到账1亿元\n\n\n# 元宇宙\n\n符合元宇宙特征的几个网站，允许用户拥有虚拟土地，在上面构建自己的世界：\n\n * 沙盒\n * 梦境空间\n * decentraland\n\n\n# 教程\n\n * npx 教你怎么合理的使用 npx\n * hacksplaining 网络安全学习网站\n * mobile-web-best-practice 移动 web 最佳实践\n\n\n# 产品\n\n * 产品大牛 什么有很多完整的产品原型可以借鉴\n * 磨刀 快速出 ui 原型\n\n\n# 实用\n\n * webden 在线网页编辑器，轻便快捷\n * browser-update 浏览器版本更新提示插件\n * typeform 一个国外的在线调查问卷网站\n * videofk videofk 视频在线解析下载\n * 全历史 历史内容聚合网站\n * uzerme 云端办公工具\n * sobooks 强大的电子书资源网站\n * 稿定设计 键式设计工具+智能抠图\n * 大力盘 百度网盘搜索\n * enfi 下载器 不限速下载器\n * 来画视频 像做 ppt 一样做短视频\n * arkie 海报制作工具\n * 优品 ppt\n * 比格 ppt\n * 高清免费图片\n * 高清免费图片 2\n * shapedivider 生成波浪分隔线\n * notion 知识库、快速笔记、tasklist、日记、读书清单，各种类型，应有尽有\n\n\n# talk\n\n * peerigon-talks 收集了不少有意思的 talks\n\n\n# 算法\n\n * leetcode 用 js 刷 leetcode\n\n\n# nginx\n\n * nginx 可视化配置工具\n\n\n# 生活\n\n * ventusky 风雨气温图",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"常用的前端轮子",frontmatter:{title:"常用的前端轮子",date:"2021-10-09T19:42:02.000Z",permalink:"/pages/47cf96/",article:!1},regularPath:"/06.%E6%94%B6%E8%97%8F%E5%A4%B9/02.%E5%B8%B8%E7%94%A8%E7%9A%84%E5%89%8D%E7%AB%AF%E8%BD%AE%E5%AD%90.html",relativePath:"06.收藏夹/02.常用的前端轮子.md",key:"v-73e22b34",path:"/pages/47cf96/",headers:[{level:2,title:"React UI 组件库",slug:"react-ui-组件库",normalizedTitle:"react ui 组件库",charIndex:2},{level:2,title:"Vue UI组件库",slug:"vue-ui组件库",normalizedTitle:"vue ui组件库",charIndex:68},{level:2,title:"常用效果组件",slug:"常用效果组件",normalizedTitle:"常用效果组件",charIndex:124},{level:2,title:"工具类",slug:"工具类",normalizedTitle:"工具类",charIndex:201},{level:2,title:"Vue工具类",slug:"vue工具类",normalizedTitle:"vue工具类",charIndex:331},{level:2,title:"其他",slug:"其他",normalizedTitle:"其他",charIndex:469}],headersStr:"React UI 组件库 Vue UI组件库 常用效果组件 工具类 Vue工具类 其他",content:"# React UI 组件库\n\n * Ant Design\n * React Bootstrap\n * MATERIAL-UI\n\n\n# Vue UI组件库\n\n * Element UI PC端\n * Vant 移动端\n * View UI\n\n\n# 常用效果组件\n\n * Animate.css 动画库\n * Swiper 轮播组件\n * mescroll 下拉刷新和上拉加载框架-基于原生JS\n\n\n# 工具类\n\n * Lodash.js\n * Day.js 处理日期\n * Timeago.js 相对时间，如N小时前\n * Echarts 百度图表\n * Meditor.md 开源在线 Markdown 编辑器\n * validator.js 验证库\n\n\n# Vue工具类\n\n * vue-draggable 基于Sortable.js实现的vue拖拽插件\n * vue-qr 文本转二维码\n * vue-cropper 图片裁剪插件\n * vue-lazyload 懒加载\n * vue-simple-upload 上传组件\n\n\n# 其他\n\n * H5带笔锋手写签名，支持PC端和移动端",normalizedContent:"# react ui 组件库\n\n * ant design\n * react bootstrap\n * material-ui\n\n\n# vue ui组件库\n\n * element ui pc端\n * vant 移动端\n * view ui\n\n\n# 常用效果组件\n\n * animate.css 动画库\n * swiper 轮播组件\n * mescroll 下拉刷新和上拉加载框架-基于原生js\n\n\n# 工具类\n\n * lodash.js\n * day.js 处理日期\n * timeago.js 相对时间，如n小时前\n * echarts 百度图表\n * meditor.md 开源在线 markdown 编辑器\n * validator.js 验证库\n\n\n# vue工具类\n\n * vue-draggable 基于sortable.js实现的vue拖拽插件\n * vue-qr 文本转二维码\n * vue-cropper 图片裁剪插件\n * vue-lazyload 懒加载\n * vue-simple-upload 上传组件\n\n\n# 其他\n\n * h5带笔锋手写签名，支持pc端和移动端",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"分类",frontmatter:{categoriesPage:!0,title:"分类",permalink:"/categories/",article:!1},regularPath:"/@pages/categoriesPage.html",relativePath:"@pages/categoriesPage.md",key:"v-6d447165",path:"/categories/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"关于",frontmatter:{title:"关于",date:"2019-12-25T14:27:01.000Z",permalink:"/about",sidebar:!1,article:!1},regularPath:"/05.%E5%85%B3%E4%BA%8E/01.%E5%85%B3%E4%BA%8E.html",relativePath:"05.关于/01.关于.md",key:"v-d552c9e2",path:"/about/",headers:[{level:2,title:"📚Blog",slug:"📚blog",normalizedTitle:"📚blog",charIndex:2},{level:2,title:"🎨Theme",slug:"🎨theme",normalizedTitle:"🎨theme",charIndex:140},{level:2,title:"🐼Me",slug:"🐼me",normalizedTitle:"🐼me",charIndex:292},{level:3,title:"技能",slug:"技能",normalizedTitle:"技能",charIndex:311},{level:2,title:"公众号",slug:"公众号",normalizedTitle:"公众号",charIndex:440},{level:2,title:"前端学习",slug:"前端学习",normalizedTitle:"前端学习",charIndex:556},{level:2,title:"✉️ 联系",slug:"联系",normalizedTitle:"✉️ 联系",charIndex:597}],headersStr:"📚Blog 🎨Theme 🐼Me 技能 公众号 前端学习 ✉️ 联系",content:"# 📚Blog\n\n这是一个兼具博客文章、知识管理、文档查找的个人网站，主要内容是Web前端技术。如果你喜欢这个博客&主题欢迎到GitHub点个Star，或者交换友链 ( •̀ ω •́ )✧\n\n提示\n\n文章内容仅是我个人的小总结，资历尚浅，如有误还请指正。\n\n更新日志\n\n\n# 🎨Theme\n\n\n\n本站主题是 Vdoing，这是一款简洁高效的VuePress 知识管理&博客 主题。旨在轻松打造一个结构化与碎片化并存的个人在线知识库&博客，让你的知识海洋像一本本书一样清晰易读。配合多维索引，让每一个知识点都可以快速定位！ Github地址 | 在线vscode预览源码\n\n\n# 🐼Me\n\nweb前端小学生\n\n\n# 技能\n\n * 熟悉 JavaScript、HTML、CSS、Vue、React 的拼写\n * 了解 Linux、windows、macOS 的开关机方式\n * 精通 Git 的 pull 和 push，并注册了 GitHub 帐号刷了一些 star\n\n\n# 公众号\n\n有趣研究社是本人对各种有趣的、好玩的、沙雕的创意和想法以在线小网站或者文章的形式表达出来，比如80、90后朋友小时候玩的小霸王游戏机：https://game.xugaoyi.com，还有更多好玩的等你去探索吧~\n\n\n# 前端学习\n\n关注上面的公众号，回复前端资源，即可获取这些 前端学习资源。\n\n\n# ✉️ 联系\n\n * WeChat or QQ: 894072666\n * Email: 894072666@qq.com\n * GitHub: https://github.com/xugaoyi\n * Vdoing主题文档：https://doc.xugaoyi.com\n * Vdoing交流QQ群：694387113",normalizedContent:"# 📚blog\n\n这是一个兼具博客文章、知识管理、文档查找的个人网站，主要内容是web前端技术。如果你喜欢这个博客&主题欢迎到github点个star，或者交换友链 ( • ω • )✧\n\n提示\n\n文章内容仅是我个人的小总结，资历尚浅，如有误还请指正。\n\n更新日志\n\n\n# 🎨theme\n\n\n\n本站主题是 vdoing，这是一款简洁高效的vuepress 知识管理&博客 主题。旨在轻松打造一个结构化与碎片化并存的个人在线知识库&博客，让你的知识海洋像一本本书一样清晰易读。配合多维索引，让每一个知识点都可以快速定位！ github地址 | 在线vscode预览源码\n\n\n# 🐼me\n\nweb前端小学生\n\n\n# 技能\n\n * 熟悉 javascript、html、css、vue、react 的拼写\n * 了解 linux、windows、macos 的开关机方式\n * 精通 git 的 pull 和 push，并注册了 github 帐号刷了一些 star\n\n\n# 公众号\n\n有趣研究社是本人对各种有趣的、好玩的、沙雕的创意和想法以在线小网站或者文章的形式表达出来，比如80、90后朋友小时候玩的小霸王游戏机：https://game.xugaoyi.com，还有更多好玩的等你去探索吧~\n\n\n# 前端学习\n\n关注上面的公众号，回复前端资源，即可获取这些 前端学习资源。\n\n\n# ✉️ 联系\n\n * wechat or qq: 894072666\n * email: 894072666@qq.com\n * github: https://github.com/xugaoyi\n * vdoing主题文档：https://doc.xugaoyi.com\n * vdoing交流qq群：694387113",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"归档",frontmatter:{archivesPage:!0,title:"归档",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-4849b605",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"标签",frontmatter:{tagsPage:!0,title:"标签",permalink:"/tags/",article:!1},regularPath:"/@pages/tagsPage.html",relativePath:"@pages/tagsPage.md",key:"v-c07cc8f6",path:"/tags/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"Home",frontmatter:{home:!0,heroText:"梵一的博客",tagline:"人生没有白走的路，每一步都算数！",bannerBg:"/img/home-bg-jeep.jpg",background:"自定义背景样式       提示：如发现文本颜色不适应你的背景时可以到palette.styl修改$bannerTextColor变量",features:[{title:"数据库",details:"ElasticSearch、MySQL、PostgresSQL框架等数据库技术",link:"/db/",imgUrl:null},{title:"中间件",details:"Kubernetes，相关中间件技术",link:"/midware/",imgUrl:null},{title:"生活",details:"技术文档、教程、技巧、总结等文章",link:"/life/",imgUrl:null}]},regularPath:"/",relativePath:"index.md",key:"v-70b16331",path:"/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"你知道的越多，不知道的也就越多",frontmatter:{title:"你知道的越多，不知道的也就越多",date:"2020-05-06T15:52:40.000Z",permalink:"/pages/f2e63f",sidebar:"auto",categories:["随笔"],tags:["学习","知识","鸡汤"],author:{name:"xugaoyi",link:"https://github.com/xugaoyi"}},regularPath:"/_posts/%E9%9A%8F%E7%AC%94/%E4%BD%A0%E7%9F%A5%E9%81%93%E7%9A%84%E8%B6%8A%E5%A4%9A%EF%BC%8C%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E4%B9%9F%E5%B0%B1%E8%B6%8A%E5%A4%9A.html",relativePath:"_posts/随笔/你知道的越多，不知道的也就越多.md",key:"v-60f6b60a",path:"/pages/f2e63f/",excerpt:"<p>不知道大家有没有发现，我们身边经常有这样的人，他们越是有能力的，越是有知识的，越是低调，越是谦逊，因为他们深知，知道的越多，不知道的也就越多。</p>\n",headersStr:null,content:"不知道大家有没有发现，我们身边经常有这样的人，他们越是有能力的，越是有知识的，越是低调，越是谦逊，因为他们深知，知道的越多，不知道的也就越多。\n\n你知道的越多，你不知道的也就越多，这是一句非常有哲理的话。\n\n每个人的知识面都是有限的，你有可能在某个领域会有较深的研究，成为这个领域里的专家，等到你站在高处的时候，才会发现，自己是多么的渺小，才知道自己有多少没有涉及的领域。知道的越多，疑惑、问题就会越来越多，对已知的质疑、疑虑、困惑就会越来越多。\n\n即使如此，我们也应该努力，至少可以成为某个领域的佼佼者。\n\n鸡汤1\n\n弱小的人，才习惯嘲讽与否定，而内心强大的人，从不吝啬赞美与鼓励。\n\n鸡汤2\n\n当代青年人都应该摆脱冷气，只管向上走，不必听从自暴自弃者的流言。能做事的做事，能发声的发声。有一份热，发一份光，就像萤火一般，也可以在黑暗里发一点光，不必等候炬火。",normalizedContent:"不知道大家有没有发现，我们身边经常有这样的人，他们越是有能力的，越是有知识的，越是低调，越是谦逊，因为他们深知，知道的越多，不知道的也就越多。\n\n你知道的越多，你不知道的也就越多，这是一句非常有哲理的话。\n\n每个人的知识面都是有限的，你有可能在某个领域会有较深的研究，成为这个领域里的专家，等到你站在高处的时候，才会发现，自己是多么的渺小，才知道自己有多少没有涉及的领域。知道的越多，疑惑、问题就会越来越多，对已知的质疑、疑虑、困惑就会越来越多。\n\n即使如此，我们也应该努力，至少可以成为某个领域的佼佼者。\n\n鸡汤1\n\n弱小的人，才习惯嘲讽与否定，而内心强大的人，从不吝啬赞美与鼓励。\n\n鸡汤2\n\n当代青年人都应该摆脱冷气，只管向上走，不必听从自暴自弃者的流言。能做事的做事，能发声的发声。有一份热，发一份光，就像萤火一般，也可以在黑暗里发一点光，不必等候炬火。",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"拥抱生活，拥抱快乐",frontmatter:{title:"拥抱生活，拥抱快乐",date:"2020-06-26T20:40:38.000Z",permalink:"/pages/cd8bde/",sidebar:"auto",categories:["随笔"],tags:["鸡汤"],author:{name:"xugaoyi",link:"https://github.com/xugaoyi"}},regularPath:"/_posts/%E9%9A%8F%E7%AC%94/%E6%8B%A5%E6%8A%B1%E7%94%9F%E6%B4%BB%EF%BC%8C%E6%8B%A5%E6%8A%B1%E5%BF%AB%E4%B9%90.html",relativePath:"_posts/随笔/拥抱生活，拥抱快乐.md",key:"v-91f7fbec",path:"/pages/cd8bde/",excerpt:"<p>生活在后现代的今天，很多人都有一种虚无感，认为人生没有意义。但是，人生不可能没有意义，因为当你认为没有意义的时候，一定有一个与之相对应的概念叫有意义。</p>\n",headersStr:null,content:"生活在后现代的今天，很多人都有一种虚无感，认为人生没有意义。但是，人生不可能没有意义，因为当你认为没有意义的时候，一定有一个与之相对应的概念叫有意义。\n\n当你怀疑人生没有意义时，难道怀疑本身不值得怀疑吗？\n\n不要任由你内心的虚无感蔓延，我们需要去拥抱真实的生活。\n\n所有真实的快乐，都离不开艰辛的努力，无论是金榜题名的快乐，还是事业成功的喜悦，甚至包括洞房花烛的激动。所有真实的快乐，都需要长久的铺垫与努力，没有辛勤的汗水，快乐也就不再真实。\n\n如果快乐触手可及，这种廉价的快乐也就不值得珍惜，随时都可能抛弃。因此，对于年轻人而言，一个重要的功课就是学会去节制欲望。\n\n所有通过捷径所带来的快乐，都是廉价的，以至于所有追求都变得毫无意义，人生就了无生趣。我们需要在每天真实的努力中去拥抱生活，追寻真实的快乐。\n\n\n\n> 文章摘录自:B站视频《罗翔说刑法》，链接https://b23.tv/K8ulrE",normalizedContent:"生活在后现代的今天，很多人都有一种虚无感，认为人生没有意义。但是，人生不可能没有意义，因为当你认为没有意义的时候，一定有一个与之相对应的概念叫有意义。\n\n当你怀疑人生没有意义时，难道怀疑本身不值得怀疑吗？\n\n不要任由你内心的虚无感蔓延，我们需要去拥抱真实的生活。\n\n所有真实的快乐，都离不开艰辛的努力，无论是金榜题名的快乐，还是事业成功的喜悦，甚至包括洞房花烛的激动。所有真实的快乐，都需要长久的铺垫与努力，没有辛勤的汗水，快乐也就不再真实。\n\n如果快乐触手可及，这种廉价的快乐也就不值得珍惜，随时都可能抛弃。因此，对于年轻人而言，一个重要的功课就是学会去节制欲望。\n\n所有通过捷径所带来的快乐，都是廉价的，以至于所有追求都变得毫无意义，人生就了无生趣。我们需要在每天真实的努力中去拥抱生活，追寻真实的快乐。\n\n\n\n> 文章摘录自:b站视频《罗翔说刑法》，链接https://b23.tv/k8ulre",charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3},{title:"《数据结构与算法之美》读书笔记",frontmatter:{title:"《数据结构与算法之美》读书笔记",date:"2022-01-20T22:37:25.000Z",permalink:"/pages/98f6c7/",categories:["编程","数据结构和算法"],tags:[null]},regularPath:"/03.%E7%BC%96%E7%A8%8B/80.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/01.%E3%80%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html",relativePath:"03.编程/80.数据结构和算法/01.《数据结构与算法之美》读书笔记.md",key:"v-233673d3",path:"/pages/98f6c7/",headers:[{level:2,title:"前序",slug:"前序",normalizedTitle:"前序",charIndex:125},{level:2,title:"D21(2020/10/07)",slug:"d21-2020-10-07",normalizedTitle:"d21(2020/10/07)",charIndex:532},{level:2,title:"如何抓住重点，系统高效地学习数据结构与算法",slug:"如何抓住重点-系统高效地学习数据结构与算法",normalizedTitle:"如何抓住重点，系统高效地学习数据结构与算法",charIndex:573},{level:3,title:"什么是数据结构？什么是算法？",slug:"什么是数据结构-什么是算法",normalizedTitle:"什么是数据结构？什么是算法？",charIndex:627},{level:3,title:"一些可以让你事半功倍的学习技巧",slug:"一些可以让你事半功倍的学习技巧",normalizedTitle:"一些可以让你事半功倍的学习技巧",charIndex:1486},{level:2,title:"复杂度分析(上)：如何分析、统计算法的执行效率和资源消耗？",slug:"复杂度分析-上-如何分析、统计算法的执行效率和资源消耗",normalizedTitle:"复杂度分析(上)：如何分析、统计算法的执行效率和资源消耗？",charIndex:1954},{level:3,title:"为什么需要复杂度分析？",slug:"为什么需要复杂度分析",normalizedTitle:"为什么需要复杂度分析？",charIndex:1988},{level:3,title:"大O复杂度表示法",slug:"大o复杂度表示法",normalizedTitle:"大o复杂度表示法",charIndex:2302},{level:3,title:"大O表示法",slug:"大o表示法",normalizedTitle:"大o表示法",charIndex:296},{level:3,title:"时间复杂度分析",slug:"时间复杂度分析",normalizedTitle:"时间复杂度分析",charIndex:3859},{level:3,title:"几种常见时间复杂度案例分析",slug:"几种常见时间复杂度案例分析",normalizedTitle:"几种常见时间复杂度案例分析",charIndex:6474},{level:3,title:"空间复杂度分析",slug:"空间复杂度分析",normalizedTitle:"空间复杂度分析",charIndex:2046},{level:3,title:"内容小结",slug:"内容小结",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D22(2020/10/08)",slug:"d22-2020-10-08",normalizedTitle:"d22(2020/10/08)",charIndex:9663},{level:2,title:"最好、最坏情况时间复杂度",slug:"最好、最坏情况时间复杂度",normalizedTitle:"最好、最坏情况时间复杂度",charIndex:9874},{level:2,title:"平均情况时间复杂度",slug:"平均情况时间复杂度",normalizedTitle:"平均情况时间复杂度",charIndex:9851},{level:2,title:"均摊时间复杂度",slug:"均摊时间复杂度",normalizedTitle:"均摊时间复杂度",charIndex:9710},{level:2,title:"内容小结",slug:"内容小结-2",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D23(2020/10/09)",slug:"d23-2020-10-09",normalizedTitle:"d23(2020/10/09)",charIndex:15216},{level:2,title:"如何实现随机访问?",slug:"如何实现随机访问",normalizedTitle:"如何实现随机访问?",charIndex:15434},{level:3,title:"第一是线性表",slug:"第一是线性表",normalizedTitle:"第一是线性表",charIndex:15519},{level:3,title:"第二个是连续的内存空间和相同类型的数据",slug:"第二个是连续的内存空间和相同类型的数据",normalizedTitle:"第二个是连续的内存空间和相同类型的数据",charIndex:15666},{level:3,title:"随机访问实现原理",slug:"随机访问实现原理",normalizedTitle:"随机访问实现原理",charIndex:15794},{level:3,title:"随机访问的注意",slug:"随机访问的注意",normalizedTitle:"随机访问的注意",charIndex:16244},{level:2,title:'低效的"插入"和"删除"',slug:"低效的-插入-和-删除",normalizedTitle:"低效的&quot;插入&quot;和&quot;删除&quot;",charIndex:null},{level:3,title:'"插入"操作',slug:"插入-操作",normalizedTitle:"&quot;插入&quot;操作",charIndex:null},{level:3,title:'降低"插入"时间复杂度的技巧',slug:"降低-插入-时间复杂度的技巧",normalizedTitle:"降低&quot;插入&quot;时间复杂度的技巧",charIndex:null},{level:3,title:"删除操作",slug:"删除操作",normalizedTitle:"删除操作",charIndex:17427},{level:3,title:"连续删除更高效",slug:"连续删除更高效",normalizedTitle:"连续删除更高效",charIndex:17595},{level:3,title:"警惕数组的访问越界问题",slug:"警惕数组的访问越界问题",normalizedTitle:"警惕数组的访问越界问题",charIndex:17960},{level:2,title:"容器能否完全替代数组？",slug:"容器能否完全替代数组",normalizedTitle:"容器能否完全替代数组？",charIndex:19091},{level:2,title:"单独使用数组的场景",slug:"单独使用数组的场景",normalizedTitle:"单独使用数组的场景",charIndex:19795},{level:2,title:"答疑开篇问题",slug:"答疑开篇问题",normalizedTitle:"答疑开篇问题",charIndex:20262},{level:3,title:"内容小结",slug:"内容小结-3",normalizedTitle:"内容小结",charIndex:9493},{level:3,title:"二维数组的寻址公式",slug:"二维数组的寻址公式",normalizedTitle:"二维数组的寻址公式",charIndex:21093},{level:2,title:"D24(2020/10/10)",slug:"d24-2020-10-10",normalizedTitle:"d24(2020/10/10)",charIndex:21342},{level:2,title:"链表(上)：如何实现LRU缓存淘汰算法？",slug:"链表-上-如何实现lru缓存淘汰算法",normalizedTitle:"链表(上)：如何实现lru缓存淘汰算法？",charIndex:21442},{level:3,title:"链表与数组的区别",slug:"链表与数组的区别",normalizedTitle:"链表与数组的区别",charIndex:21730},{level:3,title:"单链表基本概念",slug:"单链表基本概念",normalizedTitle:"单链表基本概念",charIndex:22057},{level:3,title:"单链表的查找/插入/删除",slug:"单链表的查找-插入-删除",normalizedTitle:"单链表的查找/插入/删除",charIndex:22629},{level:3,title:"循环链表",slug:"循环链表",normalizedTitle:"循环链表",charIndex:22109},{level:3,title:"双向链表的概念",slug:"双向链表的概念",normalizedTitle:"双向链表的概念",charIndex:23321},{level:3,title:"双向链表的优势",slug:"双向链表的优势",normalizedTitle:"双向链表的优势",charIndex:23580},{level:3,title:"双向链表总结",slug:"双向链表总结",normalizedTitle:"双向链表总结",charIndex:24672},{level:3,title:"空间换时间or时间换空间",slug:"空间换时间or时间换空间",normalizedTitle:"空间换时间or时间换空间",charIndex:24847},{level:3,title:"双向循环链表",slug:"双向循环链表",normalizedTitle:"双向循环链表",charIndex:25208},{level:3,title:"数组和链表性能比较",slug:"数组和链表性能比较",normalizedTitle:"数组和链表性能比较",charIndex:25403},{level:3,title:"解答开篇的问题",slug:"解答开篇的问题",normalizedTitle:"解答开篇的问题",charIndex:26137},{level:3,title:"内容小结",slug:"内容小结-4",normalizedTitle:"内容小结",charIndex:9493},{level:3,title:"问题思考",slug:"问题思考",normalizedTitle:"问题思考",charIndex:26718},{level:2,title:"D25(2020/10/11)",slug:"d25-2020-10-11",normalizedTitle:"d25(2020/10/11)",charIndex:28613},{level:2,title:"技巧一：理解指针或引用的含义",slug:"技巧一-理解指针或引用的含义",normalizedTitle:"技巧一：理解指针或引用的含义",charIndex:28945},{level:2,title:"技巧二：警惕指针丢失和内存泄漏",slug:"技巧二-警惕指针丢失和内存泄漏",normalizedTitle:"技巧二：警惕指针丢失和内存泄漏",charIndex:29461},{level:2,title:"技巧三：利用哨兵简化实现难度",slug:"技巧三-利用哨兵简化实现难度",normalizedTitle:"技巧三：利用哨兵简化实现难度",charIndex:30141},{level:2,title:"技巧四：重点留意边界条件处理",slug:"技巧四-重点留意边界条件处理",normalizedTitle:"技巧四：重点留意边界条件处理",charIndex:32190},{level:2,title:"技巧五：举例画图，辅助思考",slug:"技巧五-举例画图-辅助思考",normalizedTitle:"技巧五：举例画图，辅助思考",charIndex:32549},{level:2,title:"技巧六：多写多练，没有捷径",slug:"技巧六-多写多练-没有捷径",normalizedTitle:"技巧六：多写多练，没有捷径",charIndex:32701},{level:2,title:"内容小结",slug:"内容小结-5",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"问题思考",slug:"问题思考-2",normalizedTitle:"问题思考",charIndex:26718},{level:2,title:"D26(2020/10/12)",slug:"d26-2020-10-12",normalizedTitle:"d26(2020/10/12)",charIndex:33392},{level:2,title:'如何理解"栈"？',slug:"如何理解-栈",normalizedTitle:"如何理解&quot;栈&quot;？",charIndex:null},{level:2,title:'如何实现一个"栈"？',slug:"如何实现一个-栈",normalizedTitle:"如何实现一个&quot;栈&quot;？",charIndex:null},{level:2,title:"支持动态扩容的顺序栈",slug:"支持动态扩容的顺序栈",normalizedTitle:"支持动态扩容的顺序栈",charIndex:34412},{level:2,title:"栈在函数调用时的应用",slug:"栈在函数调用时的应用",normalizedTitle:"栈在函数调用时的应用",charIndex:35517},{level:2,title:"栈在表达式求值中的应用",slug:"栈在表达式求值中的应用",normalizedTitle:"栈在表达式求值中的应用",charIndex:36027},{level:2,title:"栈在括号匹配中的应用",slug:"栈在括号匹配中的应用",normalizedTitle:"栈在括号匹配中的应用",charIndex:36371},{level:2,title:"解答开篇",slug:"解答开篇",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-6",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"问题思考",slug:"问题思考-3",normalizedTitle:"问题思考",charIndex:26718},{level:2,title:"D27(2020/10/13)",slug:"d27-2020-10-13",normalizedTitle:"d27(2020/10/13)",charIndex:38035},{level:2,title:'如何理解"队列"？',slug:"如何理解-队列",normalizedTitle:"如何理解&quot;队列&quot;？",charIndex:null},{level:2,title:"顺序队列和链式队列",slug:"顺序队列和链式队列",normalizedTitle:"顺序队列和链式队列",charIndex:38800},{level:2,title:"链表队列的实现",slug:"链表队列的实现",normalizedTitle:"链表队列的实现",charIndex:39819},{level:2,title:"循环队列",slug:"循环队列",normalizedTitle:"循环队列",charIndex:38664},{level:2,title:"阻塞队列和并发队列",slug:"阻塞队列和并发队列",normalizedTitle:"阻塞队列和并发队列",charIndex:40720},{level:2,title:"解答开篇",slug:"解答开篇-2",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-7",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D28(2020/10/14)",slug:"d28-2020-10-14",normalizedTitle:"d28(2020/10/14)",charIndex:42417},{level:2,title:'如何理解"递归"？',slug:"如何理解-递归",normalizedTitle:"如何理解&quot;递归&quot;？",charIndex:null},{level:2,title:"递归需要满足的三个条件",slug:"递归需要满足的三个条件",normalizedTitle:"递归需要满足的三个条件",charIndex:42924},{level:2,title:"如何编写递归代码？",slug:"如何编写递归代码",normalizedTitle:"如何编写递归代码？",charIndex:43345},{level:2,title:"递归代码要警惕堆栈溢出",slug:"递归代码要警惕堆栈溢出",normalizedTitle:"递归代码要警惕堆栈溢出",charIndex:44902},{level:2,title:"递归代码要警惕重复计算",slug:"递归代码要警惕重复计算",normalizedTitle:"递归代码要警惕重复计算",charIndex:45535},{level:2,title:"怎么将递归代码改写为非递归代码？",slug:"怎么将递归代码改写为非递归代码",normalizedTitle:"怎么将递归代码改写为非递归代码？",charIndex:46398},{level:2,title:"解答开篇",slug:"解答开篇-3",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"D29(2020/10/15)",slug:"d29-2020-10-15",normalizedTitle:"d29(2020/10/15)",charIndex:47327},{level:2,title:'如何分析一个"排序算法"？',slug:"如何分析一个-排序算法",normalizedTitle:"如何分析一个&quot;排序算法&quot;？",charIndex:null},{level:3,title:"排序算法的执行效率",slug:"排序算法的执行效率",normalizedTitle:"排序算法的执行效率",charIndex:47814},{level:3,title:"排序算法的内存消耗",slug:"排序算法的内存消耗",normalizedTitle:"排序算法的内存消耗",charIndex:48452},{level:3,title:"排序算法的稳定性",slug:"排序算法的稳定性",normalizedTitle:"排序算法的稳定性",charIndex:48604},{level:2,title:"冒泡排序",slug:"冒泡排序",normalizedTitle:"冒泡排序",charIndex:47382},{level:3,title:"第一，冒泡排序是原地排序算法吗？",slug:"第一-冒泡排序是原地排序算法吗",normalizedTitle:"第一，冒泡排序是原地排序算法吗？",charIndex:49918},{level:3,title:"第二，冒泡排序是稳定的排序算法吗？",slug:"第二-冒泡排序是稳定的排序算法吗",normalizedTitle:"第二，冒泡排序是稳定的排序算法吗？",charIndex:49996},{level:3,title:"第三，冒泡排序的时间复杂度是多少？",slug:"第三-冒泡排序的时间复杂度是多少",normalizedTitle:"第三，冒泡排序的时间复杂度是多少？",charIndex:50119},{level:2,title:"插入排序",slug:"插入排序",normalizedTitle:"插入排序",charIndex:31134},{level:3,title:"插入排序的实现",slug:"插入排序的实现",normalizedTitle:"插入排序的实现",charIndex:52200},{level:2,title:"选择排序",slug:"选择排序",normalizedTitle:"选择排序",charIndex:47551},{level:2,title:"解答开篇",slug:"解答开篇-4",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-8",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D30(2020/10/17)",slug:"d30-2020-10-17",normalizedTitle:"d30(2020/10/17)",charIndex:57328},{level:2,title:"学习方法",slug:"学习方法",normalizedTitle:"学习方法",charIndex:47627},{level:3,title:"数据结构和算法的概念",slug:"数据结构和算法的概念",normalizedTitle:"数据结构和算法的概念",charIndex:57564},{level:3,title:"数据结构和算法的关系是什么呢？",slug:"数据结构和算法的关系是什么呢",normalizedTitle:"数据结构和算法的关系是什么呢？",charIndex:57663},{level:3,title:"学习重点是什么",slug:"学习重点是什么",normalizedTitle:"学习重点是什么",charIndex:57844},{level:3,title:"学习思维",slug:"学习思维",normalizedTitle:"学习思维",charIndex:58123},{level:3,title:"学习技巧",slug:"学习技巧",normalizedTitle:"学习技巧",charIndex:1497},{level:2,title:"复杂度分析",slug:"复杂度分析",normalizedTitle:"复杂度分析",charIndex:307},{level:3,title:"为什么需要复杂度分析？",slug:"为什么需要复杂度分析-2",normalizedTitle:"为什么需要复杂度分析？",charIndex:1988},{level:3,title:"引入大O表示法",slug:"引入大o表示法",normalizedTitle:"引入大o表示法",charIndex:59059},{level:3,title:"分析大O表示法",slug:"分析大o表示法",normalizedTitle:"分析大o表示法",charIndex:60021},{level:3,title:"时间复杂度分析",slug:"时间复杂度分析-2",normalizedTitle:"时间复杂度分析",charIndex:3859},{level:3,title:"常见时间复杂度分析",slug:"常见时间复杂度分析",normalizedTitle:"常见时间复杂度分析",charIndex:60765},{level:3,title:"空间复杂度分析",slug:"空间复杂度分析-2",normalizedTitle:"空间复杂度分析",charIndex:2046},{level:3,title:"低阶到高阶的排序",slug:"低阶到高阶的排序",normalizedTitle:"低阶到高阶的排序",charIndex:62602},{level:2,title:"D31(2020/10/19)",slug:"d31-2020-10-19",normalizedTitle:"d31(2020/10/19)",charIndex:62680},{level:2,title:"复杂度分析",slug:"复杂度分析-2",normalizedTitle:"复杂度分析",charIndex:307},{level:3,title:"最好/最坏时间复杂度",slug:"最好-最坏时间复杂度",normalizedTitle:"最好/最坏时间复杂度",charIndex:62777},{level:3,title:"平均时间复杂度",slug:"平均时间复杂度",normalizedTitle:"平均时间复杂度",charIndex:11034},{level:3,title:"均摊时间复杂度",slug:"均摊时间复杂度-2",normalizedTitle:"均摊时间复杂度",charIndex:9710},{level:3,title:"insert()与find()代码比较",slug:"insert-与find-代码比较",normalizedTitle:"insert()与find()代码比较",charIndex:65678},{level:2,title:"数组",slug:"数组",normalizedTitle:"数组",charIndex:937},{level:3,title:"基本概念",slug:"基本概念",normalizedTitle:"基本概念",charIndex:22060},{level:3,title:"随机访问实现原理",slug:"随机访问实现原理-2",normalizedTitle:"随机访问实现原理",charIndex:15794},{level:3,title:"数组的时间复杂度描述",slug:"数组的时间复杂度描述",normalizedTitle:"数组的时间复杂度描述",charIndex:66424},{level:3,title:'低效的"插入"',slug:"低效的-插入",normalizedTitle:"低效的&quot;插入&quot;",charIndex:null},{level:3,title:'低效的"删除"',slug:"低效的-删除",normalizedTitle:"低效的&quot;删除&quot;",charIndex:null},{level:3,title:"数组越界问题",slug:"数组越界问题",normalizedTitle:"数组越界问题",charIndex:67523},{level:3,title:"高级语言的容器类与数组",slug:"高级语言的容器类与数组",normalizedTitle:"高级语言的容器类与数组",charIndex:68053},{level:3,title:"为何数组从0开始",slug:"为何数组从0开始",normalizedTitle:"为何数组从0开始",charIndex:68527},{level:3,title:"二维数组寻址公式",slug:"二维数组寻址公式",normalizedTitle:"二维数组寻址公式",charIndex:68757},{level:2,title:"链表",slug:"链表",normalizedTitle:"链表",charIndex:976},{level:3,title:"引入链表",slug:"引入链表",normalizedTitle:"引入链表",charIndex:69002},{level:3,title:"单链表",slug:"单链表",normalizedTitle:"单链表",charIndex:22057},{level:3,title:"循环链表",slug:"循环链表-2",normalizedTitle:"循环链表",charIndex:22109},{level:3,title:"双向链表",slug:"双向链表",normalizedTitle:"双向链表",charIndex:22104},{level:3,title:"数组与链表的比较",slug:"数组与链表的比较",normalizedTitle:"数组与链表的比较",charIndex:70961},{level:3,title:"书写链表代码",slug:"书写链表代码",normalizedTitle:"书写链表代码",charIndex:71160},{level:2,title:"栈",slug:"栈",normalizedTitle:"栈",charIndex:721},{level:3,title:"什么是栈",slug:"什么是栈",normalizedTitle:"什么是栈",charIndex:72241},{level:3,title:"栈存在的意义",slug:"栈存在的意义",normalizedTitle:"栈存在的意义",charIndex:72307},{level:3,title:'如何实现一个"栈"',slug:"如何实现一个-栈-2",normalizedTitle:"如何实现一个&quot;栈&quot;",charIndex:null},{level:3,title:"栈在函数调用中的应用",slug:"栈在函数调用中的应用",normalizedTitle:"栈在函数调用中的应用",charIndex:72677},{level:3,title:"栈在括号匹配中的应用",slug:"栈在括号匹配中的应用-2",normalizedTitle:"栈在括号匹配中的应用",charIndex:36371},{level:3,title:"栈在页面前进后退中的应用",slug:"栈在页面前进后退中的应用",normalizedTitle:"栈在页面前进后退中的应用",charIndex:73048},{level:2,title:"D32(2020/10/21)",slug:"d32-2020-10-21",normalizedTitle:"d32(2020/10/21)",charIndex:73256},{level:2,title:"队列",slug:"队列",normalizedTitle:"队列",charIndex:718},{level:3,title:"引出队列",slug:"引出队列",normalizedTitle:"引出队列",charIndex:73311},{level:3,title:'理解"队列"',slug:"理解-队列",normalizedTitle:"理解&quot;队列&quot;",charIndex:null},{level:3,title:"顺序队列/链式队列",slug:"顺序队列-链式队列",normalizedTitle:"顺序队列/链式队列",charIndex:73466},{level:2,title:"递归",slug:"递归",normalizedTitle:"递归",charIndex:1385},{level:3,title:"递归的三个条件",slug:"递归的三个条件",normalizedTitle:"递归的三个条件",charIndex:75297},{level:3,title:'如何写"递归"代码',slug:"如何写-递归-代码",normalizedTitle:"如何写&quot;递归&quot;代码",charIndex:null},{level:3,title:"递归代码的注意点",slug:"递归代码的注意点",normalizedTitle:"递归代码的注意点",charIndex:75525},{level:2,title:"排序",slug:"排序",normalizedTitle:"排序",charIndex:1388},{level:3,title:'如何分析一个"排序算法"',slug:"如何分析一个-排序算法-2",normalizedTitle:"如何分析一个&quot;排序算法&quot;",charIndex:null},{level:3,title:"有序度",slug:"有序度",normalizedTitle:"有序度",charIndex:48067},{level:3,title:"冒泡排序",slug:"冒泡排序-2",normalizedTitle:"冒泡排序",charIndex:47382},{level:3,title:"插入排序",slug:"插入排序-2",normalizedTitle:"插入排序",charIndex:31134},{level:3,title:"选择排序",slug:"选择排序-2",normalizedTitle:"选择排序",charIndex:47551},{level:3,title:"插入排序比冒泡排序更好",slug:"插入排序比冒泡排序更好",normalizedTitle:"插入排序比冒泡排序更好",charIndex:80055},{level:2,title:"D33(2020/10/22)",slug:"d33-2020-10-22",normalizedTitle:"d33(2020/10/22)",charIndex:80394},{level:2,title:"抽象数据结构类型",slug:"抽象数据结构类型",normalizedTitle:"抽象数据结构类型",charIndex:80575},{level:3,title:"数据类型",slug:"数据类型",normalizedTitle:"数据类型",charIndex:15272},{level:3,title:"抽象数据类型",slug:"抽象数据类型",normalizedTitle:"抽象数据类型",charIndex:80765},{level:2,title:"数组(线性表)",slug:"数组-线性表",normalizedTitle:"数组(线性表)",charIndex:81091},{level:3,title:"线性表的抽象数据类型",slug:"线性表的抽象数据类型",normalizedTitle:"线性表的抽象数据类型",charIndex:81103},{level:3,title:"线性表的顺序存储结构",slug:"线性表的顺序存储结构",normalizedTitle:"线性表的顺序存储结构",charIndex:81849},{level:3,title:"获得元素操作",slug:"获得元素操作",normalizedTitle:"获得元素操作",charIndex:82321},{level:3,title:"插入操作",slug:"插入操作-2",normalizedTitle:"插入操作",charIndex:14237},{level:3,title:"删除操作",slug:"删除操作-3",normalizedTitle:"删除操作",charIndex:17427},{level:2,title:"D34(2020/10/23)",slug:"d34-2020-10-23",normalizedTitle:"d34(2020/10/23)",charIndex:85885},{level:2,title:"typedef与struct",slug:"typedef与struct",normalizedTitle:"typedef与struct",charIndex:85927},{level:3,title:"定义结构体/无变量/无类型",slug:"定义结构体-无变量-无类型",normalizedTitle:"定义结构体/无变量/无类型",charIndex:86059},{level:3,title:"定义结构体(包含结构体变量)",slug:"定义结构体-包含结构体变量",normalizedTitle:"定义结构体(包含结构体变量)",charIndex:86247},{level:3,title:"定义结构体(typedef定义了这种结构体的别名)",slug:"定义结构体-typedef定义了这种结构体的别名",normalizedTitle:"定义结构体(typedef定义了这种结构体的别名)",charIndex:86616},{level:3,title:"typedef定义符合类型(指针或数组)",slug:"typedef定义符合类型-指针或数组",normalizedTitle:"typedef定义符合类型(指针或数组)",charIndex:86855},{level:3,title:"定义结构体(内含同类型结构体指针变量)",slug:"定义结构体-内含同类型结构体指针变量",normalizedTitle:"定义结构体(内含同类型结构体指针变量)",charIndex:87330},{level:3,title:"定义结构体(typdef与内含结构体指针的结合)",slug:"定义结构体-typdef与内含结构体指针的结合",normalizedTitle:"定义结构体(typdef与内含结构体指针的结合)",charIndex:87523},{level:3,title:"结构体指针初始化",slug:"结构体指针初始化",normalizedTitle:"结构体指针初始化",charIndex:87893},{level:2,title:"单链表",slug:"单链表-2",normalizedTitle:"单链表",charIndex:22057},{level:3,title:"单链表的读取1(大话系列)",slug:"单链表的读取1-大话系列",normalizedTitle:"单链表的读取1(大话系列)",charIndex:88333},{level:3,title:"单链表(按位序查找)",slug:"单链表-按位序查找",normalizedTitle:"单链表(按位序查找)",charIndex:89593},{level:3,title:"单链表查询(按值查找)",slug:"单链表查询-按值查找",normalizedTitle:"单链表查询(按值查找)",charIndex:90003},{level:3,title:"单链表的插入",slug:"单链表的插入",normalizedTitle:"单链表的插入",charIndex:23703},{level:3,title:"单链表的创建",slug:"单链表的创建",normalizedTitle:"单链表的创建",charIndex:91923},{level:3,title:"单链表的单个元素删除",slug:"单链表的单个元素删除",normalizedTitle:"单链表的单个元素删除",charIndex:94878},{level:3,title:"单链表整体删除",slug:"单链表整体删除",normalizedTitle:"单链表整体删除",charIndex:96403},{level:2,title:"D35(2020/10/26)",slug:"d35-2020-10-26",normalizedTitle:"d35(2020/10/26)",charIndex:97041},{level:2,title:"静态链表的概念",slug:"静态链表的概念",normalizedTitle:"静态链表的概念",charIndex:97082},{level:3,title:"静态链表的定义",slug:"静态链表的定义",normalizedTitle:"静态链表的定义",charIndex:97236},{level:3,title:"初始化静态列表",slug:"初始化静态列表",normalizedTitle:"初始化静态列表",charIndex:97712},{level:3,title:"静态链表的插入操作",slug:"静态链表的插入操作",normalizedTitle:"静态链表的插入操作",charIndex:98097},{level:3,title:"静态链表的删除操作",slug:"静态链表的删除操作",normalizedTitle:"静态链表的删除操作",charIndex:99780},{level:2,title:"D36(2020/10/27)",slug:"d36-2020-10-27",normalizedTitle:"d36(2020/10/27)",charIndex:100807},{level:2,title:"循环链表",slug:"循环链表-3",normalizedTitle:"循环链表",charIndex:22109},{level:2,title:"双向链表+双向循环链表",slug:"双向链表-双向循环链表",normalizedTitle:"双向链表+双向循环链表",charIndex:101127},{level:3,title:"双向链表的结构",slug:"双向链表的结构",normalizedTitle:"双向链表的结构",charIndex:101246},{level:3,title:"双向循环链表带头结点的空链表",slug:"双向循环链表带头结点的空链表",normalizedTitle:"双向循环链表带头结点的空链表",charIndex:101469},{level:3,title:"插入一个结点",slug:"插入一个结点",normalizedTitle:"插入一个结点",charIndex:24479},{level:3,title:"删除一个结点",slug:"删除一个结点",normalizedTitle:"删除一个结点",charIndex:101798},{level:2,title:"栈",slug:"栈-2",normalizedTitle:"栈",charIndex:721},{level:3,title:"栈的抽象数据类型",slug:"栈的抽象数据类型",normalizedTitle:"栈的抽象数据类型",charIndex:102219},{level:3,title:"顺序栈的概念",slug:"顺序栈的概念",normalizedTitle:"顺序栈的概念",charIndex:102648},{level:3,title:"顺序栈的类定义",slug:"顺序栈的类定义",normalizedTitle:"顺序栈的类定义",charIndex:103004},{level:3,title:"顺序栈入栈算法",slug:"顺序栈入栈算法",normalizedTitle:"顺序栈入栈算法",charIndex:103525},{level:3,title:"顺序栈出栈算法",slug:"顺序栈出栈算法",normalizedTitle:"顺序栈出栈算法",charIndex:103779},{level:3,title:"链式栈的概念",slug:"链式栈的概念",normalizedTitle:"链式栈的概念",charIndex:104080},{level:3,title:"链栈的类定义",slug:"链栈的类定义",normalizedTitle:"链栈的类定义",charIndex:104409},{level:3,title:"链栈入栈算法",slug:"链栈入栈算法",normalizedTitle:"链栈入栈算法",charIndex:104965},{level:3,title:"链栈出栈算法",slug:"链栈出栈算法",normalizedTitle:"链栈出栈算法",charIndex:105308},{level:2,title:"D37(2020/10/28)",slug:"d37-2020-10-28",normalizedTitle:"d37(2020/10/28)",charIndex:105681},{level:2,title:"队列的概念",slug:"队列的概念",normalizedTitle:"队列的概念",charIndex:38598},{level:2,title:"队列的抽象数据类型",slug:"队列的抽象数据类型",normalizedTitle:"队列的抽象数据类型",charIndex:105852},{level:2,title:"顺序队列",slug:"顺序队列",normalizedTitle:"顺序队列",charIndex:38800},{level:3,title:"顺序队列的存储不足",slug:"顺序队列的存储不足",normalizedTitle:"顺序队列的存储不足",charIndex:106479},{level:3,title:"循环队列定义",slug:"循环队列定义",normalizedTitle:"循环队列定义",charIndex:107009},{level:3,title:"循环队列的顺序存储结构",slug:"循环队列的顺序存储结构",normalizedTitle:"循环队列的顺序存储结构",charIndex:107852},{level:3,title:"初始化一个空队列",slug:"初始化一个空队列",normalizedTitle:"初始化一个空队列",charIndex:108120},{level:3,title:"返回队列的当前长度",slug:"返回队列的当前长度",normalizedTitle:"返回队列的当前长度",charIndex:108272},{level:3,title:"循环队列的入队列",slug:"循环队列的入队列",normalizedTitle:"循环队列的入队列",charIndex:108429},{level:3,title:"循环队列的出队列",slug:"循环队列的出队列",normalizedTitle:"循环队列的出队列",charIndex:109028},{level:2,title:"链式队列",slug:"链式队列",normalizedTitle:"链式队列",charIndex:38805},{level:3,title:"链队列的结构",slug:"链队列的结构",normalizedTitle:"链队列的结构",charIndex:109877},{level:3,title:"链队列的入队操作",slug:"链队列的入队操作",normalizedTitle:"链队列的入队操作",charIndex:110188},{level:3,title:"链队列的出队操作",slug:"链队列的出队操作",normalizedTitle:"链队列的出队操作",charIndex:111206},{level:2,title:"D38(2020/10/29)",slug:"d38-2020-10-29",normalizedTitle:"d38(2020/10/29)",charIndex:112354},{level:2,title:"归并排序的原理",slug:"归并排序的原理",normalizedTitle:"归并排序的原理",charIndex:112615},{level:3,title:"归并排序(Merge Sort)",slug:"归并排序-merge-sort",normalizedTitle:"归并排序(merge sort)",charIndex:112627},{level:3,title:"递归代码来实现归并排序",slug:"递归代码来实现归并排序",normalizedTitle:"递归代码来实现归并排序",charIndex:112857},{level:2,title:"归并排序的性能分析",slug:"归并排序的性能分析",normalizedTitle:"归并排序的性能分析",charIndex:114708},{level:3,title:"第一，归并排序是稳定的排序算法吗？",slug:"第一-归并排序是稳定的排序算法吗",normalizedTitle:"第一，归并排序是稳定的排序算法吗？",charIndex:114741},{level:3,title:"第二，归并排序的时间复杂度是多少？",slug:"第二-归并排序的时间复杂度是多少",normalizedTitle:"第二，归并排序的时间复杂度是多少？",charIndex:114961},{level:3,title:"第三，归并排序的空间复杂度是多少？",slug:"第三-归并排序的空间复杂度是多少",normalizedTitle:"第三，归并排序的空间复杂度是多少？",charIndex:116042},{level:2,title:"快速排序的原理",slug:"快速排序的原理",normalizedTitle:"快速排序的原理",charIndex:116522},{level:2,title:"快速排序的性能分析",slug:"快速排序的性能分析",normalizedTitle:"快速排序的性能分析",charIndex:118601},{level:2,title:"解答开篇",slug:"解答开篇-5",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-9",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D39(2020/11/02)",slug:"d39-2020-11-02",normalizedTitle:"d39(2020/11/02)",charIndex:120359},{level:2,title:"归并排序",slug:"归并排序",normalizedTitle:"归并排序",charIndex:8222},{level:3,title:"二路归并排序的操作步骤如下",slug:"二路归并排序的操作步骤如下",normalizedTitle:"二路归并排序的操作步骤如下",charIndex:120742},{level:2,title:"快速排序",slug:"快速排序",normalizedTitle:"快速排序",charIndex:8227},{level:3,title:"快速排序的基本思想",slug:"快速排序的基本思想",normalizedTitle:"快速排序的基本思想",charIndex:121143},{level:3,title:"快速排序与冒泡排序",slug:"快速排序与冒泡排序",normalizedTitle:"快速排序与冒泡排序",charIndex:121269},{level:3,title:"快速排序的流程图",slug:"快速排序的流程图",normalizedTitle:"快速排序的流程图",charIndex:121468},{level:3,title:"快速排序的基本步骤",slug:"快速排序的基本步骤",normalizedTitle:"快速排序的基本步骤",charIndex:121549},{level:3,title:"快速排序代码",slug:"快速排序代码",normalizedTitle:"快速排序代码",charIndex:122175},{level:2,title:"D40(2020/11/03)",slug:"d40-2020-11-03",normalizedTitle:"d40(2020/11/03)",charIndex:123057},{level:2,title:"桶排序(Bucket sort)",slug:"桶排序-bucket-sort",normalizedTitle:"桶排序(bucket sort)",charIndex:123364},{level:3,title:"桶排序看起来很优秀，是不是可以替代之前讲的排序算法？",slug:"桶排序看起来很优秀-是不是可以替代之前讲的排序算法",normalizedTitle:"桶排序看起来很优秀，是不是可以替代之前讲的排序算法？",charIndex:123708},{level:2,title:"计数排序(Counting sort)",slug:"计数排序-counting-sort",normalizedTitle:"计数排序(counting sort)",charIndex:124834},{level:2,title:"基数排序(Radix sort)",slug:"基数排序-radix-sort",normalizedTitle:"基数排序(radix sort)",charIndex:126208},{level:2,title:"解答开篇",slug:"解答开篇-6",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-10",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D41(2020/11/04)",slug:"d41-2020-11-04",normalizedTitle:"d41(2020/11/04)",charIndex:127531},{level:2,title:"如何选择合适的排序函数？",slug:"如何选择合适的排序函数",normalizedTitle:"如何选择合适的排序函数？",charIndex:127711},{level:3,title:"冒泡排序",slug:"冒泡排序-3",normalizedTitle:"冒泡排序",charIndex:47382},{level:3,title:"插入排序",slug:"插入排序-3",normalizedTitle:"插入排序",charIndex:31134},{level:3,title:"选择排序",slug:"选择排序-3",normalizedTitle:"选择排序",charIndex:47551},{level:3,title:"归并排序",slug:"归并排序-2",normalizedTitle:"归并排序",charIndex:8222},{level:3,title:"快速排序",slug:"快速排序-2",normalizedTitle:"快速排序",charIndex:8227},{level:3,title:"桶排序",slug:"桶排序",normalizedTitle:"桶排序",charIndex:47576},{level:3,title:"计数排序",slug:"计数排序",normalizedTitle:"计数排序",charIndex:47566},{level:3,title:"基数排序",slug:"基数排序",normalizedTitle:"基数排序",charIndex:47571},{level:3,title:"整体分析",slug:"整体分析",normalizedTitle:"整体分析",charIndex:140073},{level:2,title:"如何优化快速排序？",slug:"如何优化快速排序",normalizedTitle:"如何优化快速排序？",charIndex:140597},{level:3,title:"三数取中法",slug:"三数取中法",normalizedTitle:"三数取中法",charIndex:140882},{level:3,title:"随机法",slug:"随机法",normalizedTitle:"随机法",charIndex:141037},{level:2,title:"举例分析排序函数",slug:"举例分析排序函数",normalizedTitle:"举例分析排序函数",charIndex:141350},{level:2,title:"D42(2020/11/10)",slug:"d42-2020-11-10",normalizedTitle:"d42(2020/11/10)",charIndex:142358},{level:2,title:"无处不在的二分思想",slug:"无处不在的二分思想",normalizedTitle:"无处不在的二分思想",charIndex:142478},{level:2,title:"O(logn)惊人的查找速度",slug:"o-logn-惊人的查找速度",normalizedTitle:"o(logn)惊人的查找速度",charIndex:142779},{level:2,title:"二分查找的递归与非递归实现",slug:"二分查找的递归与非递归实现",normalizedTitle:"二分查找的递归与非递归实现",charIndex:143375},{level:2,title:"二分查找应用场景的局限性",slug:"二分查找应用场景的局限性",normalizedTitle:"二分查找应用场景的局限性",charIndex:144905},{level:2,title:"解答开篇",slug:"解答开篇-7",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"D43(2020/11/11)",slug:"d43-2020-11-11",normalizedTitle:"d43(2020/11/11)",charIndex:146147},{level:2,title:"变体一：查找第一个值等于给定值的元素",slug:"变体一-查找第一个值等于给定值的元素",normalizedTitle:"变体一：查找第一个值等于给定值的元素",charIndex:146600},{level:2,title:"变体二：查找最后一个值等于给定值的元素",slug:"变体二-查找最后一个值等于给定值的元素",normalizedTitle:"变体二：查找最后一个值等于给定值的元素",charIndex:148052},{level:2,title:"变体三：查找第一个大于等于给定值的元素",slug:"变体三-查找第一个大于等于给定值的元素",normalizedTitle:"变体三：查找第一个大于等于给定值的元素",charIndex:148983},{level:2,title:"变体四：查找最后一个小于等于给定值的元素",slug:"变体四-查找最后一个小于等于给定值的元素",normalizedTitle:"变体四：查找最后一个小于等于给定值的元素",charIndex:149911},{level:2,title:"解答开篇",slug:"解答开篇-8",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-11",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D44(2020/11/12)",slug:"d44-2020-11-12",normalizedTitle:"d44(2020/11/12)",charIndex:150874},{level:2,title:"如何理解跳表",slug:"如何理解跳表",normalizedTitle:"如何理解跳表",charIndex:151231},{level:2,title:"用跳表查询到底有多快？",slug:"用跳表查询到底有多快",normalizedTitle:"用跳表查询到底有多快？",charIndex:152029},{level:2,title:"跳表是不是很浪费内存",slug:"跳表是不是很浪费内存",normalizedTitle:"跳表是不是很浪费内存",charIndex:152889},{level:2,title:"高效的动态插入和删除",slug:"高效的动态插入和删除",normalizedTitle:"高效的动态插入和删除",charIndex:153689},{level:3,title:"动态插入",slug:"动态插入",normalizedTitle:"动态插入",charIndex:77580},{level:3,title:"动态删除",slug:"动态删除",normalizedTitle:"动态删除",charIndex:154034},{level:2,title:"跳表索引动态更新",slug:"跳表索引动态更新",normalizedTitle:"跳表索引动态更新",charIndex:154176},{level:2,title:"解答开篇",slug:"解答开篇-9",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-12",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D45(2020/11/19)",slug:"d45-2020-11-19",normalizedTitle:"d45(2020/11/19)",charIndex:155486},{level:2,title:"散列思想",slug:"散列思想",normalizedTitle:"散列思想",charIndex:155528},{level:2,title:"散列函数",slug:"散列函数",normalizedTitle:"散列函数",charIndex:156380},{level:2,title:"散列冲突",slug:"散列冲突",normalizedTitle:"散列冲突",charIndex:157360},{level:3,title:"开放寻址法",slug:"开放寻址法",normalizedTitle:"开放寻址法",charIndex:157502},{level:3,title:"链表法",slug:"链表法",normalizedTitle:"链表法",charIndex:157508},{level:2,title:"解答开篇",slug:"解答开篇-10",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-13",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D46(2020/11/23)",slug:"d46-2020-11-23",normalizedTitle:"d46(2020/11/23)",charIndex:159710},{level:2,title:"如何设计散列函数？",slug:"如何设计散列函数",normalizedTitle:"如何设计散列函数？",charIndex:160197},{level:2,title:"装载因子过大了怎么办？",slug:"装载因子过大了怎么办",normalizedTitle:"装载因子过大了怎么办？",charIndex:160881},{level:2,title:"如何避免低效的扩容？",slug:"如何避免低效的扩容",normalizedTitle:"如何避免低效的扩容？",charIndex:161985},{level:2,title:"如何选择冲突解决方法？",slug:"如何选择冲突解决方法",normalizedTitle:"如何选择冲突解决方法？",charIndex:162639},{level:3,title:"开放寻址法",slug:"开放寻址法-2",normalizedTitle:"开放寻址法",charIndex:157502},{level:3,title:"链表法",slug:"链表法-2",normalizedTitle:"链表法",charIndex:157508},{level:2,title:"工业级散列表举例分析",slug:"工业级散列表举例分析",normalizedTitle:"工业级散列表举例分析",charIndex:163815},{level:3,title:"初始大小",slug:"初始大小",normalizedTitle:"初始大小",charIndex:163866},{level:3,title:"装载因子和动态扩容",slug:"装载因子和动态扩容",normalizedTitle:"装载因子和动态扩容",charIndex:163966},{level:3,title:"散列冲突解决办法",slug:"散列冲突解决办法",normalizedTitle:"散列冲突解决办法",charIndex:158995},{level:3,title:"散列函数",slug:"散列函数-2",normalizedTitle:"散列函数",charIndex:156380},{level:2,title:"解答开篇",slug:"解答开篇-11",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-14",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D47(2020/11/24)",slug:"d47-2020-11-24",normalizedTitle:"d47(2020/11/24)",charIndex:165671},{level:2,title:"LRU缓存淘汰算法",slug:"lru缓存淘汰算法",normalizedTitle:"lru缓存淘汰算法",charIndex:21427},{level:2,title:"Redis有序集合",slug:"redis有序集合",normalizedTitle:"redis有序集合",charIndex:165857},{level:2,title:"Java LinkedHashMap",slug:"java-linkedhashmap",normalizedTitle:"java linkedhashmap",charIndex:168271},{level:3,title:"按照插入的顺序打印",slug:"按照插入的顺序打印",normalizedTitle:"按照插入的顺序打印",charIndex:168548},{level:3,title:"按照访问顺序来遍历",slug:"按照访问顺序来遍历",normalizedTitle:"按照访问顺序来遍历",charIndex:168531},{level:2,title:"解答开篇&内容小结",slug:"解答开篇-内容小结",normalizedTitle:"解答开篇&amp;内容小结",charIndex:null},{level:2,title:"D48(2020/11/25)",slug:"d48-2020-11-25",normalizedTitle:"d48(2020/11/25)",charIndex:170233},{level:2,title:"什么是哈希算法？",slug:"什么是哈希算法",normalizedTitle:"什么是哈希算法？",charIndex:170291},{level:2,title:"应用一：安全加密",slug:"应用一-安全加密",normalizedTitle:"应用一：安全加密",charIndex:171077},{level:2,title:"应用二：唯一标识",slug:"应用二-唯一标识",normalizedTitle:"应用二：唯一标识",charIndex:171948},{level:2,title:"应用三：数据校验",slug:"应用三-数据校验",normalizedTitle:"应用三：数据校验",charIndex:172608},{level:2,title:"应用四：散列函数",slug:"应用四-散列函数",normalizedTitle:"应用四：散列函数",charIndex:173114},{level:2,title:"内容小结",slug:"内容小结-15",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D49(2020/11/26)",slug:"d49-2020-11-26",normalizedTitle:"d49(2020/11/26)",charIndex:173670},{level:2,title:"应用五：负载均衡",slug:"应用五-负载均衡",normalizedTitle:"应用五：负载均衡",charIndex:173813},{level:2,title:"应用六：数据分片",slug:"应用六-数据分片",normalizedTitle:"应用六：数据分片",charIndex:174264},{level:3,title:'如何统计"搜索关键词"出现的次数',slug:"如何统计-搜索关键词-出现的次数",normalizedTitle:"如何统计&quot;搜索关键词&quot;出现的次数",charIndex:null},{level:3,title:"如何快速判断图片是否在图库中",slug:"如何快速判断图片是否在图库中",normalizedTitle:"如何快速判断图片是否在图库中",charIndex:174723},{level:2,title:"应用七：分布式存储",slug:"应用七-分布式存储",normalizedTitle:"应用七：分布式存储",charIndex:175522},{level:2,title:"解答开篇&内容小结",slug:"解答开篇-内容小结-2",normalizedTitle:"解答开篇&amp;内容小结",charIndex:null},{level:2,title:"D50(2020/11/28)(二叉树)",slug:"d50-2020-11-28-二叉树",normalizedTitle:"d50(2020/11/28)(二叉树)",charIndex:176446},{level:2,title:"树(Tree)",slug:"树-tree",normalizedTitle:"树(tree)",charIndex:176658},{level:2,title:"二叉树",slug:"二叉树",normalizedTitle:"二叉树",charIndex:1362},{level:2,title:"二叉树的遍历",slug:"二叉树的遍历",normalizedTitle:"二叉树的遍历",charIndex:178638},{level:2,title:"解答开篇&内容小结",slug:"解答开篇-内容小结-3",normalizedTitle:"解答开篇&amp;内容小结",charIndex:null},{level:2,title:"D51(2020/11/29) (二叉查找树)",slug:"d51-2020-11-29-二叉查找树",normalizedTitle:"d51(2020/11/29) (二叉查找树)",charIndex:180301},{level:2,title:"二叉查找树",slug:"二叉查找树",normalizedTitle:"二叉查找树",charIndex:150711},{level:3,title:"二叉查找树的查找操作",slug:"二叉查找树的查找操作",normalizedTitle:"二叉查找树的查找操作",charIndex:180735},{level:3,title:"二叉查找树的插入操作",slug:"二叉查找树的插入操作",normalizedTitle:"二叉查找树的插入操作",charIndex:181354},{level:3,title:"二叉查找树的删除操作",slug:"二叉查找树的删除操作",normalizedTitle:"二叉查找树的删除操作",charIndex:182064},{level:3,title:"二叉查找树的其他操作",slug:"二叉查找树的其他操作",normalizedTitle:"二叉查找树的其他操作",charIndex:183535},{level:2,title:"支持重复数据的二叉查找树",slug:"支持重复数据的二叉查找树",normalizedTitle:"支持重复数据的二叉查找树",charIndex:183690},{level:2,title:"二叉查找树的时间复杂度分析",slug:"二叉查找树的时间复杂度分析",normalizedTitle:"二叉查找树的时间复杂度分析",charIndex:184226},{level:2,title:"解答开篇",slug:"解答开篇-12",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-16",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D52(2020/12/01) 红黑树",slug:"d52-2020-12-01-红黑树",normalizedTitle:"d52(2020/12/01) 红黑树",charIndex:186196},{level:2,title:'什么是"平衡二叉查找树"',slug:"什么是-平衡二叉查找树",normalizedTitle:"什么是&quot;平衡二叉查找树&quot;",charIndex:null},{level:2,title:'如何定义一棵"红黑树"',slug:"如何定义一棵-红黑树",normalizedTitle:"如何定义一棵&quot;红黑树&quot;",charIndex:null},{level:2,title:'为什么说红黑树是"近似平衡"的？',slug:"为什么说红黑树是-近似平衡-的",normalizedTitle:"为什么说红黑树是&quot;近似平衡&quot;的？",charIndex:null},{level:2,title:"解答开篇",slug:"解答开篇-13",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-17",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"数据结构整理",slug:"数据结构整理",normalizedTitle:"数据结构整理",charIndex:189452},{level:2,title:"D53(2020/12/03) 递归树",slug:"d53-2020-12-03-递归树",normalizedTitle:"d53(2020/12/03) 递归树",charIndex:190281},{level:2,title:"递归树与时间复杂度分析",slug:"递归树与时间复杂度分析",normalizedTitle:"递归树与时间复杂度分析",charIndex:190516},{level:2,title:"实战一： 分析快速排序的时间复杂度",slug:"实战一-分析快速排序的时间复杂度",normalizedTitle:"实战一： 分析快速排序的时间复杂度",charIndex:191333},{level:2,title:"实战二： 分析斐波那契数列的时间复杂度",slug:"实战二-分析斐波那契数列的时间复杂度",normalizedTitle:"实战二： 分析斐波那契数列的时间复杂度",charIndex:192507},{level:2,title:"实战三：分析全排列的时间复杂度",slug:"实战三-分析全排列的时间复杂度",normalizedTitle:"实战三：分析全排列的时间复杂度",charIndex:193158},{level:2,title:"D54(2020/12/05) 堆排序",slug:"d54-2020-12-05-堆排序",normalizedTitle:"d54(2020/12/05) 堆排序",charIndex:193189},{level:2,title:'如何理解"堆"？',slug:"如何理解-堆",normalizedTitle:"如何理解&quot;堆&quot;？",charIndex:null},{level:2,title:"如何实现一个堆？",slug:"如何实现一个堆",normalizedTitle:"如何实现一个堆？",charIndex:193863},{level:3,title:"往堆中插入一个元素",slug:"往堆中插入一个元素",normalizedTitle:"往堆中插入一个元素",charIndex:194142},{level:3,title:"删除堆顶元素",slug:"删除堆顶元素",normalizedTitle:"删除堆顶元素",charIndex:194152},{level:2,title:"如何基于堆实现排序？",slug:"如何基于堆实现排序",normalizedTitle:"如何基于堆实现排序？",charIndex:196066},{level:3,title:"建堆",slug:"建堆",normalizedTitle:"建堆",charIndex:196261},{level:3,title:"排序",slug:"排序-2",normalizedTitle:"排序",charIndex:1388},{level:2,title:"解答开篇",slug:"解答开篇-14",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-18",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D55(2020/12/09) 堆应用",slug:"d55-2020-12-09-堆应用",normalizedTitle:"d55(2020/12/09) 堆应用",charIndex:199340},{level:2,title:"堆的应用一：优先级队列",slug:"堆的应用一-优先级队列",normalizedTitle:"堆的应用一：优先级队列",charIndex:199517},{level:3,title:"合并有序小文件",slug:"合并有序小文件",normalizedTitle:"合并有序小文件",charIndex:199976},{level:3,title:"高性能定时器",slug:"高性能定时器",normalizedTitle:"高性能定时器",charIndex:200550},{level:2,title:"堆的应用二：利用堆求Top K",slug:"堆的应用二-利用堆求top-k",normalizedTitle:"堆的应用二：利用堆求top k",charIndex:201137},{level:2,title:"堆的应用三：利用堆求中位数",slug:"堆的应用三-利用堆求中位数",normalizedTitle:"堆的应用三：利用堆求中位数",charIndex:201791},{level:2,title:"解答开篇",slug:"解答开篇-15",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-19",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D56(2020/12/12) 图的存储",slug:"d56-2020-12-12-图的存储",normalizedTitle:"d56(2020/12/12) 图的存储",charIndex:205288},{level:2,title:'如何理解"图"',slug:"如何理解-图",normalizedTitle:"如何理解&quot;图&quot;",charIndex:null},{level:2,title:"邻接矩阵存储方法",slug:"邻接矩阵存储方法",normalizedTitle:"邻接矩阵存储方法",charIndex:206348},{level:2,title:"邻接表存储方法",slug:"邻接表存储方法",normalizedTitle:"邻接表存储方法",charIndex:207033},{level:2,title:"解答开篇",slug:"解答开篇-16",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-20",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D57(2020/12/13) 深度和广度优先搜索",slug:"d57-2020-12-13-深度和广度优先搜索",normalizedTitle:"d57(2020/12/13) 深度和广度优先搜索",charIndex:209327},{level:2,title:'什么是"搜索"算法？',slug:"什么是-搜索-算法",normalizedTitle:"什么是&quot;搜索&quot;算法？",charIndex:null},{level:2,title:"广度优先搜索(BFS)",slug:"广度优先搜索-bfs",normalizedTitle:"广度优先搜索(bfs)",charIndex:210356},{level:2,title:"深度优先搜索(DFS)",slug:"深度优先搜索-dfs",normalizedTitle:"深度优先搜索(dfs)",charIndex:212286},{level:2,title:"解答开篇",slug:"解答开篇-17",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-21",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D58(2020/12/14)字符串匹配",slug:"d58-2020-12-14-字符串匹配",normalizedTitle:"d58(2020/12/14)字符串匹配",charIndex:214265},{level:2,title:"BF算法",slug:"bf算法",normalizedTitle:"bf算法",charIndex:214455},{level:2,title:"RK算法",slug:"rk算法",normalizedTitle:"rk算法",charIndex:214460},{level:2,title:"解答开篇&内容小结",slug:"解答开篇-内容小结-4",normalizedTitle:"解答开篇&amp;内容小结",charIndex:null},{level:2,title:"D59(2020/12/16) 字符串匹配",slug:"d59-2020-12-16-字符串匹配",normalizedTitle:"d59(2020/12/16) 字符串匹配",charIndex:218248},{level:2,title:"BM算法的核心思想",slug:"bm算法的核心思想",normalizedTitle:"bm算法的核心思想",charIndex:218657},{level:2,title:"BM算法原理分析",slug:"bm算法原理分析",normalizedTitle:"bm算法原理分析",charIndex:219029},{level:3,title:"坏字符规则",slug:"坏字符规则",normalizedTitle:"坏字符规则",charIndex:219052},{level:3,title:"好后缀规则",slug:"好后缀规则",normalizedTitle:"好后缀规则",charIndex:219058},{level:3,title:"BM算法代码实现",slug:"bm算法代码实现",normalizedTitle:"bm算法代码实现",charIndex:221121},{level:3,title:"BM算法的性能分析及优化",slug:"bm算法的性能分析及优化",normalizedTitle:"bm算法的性能分析及优化",charIndex:221134},{level:2,title:"解答开篇&内容小结",slug:"解答开篇-内容小结-5",normalizedTitle:"解答开篇&amp;内容小结",charIndex:null},{level:2,title:"D60(2020/12/18) 字符串匹配",slug:"d60-2020-12-18-字符串匹配",normalizedTitle:"d60(2020/12/18) 字符串匹配",charIndex:221785},{level:2,title:"KMP算法基本原理",slug:"kmp算法基本原理",normalizedTitle:"kmp算法基本原理",charIndex:222100},{level:2,title:"D61(2020/12/18) Trie树",slug:"d61-2020-12-18-trie树",normalizedTitle:"d61(2020/12/18) trie树",charIndex:222486},{level:2,title:'什么是"Trie"树',slug:"什么是-trie-树",normalizedTitle:"什么是&quot;trie&quot;树",charIndex:null},{level:2,title:"如何实现一棵Trie树？",slug:"如何实现一棵trie树",normalizedTitle:"如何实现一棵trie树？",charIndex:223573},{level:2,title:"Trie树真的很耗内存吗？",slug:"trie树真的很耗内存吗",normalizedTitle:"trie树真的很耗内存吗？",charIndex:225794},{level:2,title:"Trie树与散列表、红黑树的比较",slug:"trie树与散列表、红黑树的比较",normalizedTitle:"trie树与散列表、红黑树的比较",charIndex:226764},{level:2,title:"解答开篇",slug:"解答开篇-18",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-22",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D62(2020/12/21) AC自动机",slug:"d62-2020-12-21-ac自动机",normalizedTitle:"d62(2020/12/21) ac自动机",charIndex:228370},{level:2,title:"基于单模式串和Trie树实现的敏感词过滤",slug:"基于单模式串和trie树实现的敏感词过滤",normalizedTitle:"基于单模式串和trie树实现的敏感词过滤",charIndex:228770},{level:2,title:"经典的多模式串匹配算法：AC自动机",slug:"经典的多模式串匹配算法-ac自动机",normalizedTitle:"经典的多模式串匹配算法：ac自动机",charIndex:229683},{level:2,title:"解答开篇",slug:"解答开篇-19",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-23",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D63(2020/12/22) 贪心算法",slug:"d63-2020-12-22-贪心算法",normalizedTitle:"d63(2020/12/22) 贪心算法",charIndex:229849},{level:2,title:'如何理解"贪心算法"',slug:"如何理解-贪心算法",normalizedTitle:"如何理解&quot;贪心算法&quot;",charIndex:null},{level:2,title:"贪心算法实战分析",slug:"贪心算法实战分析",normalizedTitle:"贪心算法实战分析",charIndex:231299},{level:3,title:"分糖果",slug:"分糖果",normalizedTitle:"分糖果",charIndex:231378},{level:3,title:"钱币找零",slug:"钱币找零",normalizedTitle:"钱币找零",charIndex:231915},{level:3,title:"区间覆盖",slug:"区间覆盖",normalizedTitle:"区间覆盖",charIndex:232211},{level:2,title:"解答开篇",slug:"解答开篇-20",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-24",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D64(2020/12/23) 分治算法",slug:"d64-2020-12-23-分治算法",normalizedTitle:"d64(2020/12/23) 分治算法",charIndex:234095},{level:2,title:"如何理解分治算法？",slug:"如何理解分治算法",normalizedTitle:"如何理解分治算法？",charIndex:234276},{level:2,title:"分治算法应用举例分析",slug:"分治算法应用举例分析",normalizedTitle:"分治算法应用举例分析",charIndex:234821},{level:2,title:"分治思想在海量数据处理中的应用",slug:"分治思想在海量数据处理中的应用",normalizedTitle:"分治思想在海量数据处理中的应用",charIndex:236716},{level:2,title:"解答开篇",slug:"解答开篇-21",normalizedTitle:"解答开篇",charIndex:26137},{level:2,title:"内容小结",slug:"内容小结-25",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D65(2020/12/25) 回溯算法",slug:"d65-2020-12-25-回溯算法",normalizedTitle:"d65(2020/12/25) 回溯算法",charIndex:238330},{level:2,title:'如何理解"回溯算法"？',slug:"如何理解-回溯算法",normalizedTitle:"如何理解&quot;回溯算法&quot;？",charIndex:null},{level:2,title:"两个回溯算法的经典应用",slug:"两个回溯算法的经典应用",normalizedTitle:"两个回溯算法的经典应用",charIndex:240708},{level:3,title:"0-1背包",slug:"_0-1背包",normalizedTitle:"0-1背包",charIndex:238510},{level:3,title:"正则表达式",slug:"正则表达式",normalizedTitle:"正则表达式",charIndex:238454},{level:2,title:"内容小结",slug:"内容小结-26",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D66(2020/12/26) 动态规划",slug:"d66-2020-12-26-动态规划",normalizedTitle:"d66(2020/12/26) 动态规划",charIndex:243644},{level:2,title:"动态规划学习路线",slug:"动态规划学习路线",normalizedTitle:"动态规划学习路线",charIndex:243820},{level:2,title:"0-1背包问题",slug:"_0-1背包问题",normalizedTitle:"0-1背包问题",charIndex:240880},{level:2,title:"内容小结",slug:"内容小结-27",normalizedTitle:"内容小结",charIndex:9493},{level:2,title:"D67(2020/12/26) B+树索引",slug:"d67-2020-12-26-b-树索引",normalizedTitle:"d67(2020/12/26) b+树索引",charIndex:245302},{level:2,title:"算法解析",slug:"算法解析",normalizedTitle:"算法解析",charIndex:245360},{level:3,title:"解决问题的前提是定义清楚问题",slug:"解决问题的前提是定义清楚问题",normalizedTitle:"解决问题的前提是定义清楚问题",charIndex:245369},{level:3,title:"尝试用学过的数据结构解决这个问题",slug:"尝试用学过的数据结构解决这个问题",normalizedTitle:"尝试用学过的数据结构解决这个问题",charIndex:245742},{level:3,title:"改造二叉查找树来解决这个问题",slug:"改造二叉查找树来解决这个问题",normalizedTitle:"改造二叉查找树来解决这个问题",charIndex:246308},{level:2,title:"总结引申",slug:"总结引申",normalizedTitle:"总结引申",charIndex:249623},{level:2,title:"D68(2020/12/27) 索引：如何在海量数据中快速查找某个数据",slug:"d68-2020-12-27-索引-如何在海量数据中快速查找某个数据",normalizedTitle:"d68(2020/12/27) 索引：如何在海量数据中快速查找某个数据",charIndex:250142},{level:2,title:"为什么需要索引？",slug:"为什么需要索引",normalizedTitle:"为什么需要索引？",charIndex:250367},{level:2,title:"索引的需求定义",slug:"索引的需求定义",normalizedTitle:"索引的需求定义",charIndex:245481},{level:3,title:"功能性需求",slug:"功能性需求",normalizedTitle:"功能性需求",charIndex:250886},{level:3,title:"非功能性需求",slug:"非功能性需求",normalizedTitle:"非功能性需求",charIndex:250892},{level:2,title:"构建索引常用的数据结构有哪些？",slug:"构建索引常用的数据结构有哪些",normalizedTitle:"构建索引常用的数据结构有哪些？",charIndex:252172},{level:2,title:"总结引申",slug:"总结引申-2",normalizedTitle:"总结引申",charIndex:249623},{level:2,title:"D69(2020/12/28) 并行算法",slug:"d69-2020-12-28-并行算法",normalizedTitle:"d69(2020/12/28) 并行算法",charIndex:253347},{level:2,title:"并行排序",slug:"并行排序",normalizedTitle:"并行排序",charIndex:253631},{level:3,title:"归并排序中并行",slug:"归并排序中并行",normalizedTitle:"归并排序中并行",charIndex:253816},{level:3,title:"快速排序中并行",slug:"快速排序中并行",normalizedTitle:"快速排序中并行",charIndex:253950},{level:2,title:"并行查找",slug:"并行查找",normalizedTitle:"并行查找",charIndex:254235},{level:2,title:"并行字符串匹配",slug:"并行字符串匹配",normalizedTitle:"并行字符串匹配",charIndex:254888},{level:2,title:"并行搜索",slug:"并行搜索",normalizedTitle:"并行搜索",charIndex:255435},{level:2,title:"总结引申",slug:"总结引申-3",normalizedTitle:"总结引申",charIndex:249623},{level:2,title:"D70(2020/12/29) 算法实战：Redis",slug:"d70-2020-12-29-算法实战-redis",normalizedTitle:"d70(2020/12/29) 算法实战：redis",charIndex:256185},{level:2,title:"Redis数据库介绍",slug:"redis数据库介绍",normalizedTitle:"redis数据库介绍",charIndex:256263},{level:2,title:"列表(list)",slug:"列表-list",normalizedTitle:"列表(list)",charIndex:256701},{level:2,title:"字典(hash)",slug:"字典-hash",normalizedTitle:"字典(hash)",charIndex:257735},{level:2,title:"集合(set)",slug:"集合-set",normalizedTitle:"集合(set)",charIndex:258287},{level:2,title:"有序集合(sortedset)",slug:"有序集合-sortedset",normalizedTitle:"有序集合(sortedset)",charIndex:258482},{level:2,title:"数据结构持久化",slug:"数据结构持久化",normalizedTitle:"数据结构持久化",charIndex:258739},{level:2,title:"总结引申",slug:"总结引申-4",normalizedTitle:"总结引申",charIndex:249623},{level:2,title:"D71(2020/12/31) 高性能队列Disruptor",slug:"d71-2020-12-31-高性能队列disruptor",normalizedTitle:"d71(2020/12/31) 高性能队列disruptor",charIndex:259600},{level:2,title:'基于循环队列的"生产者——消费者模型"',slug:"基于循环队列的-生产者-消费者模型",normalizedTitle:"基于循环队列的&quot;生产者——消费者模型&quot;",charIndex:null},{level:2,title:'基于加锁的并发"生产者-消费者模型"',slug:"基于加锁的并发-生产者-消费者模型",normalizedTitle:"基于加锁的并发&quot;生产者-消费者模型&quot;",charIndex:null},{level:2,title:'基于无锁的并发"生产者 —— 消费者模型"',slug:"基于无锁的并发-生产者-消费者模型",normalizedTitle:"基于无锁的并发&quot;生产者 —— 消费者模型&quot;",charIndex:null},{level:2,title:"总结引申",slug:"总结引申-5",normalizedTitle:"总结引申",charIndex:249623}],headersStr:'前序 D21(2020/10/07) 如何抓住重点，系统高效地学习数据结构与算法 什么是数据结构？什么是算法？ 一些可以让你事半功倍的学习技巧 复杂度分析(上)：如何分析、统计算法的执行效率和资源消耗？ 为什么需要复杂度分析？ 大O复杂度表示法 大O表示法 时间复杂度分析 几种常见时间复杂度案例分析 空间复杂度分析 内容小结 D22(2020/10/08) 最好、最坏情况时间复杂度 平均情况时间复杂度 均摊时间复杂度 内容小结 D23(2020/10/09) 如何实现随机访问? 第一是线性表 第二个是连续的内存空间和相同类型的数据 随机访问实现原理 随机访问的注意 低效的"插入"和"删除" "插入"操作 降低"插入"时间复杂度的技巧 删除操作 连续删除更高效 警惕数组的访问越界问题 容器能否完全替代数组？ 单独使用数组的场景 答疑开篇问题 内容小结 二维数组的寻址公式 D24(2020/10/10) 链表(上)：如何实现LRU缓存淘汰算法？ 链表与数组的区别 单链表基本概念 单链表的查找/插入/删除 循环链表 双向链表的概念 双向链表的优势 双向链表总结 空间换时间or时间换空间 双向循环链表 数组和链表性能比较 解答开篇的问题 内容小结 问题思考 D25(2020/10/11) 技巧一：理解指针或引用的含义 技巧二：警惕指针丢失和内存泄漏 技巧三：利用哨兵简化实现难度 技巧四：重点留意边界条件处理 技巧五：举例画图，辅助思考 技巧六：多写多练，没有捷径 内容小结 问题思考 D26(2020/10/12) 如何理解"栈"？ 如何实现一个"栈"？ 支持动态扩容的顺序栈 栈在函数调用时的应用 栈在表达式求值中的应用 栈在括号匹配中的应用 解答开篇 内容小结 问题思考 D27(2020/10/13) 如何理解"队列"？ 顺序队列和链式队列 链表队列的实现 循环队列 阻塞队列和并发队列 解答开篇 内容小结 D28(2020/10/14) 如何理解"递归"？ 递归需要满足的三个条件 如何编写递归代码？ 递归代码要警惕堆栈溢出 递归代码要警惕重复计算 怎么将递归代码改写为非递归代码？ 解答开篇 D29(2020/10/15) 如何分析一个"排序算法"？ 排序算法的执行效率 排序算法的内存消耗 排序算法的稳定性 冒泡排序 第一，冒泡排序是原地排序算法吗？ 第二，冒泡排序是稳定的排序算法吗？ 第三，冒泡排序的时间复杂度是多少？ 插入排序 插入排序的实现 选择排序 解答开篇 内容小结 D30(2020/10/17) 学习方法 数据结构和算法的概念 数据结构和算法的关系是什么呢？ 学习重点是什么 学习思维 学习技巧 复杂度分析 为什么需要复杂度分析？ 引入大O表示法 分析大O表示法 时间复杂度分析 常见时间复杂度分析 空间复杂度分析 低阶到高阶的排序 D31(2020/10/19) 复杂度分析 最好/最坏时间复杂度 平均时间复杂度 均摊时间复杂度 insert()与find()代码比较 数组 基本概念 随机访问实现原理 数组的时间复杂度描述 低效的"插入" 低效的"删除" 数组越界问题 高级语言的容器类与数组 为何数组从0开始 二维数组寻址公式 链表 引入链表 单链表 循环链表 双向链表 数组与链表的比较 书写链表代码 栈 什么是栈 栈存在的意义 如何实现一个"栈" 栈在函数调用中的应用 栈在括号匹配中的应用 栈在页面前进后退中的应用 D32(2020/10/21) 队列 引出队列 理解"队列" 顺序队列/链式队列 递归 递归的三个条件 如何写"递归"代码 递归代码的注意点 排序 如何分析一个"排序算法" 有序度 冒泡排序 插入排序 选择排序 插入排序比冒泡排序更好 D33(2020/10/22) 抽象数据结构类型 数据类型 抽象数据类型 数组(线性表) 线性表的抽象数据类型 线性表的顺序存储结构 获得元素操作 插入操作 删除操作 D34(2020/10/23) typedef与struct 定义结构体/无变量/无类型 定义结构体(包含结构体变量) 定义结构体(typedef定义了这种结构体的别名) typedef定义符合类型(指针或数组) 定义结构体(内含同类型结构体指针变量) 定义结构体(typdef与内含结构体指针的结合) 结构体指针初始化 单链表 单链表的读取1(大话系列) 单链表(按位序查找) 单链表查询(按值查找) 单链表的插入 单链表的创建 单链表的单个元素删除 单链表整体删除 D35(2020/10/26) 静态链表的概念 静态链表的定义 初始化静态列表 静态链表的插入操作 静态链表的删除操作 D36(2020/10/27) 循环链表 双向链表+双向循环链表 双向链表的结构 双向循环链表带头结点的空链表 插入一个结点 删除一个结点 栈 栈的抽象数据类型 顺序栈的概念 顺序栈的类定义 顺序栈入栈算法 顺序栈出栈算法 链式栈的概念 链栈的类定义 链栈入栈算法 链栈出栈算法 D37(2020/10/28) 队列的概念 队列的抽象数据类型 顺序队列 顺序队列的存储不足 循环队列定义 循环队列的顺序存储结构 初始化一个空队列 返回队列的当前长度 循环队列的入队列 循环队列的出队列 链式队列 链队列的结构 链队列的入队操作 链队列的出队操作 D38(2020/10/29) 归并排序的原理 归并排序(Merge Sort) 递归代码来实现归并排序 归并排序的性能分析 第一，归并排序是稳定的排序算法吗？ 第二，归并排序的时间复杂度是多少？ 第三，归并排序的空间复杂度是多少？ 快速排序的原理 快速排序的性能分析 解答开篇 内容小结 D39(2020/11/02) 归并排序 二路归并排序的操作步骤如下 快速排序 快速排序的基本思想 快速排序与冒泡排序 快速排序的流程图 快速排序的基本步骤 快速排序代码 D40(2020/11/03) 桶排序(Bucket sort) 桶排序看起来很优秀，是不是可以替代之前讲的排序算法？ 计数排序(Counting sort) 基数排序(Radix sort) 解答开篇 内容小结 D41(2020/11/04) 如何选择合适的排序函数？ 冒泡排序 插入排序 选择排序 归并排序 快速排序 桶排序 计数排序 基数排序 整体分析 如何优化快速排序？ 三数取中法 随机法 举例分析排序函数 D42(2020/11/10) 无处不在的二分思想 O(logn)惊人的查找速度 二分查找的递归与非递归实现 二分查找应用场景的局限性 解答开篇 D43(2020/11/11) 变体一：查找第一个值等于给定值的元素 变体二：查找最后一个值等于给定值的元素 变体三：查找第一个大于等于给定值的元素 变体四：查找最后一个小于等于给定值的元素 解答开篇 内容小结 D44(2020/11/12) 如何理解跳表 用跳表查询到底有多快？ 跳表是不是很浪费内存 高效的动态插入和删除 动态插入 动态删除 跳表索引动态更新 解答开篇 内容小结 D45(2020/11/19) 散列思想 散列函数 散列冲突 开放寻址法 链表法 解答开篇 内容小结 D46(2020/11/23) 如何设计散列函数？ 装载因子过大了怎么办？ 如何避免低效的扩容？ 如何选择冲突解决方法？ 开放寻址法 链表法 工业级散列表举例分析 初始大小 装载因子和动态扩容 散列冲突解决办法 散列函数 解答开篇 内容小结 D47(2020/11/24) LRU缓存淘汰算法 Redis有序集合 Java LinkedHashMap 按照插入的顺序打印 按照访问顺序来遍历 解答开篇&内容小结 D48(2020/11/25) 什么是哈希算法？ 应用一：安全加密 应用二：唯一标识 应用三：数据校验 应用四：散列函数 内容小结 D49(2020/11/26) 应用五：负载均衡 应用六：数据分片 如何统计"搜索关键词"出现的次数 如何快速判断图片是否在图库中 应用七：分布式存储 解答开篇&内容小结 D50(2020/11/28)(二叉树) 树(Tree) 二叉树 二叉树的遍历 解答开篇&内容小结 D51(2020/11/29) (二叉查找树) 二叉查找树 二叉查找树的查找操作 二叉查找树的插入操作 二叉查找树的删除操作 二叉查找树的其他操作 支持重复数据的二叉查找树 二叉查找树的时间复杂度分析 解答开篇 内容小结 D52(2020/12/01) 红黑树 什么是"平衡二叉查找树" 如何定义一棵"红黑树" 为什么说红黑树是"近似平衡"的？ 解答开篇 内容小结 数据结构整理 D53(2020/12/03) 递归树 递归树与时间复杂度分析 实战一： 分析快速排序的时间复杂度 实战二： 分析斐波那契数列的时间复杂度 实战三：分析全排列的时间复杂度 D54(2020/12/05) 堆排序 如何理解"堆"？ 如何实现一个堆？ 往堆中插入一个元素 删除堆顶元素 如何基于堆实现排序？ 建堆 排序 解答开篇 内容小结 D55(2020/12/09) 堆应用 堆的应用一：优先级队列 合并有序小文件 高性能定时器 堆的应用二：利用堆求Top K 堆的应用三：利用堆求中位数 解答开篇 内容小结 D56(2020/12/12) 图的存储 如何理解"图" 邻接矩阵存储方法 邻接表存储方法 解答开篇 内容小结 D57(2020/12/13) 深度和广度优先搜索 什么是"搜索"算法？ 广度优先搜索(BFS) 深度优先搜索(DFS) 解答开篇 内容小结 D58(2020/12/14)字符串匹配 BF算法 RK算法 解答开篇&内容小结 D59(2020/12/16) 字符串匹配 BM算法的核心思想 BM算法原理分析 坏字符规则 好后缀规则 BM算法代码实现 BM算法的性能分析及优化 解答开篇&内容小结 D60(2020/12/18) 字符串匹配 KMP算法基本原理 D61(2020/12/18) Trie树 什么是"Trie"树 如何实现一棵Trie树？ Trie树真的很耗内存吗？ Trie树与散列表、红黑树的比较 解答开篇 内容小结 D62(2020/12/21) AC自动机 基于单模式串和Trie树实现的敏感词过滤 经典的多模式串匹配算法：AC自动机 解答开篇 内容小结 D63(2020/12/22) 贪心算法 如何理解"贪心算法" 贪心算法实战分析 分糖果 钱币找零 区间覆盖 解答开篇 内容小结 D64(2020/12/23) 分治算法 如何理解分治算法？ 分治算法应用举例分析 分治思想在海量数据处理中的应用 解答开篇 内容小结 D65(2020/12/25) 回溯算法 如何理解"回溯算法"？ 两个回溯算法的经典应用 0-1背包 正则表达式 内容小结 D66(2020/12/26) 动态规划 动态规划学习路线 0-1背包问题 内容小结 D67(2020/12/26) B+树索引 算法解析 解决问题的前提是定义清楚问题 尝试用学过的数据结构解决这个问题 改造二叉查找树来解决这个问题 总结引申 D68(2020/12/27) 索引：如何在海量数据中快速查找某个数据 为什么需要索引？ 索引的需求定义 功能性需求 非功能性需求 构建索引常用的数据结构有哪些？ 总结引申 D69(2020/12/28) 并行算法 并行排序 归并排序中并行 快速排序中并行 并行查找 并行字符串匹配 并行搜索 总结引申 D70(2020/12/29) 算法实战：Redis Redis数据库介绍 列表(list) 字典(hash) 集合(set) 有序集合(sortedset) 数据结构持久化 总结引申 D71(2020/12/31) 高性能队列Disruptor 基于循环队列的"生产者——消费者模型" 基于加锁的并发"生产者-消费者模型" 基于无锁的并发"生产者 —— 消费者模型" 总结引申',content:'# 《数据结构与算法之美》读书笔记\n\n从今天开始要开始学习王争老师的《数据结构与算法之美》，重要的坚持到底，深入的思考，为学习数据结构打下坚实的基础。\n\n第一遍先听录音，第二遍重点词汇和语句的摘抄，第三遍查看别人的重点标记，第四遍查看留言。\n\n\n# 前序\n\n今天主要学习的第一篇，入门篇，主要描述的学习数据结构与算法的重要意义。\n\n 1. 基础知识就像是一座大楼的地基，它决定了我们的技术高度。而要想快速做出点事情，前提前脚一定是基础能力过硬，“内功”要到位。\n 2. 为了由浅入深地学习，把专栏分成四个递进的模块。\n    * 第一部分是入门篇，通过这一模块，能掌握时间、空间复杂度的概念，大O表示法的由来，各种复杂度分析技巧，以及最好、最坏、平均、均摊复杂度分析方法。\n    * 第二部分是基础篇，是学习的重点。介绍了最基础、最常用的数据结构和算法。\n    * 第三部分是高级篇，主要会讲解一些不是那么常用的数据结构和算法。\n    * 第四部分是实战篇，主要围绕数据结构和算法在具体软件实践中的应用来讲的。\n 3. 人生道路上，我们会遇到很多的坎。跨过去，你就可以成长，跨不过去就是困难和停滞。而在后面很长的一段时间里，你都需要为这个困难买单。\n\n\n# D21(2020/10/07)\n\n今天是学习的是《数据结构与算法之美》的准备课，“如何抓住重点，系统高效地学习数据结构与算法”；\n\n\n# 如何抓住重点，系统高效地学习数据结构与算法\n\n\n# 什么是数据结构？什么是算法？\n\n从广义上来讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。\n\n从狭义上来讲，也就是我们专栏要讲的，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。这些经典的数据结构和算法，都是前人从很多实际操作场景中抽象出来的，可以高效的解决很多实际开发问题。\n\n# 数据结构与算法的关系\n\n数据结构与算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。\n\n数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构是没用的。\n\n例如，因为数组具有随机访问的特点，常用的二分查找算法需要用数组来存储数据。但如果我们选择链表这种数据结构，二分查找算法就无法工作了，因为链表并不支持随机访问。\n\n# 学习的重点在什么地方？\n\n梳理学习的重点，就是要了解应该先学什么，后学什么。\n\n要想学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念 ---- 复杂度分析。\n\n数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。\n\n如果我们只是掌握了数据结构与算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没有掌握心法。只有把心法了然于胸，才能做到无招胜有招。\n\n所以，结合我自己的学习心得，还有这些年的面试、开发经验，总结了20个最常用的、最基础的数据结构与算法，不管是应付面试还是工作需要，只要集中精力逐一攻克这20个知识点就足够了。\n\n这里面有10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树；10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。\n\n在学习的过程中，要学习它的"来历" "自身的特点" "适合解决的问题"以及"实际的应用场景"。\n\n\n# 一些可以让你事半功倍的学习技巧\n\n 1. 边学边练，适度刷题\n\n 2. 多问、多思考、多互动\n    \n    可以多在留言区写下自己的疑问、思考和总结，也可以经常看看别人的留言，和他们进行互动。\n\n 3. 打怪升级学习法\n    \n    学习的过程中，我们碰到最大的问题就是，坚持不下来。\n    \n    在枯燥的学习过程中，也可以给自己设立一个切实可行的目标，就像打怪升级一样。\n    \n    比如，针对这个专栏，可以设立这样一个目标：每节课后的思考题都认真思考，并且回复到留言区。当你看到很多人给你点赞之后，你就会为了每次都能发一个漂亮的留言，而更加认真地学习。\n    \n    比如，每节课后都写一篇学习笔记或学习心得；或者你还可以每节课都找下讲的不对的、不合理的地方。\n\n 4. 知识需要沉淀，不要想试图一下子掌握所有\n    \n    学习知识的过程是反复迭代、不断沉淀的过程。\n    \n    如果碰到"拦路虎"，你可以尽情在留言区问我，也可以先沉淀一下，过几天再重新学一遍。所谓，书读百遍其义自见。\n\n\n# 复杂度分析(上)：如何分析、统计算法的执行效率和资源消耗？\n\n\n# 为什么需要复杂度分析？\n\n事后统计法：\n\n通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？\n\n上面的评估算法执行效率的方法是正确的。可以称为事后统计法，但是这种统计方法有非常大的局限性。\n\n 1. 测试结果非常依赖测试环境\n    \n    测试结果，依赖于服务器的环境的好坏。\n\n 2. 测试结果受数据规模的影响很大\n    \n    数据是否有序，数据规模的大小，都可能会对测试结果产生影响，可能无法真实地反映算法的性能。\n    \n    我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。\n\n\n# 大O复杂度表示法\n\n算法的执行效率，粗略地讲，就是算法代码执行的时间。\n\n下面的一行代码，就是求1,2,3 ... n的累加和。\n\nint cal(int n) {\n\tint sum = 0;\n\tint i = 1;\n\tfor(; i<=n; ++i) {\n\t\tsum  = sum + i;\n\t}\n\treturn sum;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n从CPU的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的CPU执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？\n\n第1行int cal(int n) {是代码程序的入口，如果要算是代码的执行时间的话，那就是1次(这里我们考虑的是单次调用的场景下的分析)。\n\n第2行int sum=0;和第3行int i=1;代码分别需要1个unit_time的执行时间，第4、5行都运行了n遍(分别i从1到n)，所以需要2n* unit_time的执行时间，所以这段代码总的执行时间就是(2n+2)*unit_time。可以看出来，所有代码的执行时间T(n)与每行代码的执行次数成正比。\n\n# 示例分析2\n\n按照上面分析的思路，来看下面的这段代码。\n\nint cal(int n) {\n\tint sum = 0;\n\tint i = 1;\n\tint j = 1;\n\tfor (; i<=n; ++i) {\n\t\tj = 1;\n\t\tfor (; j <=n; ++j) {\n\t\t\tsum  = sum + i*j;\n\t\t}\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n我们依旧假设每个语句的执行时间是unit_time。那么这段代码的总执行时间T(n)是多少呢？\n\n第2、3、4行代码，每行都需要1个unit_time的执行时间，第5、6行代码循环执行了n遍，需要2n* unit_time的执行时间，第7、8行代码循环执行了$n^2$ 遍，所以需要2$n^2$ * unit_time的执行时间。所以，整段代码总的执行时间T(n) = (2$n^2$ + 2n +3) * unit_time.\n\n\n# 大O表示法\n\n尽管我们不知道unit_time的具体值，但是通过这两段的代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，所有代码的执行时间T(n)与每行代码的执行次数n成正比。\n\n我们可以把这个规律总结成一个公式。\n\nT(n) = O(f(n))\n\n其中，T(n)表示代码执行的时间；n表示数据规模的大小；f(n)表示每行代码执行的次数总和。公式中的O，表示代码的执行时间T(n)与f(n)表达式成正比。\n\n所以，第一个列子中的T(n) = O(2n+2)，第二个例子中的T(n) = O(2$n^2$ + 2n +3)。这就是大O时间复杂度表示法。大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫做渐进时间复杂度，简称为时间复杂度。\n\n当n很大时，可以把它想象成10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大O表示法刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n); T(n) = O($n^2$) .\n\n分析代码的执行时间，假设每行的执行时间一样。--\x3e 每行代码的执行次数 --\x3e 用大O来表示执行次数和时间时间的正比关系 --\x3e 去除低阶、常量、系数，真正成为大O表示法 。\n\n\n# 时间复杂度分析\n\n前面介绍了大O时间复杂度的由来和表示方法。\n\n现在来看下，如何分析一段代码的时间复杂度？这里有三个比较实用的方法可以分享。\n\n 1. 只关注循环执行次数最多的一段代码\n    \n    刚才所说的，大O这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。\n    \n    所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环次数最多的那一段代码就可以了。这段核心代码执行次数的n的量级，就是整段要分析代码的时间复杂度。\n    \n    int cal(int n) {\n    \tint sum = 0;\n    \tint i = 1;\n    \tfor(; i<=n; ++i) {\n    \t\tsum  = sum + i;\n    \t}\n    \treturn sum;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    其中第2、3行代码都是常量级的执行时间，与n的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第4、5行代码，所以这块代码要重点分析。这两行代码被执行了n次，所以总的时间复杂度就是O(n)。\n\n 2. 加法法则：总复杂度等于量级最大的那段代码的复杂度\n    \n    int cal(int n) {\n    \tint sum_1 = 0;\n    \tint p = 1;\n    \tfor (; p<100; ++p) {\n    \t\tsum_1 = sum_1 + p;\n    \t}\n    \tint sum_2 =0;\n    \tint q = 1;\n    \tfor (; q<n; ++q) {\n    \t\tsum_2 = sum_2 + q;\n    \t}\n    \tint sum_3 = 0;\n    \tint i =1;\n    \tint j =1;\n    \tfor (; i<=n; ++i) {\n    \t\tj = 1;\n    \t\tfor (; j<=n; ++j) {\n    \t\t\tsum_3 = sum_3 + i*j;\n    \t\t}\n    \t}\n    \treturn sum_1 + sum_2 + sum_3;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    \n    \n    这个代码分为三个部分，分别是求sum_1、sum_2、sum_3。我们可以分别分析每一部分的时间复杂度，然后把它们放在一块，再去一个量级最大的作为整段代码的复杂度。\n    \n    第一段代码求sum_1，for循环中执行了100次，所以是一个常量的执行时间，跟n的规模无关。\n    \n    需要强调的是，即便这段代码循环了10000次、100000次，只要是一个已知的数，跟n无关，照样也是常数级的执行时间。当n无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率于数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。\n    \n    那第二段代码和第三段代码的时间复杂度是多少呢？答案是O(n)和O($n^2$)。\n    \n    综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就是O($n^2$) . 也就是说：总的时间复杂度就等于量级最大的那段代码的时间复杂度。\n    \n    如果T1(n) = O(f(n))，T2(n) = O(g(n))；那么T(n) = T1(n) + T2(n) = max(O(f(n)), O(g(n))) = O(max(f(n), g(n))).\n\n 3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积\n    \n    对应上面的加法法则，这里还有一个乘法法则。也就是如果T1(n)=O(f(n))，T2(n)=O(g(n)); 那么T(n)=T1(n)*T2(n) = O(f(n))*O(g(n)) = O(f(n)*g(n))\n    \n    也就是说，假设T1(n) = O(n)，T2(n) = O($n^2$), 则T1(n)*T2(n) = O($n^3$)。落实到具体的代码上，我们可以把乘法法则看成是嵌套循环。\n    \n    int cal(int n) {\n    \tint ret = 0;\n    \tint i = 1;\n    \tfor (; i<n; ++i) {\n    \t\tret = ret + f(i);\n    \t}\n    }\n    \n    int f(int n) {\n    \tint sum = 0;\n    \tint i = 1;\n    \tfor (; i<n; ++i) {\n    \t\tsum = sum +i;\n    \t}\n    \treturn sum;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n    \n    我们单独看cal()函数。假设f()只是一个普通的操作，那第4~6行的时间复杂度就是，T1(n) = O(n)。但f()函数本身不是一个简单的操作，它的时间复杂度是T2(n) = O(n)，所以整个cal()函数的时间复杂度就是，T(n) =T1(n) * T2(n) = O(n*n) =O($n^2$) .\n    \n    上面的三种复杂度分析技巧，不需要刻意记忆。实际上，复杂度分析这个东西关键在于"熟练"。只要多看案例，多分析，就能做到"无招胜有招"。\n\n\n# 几种常见时间复杂度案例分析\n\n下面的图示，展示的是常见的时间复杂度的各个量级，按照数量级递增。\n\n\n\n对于上面罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O($2^n$) 和O(n!)\n\n我们把时间复杂度为非多项式量级的算法问题叫做NP(非确定多项式)问题。\n\n当数据规模n越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。所以，关于NP的时间复杂度就不展开讲了。下面主要讨论的是常见的多项式时间复杂度。\n\n 1. O(1)\n    \n    首先必须明确一个概念，O(1)只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。\n    \n    比如下面的这段代码中，即便有3行，它的时间复杂度也是O(1)，而不是O(3)。\n    \n    int i = 8;\n    int j = 6;\n    int sum = i + j;\n    \n    \n    1\n    2\n    3\n    \n    \n    需要了解的是，只要代码的执行时间不随n的增大而增长，这样代码的时间复杂度我们都记住O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是O(1)。\n\n 2. O(logn)、O(nlogn)\n    \n    对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。\n    \n    i = 1;\n    while (i<=n) {\n    \ti = i*2;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    根据我们前面讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。\n    \n    从代码中可以看出，变量i的值从1开始取，每循环一次就乘以2。当大于n时，循环结束。这个就类似于高中所学的等比数列。实际上，变量i的取值就是一个等比数列。如果我们把它一个个列出来，就应该是下面这个样子:\n    \n    $2^0$ $2^1$ $2^2$ $2^3$ ... $2^k$ ... $2^x$ ，最终一直到i<=n才停止。\n    \n    所以，我们只要知道x值是多少，就知道这行代码执行的次数是多少了。通过求解$2^x$ = n，来求解x的值。根据对数可知，x = $\\log_2{n}$ ,所以，这段代码的时间复杂度就是O($\\log_2{n}$)\n    \n    那么现在，我们把代码稍微修改一下，再观察一下下面代码的时间复杂度是多少？\n    \n    i = 1;\n    while (i<=n) {\n    \ti = i*3;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    根据上面同样的算法，我们可以得出这段代码的时间复杂度为O($\\log_3{n}$).\n    \n    实际上，不管是以2为底、以3为底，还是以10为底，我们可以把所有对数阶的时间复杂度都记为O($\\log n$). 为什么呢？\n    \n    我们知道，对数之间是可以相互转换的，$\\log_3 n$ 就等于$\\log_3 2$ * $\\log_2 n$ ,所以O($\\log_3 n$) = O(C * $\\log_2 n$) ，其中C = $\\log_3 2$ 是一个常量。\n    \n    基于我们前面的一个理论：在采用大O标记复杂度的时候，可以忽略系数。所以，O($\\log_2 n$)就等于O($\\log_3 n$)。因此，在对数阶时间复杂度的表示方法里，我们忽略了对数的"底"，统一表示为O(logn).\n    \n    进而对于O(n$\\log n$)就很容易理解了，这个就类似于之前学习的乘法法则。如果一段代码的时间复杂度是O($\\log n$)，我们循环执行n遍，时间复杂度就是O(n$\\log n$)，这也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是O(n$\\log n$) 。\n\n 3. O(m+n)、O(m*n)\n    \n    下面的例子是不一样的时间复杂度，代码的复杂度是由两个数据的规模来决定的。\n    \n    int cal(int m, int n) {\n    \tint sum_1 = 0;\n    \tint i = 1;\n    \tfor (; i<m; ++i) {\n    \t\tsum_1 = sum_1 + i;\n    \t}\n    \tint sum_2 = 0;\n    \tint j = 1;\n    \tfor (; j<n; ++j) {\n    \t\tsum_2 = sum_2 + j;\n    \t}\n    \treturn sum_1 + sum_2;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n    \n    从代码中可以看出，m和n是表示两个数据规模。我们无法事先评估m和n谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是O(m+n).\n    \n    针对这种情况下，原来的加法法则就不正确了，我们需要将加法法则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则则继续有效：T1(m)* T2(n) = O(f(m)*f(n)).\n\n\n# 空间复杂度分析\n\n前面我们讲到，时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度的全称就是渐进式空间复杂度，表示算法的存储空间与数据规模之间的增长关系。\n\nvoid print(int n) {\n\tint i = 0;\n\tint[] a = new int[n];\n\tfor (i; i<n; ++i) {\n\t\ta[i] = i*i;\n\t}\n\tfor (i = n-1; i>=0; --i) {\n\t\tprint out a[i]\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n空间时间复杂度，看的是代码申请的内存空间的情况，表示的是这个代码使用的内存空间与数据规模之间增长的关系；而时间复杂度看的是算法的执行时间，或者更进一步说是代码中某一行的执行次数与数据规模之间的增长关系。\n\n跟时间复杂度分析一样，我们可以看到，第2行代码中，我们申请了一个空间存储变量i，但是它是常量阶的，跟数据规模n没有关系，所以我们可以忽略。第3行申请了一个大小为n的int类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是O(n)。\n\n我们常用的空间复杂度就是O(1)、O(n)、O($n^2$)，像O($logn$)、O($nlogn$)这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单得多。\n\n\n# 内容小结\n\n复杂度分析也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算分，执行效率越低。\n\n常见的复杂度并不多，从低阶到高阶有：O(1)、O($\\log n$)、O(n)、O(n$\\log n$)、O($n^2$)。\n\n\n\n复杂度分析并不难，关键在于多练。\n\n\n# D22(2020/10/08)\n\n今天主要是要学习复杂度分析的下半部分，浅析最好、最坏、平均、均摊时间复杂度的内容。\n\n上一节，我们讲了复杂度的大O表示法和几个分析技巧，还举了一些常见复杂度分析的例子，比如O(1)、O($\\log n$)、O(n)、O(n$\\log n$)复杂度分析。\n\n今天我会继续给你讲四个复杂度分析方面的知识点，最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。\n\n\n# 最好、最坏情况时间复杂度\n\n先分析如下的代码的时间复杂度。\n\nint find(int[] array, int n, int x) {\n\tint i = 0;\n\tint pos = -1;\n\tfor (; i<n; ++i) {\n\t\tif (array[i] == x) {\n\t\t\tpos = i;\n\t\t}\n\t}\n\treturn pos;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n我们应该可以看出来，这段代码要实现的功能是，在一个无序的数组(arry)中，查找变量x出现的位置。如果没有找到，就返回-1。按照上节课讲的分析方法，这段代码的复杂度是O(n)，其中，n代表数组的长度。\n\n其实，我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。所以说，我们可以如下优化一下这段查找代码。\n\nint find(int[] array, int n, int x) {\n\tint i = 0;\n\tint pos = -1;\n\tfor (; i<n; ++i) {\n\t\tif (array[i] == x) {\n\t\t\tpos = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn pos;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n这个时候，问题来了。我们优化后，这段代码的时间复杂度还是O(n)吗？很显然，我们上一节学习的分析方法，解决不了这个问题。\n\n因为，要查找的变量x可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量x，那就不需要继续遍历剩下的n-1个数据了，那时间复杂度就是O(1)。\n\n但如果数组中不存在变量x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了O(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。\n\n为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度。\n\n顾名思义，最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。就像我们刚刚了解的，在最理想的情况下，要查找的变量x正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。\n\n同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。就像刚举的那个例子，如果数组中没有要查找的变量x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。\n\n\n# 平均情况时间复杂度\n\n我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面我们简称为平均时间复杂度。\n\n平均时间复杂度又该怎么分析呢，继续上面的查找变量x的例子来说。\n\n要查找的变量x在数组中的位置，有n+1种情况：在数组的0~ n-1位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数(或者说执行代码的次数)累加起来，然后再除以n+1种情况，就可以得到需要遍历的元素个数的平均值，即：\n\n$\\frac{1+2+3+...+n+n}{n+1}$ = $\\frac{n(n+3)}{2(n+1)}$\n\n我们知道，在时间复杂度的大O标记法中，可以省略掉系数、低阶、常量，所以，咋们把刚刚这个公式简化之后，得到的平均时间复杂度就是O(n).\n\n这个结果虽然是正确的，但是计算过程稍微有点问题。我们刚讲的这个n+1种情况，出现的概率并不是一样的。\n\n我们知道，要查找的变量x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便理解，我们假设在数组中与不再数组中的概率都是1/2。另外，要查找的数据出现在0~n-1这n个位置的概率也是一样的，为1/n。所以，根据概率乘法法则，要查找的数据出现在0~ n-1中任意位置的概率就是1/(2n).\n\n我们前面在推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了下面的这样：\n\n1$\\times$$\\frac{1}{2n}$ + 2$\\times$$\\frac{1}{2n}$ + 3$\\times$$\\frac{1}{2n}$ + ... + n$\\times$$\\frac{1}{2n}$ + n$\\times$$\\frac{1}{2}$ = $\\frac{3n+1}{4}$\n\n这个值就是概率论中的加权平均值，也叫做期望值，所以平均时间复杂度的全称应该叫做加权平均时间复杂度 或者 期望时间复杂度。\n\n引入概率之后，前面那段代码的加权平均值为(3n+1)/4。用大O表示法来表示，去掉系数和常量，这段代码的加权评平均时间复杂度仍然是O(n).\n\n我们可能会感觉到，平均时间复杂度貌似比较复杂，还要涉及到概率论的知识。实际上，在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。很多时间就像上一节课举的那些例子那样，使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。\n\n\n# 均摊时间复杂度\n\n到此为止，我们已经掌握了算法复杂度分析的大部分内容了。下面需要描述的是一个更加高级的概念，均摊时间复杂度，以及它对应的分析方法，摊还分析(或者叫平摊分析)。\n\n均摊时间复杂度，听起来跟平均时间复杂度有点像。对于初学者来说，这两个概念确实非常容易弄混。大部分的情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。\n\n// array 表示一个长度为n的数组\n// 代码中的array.length就等于n \nint[] array = new int[n];\nint count = 0;\n\nvoid insert(int val) {\n\tif (count == array.length) {\n\t\tint sum = 0;\n\t\tfor (int i =0; i < array.length; ++i) {\n\t\t\tsum = sum + array[i];\n\t\t}\n\t\tarray[0] = sum;\n\t\tcount = 1;\n\t}\n\tarray[count] = val;\n\t++count;\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n这个代码是外层有个循环在反复的调用的。\n\n这段代码实现了一个往数组中插入数据的功能。一开始count=0，先执行array[count] = val和++count，也就是如下的情况：\n\n这里假设array数组的长度为5。\ncount=0时，array[0] = val, count =1\ncount=1时，array[1] = val, count =2 \ncount=2时，array[2] = val, count =3\ncount=3时，array[3] = val, count =4\ncount=4时，array[4] = val, count =5\n\n这个时候count == array.length了，执行for循环5次(从i=0到i=4)，其中将前面几个array数组中的各个数组元素都相加起来，变成了sum。\n随后将sum赋值给了array[0]，原来的array[0]的值被替换掉了。\n将count 赋值为1，这个很重要。**实现了数组的清空**\n紧接着又开始了。。。\n\ncount=1时，array[1] = val, count =2 \ncount=2时，array[2] = val, count =3\ncount=3时，array[3] = val, count =4\ncount=4时，array[4] = val, count =5\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的count == array.length时，我们用for循环遍历数组求和，并清空数组(count = 1;)，将求和之后的sum值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。\n\n那这段代码的时间复杂度是多少呢？我们可以先用我们刚学的三种时间复杂度的分析方法来分析一下。\n\n最理想的情况下，数组中有空闲时间，我们只需要将数据插入到数组下标为count的位置就可以了，所以最好情况时间复杂度为O(1)。最坏情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为O(n)。\n\n那平均时间复杂度是多少呢？答案是O(1)。我们还是可以通过前面讲的概率论的方法来分析。\n\n假设数组的长度是n，根据数据插入的位置的不同，我们可以分为n种情况，每种情况的时间复杂度是O(1)。除此之外，还有一种"额外"的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是O(n)。而且，这n+1种情况发生的概率一样，都是1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是：\n\n1$\\times$$\\frac{1}{n+1}$ + 1$\\times$$\\frac{1}{n+1}$ +... + 1$\\times$$\\frac{1}{n+1}$ + n$\\times$$\\frac{1}{n+1}$ --\x3e O(1)\n\n到此为止，前面的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是这个例子里的平均复杂度分析其实并不需要这么复杂，不需要引入概率论的知识。这是为什么呢？\n\n我们先来对比一下这个insert()的例子和前面那个find()的例子，我们就会发现这两者有很大差别。\n\n首先，find()函数在极端情况下，复杂度才为O(1)。但insert()在大部分情况下，时间复杂度都为O(1)。只有个别的情况下，复杂度才比较高，为O(n)。这是insert()第一个区别于find()的地方。\n\n第二个不同的地方。对于insert()函数来说，O(1)时间复杂度的插入和O(n)时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序的关系，一般都是一个O(n)插入之后，紧跟着n-1个O(1)的插入操作(是指已经满的情况下产生的规律)，循环往复。\n\n所以，针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。\n\n针对这种特殊的场景，我们引入了一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫做均摊时间复杂度。\n\n那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？\n\n我们还是继续看在数组中插入数据的这个例子。每一次O(n)的插入操作，都会跟着n-1次 O(1)的插入操作，所以把耗时多的那次操作摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是O(1)。(我的理解下，那一次的O(n)的插入操作的循环代码的次数为n，也就是n个unit_time，将n个单位的unit_time平均摊派到前面的n-1次中，大致每一次都分到一个unit_time，还是O(1)的时间复杂度。由于这个每一次的插入操作的概率都是相同的，所以这种均摊是可以成立的)\n\n均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。为了方便我们理解和记忆，我们需要简单总结一下它们的应用场景。\n\n对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够引用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。\n\n尽管很多数据结构和算法书籍都花了很大力气来区分平均时间复杂度和均摊时间复杂度，但其实我个人认为，均摊时间复杂度就是一种特殊的平均时间复杂度，我们没必要花太多精力去区分它们。我们最应该掌握的是它的分析方法，摊还分析。\n\n\n# 内容小结\n\n今天我们学习了几个复杂度分析相关的概念，分别有：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。之所以引入这几个复杂度概念，是因为，同一段代码，在不同输入的情况下，复杂度量级有可能是不一样的。\n\n在引入这几个概念之后，我们可以更加全面地表示一段代码的执行效率。而且，这几个概念理解起来都不难。\n\n\n# D23(2020/10/09)\n\n今天主要是要学习的是数组方面的内容。\n\n在每一种编程语言中，基本都会有数组这种数据类型。不过，它不仅仅是一种编程语言中的数据类型，还是一种最基础的数据结构。尽管数组看起来非常基础、简单，但是我估计很多人都并没有理解这个基础数据结构的精髓。\n\n在大部分的编程语言中，数组都是从0开始编号的，但是是否下意识地想过，为什么数组要从0开始编号，而不是从1开始呢？ 从1开始不是更符合人类的思维习惯吗？\n\n\n# 如何实现随机访问?\n\n什么是数组呢？数组(Array)是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。\n\n这个定义中，有几个关键词。\n\n\n# 第一是线性表\n\n顾名思义，线性表就是数据排成像一条线一样的结果。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。\n\n\n\n而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。\n\n\n\n\n# 第二个是连续的内存空间和相同类型的数据\n\n正是因为这两个限制，它才有了一个堪称"杀手锏"的特性："随机访问"。但有利也有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。\n\n\n# 随机访问实现原理\n\n说到数据的访问，数组是如何实现根据下标随机访问数组元素的？\n\n我们拿一个长度为10的int类型的数组int[] a = new int[10]来举例。在下面画的图示中，计算机给数组a[10]，分配了一块连续内存空间1000~1039，其中，内存块的首地址为base_address = 1000。1个字节理解为1个房间，一个int类型的元素会占据4个房间。\n\n\n\n我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素的时候，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：\n\na[i]_address = base_address + i * data_type_size\n\n\n1\n\n\n其中data_type_size 表示数组中每个元素的大小。根据这里的例子来看，数组中存储的是int类型的数据，所以data_type_size就是4个字节。\n\n根据这个公式，我们就能实现了数组元素的快速访问了。\n\n\n# 随机访问的注意\n\n这里需要纠正一个"错误"。在面试的时候，常常会问数组和链表的区别，很多人都回答说，"链表适合插入、删除，时间复杂度O(1)；数组适合查找，查找时间复杂度为O(1)".\n\n实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为O(1)。即使是排好序的数组，我们用二分查找，时间复杂度也是O(log n)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为O(1)。\n\n\n# 低效的"插入"和"删除"\n\n前面概念部分我们提到，数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。为什么会导致低效呢？又有哪些改进方法呢？\n\n\n# "插入"操作\n\n假设数组的长度是n，现在，如果我们需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，给新来的数据，我们需要将第k ~ n这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？\n\n如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度是O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是O(n)。因为我在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为(1+2+...+n)/n = O(n)。\n\n> 我的理解是，n个线性表的元素中，有n+1个插槽，每个位置的概率都是一样的，都是1/(n+1)。 而出现在不同的位置上，需要移动的元素的个数是不一样的，在末尾是0，倒数第二位是1，倒数第二位是2 。。。 直至开头是n。 根据加权平均数的求解，那就是1 * (1/(n+1)) + 2* (1/(n+1)) .... + n* (1/(n+1)) = {(1+n)n}/{2(n+1)} = n/2 ，也就是O(n)\n\n这里的场景中，数组初始化后默认都是每个位置都是有元素的，默认的没有填入的元素的值，就是0。\n\n\n# 降低"插入"时间复杂度的技巧\n\n如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移k之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第k个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。\n\n为了更好地理解，我们举一个例子。假设数组a[10]中存储了如下5个元素：a,b,c,d,e\n\n我们现在需要将元素x插入到第3个位置。我们只需要将c放入到a[5]，将a[2]赋值为x即可。最后，数组中的元素如下：a，b, x,d,e,c\n\n\n\n利用这种处理技巧，在特定的场景下，在第k个位置插入一个元素的时间复杂度就会降为O(1)。这个处理思想会在快排中用到。\n\n\n# 删除操作\n\n跟插入数据类似，如果我们要删除第k个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。\n\n和插入类似，如果删除数组末尾的数据，则最好情况下时间复杂度为O(1)；如果删除开头的数据，则最坏情况时间复杂度为O(n)；平均情况时间复杂度也为O(n)，这个推导的原理和上面的插入是一样的。\n\n\n# 连续删除更高效\n\n实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？\n\n观看下面的例子。数组a[10]中存储了8个元素：a,b,c,d,e,f,g,h。现在，我们要依次删除a,b,c三个元素。\n\n\n\n为了避免d,e,f,g,h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据的时候，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。\n\n如果了解JVM，就会发现，这就是JVM标记清除垃圾回收算法的核心思想。很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。\n\n\n# 警惕数组的访问越界问题\n\n先看下面C语言代码的例子：\n\n# include <stdio.h>\nint main(int argc, char* argv[]) {\n\tint i =0;\n\tint arr[3] = {0};\n\tfor (; i<=3; i++) {\n\t\tarr[i] = 0;\n\t\tprintf("%s","hello world\\n");\n\t}\n\treturn 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n我们会发现代码的运行结果并非是打印三行"hello word"，而是会无限打印"hello world"，这是为什么呢？\n\n因为，数组大小为3，a[0]，a[1]，a[2]，而我们的代码因为书写错误，导致for循环的结束条件错写为了i<=3 而非 i<3，所以 当i=3的时候，数组a[3]访问越界。\n\n我们知道，在C语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。根据我们前面讲的数组寻址公式，a[3]也会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量i的内存地址，\n\n数组寻址公式是a[i]_address = base_address + i * data_type_size，\n\n> 函数体内的局部变量存在栈上，且是连续压栈。在Linux进程的内存布局中，栈区在高地址空间，从高向低增长。变量i和arr在相邻地址，且i比arr的地址大，所以arr越界正好访问到i。当然，前提是i和arr元素同类型，否则那段代码仍是未决行为。\n> \n> 我觉得那个例子，栈是由高到低位增长的，所以，i和数组的数据从高位地址到低位地址依次是：i, a[2], a[1], a[0]。a[3]通过寻址公式，计算得到地址正好是i的存储地址，所以a[3]=0，就相当于i=0.\n\n数组越界在C语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。\n\n这种情况下，一般都会出现莫名其妙的逻辑错误，就像我们刚刚举的那个例子，debug的难度非常的大。而且，很多计算机病毒也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统，所以写代码的时候一定要警惕数组越界。\n\n但并非所有的语言都像C一样，把数组越界检查的工作丢给程序员来做，像Java本身就会做越界检查，比如下面这几行Java代码，就会抛出java.lang.ArrayIndexOutOfBoundsException 。\n\nint[] a = new int[3];\na[3] = 10;\n\n\n1\n2\n\n\n\n# 容器能否完全替代数组？\n\n针对数组类型，很多语言都提供了容器类，比如Java中的ArrayList、C++ STL中的vector。在项目开发中，什么时候适合用数组，什么时候适合用容器呢？\n\n用Java语言来描述，ArrayList和数组相比，到底有哪些优势呢？\n\n个人觉得，ArrayList最大的优势就是可以将很多数组操作的细节封装起来。 比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容 。\n\n数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为10的数组，当第11个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。\n\n如果使用ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList已经帮我们实现好了。每次存储空间不够的时候，它都会将空间自动扩容为1.5倍大小。\n\n不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建ArrayList的时候事先指定数据大小。\n\n比如下面的情况下，我们要从数据库中取出10000条数据放入ArrayList。我们看下面这几行代码，就会发现，相比之下，事先指定数据大小可以省略掉很多次内存申请和数据搬移操作。\n\nArrayList<User> users =  new ArrayList(10000);\nfor (int i = 0; i < 10000; ++i) {\n   users.add(xxx);\n}\n\n\n1\n2\n3\n4\n\n\n\n# 单独使用数组的场景\n\n作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有些时候，用数组会更合适一些，总结了如下几点自己的经验。\n\n 1. Java ArrayList无法存储基本类型，比如int、long，需要封装为Integer、Long类，而Autoboxing、Unboxing则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。\n 2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以直接使用数组。\n 3. 还有就是个人喜好，当要表示多维数组的时候，用数组往往会更加直观。比如Object [][] array ; 而用容器的话则需要这样定义：ArrayList <ArrayList<object>> array.\n\n对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果我们是做一些非常底层的开发，比如开发网络架构，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。\n\n\n# 答疑开篇问题\n\n现在来解答一下开篇的问题：为什么大多数编程语言中，数组要从0开始编号，而不是从开始呢？\n\n从数组存储的内存模型上来看，"下标"最确切的定义应该是"偏移(offset)"。前面也提到，如果用a来表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址，a[k]就表示偏移k个type_size的位置，所以计算a[k]的内存地址只需要也能够这个公式：\n\na[k]_address = base_address + k * data_type_size\n\n但是，如果数组从1开始计数，那我们计算数组元素a[k]的内存地址就会变成：\n\na[k]_address = base_address + (k-1) * data_type_size\n\n对比两个公式，我们不难发现，从1开始编号，每次随机访问数组元素都多了一次减法运算，对于CPU来说，就是多了一次减法指令。\n\n数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从0开始编号，而不是从1开始。\n\n其实说，数组起始编号非0开始不是一定不可，最重要的可能是历史原因。\n\nC语言设计者用0开始计数数组下标，之后的Java、JavaScript等高级语言都效仿了C语言，或者说，为了在一定程度上减少C语言程序员学习Java的学习成本，因此急需沿用了从0开始计数的习惯。实际上，很多语言中数组也并不是从0开始计数的，甚至还有一些语言支持负数下标，比如Python。\n\n\n# 内容小结\n\n我们今天学习了数组。它可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特定就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为O(n)。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的开发，直接使用数组可能会更合适。\n\n\n# 二维数组的寻址公式\n\n那么对于二维数组 x[]来说，求x[i][j]的时候（不会考虑i j越界的情况），要到i的时候，一定走完了i*a2的长度，在x[i][0]往后找j个长度就是x[i][j]，所以会从初始地址增加 （i*a2+j）个单位长度\n\n一维数组：（a1）x[i]_address = base_address + i * type_size\n\n二维数组：（a1a2）x[i][j]_address = base_address + (i * a2 + j ) type_size\n\n\n# D24(2020/10/10)\n\n今天需要学习的是链表linked list这个数据结构。学习链表又有什么作用呢？为了回答这个问题，先要讨论一个经典的链表应用场景，那就是LRU缓存淘汰算法。\n\n\n# 链表(上)：如何实现LRU缓存淘汰算法？\n\n缓存是一种提供数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的CPU缓存、数据库缓存、浏览器缓存等。\n\n缓存的大小有限，当缓存被用满的时候，哪些数据应该被清理出去，哪些数据应该被保留呢？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略FIFO(First In, First Out)、最少使用策略LFU(Least Frequently Used)、最近最少使用策略LRU(Least Recently Used)。\n\n这里就需要关注的一个问题：如何用链表来实现LRU缓存淘汰策略呢?\n\n\n# 链表与数组的区别\n\n相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍微难一些。数组和链表是两个非常基础、非常常用的数据结构，我们会常常放到一起比较。\n\n从底层的存储结构上来看：\n\n参考如下的图示，从图中我们可以看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请了一个100MB大小的数组，当内存中没有连续的、足够大的存储空间的时候，即便内存的剩余总可用空间大于100MB，仍然会申请失败。(Java中的ArrayList可用不连续)\n\n而链表恰恰相反，它并不需要一块连续的内存空间，它通过"指针"将一组零散的内存块串联起来使用，所以如果我们申请的是100MB大小的链表，根本不会有问题。\n\n\n\n\n# 单链表基本概念\n\n链表结构五花八门，重点需要学习的是三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。\n\n链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的"结点"。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如下面的图示，我们把这个记录下个结点地址的指针叫做后继指针next。\n\n\n\n从上面的单链表的图中，可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性把第一个结点叫做头结点，把最后一个结点叫做尾结点。其中头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是只想下一个结点，而是指向一个空地址NULL，表示这时候链表上最后一个结点。\n\n> 参考《大话数据结构》中的内容，对于头结点，头指针的区别和联系。\n> \n> 参考URL：http://data.biancheng.net/view/103.html\n> \n> 头结点：是放在第一个元素结点之前的一个节点，其数据域一般无意义(也可以存放链表的长度)。头结点可有可无。\n> \n> 头指针：如果存在头节点，那么头指针则是指向头结点的指针。如果不存在头节点，那么头指针则是执行第一个结点元素的指针。头指针是链表必须存在的。\n\n\n\n\n# 单链表的查找/插入/删除\n\n与数组一样，链表也支持数据的查找、插入和删除操作。\n\n我们知道，在进行数组的插入、删除操作的时候，为了保证内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是O(n)。\n\n而在链表中插入或删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。\n\n从下面的图示中，可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针的改变，所以对应的时间复杂度是O(1).\n\n\n\n但是，有利就有弊。链表要想随机访问第k个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。\n\n我们把单链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第k位的人是谁的时候，我们就需要从第一个人开始，一个个第往下数。所以，链表随机访问的性能没有数组好，需要O(n)的时间复杂度。\n\n\n# 循环链表\n\n循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。\n\n\n\n而循环链表的尾结点指针是指向链表的头结点。从上面的循环链表的图示中，可以看出，它像一个环一样首尾相连，所以叫做"循环"链表。\n\n和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环形结构特点时，就特别适合采用循环链表。\n\n\n# 双向链表的概念\n\n在实际的软件开发中，也更加常用的链表结构：双向链表。\n\n单向链表只有一个方向，结点只有一个后继指针next指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。\n\n\n\n从上面的图示中可以看出，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。\n\n\n# 双向链表的优势\n\n那相比单链表，双向链表适合解决哪种问题呢？\n\n从结构上来看，双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。\n\n可能我们会说，我们刚才讲的单链表的插入、删除操作的时间复杂度已经是O(1)了，双向链表还能再怎么高效呢？\n\n刚才我们的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。接下来，我们一起分析一下链表的两个操作。\n\n# 双向链表的删除操作\n\n在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：\n\n * 删除结点中"值等于某个给定值"的结点；(如果是这种情况的删除，我们需要先定位到要删除的元素，然后再执行删除操作。定位到要删除的元素的平均复杂度是O(n)，执行删除操作的时间复杂度是O(1))\n * 删除给定指针指向的结点。(这就意味着不用我们再去遍历找到被删除的元素。直接删除指向这个元素的相应的指针。)\n\n对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我们前面讲的指针操作将其删除。\n\n尽管单纯的删除操作时间复杂度是O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。\n\n对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点q需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到p->next = q, 说明p是q的前驱结点。\n\n但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定了。\n\n# 双向链表的插入操作\n\n同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。\n\n双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。\n\n# 双向链表的查询\n\n除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。\n\n因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。\n\n\n# 双向链表总结\n\n从上面的例子中，可以看出双向链表确实是要比单链表要更加高效一些。这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。\n\n如果我们熟悉Java语言，我们肯定就用过LinkedHashMap这个容器。深入了解LinkedHashMap的实现原理，就会发现其中就用到了双向链表这种数据结构。\n\n\n# 空间换时间or时间换空间\n\n这里用到了一个空间换时间的设计思想。\n\n当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或数据结构。相反，如果内存比较紧张，比如代码跑在手机或单片机上，这个时间，就要反过来用时间换空间的设计思路。\n\n还是开篇缓存的例子。缓存实际上就是利用了空间换时间的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。\n\n所以总结一下，对于执行较慢的程序，可以通过消耗更多的内存(空间换时间)来进行优化；而消耗过多的内存程序，可以通过消耗更多的时间(时间换空间)来降低内存的消耗。\n\n\n# 双向循环链表\n\n我们可以将循环链表和双向链表结合在一起，就得到了双向循环链表。\n\n可以参见如下的图示，实际上每个链表中的结点，都可以理解为一个结构体，这个结构体中存储了相应的数据域，还有相应的结构体类型的指针，分别可以指向前驱和后驱的结构体类型的结点。\n\n下面的图像中，第一个节点的前驱指向末尾节点的结构体内存地址的指针，而末尾节点的后继指向第一个节点的结构体内存地址的指针。\n\n\n\n\n# 数组和链表性能比较\n\n通过学习，可以知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。\n\n\n\n不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构存储数据。\n\n 1. 数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存不友好，没有办法有效预读。\n\n 2. 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致"内存不足(out of memory)"。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，这是它与数组最大的区别。\n    \n    上一节的学习中，我们知道了Java中ArrayList容器，也可以支持动态扩容。根据上一节的内容，我们知道，当我们往支持动态扩容的数组中插入一个数据的时候，如果数组中没有空闲空间时，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。\n\n 3. 除此之外，如果我们的代码对内存的使用非常苛刻，那就使用数组更合适。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是Java语言，就有可能导致频繁的GC(Garbage Collection，垃圾回收)\n\n\n# 解答开篇的问题\n\n如何基于链表实现LRU缓存淘汰算法？\n\n我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。\n\n 1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。\n 2. 如果此数据没有在缓存链表中，又可以分为两种情况：\n    * 如果此时缓存未满，则将此结点直接插入到链表的头部；\n    * 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。\n\n这样我们就用链表实现了一个LRU缓存。\n\n现在我们再来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为O(n)。\n\n实际上，我们可以继续优化这个实现思路，如果引入散列表(Hash table) 记录每个数据的位置，将缓存访问的时间复杂度降到O(1)。\n\n\n# 内容小结\n\n今天我们学习了一种跟数组"相反"的数据解雇，链表。它跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数据稍微复杂，从普通的单链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。\n\n和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。\n\n\n# 问题思考\n\n 1. C语言中指针占用几个字节\n    \n    指针即为地址，指针几个字节跟语言无关，而是跟系统的寻址能力有关。现在一般是32位系统，所以是4个字节，64为的系统，就是8个字节。可以通过如下的代码来查看运行环境中指针占多大字节。\n    \n    #include <stdio.h>   \n    int main(void)  \n    {  \n        int a=1;  \n        char b=\'a\';  \n        float c=1.0;  \n        void *p;  \n        p=&a;  \n        printf("a的地址为：0x%x，其字节数为：%d\\n",p,sizeof(p));  \n        p=&b;  \n        printf("b的地址为：0x%x，其字节数为：%d\\n",p,sizeof(p));  \n        p=&c;  \n        printf("c的地址为：0x%x，其字节数为：%d\\n",p,sizeof(p));  \n        return 0;  \n    } \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n    \n    参考如下的网上资源：\n    \n    > https://blog.csdn.net/IOSSHAN/article/details/88944637\n    > \n    > https://blog.csdn.net/koches/article/details/7627381#\n\n 2. 单链表中尾结点的后继指针问题\n    \n    在单链表中，尾结点的后继指针next指向的不再是下一个结点。而是指向一个空地址null。这样做的好处在于：防止尾结点的后继指针next成为一个野指针，导致遍历链表根本停不下来，或者出现一些本不属于该链表的垃圾数据。\n\n 3. 参考《大话数据结构》中的内容，对于头结点，头指针的区别和联系。\n    \n    参考URL：http://data.biancheng.net/view/103.html\n    \n    头结点：是放在第一个元素结点之前的一个节点，其数据域一般无意义(也可以存放链表的长度)。头结点可有可无。\n    \n    头指针：如果存在头节点，那么头指针则是指向头结点的指针。如果不存在头节点，那么头指针则是执行第一个结点元素的指针。头指针是链表必须存在的。\n    \n    \n\n 4. 单链表实现LRU，数组实现LRU\n    \n    单链表LRU：我们约定，越靠近链表尾部的结点是越早之前访问的(当然也可以是链尾是越后访问的)。当有一个新的数据被访问的时候，我们从链表头开始顺序遍历链表。\n    \n    * 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部；\n    * 如果此数据没有在缓存链表中，可以分为两种情况：如果此时缓存未满，则将此结点直接插入到链表的头部；\n    * 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。\n    \n    数组LRU：同样我们约定，越靠近数组尾部的元素是越迟访问的。当有一个新的数据被访问的时候，我们就从数组的第一个元素开始顺序遍历数组。\n    \n    * 如果此数据之前已经被缓存在数组中了，我们遍历得到这个数据对应的元素的位置，并将其从原来的位置删除，然后再插入到数组的尾部。\n    * 如果此数据没有在缓存的数组中，可以分为两种情况：如果此时缓存未满，则将此元素直接插入到数组的末尾；\n    * 如果此时缓存已满，则将数组的第一个元素删除，将新的元素插入到数组的尾部。\n\n 5. 判断一个字符串是否是回文字符串，这个字符串是通过单链表来存储的。\n    \n    使用快慢两个指针找到链表中点，\n    慢指针每次前进一步，快指针每次前进两步。\n    在慢指针前进的过程中，同时修改其next指针，\n    使得链表前半部分反序。\n    最后比较中点两侧的链表是否相等。\n    时间复杂度：O(n)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n\n# D25(2020/10/11)\n\n今天主要是要学习的是链表的下半部分，主要学习和描述的内容是如何轻松地写出正确的链表代码。\n\n要想写好链表代码并不是容易的事，尤其是那些复杂的链表操作，比如链表反转、有序链表合并等，写的时候非常容易出错。\n\n为什么链表代码这么难写？究竟怎样才能轻松地写出正确的链表代码呢？\n\n只要愿意投入时间，我觉得大多数人都是可以学会的。比如说，如果你真的能花上一个周末或者一整天的时间，就去写链表反转这一代码。\n\n当然，自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要一些方法和技巧。我根据自己的学习经历和工作经验，总结了几个写链表代码技巧。如果你能熟练掌握这几个技巧，加上自己的主动和坚持，轻松拿下链表代码完全没有问题。\n\n\n# 技巧一：理解指针或引用的含义\n\n事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以，要想写对链表代码，首先就要理解好指针。\n\n我们知道，有些语言有"指针"的概念，比如C语言；有些语言没有指针，取而代之的是"引用"，比如Java、Python。不管是"指针"还是"引用"，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。\n\n接下来，我会拿C语言中的"指针"来讲解，如果你用的是Java或者其他没有指针的语言也没有关系，你把它理解成"引用"就可以了。\n\n实际上，对应指针的理解，我们只需要记住下面这句话就可以了：\n\n将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。\n\n在编写链表代码的时候，我们经常会有这样的代码：p->next = q。这行代码是说，p结点中的next指针存储了q结点的内存地址。\n\n还有一个更复杂的，也是我们写链表代码经常会用到的：p->next = p->next->next . 这行代码表示，p结点的next指针存储了p结点的下下一个结点的内存地址。\n\n\n# 技巧二：警惕指针丢失和内存泄漏\n\n很多时候，在写链表代码的时候，很容易不知道指针指向哪里了。\n\n指针往往都是怎么弄丢的呢？下面看一个单链表插入的操作例子：\n\n\n\n如上图所示，我们希望在结点a和相邻的结点b之间插入结点x，假设当前指针p指向结点a。如果我们将代码实现变成下面的样子，就会发生指针丢失和内存泄漏.\n\np->next = x;   //将p的next指针指向x结点\nx->next = p->next; //将x的结点的next指针指向b\n\n\n1\n2\n\n\n在这里是错误的。p->next指针在完成第一步操作之后，已经不再指向结点b了，而是指向结点x。第2行代码相当于将x赋值给x->next，自己指向自己。因此，整个链表也就断成了两半，从结点b往后的所有结点都无法访问到了。(这里的意思是，由于指向性的丢失，结点b往后的所有结点都无法访问到了，也就是说无法去释放它们了。没有了指针，程序也不知道从哪个位置开始取释放内存空间)\n\n对于有些语言来说，比如C语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄漏。所以，我们插入结点时，一定要注意操作的顺序，要先将结点x的next指针指向结点b，再把结点a的next指针指向结点x，这样才不会丢失指针，导致内存泄漏。所以，对于刚刚的插入代码，我们只需要把第1行和第2行代码的顺序颠倒一下就可以了。\n\n同理，删除链表结点的时候，也一定要记得手动释放内存空间。否则，也会出现内存泄漏的问题。当然，对于像Java这种虚拟机自动管理内存的编程语言来说，就不需要考虑这么多了。\n\n\n# 技巧三：利用哨兵简化实现难度\n\n首先，我们先来回顾一下单链表的插入和删除操作。如果我们在结点p后面插入一个新的结点，只需要下面两行代码就可以搞定。\n\nnew_node->next = p->next; \n//原先p的后继指针里面的地址位置赋值给了新节点的后继指针里面\np->next = new_node;  \n//将新节点的地址值，赋给p的next后继指针\n\n\n1\n2\n3\n4\n\n\n但是，当我们要向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。我们需要进行下面这样的特殊处理，其中head表示链表的头结点。所以，从这段代码，我们可以发现，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不一样的。\n\nif (head == null) {\n    head = new_node;\n}\n\n\n1\n2\n3\n\n\n我们再来看单链表结点删除操作。如果要删除结点p的后继结点，我们只需要一行代码就可以了。\n\np->next = p->next->next;\n\n\n1\n\n\n但是，如果我们要删除链表中的最后一个结点，前面的删除代码就不能工作了。跟插入操作类似，我们也需要对于这种情况特殊处理。代码如下：\n\nif (head->next == null) {\n    head = null;\n}\n\n\n1\n2\n3\n\n\n从前面的一步一步分析来看，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。 这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。可以参考如下的方法来解决问题。\n\n可以利用哨兵，哨兵，解决的是国家之间的边界问题。同理，这里说的哨兵也是解决"边界问题"的，不直接参与业务逻辑。\n\n上面用head = null来表示链表中没有结点了。其中head表示头结点指针，指向链表中的第一个结点。\n\n如果我们引入哨兵结点，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。\n\n下面的图示是一个带头链表，我们可以发现，哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。\n\n\n\n实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。下面是用C写的例子。\n\n//在数组a中，查找key，返回key所在的位置\n// 其中，n表示数组a的长度\nint find(char* a, int n, char key) {\n\t//边界条件处理，如果a为空，或者n<=0，说明数组中没有数据，就不用while循环比较了\n\tif (a == null || n <=0) {\n\t\treturn -1;\n\t}\n\tint i = 0;\n\t// 这里有两个比较操作：i<n和a[i] == key.\n\twhile (i<n) {\n\t\tif (a[i] == key) {\n\t\t\treturn i;\n\t\t}\n\t\t++i;\n\t} \n\treturn -1;\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n代码二：\n\n// 在数组a中，查找key，返回key所在的位置\n// 其中，n表示数组a的长度\nint find(char* a, int n, char key) {\n\tif (a == null || n <=0) {\n\t\treturn -1;\n\t}\n\tif (a[n-1] == key) {\n\t\treturn n-1;\n\t}\n\tchar tmp = a[n-1];\n\ta[n-1] = key;\n\tint i = 0;\n\twhile (a[i] != key) {\n\t\t++i;\n\t}\n\ta[n-1] = tmp;\n\tif (i == n-1) {\n\t\treturn -1;\n\t} else {\n\t\treturn i;\n\t}\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n对比两段代码，在字符串a很长的时候，比如几万、几十万，哪段代码运行的更快点呢？答案是代码二，因为两段代码中执行次数最多就是while循环那一部分。第二段代码中，我们通过一个哨兵a[n-1] = key，成功省掉了一个比较语句i<n(i<n在while循环中存在的意义是，帮助控制循环的次数，不要无限循环下去。而这里设置了a[n-1]=key，就保证了while循环会在最后一个数组元素后退出了)。不要小看这一条语句，当累积执行万次、几十万次时，累积的时间就很明显了。\n\n当然，这只是为了举例说明哨兵的作用，我们写代码的时候千万不要写第二段那样的代码，因为可读性太差了。大部分情况下，我们并不需要如此追求极致性能。\n\n\n# 技巧四：重点留意边界条件处理\n\n软件开发中，代码在一些边界或异常的情况下，最容易产生Bug。链表代码也不例外。要想实现没有Bug的链表代码，一定要在编写的过程中以及编写完成之后，检查边界条件是否考虑全面，以及代码在边界条件下是否能正确运行。\n\n我经常用来检查链表代码是否正确的边界条件有如下几个：\n\n * 如果链表为空时，代码是否能正常工作？\n * 如果链表只包含一个结点的时候，代码是否能正常工作？\n * 如果链表只包含两个结点的时候，代码是否能正常工作？\n * 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？\n\n当我们写完链表代码之后，除了看下我们写的代码在正常的情况下能否工作，还要看下在上面我列出的几个边界条件下，代码仍然能否正确工作。如果这些边界条件下都没有问题，那基本上可以认为没有问题了。\n\n\n# 技巧五：举例画图，辅助思考\n\n对于稍微复杂的链表操作，我们可以使用举例法和画图法。\n\n我们可以找一个具体的例子，把它画在纸上，释放一些脑容量，留更多的给逻辑思考，这样就会感觉到思路清晰很多。比如往单链表中插入一个数据这样一个操作，我一般都是把各种情况都举一个例子，画出插入前和插入后的链表变化。\n\n\n# 技巧六：多写多练，没有捷径\n\n精选了5个常见的链表操作。我们需要把这几个操作都能写熟练，不熟就多写几遍。\n\n * 单链表反转\n * 链表中环的检测\n * 两个有序的链表合并\n * 删除链表倒数第n个结点\n * 求链表的中间结点\n\n\n# 内容小结\n\n这里主要罗列了写链表代码的六个技巧。分别是理解指针或引用的含义、警惕指针丢失和内存泄漏、利用哨兵简化实现难度、重点留意边界条件处理，以及举例画图、辅助思考，还有多写多练。\n\n写链表代码是最考验逻辑思维能力的。\n\n\n# 问题思考\n\n 1. C语言中，内存泄漏的问题\n    \n    对于c语言而言，删除链表的结点时，也一定要手动释放结点所占用的内存空间，否则也会容易产生内存泄漏的问题。\n\n 2. 如何获取结构体的首地址\n    \n    参考如下的代码：\n    \n    #include<stdio.h>\n    typedef struct\n    {\n     int a;\n     int b;\n    }T;\n    int  main()\n    {\n    \tT t;\n    \tvoid *p1;\n    \tvoid *p2;\n    \tp1 = &t;\n    \tp2 = &t.a;\n    \tprintf("p1 = %p, p2 = %p\\n",p1,p2);\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n\n# D26(2020/10/12)\n\n今天主要是要学习的是栈相关方面的内容。\n\n这里我先引出一个浏览器中前进和后退的例子。\n\n当我们依次访问完一串页面a-b-c之后，点击浏览器的后退按钮，既可以看到之前浏览过的页面b和a。当我们后退到页面a，点击前进按钮，就可以重新查看页面b和c。但是，如果我们后退到了页面b之后，点击了新的页面d了，那就无法再通过前进、后退功能查看页面c了。\n\n\n# 如何理解"栈"？\n\n关于"栈"，有个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个个放；取的时候，我们也是从上往下一个个的依次取，不能从中间任意抽出。后进者先出，新进者后出，这就是典型的"栈"结构 。\n\n从栈的操作特性来看，栈是一种"操作受限"的线性表，只允许在一端插入和删除数据。\n\n那对于这种栈的结构，存在的意义是什么呢？相比于数组和链表，栈带给我们的好像只有限制，并没有任何优势。我们可以直接使用数组或链表来替代栈，我们为什么还要用这个"操作受限"的栈呢？\n\n事实上，从功能上来说，数组或链表确实可以替代栈，但我们需要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就跟容易出错。\n\n当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选"栈"这种数据结构。\n\n\n# 如何实现一个"栈"？\n\n从刚才栈的定义里面，我们可以看出，栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。\n\n实际上，栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫做顺序栈，用链表来实现的栈，我们叫做链式栈。\n\n下面的代码是用java写的用数组实现的顺序栈\n\n\n\n\n1\n\n\n了解了定义和基本操作，那它的操作的时间、空间复杂度是多少呢？\n\n不管是顺序栈还是链式栈，我们存储数据只需要一个大小为n 的数组够了。在入栈和出栈的过程中，只需要一两个临时变量存储空间，所以空间复杂度是O(1).\n\n注意，这里存储数据需要一个大小为n的数组，并不是说空间复杂度就是O(n)。因为，这n个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。\n\n不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是O(1)。\n\n\n# 支持动态扩容的顺序栈\n\n刚才那个基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈的大小不受限，但要存储next指针，内存消耗相对较多。那我们如何基于数组实现一个可以支持动态扩容的栈呢？\n\n我们在数组那一节描述过，当数组空间不够的时候，我们就重新申请一块更大的内存，将原来数组中数据统统拷贝过去。这样就实现了一个支持动态扩容的数组。\n\n所以，如果要实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。如下图所示：\n\n\n\n实际上，支持动态扩容的顺序栈，在平时的开发中并不常见。我们可以根据这个场景，复习一下复杂度的分析。\n\n对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是O(1)。但是，对于入栈操作来说，情况就不一样了。当栈中有空闲空间的时候，入栈的操作的时间复杂度是O(1)。但是当空间不够的时候，就需要重新申请内存和数据搬移，所以时间复杂度就变成了O(n)。\n\n也就是说，对于入栈操作来说，最好情况时间复杂度是O(1)，最坏情况的时间复杂度是O(n)。那平均情况下的时间复杂度又是多少呢？这个入栈操作的平均情况下的时间复杂度可用用摊还分析法来分析。\n\n为了分析的方便，我们需要事先做一些假设和定义：\n\n * 栈空间不够的时候，我们重新申请一个是原来大小两倍的数组；\n * 为了简化分析，假设只有入栈操作没有出栈操作；\n * 定义不涉及内存搬移的入栈操作为simple-push操作，时间复杂度为O(1)。\n\n如果当前栈大小为k，并且已满，当再有新的数据要入栈的时候，就需要重新申请2倍大小的内存，并且做K个数据的搬移操作，然后再入栈。但是，接下来的K-1次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这K-1次入栈操作都只需要一个simple-push操作就可以完成。如下图所示：\n\n\n\n我们可以看出，这K次入栈操作，总共涉及了K个数据的搬移，以及K次simple-push操作。将K个数据搬移均摊到K次入栈操作，那每个入栈操作只需要一个数据搬移和一个simple-push操作。以此类推，入栈操作的均摊时间复杂度是O(1)。\n\n从这个例子的分析中，可以看出，均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分的情况下，入栈操作的时间复杂度O都是O(1)，只有在个别的时刻才会退化为O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况系的耗时就接近O(1)。\n\n\n# 栈在函数调用时的应用\n\n栈作为一个比较基础的数据结构，经典的一个应用场景就是函数调用栈。\n\n操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成"栈"这种结构，用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。\n\n可以查看下如下的代码：\n\n# include <stdio.h>\nint add(int x, int y) {\n\tint sum = 0;\n\tsum = x + y;\n\treturn sum;\n}\n\nint main() {\n\tint a = 1;\n\tint ret = 0;\n\tint res = 0;\n\tret = add( 3, 5);\n\tres = a + ret;\n\tprintf("%d", res);\n\treturn 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n从代码中我们可以看出，main()函数调用了add()函数，获取计算结果，并且与临时变量a相加，最后打印res的值。下面的图示中展示的，对应的函数栈里出栈、入栈的操作。\n\n\n\n\n# 栈在表达式求值中的应用\n\n这里展示的是栈的另一个常见的应用场景，编译器如何利用栈来实现表达式求值。\n\n为了方便解释，我将算术表达式简化为只包含加减乘除四则运算，比如：34+13*9+44 - 12/3。\n\n对于计算机来说，编辑器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。\n\n如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或相同，从运算符栈中取栈顶运算符，从操作数的栈顶取2个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。\n\n下面是将3+ 5*8 -6 这个表达式的计算过程画成了一张图\n\n\n\n\n# 栈在括号匹配中的应用\n\n除了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。\n\n我们同样简化一下背景。我们假设表达式中只包含三种括号，圆括号()、方括号[]和花括号{}，并且它们可以任意嵌套。比如，{[{}]} 或者是[{()}([])]等都是合法格式，而{[}()] 或[({)] 为不合法的格式。如果现在有一个包含三种括号的表达式字符串，如何检查它是否合法呢？\n\n这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号的时候，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如"("跟")"匹配，"["跟"]"匹配，"{"跟"}"匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。\n\n当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。\n\n>  1. 扫描到左括号时，将其压入栈。\n>  2. 当扫描到右括号时，从栈顶取出数据。\n>  3. 如果能够匹配，则继续扫描；反之，就是非法。\n>  4. 最后假如栈中没有数据，表示格式非法。\n\n\n# 解答开篇\n\n我们再来看看开篇的思考题，如何实现浏览器的前进、后退功能？其实，用两个栈就可以非常完美地解决这个问题。\n\n我们使用两个栈，X和Y，我们把首次浏览的页面依次压入栈X，当点击后退按钮的时候，再依次从栈X中出栈，并将出栈的数据依次放入栈Y。当我们点击前进按钮的时候，我们依次从栈Y中取出数据，放入栈X中。当栈X中没有数据的时候，那就说明没有页面可以继续后退浏览量。当栈Y中没有数据，那就说明没有页面可以点击前进按钮浏览了。\n\n> X栈是用来实现后退功能的，但是每后退依次，从X栈中取出一个数据，需要放入到Y栈中。\n> \n> 当需要实现前进功能的时候，就依次从Y栈中取出一个数据。同时将这个数据压如栈X中。\n> \n> 每浏览一个页面就将其数据压入栈X，同时清理Y栈。\n> \n> 需要注意的是，如果是访问新的页面的时候，就需要清空Y栈，不管是刚开始登录页面，还是回退到某个页面后点击的新页面，这两个时候都不能使用Y栈的数据了，也就是说，不能进行前进的操作了。\n\n具体我们可以参照下面的图示来解决问题：\n\n比如我们顺序查看了a,b,c三个页面，我们就依次把a,b,c压入栈，这个时候，两个栈的数据就是下面的样子。\n\n\n\n当我们通过浏览器的后退按钮，从页面C后退到页面a之后，我们就依次把c和b从栈X中弹出，并且依次放入到栈Y。这个时候，两个栈的数据就是下面的样子:\n\n\n\n这个时候我们又想看页面b，于是我们又点击前面按钮回到b 页面，我们就把b再从栈Y中出栈，放入栈X中。此时两个栈的数据是下面的样子了：\n\n\n\n这个时候，我们通过页面b又跳转到新的压面d上了，页面c就无法再通过前进、后退按钮重复查看了，所以需要清空栈Y。此时两个栈的数据如下样子了：\n\n\n\n\n# 内容小结\n\n栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都是O(1)。\n\n\n# 问题思考\n\n 1. 为什么函数调用要用"栈"来保存临时变量？用其他数据结构不行吗？\n    \n    其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。\n    \n    从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一次新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。\n\n 2. 内存管理上的"栈"和数据结构上的"栈"到底是不是一回事。\n    \n    不是。\n\n\n# D27(2020/10/13)\n\n今天主要是要学习的是队列方面的内容。队列在线程池等有限资源池中的应用。\n\n我们知道，CPU资源是有限的，任务的处理速度与线程个数并不是线程正相关的。(不是说，线程个数越多，任务处理的速度越快，当线程很多很多事，频繁的CPU上下文切换，也会导致性能处理下降) 相反，过多的线程反而会导致CPU频繁切换，处理性能下降。\n\n所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。\n\n当我们向固定大小的线程池中请求一个线程的时候，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求呢？是拒绝请求还是排队请求？各种处理策略又是怎么实现的？\n\n实际上，这些问题并不复杂，其底层的数据结构就是我们今天要学习的内容，队列(queue)。\n\n\n# 如何理解"队列"？\n\n队列这个概念非常好理解。可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的"队列"\n\n我们知道，栈只支持两个基本操作：入栈push()和出栈pop()。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队enqueue()，放一个数据到队列尾部；出队dequeue()，从队列头部取一个元素。\n\n\n\n所以，队列跟栈一样，也是一种操作受限的线性表数据结构\n\n队列的概念很好理解，基本操作也是很容易掌握的。作为一种非常基础的数据结构，队列的应用也是非常广泛，特别是一些具有额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。 比如高性能队列Disruptor、Linux环形缓存，都用到了循环并发队列；Java concurrent并发包利用ArrayBlockingQueue来实现公平锁等。\n\n\n# 顺序队列和链式队列\n\n如何实现一个队列呢？\n\n跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫做顺序栈，用链表实现的栈叫做链式栈。同样，用数组实现的队列叫做顺序队列，用链表实现的队列叫做链式队列。\n\n下面的是用Java代码试实现的顺序队列的内容：\n\n\n\n\n1\n\n\n相对于栈来说，我们只需要一个栈顶指针就可以了。但是队列需要两个指针：一个是head指针，指向队头；一个是tail指针，指向队尾。\n\n从下面的图示来看。当a、b、c、d依次入队之后，队列中的head指针指向下标为0的位置，tail指针指向下标为4的位置。\n\n\n\n当我们调用两次出队操作之后，队列中head指针指向下标为2的位置，tail指针仍然指向下标为4的位置。\n\n我们会发现你，随着不停地进入入队、出队操作，head和tail都会持续往后移动。当tail移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。这个问题该如何解决？(这是由于只有队头会删除元素，队尾只会增加元素的特性导致的)\n\n我们还记得，在数组那一节，也遇到类似的问题，就是数组的删除操作会导致数组中的数据不连续。我们当时只能使用数据搬移来解决这个问题。但是，每次进行出队操作都相当于删除数组下标为0的数据，要搬移整个队列中的数据，这样出队操作的时间复杂度就会从原来的O(1)变成了O(n)。\n\n实际上，我们在出队列的时候可以不用搬移数据。如果没有空闲空间了，我们只需要在入队的时候，再集中触发一次数据的搬移操作。借助这个思想，出队函数dequeue()保持不变，我们稍加改造一下入队函数enqueue()的实现，就可以轻松解决刚才的问题了。\n\n\n\n\n1\n\n\n从代码中我们看到，当队列的tail指针移动到数组的最右边后，如果有新的数据入队，我们可以将head到tail之间的数据，整体搬移到数组中0到tail-head的位置。\n\n\n\n这种实现的思路中，出队操作的时间复杂度仍然是O(1)，但入队操作的时间复杂度还是O(1)吗？ 出队的情况不考虑，不管是什么情况下的，它的时间复杂度都是O(1)。入队的操作中，最好情况下，这个队列先是空的，有n个位置，可以入队n-1个元素，这个时间复杂度是O(1)。最后当tail == n的时候，就需要移动一次了，最差情况下，需要移动n-1个元素的操作，也是就是O(n)的复杂度了，大致上好像可以理解为摊还分析法来看是O(1)。\n\n\n# 链表队列的实现\n\n我们再来看下链表队列的实现方法。\n\n基于链表的实现，我们同样需要两个指针：head指针和tail指针。它们分别指向链表的第一个结点和最后一个结点。如下图所示，入队时，tail->next = new_node，tail = tail->next；出队的时候，head = head->next。\n\n\n\n\n# 循环队列\n\n我们刚才用数组来实现队列的时候，在tail == n时，会有数据搬移操作，这样入队操作性能就会受到影响。为了避免数据搬移，这里引出了循环队列的解决思路。\n\n循环队列，顾名思义，它长得像一个环。原本数组是有头有尾的，是一条直线。我们可以把首尾相连，扮成一个环。\n\n\n\n我们可以看到，图中这个队列的大小为8，当前head=4，tail=7。当有一个新的元素a入队的时候，我们放入下标为7的位置。但这个时候，我们并不把tail更新为8，而是将其在环中后移一位，到下标为0的位置。当再有一个元素b入队的时候，我们将b放入下标为0的位置，然后tail加1更新为1。所以，在a,b依次入队之后，循环队列中的元素就变成了下面的样子。\n\n\n\n我们通过这样的方法，就成功避免了数据搬移的操作。看起来不难理解，但是循环队列的代码实现难度要比前面的非循环队列难多了。要想写出没有bug的循环队列的实现代码，个人觉得，最关键的是，确定好队空和队满的判定条件。\n\n在用数组实现的非循环队列中，队满的判断条件是tail == n，队空的判断条件是head == tail。那针对循环队列，如何判断队空和队满呢？\n\n队列为空的判断条件仍然是head == tail。但队列满的判断条件就稍微有些复杂了。可以看如下图示：\n\n\n\n就像图中画的队满的情况，tail=3，head=4，n=8的时候，可以总结一下规律如下：(3+1)%8 = 4。多判断几种情况下，我们就会发现，当队满的时候，(tail+1)%n = head.\n\n我们可以发现，当队列满的时候，图中的tail指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。\n\n可以查看如下的实现代码：\n\n\n\n\n1\n\n\n\n# 阻塞队列和并发队列\n\n队列这种数据结构很基础，平时的业务开发不太可能从零实现一个队列，设置都不会直接用到。而一些具有特殊特性的队列应用却比较广泛，比如阻塞队列和并发队列。\n\n阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。 简单来说，会在队空和队满的时候，阻塞操作。\n\n\n\n我们可以看到，上述的定义就是一个"生产者-消费者模型" 。的确，我们可以使用阻塞队列，轻松实现一个"生产者-消费者模型"！\n\n这种基于阻塞队列实现的"生产者-消费者模型"，可以有效地协调生产和消费的速度。当"生产者"生产数据的速度过快，"消费者"来不及消费的时候，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到"消费者"消费了数据，"生产者"才会被唤醒继续"生产"。\n\n而且不仅如此，基于阻塞队列，我们还可以通过协调"生产者"和"消费者"的个数，来提高数据的处理效率。基于上面的例子，我们可以多配置几个"消费者"，来应对一个"生产者"。\n\n\n\n前面讲到的阻塞队列，在多线程的情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？\n\n线程安全的队列，我们叫做并发队列。最简单直接的实现方式是直接在enqueue()、dequeue()方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存活取操作。实际上，基于数组的循环队列，利用CAS原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。\n\n\n# 解答开篇\n\n这里回到了开篇的问题。线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？\n\n我们一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。那如何存储排队的请求呢？\n\n我们希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。我们前面说过，队列有基于链表和基于数组这两种实现方式。这两种实现方式对于排队请求又有什么区别呢?\n\n基于链表的实现方式，可以实现一个支持无限排队的无界队列(unbounded queue)，但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。\n\n而基于数组实现的有界队列(bounded queue)，队列的大小有限，所以线程池中排队的请求超过队列大小的时候，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。\n\n除了前面所说的队列应用在线程池请求排队的场景之外，队列可以应用在任何有限资源池中，队列还可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源的时候，基本上都可以通过"队列"这种数据结构来实现请求排队。\n\n\n# 内容小结\n\n今天我们学习了一种和栈很相似的数据结构，队列。\n\n队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。特别是长得像一个环的循环队列。在数组实现队列的时候，会有数据搬移操作，要想聚集数据搬移的问题，我们就需要像环一样的循环队列。\n\n循环队列是重点，关键是要确定好队空和队满的判定条件，具体的代码需要能写出来。\n\n除此之外，我们还讲了几种高级的队列结构，阻塞队列、并发队列，底层都还是队列这种数据结构，只不过在之上附加了很多其他功能。阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全。\n\n\n# D28(2020/10/14)\n\n今天主要学习的是递归方面的内容。提出的问题是，如何用三行代码找到"最终推荐人"。\n\n\n# 如何理解"递归"？\n\n从学习数据结构和算法的经历来看，有个最难理解的知识点，一个是动态规划，另一个就是递归。\n\n递归是一种应用非常广泛的算法(或者编程技巧)。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如DFS深度优先搜索、前中后序二叉树遍历等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。\n\n这就是一个非常标准的递归求解问题的分解过程，去的过程叫"递"，回来的过程叫"归"。基本上，所有的递归问题都可以用递归公式来表示。上面的那个例子中，我们用递归公式来表示出来就这样：\n\nf(n)=f(n-1)+1 其中，f(1)=1\n\nf(n)表示我们想知道自己在哪一排，f(n-1)表示前面一排所在的排数，f(1)=1表示第一排的人知道自己在第一排。有了这个递推公式，我们可以将它改为递归代码。\n\nint f(int n){\n   if (n==1) return 1;\n   return f(n-1) + 1;\n}\n\n\n1\n2\n3\n4\n\n\n\n# 递归需要满足的三个条件\n\n什么样的问题可以用递归来解决呢？总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决。\n\n 1. 一个问题的解可以分解为几个子问题的解\n    \n    何为子问题？子问题就是数据规模更小的问题。比如，前面讲的电影院的例子，需要知道，"自己在哪一排"的问题，可以分解为"前一排的人在哪一排"这样一个子问题。\n\n 2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样\n    \n    比如电影院的那个例子，在求解"自己在哪一排"的思路，和前面一排人求解"自己在哪一排"的思路，是一模一样的。\n\n 3. 存在递归终止条件。\n    \n    把问题分解为子问题，把子问题再分解为子字问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。\n    \n    还是电影院的例子，第一排的人不需要再继续询问任何人，就知道自己在哪一排，也就是 f(1) =1, 这就是递归的终止条件。\n\n\n# 如何编写递归代码？\n\n写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。\n\n假如这里有n个台阶，每次你可以跨1个台阶或2个台阶，请问走这n个台阶有多少种走法？如果有7个台阶，你可以2，2，2，2，1这样子上去，也可以1，2，1，1，2这样子上去，总之走法有很多，如何用编程求得总共有多少种走法呢？\n\n我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了1个台阶，另一类是第一步走了2个台阶。所以n个台阶的走法就可简化为，先走了1个台阶后，n-1个台阶的走法，然后再加上，先走了2阶后，n-2个台阶的走法。用公式表示就是：\n\nf(n) = f(n-1) + f(n-2)\n\n有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以f(1)=1。这个递归终止条件足够吗？我们可以用n=2, n=3这样比较小的数试验一下。\n\nn=2时，f(2)=f(1)+f(0)。如果递归终止条件只有一个f(1)=1，那f(2)就无法求解了。所以除了f(1)=1这一个递归终止条件外，还要有f(0)=1，表示走0个台阶有一种走法，不过这样子看起来就不符合正常的逻辑思维了。\n\n所以，我们可以把f(2)=2作为一种终止条件，表示走2个台阶，有两种走法，一步走完或分两步来走。\n\n所以，递归终止条件就是f(1)=1, f(2)=2。这个时候，你可以再拿n=3，n=4来验证一下，这个终止条件是否足够并且正确。\n\n我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：\n\nf(1) = 1;\nf(2) =2;\nf(n) = f(n-1) + f(n-2)\n\n\n1\n2\n3\n\n\n有了这个公式，我们转化成递归代码就简单多了。最终的递归代码是这样的：\n\nint f(int n) {\n    if (n==1) return 1;\n    if (n==2) return 2;\n    return f(n-1) + f(n-2);\n}\n\n\n1\n2\n3\n4\n5\n\n\n总结一下，写递归代码的关键是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。\n\n刚才讲的电影院的例子，我们的递归调用只有一个分支，也就说"一个问题只需要分解为一个子问题"，我们很容易能够想清楚"递"和"归"的每一个步骤，所以写起来、理解起来都不难。\n\n但是，当我们面对的是一个问题要分解为多个子问题的情况，递归代码没那么好理解了。\n\n在第二个例子中，人脑几乎没有办法把整个"递"和"归"的过程一步步都想清楚。\n\n计算机擅长做重复的事情，所以递归正合它的胃口。而我们人脑更喜欢平铺直叙的思维方式。当我们看到递归的时候，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。\n\n对于递归代码，这种试图想清楚整个递归过程的做法，实际上是进入了一个思维误区。很多时候，我们理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。那正确的思维方式应该是怎样的呢？\n\n如果一个问题A可以分解为若干个子问题B、C、D，你可以假设子问题B、C、D已经解决，在此基础上思考如何解决问题A。而且，你只需要思考问题A与子问题B、C、D两层之间的关系即可，不需要一层层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。\n\n因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。\n\n\n# 递归代码要警惕堆栈溢出\n\n在实际的软件开发中，编写递归代码时，我们会遇到很多问题，比如堆栈溢出。而堆栈溢出会造成系统性崩溃，后果非常严重。为什么递归代码容易造成堆栈溢出呢？我们又该如何预防堆栈溢出呢？\n\n我们在"栈"那一节讲过，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会堆栈溢出的风险。\n\n如何避免堆栈溢出呢？\n\n我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度之后，我们就不继续往下再递归了，直接返回报错。下面是根据例子，写的伪代码，为了代码简洁，有些边界条件没有考虑，比如x <=0.\n\n// 全局变量，表示递归的深度\nint depth = 0;\n\nint f(int n) {\n\t++depth;\n\tif (depth > 1000) throw exception;\n\t\n\tif (n==1) return 1;\n\treturn f(n-1) + 1;\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n但是这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最大深度比较小，比如10，50，就可以用这种方法，否则这种方法并不是很实用。\n\n\n# 递归代码要警惕重复计算\n\n除此之外，使用递归时还会出现重复计算的问题。从刚才看到的第二个递归代码的例子，如果我们把整个递归过程分解一下的话，那就是这样的：\n\n从图中，可以直观看出，想要计算 f(5)，需要先计算f(4)和f(3)，而计算f(4)的时候还需要再次计算 f(3)，因此，f(3)就被计算了很多次，这就是重复计算问题。\n\n为了避免重复计算，我们可以通过一个数据结构(比如散列表)来保存已经求解过的f(k)。当递归调用到f(k)时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。\n\n/**\n * recursion\n */\npublic class recursion {\n\n    public int f(int n) {\n        if (n==1) return 1;\n        if (n==1) return 2;\n    }\n\n    // hasSolvedList可以理解为一个Map，key是n，value是f(n)\n    if (hasSolvedList.containsKey(n)) {\n        return hasSolvedList.get(n);\n    }\n\n    int ret = f(n-1) + f(n-2);\n    hasSolvedList.put(n, ret);\n    return ret;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n除了堆栈溢出、重复计算这两个常见的问题。递归代码还有很多别的问题。\n\n在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大的时候，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度的时候，需要额外考虑这部分的开销，比如我们前面讲的电影院递归代码，空间复杂度并不是O(1)，而是O(n)。\n\n\n# 怎么将递归代码改写为非递归代码？\n\n我们刚才说到，递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；而弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多等问题。所以，在开发过程中，我们要根据实际情况来选择是否需要用递归的方式来实现。\n\n我们可以将电影院的例子，将f(x) = f(x-1)+1这个递推公式，进行改写。\n\nint f(int n) {\n\tint ret = 1;\n\tfor (int i=2; i<=n; ++i) {\n\t\tret = ret + 1;\n\t}\n\treturn ret;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n同样的，第二个例子也可以改为非递归的实现方式。\n\nint f(int n) {\n\tif (n==1) return 1;\n\tif (n==2) return 2;\n\t\n\tint ret = 0;\n\tint pre = 2;\n\tint prepre = 1;\n\tfor (int i=3; i<=n; ++i) {\n\t\tret = pre + prepre;\n\t\tprepre = pre;\n\t\tpre = ret;\n\t}\n\treturn ret;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n笼统地讲，我们可以将递归代码改写为 迭代循环的非递归写法。因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。\n\n\n# 解答开篇\n\n解决下开篇的问题：如何找到"最终推荐人"？\n\nlong findRootRefrrerId(long actorId) {\n\tLong referrerId = select referrer_id from [table] where actor_id = actorId;\n\tif (referrerId == null) return actorId;\n\treturn findRootReferrerId(referrerId);\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# D29(2020/10/15)\n\n今天主要是学习的是排序中的第一个章节的内容，提出的问题是，"为什么插入排序比冒泡排序更受欢迎？"\n\n我们学习的第一个算法中，可能就是排序。大部分的编程语言中，也都提供了排序函数。在平常的项目中，我们也经常会用到排序，排序非常重要，我们会多花一点时间来学习。\n\n排序算法太多了，很多连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。这里只是研究众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。\n\n在这里我们按照时间复杂度把它们分成了三类，分三次来讲解。\n\n\n\n带着问题去学习，是最有效的学习方法。所以按照惯例，我们还是先提出一个思考题：插入排序和冒泡排序的时间复杂度相同，都是O($n^2$)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？\n\n\n# 如何分析一个"排序算法"？\n\n学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。那分析一个排序算法，要从哪几个方面入手呢？\n\n\n# 排序算法的执行效率\n\n对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：\n\n 1. 最好情况、最坏情况、平均情况时间复杂度\n    \n    我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。\n    \n    除此之外，我们还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。\n    \n    为什么要区分这三种时间复杂度呢？第一，有些排序算法要进行区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。\n\n 2. 时间复杂度的系数、常数、低阶\n    \n    我们知道，时间复杂度反映的是数据规模n 很大的时候的一整个增长趋势，所以它表示的时候会忽略系数、常数、低阶。\n    \n    但是实际的软件开发中，我们排序的可能是10个、100个、1000个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。\n\n 3. 比较次数和交换(或移动)次数\n    \n    这一节和下一节讲的基于比较的排序算法。基于比较的排序算法的执行过程，会涉及到两种操作，一种是元素比较大小，另一种是元素交换或移动。\n    \n    所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换(或移动)次数也考虑进去。\n\n\n# 排序算法的内存消耗\n\n我们前面也讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序(sort in place)。 原地排序算法，就是特指空间复杂度是O(1)的排序算法。今天所描述的三种排序算法，都是原地排序算法。\n\n\n# 排序算法的稳定性\n\n仅仅用执行效率和内存消耗来衡量算法的好坏是不够的的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。\n\n通过一个例子来解释。比如我们有一组数据2，9，3，4，8，3，按照大小排序之后就是2，3，3，4，8，9.\n\n这组数据里有两个3。经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫做稳定的排序算法；如果前后顺序发生了变化，那对应的排序算法就叫做不稳定的排序算法。\n\n两个3，谁在前，谁在后，有什么关系呢？稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？\n\n很多数据结构和算法课程，在讲排序的时候，都是用整数来举例，但是在真正的软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。\n\n比如说，我们现在要给电商交易系统中的"订单"排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有10万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，如何来做？\n\n最先想到的办法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间来排序。这种排序思路理解起来不难，但是实现起来会很复杂。\n\n如果我们借助于稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意这里是按照下单时间，而非金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？\n\n稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的顶你单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同的金额的订单仍然保持下单时间从早到晚有序。\n\n\n\n\n# 冒泡排序\n\n从冒泡排序开始，学习今天的三种排序算法。\n\n冒泡排序只会操作相邻的两个数据。每次冒泡排序都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序工作。\n\n从下面的例子来看下整个冒泡排序的整个过程。我们要对一组数据45，6，3，2，1 从小到大进行排序。第一次冒泡操作的详细过程如下：\n\n\n\n可以看出，经过一次冒泡排序之后，6这个元素已经存储在正确的位置上。要想完成所有数据的排序，我们只需要进行6次这样的冒泡操作就行了。\n\n\n\n实际上，刚才的冒泡过程还是可以优化的。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。参照下面的例子来看，这里面有6个元素排序，只需要4次冒泡操作就可以了。\n\n\n\n冒泡排序算法的原理比较容易理解，代码如下：\n\n\n\n\n1\n\n\n结合代码，我们可以思考三个问题。\n\n\n# 第一，冒泡排序是原地排序算法吗？\n\n冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为O(1)，是一个原地排序算法。\n\n\n# 第二，冒泡排序是稳定的排序算法吗？\n\n在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。\n\n\n# 第三，冒泡排序的时间复杂度是多少？\n\n最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行n次冒泡操作，所以最坏情况时间复杂度是O($n^2$) 。\n\n最好情况时间复杂度(在定义第1趟排序中，对n个元素，进行n-1次的比较。发现都不满足于前面数组元素要大于紧跟着的相邻的数组元素，那么始终都无法将flag调整为true，进行外层的for循环的第二趟的比较，就直接跳出了整个两层for循环)，结果只有内层for循环的n-1的比较判断操作，直接复杂度是O(n)。\n\n最坏情况时间复杂度，就是整个数据都是正好从大到小的，里面的for循环，和外面的for循环，总共共执行时间复杂度是O($n^2$) 。\n\n\n\n最好、最坏情况下的时间复杂度很容易分析，那平均情况下的时间复杂度是多少呢？我们前面讲过，平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。\n\n对于包含n个数据的数组，这n个数据就有n!种排列方式。不同的排列方式，冒泡排序执行的时间肯定时不同的。比如我们刚才举的例子中，其中一个要进行6次冒泡，而另一个只需要4次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。\n\n这里还有一个思路，通过"有序度"和"逆序度"这两个概念来进行分析。\n\n有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式就是这样：\n\n如果i < j，有序元素对：a[i] < = a[j]，就是一个有序元素对\n\n\n1\n\n\n\n\n同理，对于一个倒序排序的数组，如6,5,4,3,2,1， 其有序度是0；对于一个完全有序的数组，比如1，2，3，4，5，6，有序度就是n*(n-1) /2 ，也就是15.我们把这种完全有序的数组的有序度叫做满有序度。\n\n> 第一种理解：假设一个完全有序的数组长度是N，则第1项与其后面的各项有N-1个组合，第2项与其后面各项有N-2个组合，第3项与其后面各项有N-3个组合，第N-1项与其后面各项有1个组合，第N项与其后面各项有0个组合。所以这个公式为：(N-1)+(N-2)+(N-3)+(N-4)+....+(N-1)+0 = (首项+末项)*项数 / 2 ，所以就等于 (N-1 + 0)*N / 2\n> \n> 第二种理解：从N个元素中任意抽取2个元素，其形成的就是一个有序对，我们用Cn 2来做，怎么理解呢？ C这种排列，本身对于取的数的先后顺序是不做考虑的(取的两个数，先取哪个，后取哪个，这是一个方法)，而这里恰巧数据就是有序的，有序的意思，就是不用考虑两个数的先后顺序了。符合C排列的概念。Cn 2 = n!/(2! (n-2)!) = (n-1)n / 2\n\n逆序度的定义正好跟有序度相反(默认情况下，我们称数据从小到大为有序)，我们应该可以想到了。\n\n逆序元素对：a[i] > a[j]， 如果i < j。\n\n\n1\n\n\n关于这三个概念，我们还可以得到一个公式：逆序度 = 满有序度 - 有序度 。 我们排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序读，就说明排序完成了。\n\n我还是拿前面举的那个冒泡排序的例子来说明。要排序的数组的初始状态是4，5，6，3，2，1。其中，有序元素对有(4,5) (4,6) (5,6)，所以有序度是3。n=6，所以排序完成之后终究的满有序度为 n*(n-1) /2 = 15.\n\n\n\n冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加1。不管算法怎么改进，交换次数总是确定的，即为逆序度 ， 也就是 n*(n-1)/2 - 初始有序度。此例子中就是15-3 =12，要进行12次交换操作。\n\n对于包含n个数据的数组进行冒泡排序，平均交换次数是多少呢？ 最坏情况下，初始状态的有序度是 0，所以要进行n*(n-1) /2次交换。最好情况下，初始状态的有序度是n*(n-1)/2，就不需要进行交换。我们可以取个中间值n*(n-1)/4， 来表示初始有序度既不是很高也不是很低的平均情况。\n\n换句话说，平均情况下，需要n*(n-1)/4次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是O($n^2$) , 所以平均情况下的时间复杂度就是O($n^2$) .\n\n这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。等我们讲到快排的时候，还会再次用这种"不严格"的方法来分析平均时间复杂度。\n\n\n# 插入排序\n\n我们来看一个问题。一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。\n\n\n\n这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。\n\n\n# 插入排序的实现\n\n插入排序具体是如何借助于上面的思想来实现排序的呢？\n\n首先，我们将数组中的数据分为两个区间，已排序区间 和未排序区间 。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。\n\n如图所示，要排序的数据是4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。\n\n\n\n插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。 当我们需要将一个数据a插入到已排序区间时，需要拿a与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素a插入。\n\n对于不同的查找插入点方法(从头到尾、从尾到头)，元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。\n\n为什么说移动次数就等于逆序度呢？拿刚才的例子画了一个图表，来观察一下。满有序度是n*(n-1) /2 =15，初始序列的有序度是5，所以逆序度是10。插入排序中，数据移动的个数总和也等于10 = 3+3+4\n\n\n\n下面可以看下插入排序的代码：\n\n// 插入排序，a表示数组，n表示数组大小\npublic void insertionSort(int[] a, int n) {\n  if (n <= 1) return;\n  //外层的for循环，每次从无序的区间中取出一个元素\n  //除去首个数组元素的值，一共要取n-1次\n  for (int i = 1; i < n; ++i) {\n    // 将取出的一个无序区间的一个数组元素的值，赋值给value\n    int value = a[i];\n    // 定义变量j，将i-1的值赋给j\n    int j = i - 1;\n    // 查找插入的位置\n    // 依次从有序的区间的元素，从尾巴到头的方式去遍历\n    // 每次取尾巴的一个元素，直至取到有序区间元素的头部元素\n    for (; j >= 0; --j) {\n      // 如果取出的有序序列中的元素大于，外层for循环取出的要插入的那个无序区间的那个元素\n      if (a[j] > value) {\n        // 就将a[j]的值赋值给a[j+1]\n        // 由于有序的区间的元素都是从小到大排列的\n        // 不存在有序区间中，一些大于value，一些小于value的数\n        // 如果有序区间首个末尾的值要小于value\n        // 那么取的这个外层的无序的要插入的这个元素，就插入在原来的位置上a[j+1] = value\n        // 如果有序区间首个末尾的值要大于value\n        // 那内部的for循环，每次去和有序区间元素比较的时候，\n        // 都去将a[j]赋予到了a[j+1]的位置\n        // 目的是空出a[j]的位置\n        a[j+1] = a[j];  // 数据移动\n      } else {\n        break;\n      }\n    }\n    // 上述循环结束后，就已经找到了需要在a[j]后面的位置\n    // 是插入数据所要在的位置\n    // 注意上述for循环结束后，j是--j，不满足上面的if条件，跳出了循环\n    // 所以相应的是是a[j+1]才是新增数据的位置\n    a[j+1] = value; // 插入数据\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n现在来看三个问题：\n\n第一，插入排序是原地排序算法吗？\n\n从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是O(1)，也就是说，这是一个原地排序算法。\n\n第二，插入排序是稳定的排序算法吗？\n\n在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现的元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。\n\n第三，插入排序的时间复杂度是多少？\n\n如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为O(n)。注意，这里是从尾到头遍历已经有序的数据。\n\n如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度是O($n^2$).\n\n记得在数组中插入一个数据的平均时间复杂度是多少？是O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行n次插入操作，所以平均时间复杂度为O($n^2$)\n\n\n# 选择排序\n\n选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\n\n\n\n同理需要了解三个问题。\n\n首先，选择排序空间复杂度是O(1)，是一种原地排序算法。选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都是O($n^2$)\n\n那选择排序是稳定的排序算法吗？\n\n答案是否定的，选择排序是一种不稳定的排序算法。从我前面画的那张图中，可以看出来，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。\n\n比如5，8，5，2，9这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素2，与第一个5交换位置，那第一个5和中间的5的顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。\n\n选择排序的代码示例：\n\n// package array;\n/*\n * 选择排序：  3,1,5,2,4,9,6,8,7\n * 稳点性差\n */\npublic class SelectSort {\n\tpublic static void selectSort(int [] x) {\n\t\tif(x.length!=0) {\n\t\t\tint temp = 0;\n\t\t\tfor(int i = 0;i<x.length;i++) {\n\t\t\t\tint min = i;\n\t\t\t\tfor(int j = i;j<x.length;j++) {\n\t\t\t\t\tif(x[j] <= x[min]) {\n\t\t\t\t\t\tmin = j;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttemp = x[min];\n\t\t\t\tx[min] = x[i];\n\t\t\t\tx[i] = temp;\n\t\t\t}\n\t\t}\n\t}\n\tpublic static void main(String[] args) {\n\t\tint [] arr = {3,1,5,2,4,9,6,8,7};\n\t\tSystem.out.print("原始数组是：");\n\t\tfor(int a :arr) {\n\t\t\tSystem.out.print(a+",");\n\t\t}\n\t\tselectSort(arr);\n\t\tSystem.out.println();\n\t\tSystem.out.print("排序之后的数组是：");\n\t\tfor(int i = 0;i<arr.length;i++) {\n\t\t\tSystem.out.print(arr[i]+",");\n\t\t}\n\t}\n \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n最外面一层的for循环，表示的要进行多少次的排序，在一个n个元素的数组中，需要进行n次排序。内层的for循环，是用于在无序的各个数组元素中，通过比较选出一个最小的，然后放在有序的数列的末尾。\n\n不管是最好最坏情况下，最外层的for循环n次是少不了。里面的for循环，各个元素的比较也是少不了了，最好最坏的差异只是在于，找到内层for循环的找到最小的元素时，貌似通过第三方变量交换的步骤，也少不了。\n\n时间复杂度分析：\n\n> https://blog.csdn.net/yuzhihui_no1/article/details/44339673\n> \n> 选择排序的时间复杂度不像前面几种排序方法那样，前面几种排序方法的时间复杂度不是一眼就能看出来的，而是要通过推导计算才能得到的。一般会涉及到递归和完全二叉树，所以推导也不是那么容易。但是选择排序就不一样了，你可以很直观的看出选择排序的时间复杂度：就是两个循环消耗的时间；\n> \n> 比较时间：T = （n-1)）+ （n -2）+（n - 3）.... + 1; ===>> T = [n*(n-1) ] / 2；\n> \n> 交换时间：最好的情况全部元素已经有序，则 交换次数为0；最差的情况，全部元素逆序，就要交换 n-1 次；\n> \n> 所以最优的时间复杂度 和最差的时间复杂度 和平均时间复杂度 都为 ：O(n^2)\n\n\n# 解答开篇\n\n冒泡排序和插入排序的时间复杂度都是O($n^2$)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？\n\n我们前面分析冒泡排序和插入排序的时候讲到，冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。\n\n但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要3个赋值操作，而插入排序只需要1个。\n\n冒泡排序中数据的交换操作：\nif (a[j] > a[j+1]) {\n    // 交换\n    int tmp = a[j];\n    a[j] = a[j+1];\n    a[j+1] = tmp;\n    flag = true;\n}\n\n插入排序中数据的移动操作：\nif (a[j] > value) {\n    // 数据移动\n    a[j+1] = a[j];\n} else {\n    break;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n我们把执行一个赋值语句的时间粗略地计为单位时间(unit_time)，然后分别用冒泡排序和插入排序对同一个逆序度是K的数组进行排序。用冒泡排序，需要K次交换操作，每次需要3个赋值语句，所以交换操作总耗时就是3*K单位时间。而插入排序中数据移动操作只需要K个单位时间。\n\n这个只是非常理论的分析，为了实验，针对上面的冒泡排序和插入排序的Java代码，我们写了一个性能对比测试程序，随机生成10000个数组，每个数组中包含200个数据，然后在我们的机器上分别用冒泡和插入排序算法来排序，冒泡排序算法大约有700ms才能执行完成，而插入排序只需要100ms左右就能搞定。\n\n所以，虽然冒泡排序和插入排序在时间复杂度是是一样的，都是O($n^2$)，但是如果我们希望把性能优化做到极致，那肯定首选是插入排序。插入排序的算法思路也有很大的优化空间，我们只是讲了最基础的一种。\n\n\n# 内容小结\n\n分析、评价一个排序算法，需要从执行效率、内存消耗和稳定性三个方面来看。因此，这里分析了三种时间复杂度都是O($n^2$) 的排序算法，冒泡排序、插入排序、选择排序。\n\n\n\n在这三种时间复杂度为O($n^2$) 的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。后面讲排序优化的时候，会讲到，有些编程语言中的排序函数的实现原理会用到插入排序算法。\n\n今天学习的三种排序算法，实现代码都非常简单，对于小规模数据的排序，用起来非常高效。但是在大规模数据排序的时候，这个时间复杂度还是稍微有点高，所以我们更倾向于用下一节所用的时间复杂度为O(nlong n)的排序算法。\n\n\n# D30(2020/10/17)\n\n今天主要是对本周学习的基础知识内容，进行一次总结。\n\n整理的内容涵盖了学习方法、复杂度分析方面的内容，数组，链表，栈，队列，递归以及排序的一部分的内容。\n\n\n# 学习方法\n\n在数据结构和算法这部分的知识来看，确实比较难，比较抽象，比较考验人的耐心。对于这门难啃的知识来说，我们首先思想要重视，不要认为看一遍就可以都会了，不要以为粗粗看看就行，不要以为这些都是些过时的、没用的知识，不要以为有侥幸的心理，要学会深究，学会反复思考。\n\n\n# 数据结构和算法的概念\n\n从广义上来看，数据结构就是指一组数据的存储结构，算法就是操作数据的一组方法。\n\n从狭义上来看，是指某些著名的数据结构和算法。比如队列、栈、堆、二分查找、动态规划等。\n\n\n# 数据结构和算法的关系是什么呢？\n\n 1. 数据结构是为算法来服务的，数据结构是静态的，它只是组织数据的一种方式。\n 2. 算法要作用在特定的数据结构之上的，孤立的数据结构是没有用的。\n 3. 例如，因为数据具有随机访问的特点，常用的二分查找算法炫耀用数组来存储数据，但是如果我们选择链表这种数据结构，二分查找算法就无法工作了，因为链表并不只是随机访问。\n\n\n# 学习重点是什么\n\n 1. 先重点学习，算法的复杂度分析的内容，包括各种数据结构和相应算法的最好情况时间复杂度，最坏情况时间复杂度，平均情况时间复杂度。并且分别对应的那种情况的数据排列的情况。\n\n 2. 随后中重点是要学习的是常用的一些数据结构，以及对应的基于这些数据结构上的算法。\n\n 3. 共计20个常用的数据结构和算法。\n    \n    * 10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树。\n    * 10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算分、分治算法、回溯算法、动态规划、字符串匹配算法。\n\n\n# 学习思维\n\n 1. 首先要学习这个数据结构和算法的"来历"，怎么会有出现这种数据结构或相应的算法的，之前的不香吗，之前的是出现了什么问题，要引入新的数据结构和算法呢？\n\n 2. 学习它的"自身特点"，每种特定的数据结构和算法，都有其各自的特点。这些特点，会体现在解决问题的新的思维上。包含了具体的算法实现上，结合我目前的实际情况，我重点关注的C++的实现，以及Java的实现，顺带着考虑Python的实现。考研的笔试中，会有相应的C++的编程题，而在复试的机试中会有相应的上机题。\n    \n    特别特别要注意，各种数据结构和算法的代码实现上，不要求一开始要掌握全部，不着急，慢慢来，常用的就那么几个，手指头都算的过来。把心思和精力放在，是否真的理解了，是否自己能不看材料，自己写出来。\n\n 3. 学习它的“适合解决的问题”，以及它的“实际应用场景”。新的数据结构和算法肯定要结合具体的场景来存在的，深入理解这些场景，就是为了从这些场景中，提取算法的共性，了解其算法的本质，进而可以推广应用到其他场景中去。\n\n\n# 学习技巧\n\n 1. 边学边练，适度刷题\n 2. 学习的过程，是一个延迟享受的过程，克服自己焦躁的心态。多问、多思考、多互动，没有一个问题是愚蠢的问题。可以躲在留言区写下自己的疑问、思考和总结，也可以经常看看别人的留言，和他们进行互动。\n 3. 打怪升级法。通过设立目标，留言学习，每节课后都写一篇学习笔记或学习心得。\n 4. 知识需要沉淀，不要试图一下子掌握所有。学习知识的过程是反复迭代、不断沉淀的过程。如果碰到"拦路虎"，我们可以尽情在留言区提问，也可以先沉淀一下，过几天再重新学习一边。所谓，书读百遍其义自见。\n\n\n# 复杂度分析\n\n\n# 为什么需要复杂度分析？\n\n 1. 事后分析法的局限性\n    \n    通过统计、监控，就能得到算法执行的时间和占用的内存大小。\n    \n    但是，"事后"测试结果依赖测试环境，不同的测试环境中的测试结果是不一样的。"事后"测试的结果受到数据规模影响很大，数据量少的情况下，很难反应出程序代码的真正效率。\n\n 2. 这个时候，我们需要一个不用具体的测试数据，来粗略估计算法的执行效率的方法。\n\n\n# 引入大O表示法\n\n 1. 从算法的执行效率，粗略来看，就是算法代码执行时间。\n\n 2. 我们假设每行代码都执行类似的操作，读取数据->运算->写数据的操作。\n\n 3. 假设每行代码执行时间都是一样的unit_time。 尽管每行代码对应的CPU执行的个数、执行的时间都不一样。但是，我们这里只是粗略的估计。\n\n 4. 示例代码一分析：\n    \n    int cal(int n) {\n    \tint sum = 0;\n    \tint i = 1;\n    \tfor(; i<=n; ++i) {\n    \t\tsum  = sum + i;\n    \t}\n    \treturn sum;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    在这段代码中总的执行时间是，(2n+2)* unit_time。\n\n 5. 示例代码二分析：\n    \n    int cal(int n) {\n    \tint sum = 0;\n    \tint i = 1;\n    \tint j = 1;\n    \tfor (; i<=n; ++i) {\n    \t\tj = 1;\n    \t\tfor (; j <=n; ++j) {\n    \t\t\tsum  = sum + i*j;\n    \t\t}\n    \t}\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n    \n    在这段代码中，总的执行时间是，T(n) = (2$n^2$ + 2n +3) * unit_time.\n    \n    第2、3、4行代码，每行都需要1个unit_time的执行时间，\n    \n    第5、6行代码循环执行了n遍，需要2n 个unit_time的执行时间，\n    \n    第7、8行代码循环执行了$n^2$遍，所以需要2$n^2$个unit_time的执行时间。\n    \n    所以，整段代码总的执行时间是：T(n) = (2$n^2$ + 2n +3) * unit_time.\n\n\n# 分析大O表示法\n\n 1. T(n) = O(f(n))\n    \n    T(n)表示的是代码执行的时间，n表示数据规模的大小；f(n)表示的是每行代码执行的次数总和。公式中的O，表示的是代码的执行时间T(n)与f(n)表达式成正比。\n\n 2. 用大O来表示\n    \n    在第一个例子中的T(n) = O(2n+2), 简写为T(n) = O(n).\n    \n    在第二个例子中的T(n) = O(2$n^2$ + 2n + 3)，简写为T(n) = O($n^2$)\n\n 3. 大O时间复杂度实际上并不具体表示代码的真正的执行时间，而是表示的是代码执行时间随着数据规模增长的变化趋势，所以也叫做渐进式时间复杂度，简称为时间复杂度。\n\n 4. 大O的演进\n    \n    分析代码的执行时间，假设每行代码中的执行时间都是一样的。--\x3e 进而考虑的是每行代码的执行次数 --\x3e 用大O来表示执行次数和执行时间的正比关系 --\x3e 去除低阶、常量、系数，真正成为大O表示法。\n\n\n# 时间复杂度分析\n\n 1. 只关注循环次数最多的一段代码。大O复杂度表示法，只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。\n\n 2. 加法法则\n    \n    所谓的加法法则，是指总的时间复杂度等于量级最大的那段代码的复杂度。\n    \n    例如，一个程序中有三段代码，综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就是O($n^2$) .\n\n 3. 乘法法则\n    \n    所谓的乘法法则，是指嵌套代码的复杂度，等于嵌套内外代码复杂度的乘积。\n    \n    可以把乘法法则看成是嵌套循环。\n\n\n# 常见时间复杂度分析\n\n在我们常见的时间复杂度分析的常见量级中，大致可以分为 非多项式量级和多项式量级。\n\n# 非多项式量级\n\n所谓的非多项式量级中，只有两个：O($2^n$) 和O(n!) 。\n\n非多项式的问题，也就是所谓的NP(非确定多项式)问题。\n\n在上述的两种非多项式量级中，都是非常低效的。\n\n当数据规模n越来越大的时候，非多项式量级算法的执行时间会急剧增加，求解问题的执行施加会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。\n\n# 多项式量级\n\n在所谓的多项式量级的分类中，我们大致分为三大类：常量级O(1)，对数级O(logn)和O(nlog n)，还有就是复杂度是由两个数据的规模来决定的，O(m+n)和O(m*n)。\n\n# O(1)\n\n表达的是常量级的时间复杂度，这里的1并不是说只执行一行代码，而是指只要代码执行时间不随n的增大而增大，这样代码的时间复杂度都可以记住O(1).\n\n一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万的代码，其时间复杂度也是O(1)。\n\n# O(log n)\n\n在下面的示例代码中，就是一个O(log n)的代码复杂度。\n\ni = 1;\nwhile (i<=n) {\n\ti = i*2;\n}\n\n\n1\n2\n3\n4\n\n\n变量i的值从1开始取，每循环一次就乘以2，大于n时，循环结束。\n\n变量i的取值就是一个等比数列：\n\n$2^1$ , $2^2$ , $2^3$ , $2^4$ , ... $2^k$ ... $2^x$\n\n我们这里先假设当2的x次方的值等于 n的时候，恰好循环结束。通过求解x的值，根据对数的知识可知， x = $log_2 n$ ，所以代码复杂度就是O($log_2 n$)\n\n我们知道对数之间是可以相互转换的，例如$log_3 n$ 就等于$log_3 2$ * $log_2 n$ ，所以我们可以得知O($log_3 n$) = O(C* $log_2 n$) ，其中 C= $log_3 2$就是一个常量。\n\n所以对我们来说，我们可以忽略系数，忽略对数的"底"，所以说，我们在对数阶的时间复杂度的表示方法中，我们忽略了对数的"底"，统一表示为O($log n$)\n\n# O(nlogn)\n\n通过乘法法则，n与log n嵌套循环。\n\n# O(m+n)/O(m*n)\n\n在这里，复杂度是由两个数据的规模来决定的。我们无法事先评估m和n两个数的谁的量级大，不能简单利用加法法则，来忽略掉其中一个。\n\n例如下面的代码：\n\nint cal(int m, int n) {\n\tint sum_1 = 0;\n\tint i = 1;\n\tfor (; i<m; ++i) {\n\t\tsum_1 = sum_1 + i;\n\t}\n\tint sum_2 = 0;\n\tint j = 1;\n\tfor (; j<n; ++j) {\n\t\tsum_2 = sum_2 + j;\n\t}\n\treturn sum_1 + sum_2;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n在上面的例子中，我们无法判断是m的量级大还是n的量级大，所以，这里只能是O(m+n)。\n\n\n# 空间复杂度分析\n\n 1. 表示算法的存储空间与数据规模之间的增长关系。\n\n 2. 一般是指算法额外申请的内存空间的情况。\n\n 3. 与时间复杂度分析的区别来看，时间复杂度看的是算法的执行时间，更进一步来说是，代码中某一行的执行次数与数据规模之间的增长关系。而空间复杂度看的是算法的内存空间，是这个代码使用的内存空间与数据规模之间增长的关系。\n\n 4. 空间复杂度分析示例：\n    \n    void print(int n) {\n    \tint i = 0;\n    \tint[] a = new int[n];\n    \tfor (i; i<n; ++i) {\n    \t\ta[i] = i*i;\n    \t}\n    \tfor (i = n-1; i>=0; --i) {\n    \t\tprint out a[i]\n    \t}\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n    \n    在第3行中申请了一个大小为n的int类型数组，整段代码的空间复杂度就是O(n).\n\n\n# 低阶到高阶的排序\n\n从低阶到高阶的排序如下：\n\nO(1) < O($log n$) < O(n) < O(n$log n$) < O($n^2$)\n\n\n# D31(2020/10/19)\n\n今天继续对之前学习的数据结构和算法的知识点进行整理。\n\n主要涵盖的是复杂度分析、数组、链表、栈、队列、递归以及排序的一部分的内容。\n\n\n# 复杂度分析\n\n\n# 最好/最坏时间复杂度\n\n# 引入最好/最坏时间复杂度\n\n例如下面的代码中，主要的功能是从一个数组中，找到数据元素值为x的元素，然后返回对应的数组下标。\n\nint find(int[] array, int n, int x) {\n\tint i = 0;\n\tint pos = -1;\n\tfor (; i<n; ++i) {\n\t\tif (array[i] == x) {\n\t\t\tpos = i;\n\t\t}\n\t}\n\treturn pos;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n由于代码中没有break强制的退出，也就是说即使已经找到某个值的数组元素，也要执行完整个for循环。时间复杂度不分情况，都是O(n).\n\n# 进一步分析\n\n我们对上面的代码进行优化，在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到了几可以提前结束循环了。\n\nint find(int[] array, int n, int x) {\n\tint i = 0;\n\tint pos = -1;\n\tfor (; i<n; ++i) {\n\t\tif (array[i] == x) {\n\t\t\tpos = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn pos;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n从上面的代码可以看出，由于查找变量x 可能出现在数组的任意位置。\n\n最好情况下，如果数组中第一个元素正好是要查找的变量x，那就不需要继续遍历剩下的n-1个数据了，那此时的时间复杂度就是O(1)。\n\n但是如果数组中不存在变量x，那么这个时候我们就需要把整个数组都遍历一遍，这个时候的时间复杂度就成了O(n)了。\n\n\n# 平均时间复杂度\n\n如果我们想要考虑的是这个代码在平均情况下的时间复杂度的情况呢？\n\n还是上面从数组中find数组元素的代码，查找的变量x在数组中的位置，有n+1种的情况，也就是在数组的0~ n-1的位置中，和不在数组中。\n\n一般思维：我们统计出所有情况下，代码的执行的次数，然后将这些次数进行累加起来求和。然后再除以n+1种情况，以求解到这个代码遍历元素个数的平均数了。$\\frac{1+2+3+...+n+n}{n+1}$ = $\\frac{n(n+3)}{2(n+1)}$ ，所以这种情况下，我们可以判断出这个平均时间复杂度就是O(n).\n\n仔细思考：n+1种情况，出现的概念并不是一样的。我们可以分析出每种情况的出现的概率，然后每种情况遍历比较的次数 * 该种情况出现的概率，将所有的这些情况都累加起来，得到的就是一个加权的平均数了。\n\n例如，我们要查找的变量x，要么在数组里，要么就不在数组里面。这两种情况对应的概率统计起来很麻烦，为了方便理解，我们假设在数组中与不在数组中的概率都是1/2。另外，要查找的数据出现在0~n-1这n个位置的概率也是一样的，为1/n。所以，根据概率乘法法则，要查找的数据出现在0~n-1种任意位置的概率就是1/(2n).\n\n1$\\times$$\\frac{1}{2n}$ + 2$\\times$$\\frac{1}{2n}$ + 3$\\times$$\\frac{1}{2n}$ + ... + n$\\times$$\\frac{1}{2n}$ + n$\\times$$\\frac{1}{2}$ = $\\frac{3n+1}{4}$\n\n我们得到的加权平均值就是(3n+1)/4 ，由此可以得到平均时间复杂度仍然是O(n).\n\n\n# 均摊时间复杂度\n\n# 总体概述\n\n相比与平均时间复杂度，均摊时间复杂度的使用场景，更为特殊。有哪些场景可以用的上均摊时间复杂度呢？\n\n * 对一个数据结构进行一组连续操作中，大部分的情况下的时间复杂度都很低，只有在个别情况下的时间复杂度比较高。\n * 这些连续的操作之间，存在前后连贯的时序关系，可以将这一组操作放在一块儿分析。\n * 看的是，能否将这些较高的时间复杂度那次操作的耗时，平摊到其他的那些时间复杂度比较低的操作上。\n * 在能够引用均摊时间复杂度分析的场合中，一般均摊时间复杂度就等于最好情况下的时间复杂度。\n\n# 摊还分析法\n\n简而言之，就是利用摊还分析法，来得到均摊时间复杂度。\n\n例如，每一次O(n)的插入操作，都会跟着n-1次的O(1)的插入操作。耗时多的那次操作摊到接下来的n-1次耗时少的操作上。通过均摊下来，这一组连续的操作的均摊时间复杂度就是O(1)了。\n\n我的理解，那一次的O(n)的插入操作的循环代码的次数为n，也就是n个unit_time。将n个单位的unit_time平均摊派到前面的n-1次中，大致每一次都能分到一个unit_time，这样的话，还是O(1)的时间复杂度。由于这个每一次的插入操作的概率都是相同的，所以这种均摊是可以成立的。\n\n# 代码示例\n\n代码中，是一个连续的往数组中插入的操作，数组未满时，赋予相应的数组元素对应的值，并且将count++。当数组满的时候，将现有的数组中的各个元素求和赋予给第一个数组元素的值，同时将count置为1，以便于后面继续从1开始插入元素。\n\n// array 表示一个长度为n的数组\n// 代码中的array.length就等于n \nint[] array = new int[n];\nint count = 0;\n\nvoid insert(int val) {\n\tif (count == array.length) {\n\t\tint sum = 0;\n\t\tfor (int i =0; i < array.length; ++i) {\n\t\t\tsum = sum + array[i];\n\t\t}\n\t\tarray[0] = sum;\n\t\tcount = 1;\n\t}\n\tarray[count] = val;\n\t++count;\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n这个代码是有个循环在反复的调用的。理想的情况下，数组中有空闲的元素位置(count未到数组的大小值)，我们只需要将数据插入到数组下标为count的位置就可以了，所以最好情况时间复杂度是O(1)。\n\n最坏情况下，数组中没有空闲位置了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度是O(n)。\n\n平均情况下，假设数组的长度是n，根据数据插入的位置的不同，我们可以分为n种情况，每种情况的时间复杂度是O(1)。除此之外，还有一种"额外"的情况，就是在数组中没有空闲空间的的时候插入一个数据时，这个时候的时间复杂度是O(n)。而且，这个n+1种情况发生的概率是一样的，都是1/(n+1)。所以，根据加权平均的计算方法，我们求到的平均时间复杂度就是：1* $\\frac{1}{n+1}$ + 1* $\\frac{1}{n+1}$ +... + 1 * $\\frac{1}{n+1}$ + n * $\\frac{1}{n+1}$ --\x3e O(1)\n\n\n# insert()与find()代码比较\n\n# 第一个区别\n\n * find()函数在极端的情况下，复杂度才是O(1)\n * insert()在大部分的情况下，时间复杂度都是O(1)\n * insert()只有在个别的情况下，复杂度才比较高，为O(n)\n\n# 第二个区别\n\n对于insert()函数来说，O(1)的时间复杂度的插入和O(n)的时间复杂度的插入，出现的概率是非常有规律的，而且有一定的前后时序的关系，一般的都是一个O(n)插入之后，紧跟着n-1个O(1)的插入操作(是指已经满的情况下产生的规律)，循环往复。\n\n\n# 数组\n\n\n# 基本概念\n\n 1. 数组array是一种线性表的数据结构\n 2. 数组是用一组连续的内存空间，来存储一组具有相同类型的数据。\n 3. 重点理解:\n    * 第一是线性表。线性表就是数据排成像一条线一样的结果；每个线性表上的数据最多只有前和后两个方向；除了数组之外，链表、队列、栈等也是线性表的结构；非线性表有二叉树、堆、图，这些数据之前并不是简单的前后关系。\n    * 第二，连续内存空间和相同数据类型。这种数据结构的特点优势，就是可以"随机访问"；劣势在于，删除/插入数据，都需要数据移动。\n\n\n# 随机访问实现原理\n\n 1. 计算机会给每个内存单元分配一个地址。\n 2. 计算机通过地址来访问内存中的数据。\n 3. 随机访问数组中某个元素的时候，由于连续内存空间，可以通过寻址公式来查找。a[i]_address = base_address + i * data_type_size\n 4. 例如，int数组元素是需要有4个字节。内存块的首地址是base_address = 1000，那么a[3]地址是: 1000 + 3*4\n\n\n# 数组的时间复杂度描述\n\n 1. 数组适合查找，但是其查找的时间复杂度并非是O(1)。\n 2. 数组支持随机访问，我们可以根据下标来访问数组元素，其时间复杂度是O(1).\n 3. 即使数组元素有序时，二分查找法的时间复杂度也是O(log n)\n\n\n# 低效的"插入"\n\n# "插入"操作\n\n假设线性表的长度是n，现在，如果我们需要将一个数据插入到数组中的第K个位置。为了把第K个位置腾出来，给新插入的数据，我们需要将第k~n这部分的元素都顺序地往后挪一位。\n\n 1. 如果在线性表末尾插入元素，就不需要移动数据了，这个时候的时间复杂度为O(1)，也是最好情况的时间复杂度。\n 2. 如果我们从数组的头部来插入元素，那所有元素都需要依次往后移动一位，所以最坏的时间复杂度是O(n)\n 3. 在每个位置上插入元素的概率都是一样的，平均时间复杂度是O(n)。在一个数组大小为n中，有n个插槽，每个位置的概率都是一样的，都是1/n。而出现在不同的位置上，需要移动的元素的个数是不一样的，在末尾的话是0，倒数第二个是1，倒数第二位是2 .... 直至开头是n。根据加权平均数的求解，那就是1* 1/n + 2* 1/n ... + n* (1/n) = (n+1)/2 ，也就是O(n)。\n\n# 降低"插入"时间复杂度\n\n如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。\n\n如果要将某个数据插入到第K个位置，为了避免大规模的数据搬移。我们就需要直接将第k位的数据搬移到数组元素的最后。然后把新的元素直接放入第k个位置，这个时候的时间复杂度就降为O(1)。\n\n\n# 低效的"删除"\n\n# "删除"操作\n\n跟插入数据类似，如果我们要删除第k个位置的数据，为了保证内存的连续性，也需要搬移数据。不然的话，中间就会出现空洞，内存就不连续了。\n\n如果删除线性表末尾的数据，则最好情况下时间复杂度是O(1)；如果删除线性表开头的数据，那所有元素都需要依次往前移动一位，最坏情况时间复杂度是O(n)；每个位置删除元素的概率也是一样的，参照上面分析的插入的平均时间复杂度来看，是为O(n)。\n\n# 连续删除更高效\n\n 1. 原先的情况下，如果删除多个元素，之前的数组，就需要将数据移动多次了。\n 2. 在某些特殊的场景下，我们并不一定非得追求数组中数据的连续性。\n 3. 这个时候的话，我们每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。\n 4. 当数据中，没有更多空间来存储数据的时候，就触发执行一次真正的删除操作。\n 5. 这样的好处在于，可以大大减少了删除操作导致的数据搬移。\n\n\n# 数组越界问题\n\n在下面的代码中，由于在for循环的结束条件中错写了i < =3，而非 i<3，结果就导致了数据越界。该段代码会无限打印 "hello world"\n\n# include <stdio.h>\nint main(int argc, char* argv[]) {\n\tint i =0;\n\tint arr[3] = {0};\n\tfor (; i<=3; i++) {\n\t\tarr[i] = 0;\n\t\tprintf("%s","hello world\\n");\n\t}\n\treturn 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n原因在于，函数体内的局部变量存在栈上，且是连续压栈。\n\n在Linux进程的内存布局中，栈去在高地址空间，从高向低增长。在上面的main 函数中，首先向函数的栈区压入i = 0，紧接着是一个int attr[3]，也就是一个陆续压入了attr[2]、attr[1]、 attr[0]。我们知道数组的内存空间是从低到高的，而栈区的内存地址是从高到低的。\n\n当我们通过attr[3]寻址公式，计算得到地址正好是i的存储地址，所以当对attr[3] = 0 ,也就是对i赋值为0了，结果导致了无限循环打印。\n\n\n# 高级语言的容器类与数组\n\n# 容器类的优势\n\n 1. ArrayList最大的优势就是可以将很多数组操作的细节封装起来了。比如前面提到的数组插入、删除数据时需要搬移其他数据等。\n 2. ArrayList支持动态扩容，使用ArrayList，我们就完全不需要关心底层的扩容逻辑了，ArrayList已经帮我们实现好了，每次存储空间不够的时候，它都会将空间自动扩容为1.5倍大小。\n 3. 为了避免频繁内存的申请和数据搬移，ArrayList最好事先指定数据大小。\n 4. 在业务的开发中，直接使用容器类，可以省时省力，只会损耗一点点性能。\n\n# 单独使用数组的场景\n\n 1. ArrayList无法存储基本类型，比如int、long，需要封装为Interger、Long类，而Autoboxing、Unboxing则有一定的性能损耗。\n 2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法。\n 3. 在多维数组的使用上，更加清晰的使用。\n 4. 在基于底层架构开发的时候，可以将性能优化到极致。\n\n\n# 为何数组从0开始\n\n"下标"最确切的定义应该是"偏移(offset)"，a[0]就是偏移量为0的位置。这个时候，计算a[k]的内存地址只需要使用下面的这个公式：a[k]_address = base_address + k*data_type_size\n\n如果数组从1开始，多一层CPU减去指令。这个时候，计算数组元素a[k]的内存地址就会变成，a[k]_address = base_address + (k-1)* data_type_size.\n\n\n# 二维数组寻址公式\n\n一维数组的寻址公式为：\n\n(a1)*[i]_address = base_address + i* type_size\n\n二维数组的寻址公式为：\n\n(a1*a2)* [i][j]_address = base_address + (i*a2 +j) * type_size\n\na2是列的长度，一定要走完i * a2的长度，随后在a[i][o] 的位置开始，往后找j 个长度就是 a[i] [j] 了。\n\n\n# 链表\n\n这个章节，主要整理的是链表的知识点的内容。\n\n\n# 引入链表\n\n由于数组需要一块连续的内存空间，如果这个时候没有连续的、足够大的内存空间的话，就会申请失败。注意：Java的ArrayList不存在这个问题。\n\n这个时候，就引出了链表，申请一个链表，不需要连续的空间，链表通过"指针"将一组零散的内存块串联起来。\n\n\n# 单链表\n\n在描述单链表的信息时，从基本概念入手，随后讲解单链表的相关操作：单链表的查询、单边表的插入/删除。\n\n# 基本概念\n\n 1. "结点"，在链表中，一个内存块就被称为一个"结点"。\n 2. "后继指针next"：记录下一个结点地址的指针。\n 3. "头结点"：通常我们认为是，放在第一个元素结点之前的一个节点，其数据域一般无意义(也可以存放链表的长度)，头结点可有可无。\n 4. "头指针"：在链表中，头指针是必须要存在的。若存在头结点，头指针是指向头结点的；但是若不存在头结点，头指针指向第一个结点元素。\n 5. 第一个元素结点，顾名思义，就是第一个存放元素值的节点。\n 6. "尾结点"：是指最后一个结点。\n 7. 尾结点的next指针指向一个空地址NULL，或者可以说，当一个结点的next指针指向NULL空地址的时候，这个就表示这个结点是链表上的最后一个结点了。\n\n# 单链表的查询操作\n\n当需要随机访问链表的第K个元素的时候，这个场景下面，就和数组有很大的不同了。\n\n需要根据指针一个结点一个结点的一次遍历，单链表的随机访问没有数组好，需要O(n)的时间复杂度。\n\n# 单链表的插入/删除操作\n\n 1. 数组的插入/删除操作，需要保证内存连续性，为了保证数组内存的连续性，在做插入/删除操作的时候，要配套做大量的数据搬移工作，其时间复杂度是O(n)。\n 2. 链表的插入/删除操作，不需要考虑内存的连续性，也不需要进行数据搬移。只需要改变相邻结点的指针，理论上其时间复杂度是O(1)\n\n\n# 循环链表\n\n循环链表是特殊的单链表，与单链表唯一的区别在于尾结点。\n\n单链表的尾结点的指针指向的是NULL，循环链表的尾结点指针指向链表的头结点(或者说是第一个元素结点)，这种把链尾和链头连接了起来，适合处理环形的数据结构。\n\n\n# 双向链表\n\n# 基本概念\n\n 1. 双向链表与单链表相比，多了一个前驱指针prev。\n 2. 前驱指针prev，会指向前面的结点。\n 3. 后继指针next，指向后面的结点。\n 4. 缺点：双向链表需要额外存储两个指针。\n 5. 优点：支持向前向后(双向)遍历。\n\n# 删除操作\n\n在双向链表中，对某个结点的删除，有两种场景，第1种场景是：删除结点中"值等于某个给定值"的结点；第2种场景是：删除给定指针指向的结点。\n\n# 删除"值等于某个给定值"的结点\n\n 1. 我们要先遍历定位要删除的元素，这时候的遍历的程序代码的时间复杂度是O(n)。\n 2. 执行删除查询到的结点，这个删除的操作的时间复杂度是O(1)。\n 3. 在这种场景下，双链表的删除操作和单链表的删除操作都是一样的，都需要先一个个遍历，时间复杂度也是相同的。\n\n# 删除给定指针指向的结点\n\n在这个场景中，就意味着不用我们再去遍历，找到需要被删除的元素了，而是直接去删除指向这个元素的相应的指针就可以了。\n\n 1. 首先，我们已经找到了要删除的结点。\n 2. 但是，删除结点，需要知道该结点的前驱指针，这个结点的前驱指针指向的是前面的那个结点，我们需要改写前面结点的后继指针，将这个指针指向其他的结点。\n 3. 单链表不支持直接获取其前面结点的指针。单链表需要从头开始遍历链表，这个时候的时间复杂度是O(n)。\n 4. 但是双向链表，可以通过"删除结点"的前驱指针来找到前面的结点了。\n 5. 这个时候的双向链表的删除操作的时间复杂度是O(1)\n\n# 插入操作\n\n对于双向链表而言，可以选择插入到某个结点后面，或者是插入到某个结点前面。\n\n# 插入到某个结点后面\n\n在这个时候，单链表和双链表的时间复杂度就都是一样的了。因为这种场景下，单链表已经知道了"要插入的位置的前面结点"的信息了。\n\n# 插入到某个结点前面\n\n这个时候，如果是单链表的话，操作的步骤和删除操作类似，需要先遍历找到前面结点的信息。这个时候单链表的时间复杂度是O(n)，双链表的操作和上面的删除操作是一样的，时间复杂度是O(1)。\n\n# 查询\n\n如果链表是无序的话，单链表和双链表的查询的时间复杂度都是一样的。\n\n如果链表是有序的，双向链表按值来查询要比单链表要高，双向链表可以根据查询到的值，和目标值进行比较，决定是往前查找，还是往后查找。\n\n# 应用场景\n\n在实际的开发过程中，大多数使用的双向链表。虽然会多存储一些内存空间，这也是用空间换时间的设计体现。\n\n\n# 数组与链表的比较\n\n# 从内存是否连续上\n\n数组简单易用，需要连续的内存空间，可以借助CPU的缓存机制，预读数据，访问效率高。\n\n链表在内存中是非连续的，对CPU缓存不友好，无法预读数据。\n\n# 动态扩容上\n\n数组如果过小，需要申请更大的内存，这个时候需要原来的数组都拷贝过去，非常耗时。\n\n链表本身没有大小限制，天然的就支持动态扩容。\n\n与数组相比，链表更适合插入/删除操作频繁的场景。\n\n\n# 书写链表代码\n\n# 理解指针或引用的含义\n\n在C语言中是有指针的概念的，而在Java、Python中是没有指针的概念的，取而代之的是引用的概念。\n\n我们可以将某个变量的地址赋值给指针。这样的话，这个指针中就纯粹了这个变量的内存地址，指向了这个变量。通过指针我们就能够找到这个变量。\n\n# 警惕指针丢失和内存泄漏\n\n由于在链表的删除中，可能会由于指针指向的问题，结果就导致了有些链表分成了两个部分。或者说是某个链表结点的指向性丢失了，也就是说无法去释放这些空间的内存了。\n\n程序本身自己是无法知道这部分内存空间在哪里了，更加无法去释放了。\n\n# 利用哨兵来简化实现\n\n我们在单链表中的插入和删除操作中，如果我们在结点p后面插入一个新的结点，只需要下面两行代码就可以了。\n\nnew_node->next = p->next;\np->next = new_node;\n\n\n1\n2\n\n\n但是，当我们要向一个空链表中插入第一个结点时，上面的代码就不能用了。我们需要进行下面的特殊处理，其中 head表示链表的头结点。所以，从这段代码来看，我们可以发现，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不一样的。\n\nif (head == null) {\n    head = new_node;\n}\n\n\n1\n2\n3\n\n\n我们再来看下单链表中的结点删除操作。如果要删除结点p的后继结点，我们只需要一行代码就可以搞定。\n\np->next = p->next->next;\n\n但是，如果我们要删除链表中的最后一个结点，前面的删除代码就不work了。跟插入类似，我们也需要对这种情况特殊处理。写成代码如下：\n\nif (head->next == null) {\n    head = null;\n}\n\n\n1\n2\n3\n\n\n从前面的操作可以看出，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况来进行特殊处理。\n\n为了考虑代码的简洁，可以利用哨兵。引入哨兵结点，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫做带头链表。相反，没有哨兵结点的链表就叫做不带头链表。\n\n哨兵结点不存储数据，因为哨兵结点一直存在，原来的插入第一个结点的场景 --\x3e 就是在哨兵结点后插入结点；删除最后一个结点的场景 --\x3e 这个不是最后一个结点，删除后，还有个哨兵结点。\n\n# 留意边界条件\n\n考虑的边界条件有，链表为空的时候；链表只包含一个结点的时候；链表包含两个结点的时候；处理头结点和尾结点的时候。\n\n\n# 栈\n\n\n# 什么是栈\n\n后进者先出，先进者后出，这就是典型的"栈"结构。\n\n栈是一种"操作受限"的线性表，只允许在一端插入和删除数据。\n\n\n# 栈存在的意义\n\n从功能上来看，数组或链表都可以替代栈。\n\n了解到特定的数据结构是对特定场景的抽象，数组或链表暴露了太多的操作接口，在操作撒花姑娘的确灵活自由，但是在使用上就比较不可控了，自然就更容易出错。\n\n栈的数据结构，是应用于当某个数据集合只涉及在一端插入和删除数据的场景，并且满足于后进先出、先进后出的特性的时候，我们就应该首选"栈"这种数据结构。\n\n\n# 如何实现一个"栈"\n\n用数组来实现的栈，是顺序栈；用链表来实现的栈，是链式栈。栈一般来说有两个操作，入栈(在栈顶插入一个数据)，出栈(从栈顶删除一个数据)。\n\n在栈的使用中，我们需要强化一下空间复杂度的概念，我们所说的空间复杂度，是指除了原本的数据额外的存储空间外，算法运行还需要额外的存储空间。\n\n不管顺序栈还是链式栈，不管是入栈还是出栈，时间复杂度都是O(1)。\n\n\n# 栈在函数调用中的应用\n\n场景描述是，函数调用栈。\n\n 1. OS会给每个线程分配一块独立的内存空间。\n 2. 这块内存被组织成"栈"这种数据结构。用来存储函数调用时的临时变量。\n 3. 每进入一个函数，就会将临时变量作为一个栈帧入栈。\n 4. 当调用函数执行完成后，返回之后，会将这个函数对应的栈帧出栈。\n\n\n# 栈在括号匹配中的应用\n\n场景的描述是，我们可以借助栈来检查表达式的括号是否匹配。\n\n 1. 我们可以把([{ 这些都叫做左括号，}])这些都叫做右括号。\n 2. 我们用栈来保存未匹配的左括号\n 3. 从左到右一次扫描字符串\n 4. 当扫描到左括号的时候，将其压入栈\n 5. 当扫描到右括号的时候，从栈顶取出一个左括号，如果这是时候能匹配的上，则继续扫描剩下的字符串；如果不能匹配，或栈中没有数据的话，则说明为非法格式。\n\n\n# 栈在页面前进后退中的应用\n\n我们可以用栈，来实现网页的前进和后退。\n\n 1. 我们可以借助于两个栈来实现。\n 2. 首次浏览页面的时候，依次压入X栈。\n 3. 后退的时候，依次从X栈中弹栈，放入Y栈。\n 4. 前进按钮，依次从Y栈中取出数据，放入X栈。\n 5. 当访问新页面的时候，压入X栈，同时清空Y栈。\n 6. 当X栈没有数据的时候，表明无法进行后退了。\n 7. 当Y栈没有数据的时候，表明无法前进了。\n\n\n# D32(2020/10/21)\n\n今天主要是要完成队列、递归，以及排序的一部分的内容。\n\n\n# 队列\n\n\n# 引出队列\n\n我们在线程池，多并发的开发场景中，这个时候如果线程满了。我们该如何去处理新的线程请求呢？拒绝请求还是排队请求呢？\n\n\n# 理解"队列"\n\n先进先出，就是典型的"队列"。与栈有些类似，都有两个基本的操作，入队和出队的操作。我们是从队尾入队，从队头出队。与栈类似，队列是一种操作受限的线性表。\n\n\n# 顺序队列/链式队列\n\n从队列的定义来看，我们把用数组来实现的队列叫做顺序队列；把用链表实现的队列叫做链式队列。\n\n# 顺序队列的两个指针\n\nhead指针，指向队头元素；tail指针，指向队尾元素的下一个位置，需要注意的是tail指针指向的不是队尾元素，而是队尾的下一个，这么做是为了区分队列为空和队列中只有一个元素的两种场景。\n\n判断队列为空的条件：head == tail\n\n判断队列只有一个元素：tail - head = 1\n\n# 顺序队列入队\n\n当tail == n的时候，也就是说队列末尾没有空间了，但是，不存在当tail == n的时候的数组a[tail]。\n\n只有当tail == n并且head == 0的时候，表示队列都占满了。这个时候将触发数据搬移工作，当数据搬迁完以后，再更新head和tail .\n\nfor (int i = head; i< tail; ++i) {\n    item[i-head] = items[i];\n}\n\n\n1\n2\n3\n\n\n更新tail和head指针\n\ntail = tail - head;\nhead = 0;\n\n\n1\n2\n\n\n如果当队列中队尾还有空间，直接一个插入操作就可以了，这个时候对于顺序队列的插入而言，是最好情况下的时间复杂度，此时为O(1)。\n\n当队列中没有空间的时候，for循环来搬移元素，会执行n-1次 ??\n\n当频繁调用顺序队列入队列的时候，从队列为空开始吗，入一个队列，出一个队列的方式的话，这种情况下的程序的代码复杂度是从O(1)到O(n)，利用摊还分析法来看的话是O(1)。\n\n# 链表队列\n\n在链表队列中，head指针指向的是链表的第一个结点，tail指针指向的是最后一个结点，在入队的时候，可以做如下的操作：\n\n在入队的时候，主要有下面的两个代码：\n\ntail->next = new_node\ntail = tail->next\n\n\n1\n2\n\n\n我们将新结点的内存地址，赋值给了原来的tail->next，原来的链表的tail->next指向的是null。这样的话，新结点就可以连接上了原来的链表的后的一个元素了。最后，再修改tail的指针。\n\n出队列的时候，head = head->next\n\n# 循环队列\n\n引入循环队列：\n\n为什么要引入循环队列呢？在原有的顺序队列中，当tail == n的时候，会有数据搬移的操作，循环队列就是为了避免搬移操作。\n\n在循环队列中，我们是将顺序队列首尾相连。\n\n当达到head == tail 的场景中，这是判断队列为空的条件。\n\n当达到(tail+1)%n = head的时候，这个时候是判断队列满的条件，在这个时候tail指向的位置实际上没有存储数据的，这会浪费一个数组的元素的存储空间。\n\n入队列的时候，我们需要移动tail指针，iterms[tail] = item; tail = (tail+1)%n;\n\n出队列的时候，我们需要移动head指针，String ret = iterm[head]; head = (head+1)%n;\n\n不管是head还是tail，加上1后，然后对n取余的目的，在于逆时针移动一格。\n\n# 阻塞队列\n\n我们在队列的基础上增加了阻塞的操作。当队列为空的时候，从队头取数据会被阻塞直到队列中有了数据才能返回；当队列满的时候，插入数据的操作会被阻塞，直到队列中有空闲位置后再插入数据，再返回；基于阻塞队列可以实现"生产者 - 消费者模型"，通过协调"生产者"和"消费者"的个数，可以提高数据的处理效率。\n\n# 并发队列\n\n当几个线程同时操作队列的时候，会引发线程安全的问题。\n\n当多个线程访问某个方法的时候，不管你通过怎样的调用方式或者说这些线程如何交替的执行，我们在主程序中不需要去做任何的同步，这个类的结果行为都是我们设想的正确行为，那么我们就可以说这个类是线程安全的。\n\n但是如果多个线程操作共享变量的时候，就会出现错误了。\n\n线程安全的队列，也叫做并发队列。最简答的直接实现方式是，在enqueue()、dequeue()方法上加锁，实际上基于数组的循环队列(避免搬移数据)的CAS原子操作，可以实现高效的并发队列。\n\n\n# 递归\n\n理解递归，去的过程叫"递"，回来的过程叫"归"。\n\n所有的递归问题都可以用递推公式来表示，例如f(n) = f(n-1) +1，其中f(1)=1.\n\n\n# 递归的三个条件\n\n当打算使用递归的时候，必须满足下面三个条件的内容：\n\n 1. 一个问题的解可以分解为几个子问题的解。\n 2. 这个问题与分解之后的子问题，除了数据规模的不同，求解的思路是完全一样的。\n 3. 存在递归终止的条件。\n\n\n# 如何写"递归"代码\n\n 1. 关键在于找到如何将大问题分解为小问题的规律\n 2. 基于分解后的小问题，来写出相应的递推公式\n 3. 再写出推敲递归终止的条件。\n 4. 最后将递推公式和终止条件都翻译成代码。\n\n\n# 递归代码的注意点\n\n 1. 抽象成一个递推公式。\n 2. 不用去想一层层的调用关系。\n 3. 不要试图用人脑去分解递归的每个步骤。\n 4. 警惕堆栈的溢出，可以通过限定深度大小的方式，适用于最大深度比较小的情况下。\n 5. 警惕重复的计算，可以利用散列表来存放已经求解的值。\n 6. 警惕空间复杂度。\n\n\n# 排序\n\n\n# 如何分析一个"排序算法"\n\n# 排序算法的执行效率\n\n排序算法的执行效率来看，可以分为最好情况/最坏情况/平均情况时间的复杂度，我们需要了解到最好情况下要排序的原始数据是什么样的，最坏情况下要排序的原始数据是什么样的。\n\n当我们对同一阶时间复杂度的排序算法进行性能比较的时候，要把系数、常数、低阶也要考虑进去。\n\n在基于比较的排序算法中，这里会涉及到两种操作：一种是元素比较大小，一种是元素交换或移动。\n\n# 排序算法的内存消耗\n\n在分析一个排序算法的时候，要考虑到算法的空间复杂度的情况，当我们描述一个排序算法是一个原地的排序算法的时候，特指这个程序的空间复杂度是O(1)的排序算法。\n\n# 排序算法的稳定性\n\n一般来说，如果待排序的序列中存在值相等的元素，经过排序之后，相等的元素之间原有的先后顺序不变。如果是这种情况的话，我们就说这个排序算法是稳定的。\n\n同理而言，如果同值的两个数，排序后，前后顺序发生了变化，这个时候，我们就称这个排序算法是不稳定的排序算法。\n\n\n# 有序度\n\n有序度是指，在数组中具有有序关系的元素对的个数。\n\n如果i<j，有序元素对：a[i] <= a[j]，这种情况，就叫做一个有序对。\n\n逆序度：\n\n数组中具有逆序关系的元素对的个数。逆序度 = 满有序度 - 有序度，相对而言就是，如果i < j，逆序元素对是 a[i] > a[j]\n\n满有序度：\n\n一般来说，完全有序的数组的有序度叫做满有序度。\n\n满有序度的大小可以用： n*(n-1) / 2来表示大小。\n\n\n# 冒泡排序\n\n# 排序思想\n\n外层for循环，是排序的趟数。而内层的for循环，是对相邻的两个元素的比较饿交换。\n\n我们在对内层的for循环中，会对相邻的两个元素进行比较，如果满足条件，则需要去进行交换，每交换一次，这个数组的序列，有序度就会相应的加上1，相应的逆序度就减1了。\n\n# 冒泡排序代码\n\n具体的代码如下：\n\n/**\n * bubbleSort\n */\npublic class bubbleSort {\n\n    // 冒泡排序，a表示数组，n表示数组大小\n    public void bubbleSort(int[] a, int n) {\n        if (n < =1) return;\n\n        for (int i=0; i <n; ++i) {\n            // 提前退出冒泡循环的标志位\n            boolean flag = false;\n            for (int j =0; j< n-i-1; ++j) {\n                if (a[j] > a[j+1]) {\n                    //交换\n                    int tmp = a[j];\n                    a[j] = a[j+1];\n                    a[j+1] = tmp;\n                    // 表示有数据交换\n                    flag = true;\n                }\n            }\n            // 没有数据交换，提前退出\n            if (!flag) break;     \n        }   \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n优化的冒泡：\n\n * 冒泡排序的时候，也是将数组的元素，分为了两部分：待排序，和已排序的区间。\n * 在外层的for循环内，设置了一个boolean类型的变量flag，初始的默认值为false。\n * 在进行某一趟的，待排序的各个数组元素，进行比较的时候，发现没有移动位置，那么可以判断出这个时候待排序的区间的各个数组元素，实际上都是已经有序了。\n * 可以这样设置，如果内层的for循环中，如果发生了交换，我们就把flag的值设置为true，如果没有发生交换，那么就什么也没做，保留flag的值为false。\n * 随后，我们紧接着是一个if对flag的条件判断，如果flag的值为flase，那么我们就跳出外层的for循环，就此终止所有的排序操作。\n\n\n# 插入排序\n\n从打扑克牌的时候，不断的拿牌，不断的插入排序的思路来理解。\n\n从动态插入新数据的方式，引入了插入排序的方式。这种插入排序的方法同样使用于 静态的数据排序。\n\n# 排序思想\n\n我们把排序的数组，分为两个区间，已排序区间和未排序区间。\n\n初始的已排序的区间只有一个元素，就是数组的第一个元素。\n\n核心思想：取出未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序的区间中元素为空，这样的话，算法结束。\n\n# 插入排序的操作\n\n插入排序的操作中一共包含两种操作，一种是元素的比较，一种是元素的移动。\n\n元素的比较，是指拿需要排序的数a与已排序区间的元素依次比较比较，找到合适和插入位置。\n\n元素的移动，找到对应的插入点之后，还需要将插入带你之后的元素顺序往后移动一位，这样的话才能腾出位置给元素a来插入。\n\n不同的查找插入方法，对于元素的比较次数还是有所区别的。对于不同的查找插入点方法(从头到尾、从尾到头)，元素的比较次数是有所区别的。\n\n在给定的初始序列中，移动操作的次数总是固定的，就等于逆序度。\n\n# 分析插入排序\n\n插入排序算法的运行并不需要额外的存储空间，空间复杂度是O(1)，所以说是原地的排序算法。\n\n对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保证原有的前后顺序不变，所以说插入排序是稳定的排序算法。\n\n从时间复杂度这个角度来看，当排序的数据已经是有序了，也是不需要搬移任何数据的。当从尾到头的方式去查找插入位置后，算法的主要消耗在那未排序的区间的每个元素去和每个元素进行比较。依次比较的次数，也是一个n的等差数列，得到的算法的时间复杂度是O(n)\n\n当数组的各个元素都是倒序的情况的时候，外层的 for循环去寻找位置的O(n)是跑不掉的，内层的for循环需要对找到的位置往后的元素依次往后移动，所以总体来说，其时间复杂度是O($n^2$).\n\n这个平均时间复杂度，用加权求平均数也是可以的，一个数组为n的，有n+1个位置，每个位置出现的概率都是一样的，可以理解为1/(n+1)，然后分析下，每个槽位大致的查找的位置的时间复杂度和移动数组元素的时间复杂度，可以得到该算法的平均时间复杂度是O($n^2$).\n\n\n# 选择排序\n\n# 排序思想\n\n在选择排序中，也是分为已排序区间和未排序区间的。\n\n我们每次会从未排序区间中找到最小的元素，然后将其放到已排序区间的末尾。\n\n# 代码实现\n\n下面的代码是一个选择排序的代码示例：\n\n// package array;\n/*\n * 选择排序：  3,1,5,2,4,9,6,8,7\n * 稳点性差\n */\npublic class SelectSort {\n\tpublic static void selectSort(int [] x) {\n\t\tif(x.length!=0) {\n\t\t\tint temp = 0;\n\t\t\tfor(int i = 0;i<x.length;i++) {\n\t\t\t\tint min = i;\n\t\t\t\tfor(int j = i;j<x.length;j++) {\n\t\t\t\t\tif(x[j] <= x[min]) {\n\t\t\t\t\t\tmin = j;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttemp = x[min];\n\t\t\t\tx[min] = x[i];\n\t\t\t\tx[i] = temp;\n\t\t\t}\n\t\t}\n\t}\n\tpublic static void main(String[] args) {\n\t\tint [] arr = {3,1,5,2,4,9,6,8,7};\n\t\tSystem.out.print("原始数组是：");\n\t\tfor(int a :arr) {\n\t\t\tSystem.out.print(a+",");\n\t\t}\n\t\tselectSort(arr);\n\t\tSystem.out.println();\n\t\tSystem.out.print("排序之后的数组是：");\n\t\tfor(int i = 0;i<arr.length;i++) {\n\t\t\tSystem.out.print(arr[i]+",");\n\t\t}\n\t}\n \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n最外面的一层for循环，表示的是要进行多少次的排序，在一个n个元素的数组中，需要进行n次排序。内层的for循环中，是用于在无序的各个数组元素中，通过比较选出一个最小的，然后放在有序的数组的末尾。\n\n# 分析选择排序\n\n在选择排序中，不需要额外的存储空间，其空间复杂度是O(1)，所以说选择排序是一个原地排序算法。\n\n选择排序的思想是从未排序的区间元素中，选择一个最小值，然后将与有序区间末尾的无序区间的第一个元素，交换位置。交换位置的过程，会导致相同元素的前后顺序颠倒。\n\n举例来说，比如5，8，5，2，9这样一组数据，使用选择排序算法来排序的话，第一次找到的最小元素为2，与第一个5交换位置，那第一个5和中间的5的顺序就变了，所以就不稳定了。\n\n最好情况的时间复杂度为O($n^2$)，最外层是对n个数组元素去寻找合适的位置，需要n 趟，如果这个时候数组的各个元素是有序的，里面的for循环，还是需要依次为某个位置，去寻找合适的最小值元素，里面的比较次数n，也是少不了的。在这种情况下，数据是否有序，已经不重要了，比较的次数占了大头。\n\n最坏情况的时间复杂度是O($n^2$)，不管是最好最坏的情况下，最外层的for循环n次是少不了的。里面的for循环，各个元素的比较也是少不了的，最好最坏的差异只是在于，找到内层for循环的时候，利用第三方的变量的交换的步骤，比较的复杂度和交换的时间复杂度是一样的都是O($n^2$)\n\n平均情况下的时间复杂度是O($n^2$)\n\n\n# 插入排序比冒泡排序更好\n\n对于相同的数组元素，究竟是使用插入排序好，还是使用冒泡排序好呢？\n\n这两种排序算法中，不管是什么情况的数组元素，元素交换移动的次数是一个固定值。为什么呢？这是由于各个元素的交换移动的次数，是由数据的逆序度决定的，相同的数组元素，其逆序度是一样的。\n\n但是，在冒泡排序中，需要K次交换操作，是利用一个第三方的中间变量，来实现两个相邻元素的交换的，每次这种交换的操作都需要3个赋值语句，所以交换操作总耗时可以理解为3*K个单位时间。\n\n而在插入排序中，由于直接是用了a[j+1] = a[j]的方式直接赋值了，替换了。只需要K个单位时间。\n\n所以在最好、最坏、平均时间复杂度相同的冒泡排序和插入排序的两种排序算法的比较中，较优的是插入排序的算法。\n\n\n# D33(2020/10/22)\n\n今天开始主要是要整理已经学习的一些数据结构和算法的一些常见代码。\n\n参考github中的思路，参考《大话数据结构》中的思路，从C/C++开始总结起来，其次是Java。\n\n在github中虽然有些代码阅读起来比较麻烦，没有统一的风格和格式，甚至还无法运行，有错误。\n\n但是总体来说，还是提供了一些代码的书写的思路和方向。\n\n\n# 抽象数据结构类型\n\n\n# 数据类型\n\n数据类型：是指一组性质相同的值的集合及定义在此集合上的一些操作的总称。\n\n在C语言中，按照取值的不同，数据类型可以分为两类：\n\n * 原子类型：是不可以再分解的基本类型，包括整型、实型、字符型等。\n * 结构类型：由若干个类型组合而成，是可以再分解的。例如，整型数组是由若干整型数据组成的。\n\n抽象是指抽取出事物具有的普遍性的本质。\n\n\n# 抽象数据类型\n\n抽象数据类型(Abstract Data Type, ADT)：是指一个数学模型及定义在该模型上的一组操作。抽象数据类型的定义仅取决于它的一组逻辑特性，而与在计算机内部如何表示和实现无关。\n\n抽象数据类型体现了程序设计中问题分解、抽象和信息隐藏的特性。\n\n抽象数据类型的标准格式：\n\nADT  抽象数据类型名\nData \n     数据元素之间逻辑关系的定义\nOperation\n     操作1\n          初始条件\n          操作结果描述\n     操作2\n          ...\n     操作n\n          ...\nendADT\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 数组(线性表)\n\n\n# 线性表的抽象数据类型\n\n我感觉理解每种数据结构，及其对应的算法，就应该从抽象数据类型开始。我们大多人的学习认识都是平铺直叙的方式，适合我们的是循序渐进的认知方式，从这里开始，是认识理解代码的第一步。\n\nADT 线性表(List)\nData  \n     线性表的数据对象集合为{a1 ... an},每个元素的类型均为DataType。\n     其中，除了第一个元素a1外，每个元素有且只有一个直接前驱元素，\n     除了最后一个元素an外，每一个元素有且只有一个直接后继元素。\n     数据元素之间的关系是一对一的关系。\nOperation \n     InitList (*L): 初始化操作，建立一个空的线性表L。\n     ListEmpty (L): 若线性表为空，返回true，否则返回false.\n     ClearList (*L): 将线性表清空。\n     GetElem (L,i,*e): 将线性表L中的第i个位置元素值返回给e。\n     LocateElem (L,e): 在线性表L中查找与给定值e相等的元素，\n                       如果查找成功，返回该元素在表中序号表示成功；\n                       否则，返回0表示失败。\n     ListInsert (*L,i,e): 在线性表L中的第i个位置插入新元素e。\n     ListDelete (*L,i,*e): 删除线性表L中第i个位置元素，并用e返回其值。\n     ListLength (L): 返回线性表L的元素个数。\nendADT\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 线性表的顺序存储结构\n\n参照下面的代码来实现线性表的顺序存储结构。\n\n# define MAXSIZE 20  /*存储空间初始分配量*/\ntypedef int ElemType;  /*ElemType类型根据实际情况而定，这里假设为int*/\n\ntypedef struct\n{\n    ElemType data[MAXSIEZ];  /*数组存储数据元素，最大值为MAXSIZE*/\n    int length;          /*线性表当前长度*/\n} SqList;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n注意这里几个概念：\n\n * 存储空间的起始位置：数组data，它的存储位置就是存储空间的存储位置。\n * 线性表的最大存储容量：数组长度MaxSize.\n * 线性表的当前长度：length.\n * 数组的长度是存放线性表的存储空间的长度。\n * 线性表的长度是线性表中数据元素的长度。\n * 任意时候，线性表的长度应该小于等于数组的长度。(保证数组的空间可用，不去考虑数组空间不够的时候，数组扩容的问题)\n\n\n# 获得元素操作\n\n也就是线性表ADT中的GetElem，在这个方法中，GetElem (L,i,*e): 将线性表L中的第i个位置元素值返回给e。L是线性表的名称，i是数组的第几个位置的元素。\n\n问题1：*e是什么呢？ 准确的说，应该是ElemType *e是什么呢？\n\n * ElemType使我们自己定义的数组类型，这里我们可以大致用int来替换理解下。\n * 应该来说在我们定义的一个GetElem的方法中，Statu GetElem (SqList L, int i, ElemType *e)中，我们把ElemType *e要理解为ElemType类型的指针，这个指针的变量是e。\n * 单独出现的*e是指，取地址变量e中，所存放的值。\n * 为什么要在这个方法中，有这个指针入参呢？直接写个int e不香吗？这里涉及到一个问题，我们大多数使用数组的元素的各个方法中，可以会涉及到修改数据里面的元素的值的情况，如果不是用对应的指针变量作为方法的入参的话，那么就肯定会出现无法修改数组元素值的情况。\n * 重点：在C/C++ 中，如果在某个方法的入参中，出现了指针变量，注意了，有可能会改变这个变量值的情况。另外一种修改全局变量值的方法，就是引用，在方法的入参中，出现了符号 &.\n\n代码如下：\n\n# define OK 1\n# define ERROR 0\n# define TRUE 1\n# define FALSE 0\ntypedef int Status;\n/*这里把Status也是一个int类型的数据类型*/\n/*Status是函数的类型，其值是函数结果状态代码，如OK等*/\n/*初始条件：顺序线性表L已存在，1<=i<=ListLength(L)*/\n/*操作结果：用*e返回L中第i个数据元素的值*/\nStatus GetElem (SqList L, int i,ElemType *e)\n{\n    if (L.length==0 || i<1 || i>L.length)\n        return ERROR;\n    *e = L.dat[i-1];\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n这里的返回类型Status是一个整型，返回OK代表1，ERROR代表0。\n\n\n# 插入操作\n\n这个操作方法的主要的目的，是往线性表L的第i个位置插入新的元素e。\n\nListInsert (*L,i,e)\n\n插入算法的思路是：\n\n * 如果插入位置不合理，就抛出异常；\n * 如果线性表长度大于等于数组长度，则抛出异常或动态增加容量；\n * 从最后一个元素开始向前遍历到第i个位置，分别将它们都向后移动一个位置；\n * 将要插入元素填入位置i处；\n * 表长加1.\n\n实现代码如下：\n\n# define MAXSIZE 20  /*存储空间初始分配量*/\ntypedef int ElemType;  /*ElemType类型根据实际情况而定，这里假设为int*/\n\n/*定义了一个结构体类型SqList*/\ntypedef struct\n{\n    ElemType data[MAXSIEZ];  /*数组存储数据元素，最大值为MAXSIZE*/\n    int length;          /*线性表当前长度*/\n} SqList;\n\n# define OK 1\n# define ERROR 0\n# define TRUE 1\n# define FALSE 0\ntypedef int Status;\n/*这里把Status也是一个int类型的数据类型*/\n/*Status是函数的类型，其值是函数结果状态代码，如OK等*/\n/*初始条件：顺序线性表L已存在，1<=i<=ListLength(L)*/\n/*操作结果：用*e返回L中第i个数据元素的值*/\n\n/*初始条件：顺序线性表L已存在，1<= i <= ListLength(L)*/\n/*操作结果：在L中第i个位置上插入新的数据元素e，L的长度加1*/\n\nStatus ListInsert (SqList *L, int i, ElemType e)\n{\n    int k;\n    /*顺序线性表已经满*/\n    if (L->length == MAXSIZE)\n      return ERROR;\n    /*当i不在范围内时*/\n    if (i<1 || i>L->length+1)\n      return ERROR;\n    /*若插入数据位置不在表尾*/\n    if (i<= L->length) {\n        /*将要插入位置后数据元素向后移动一位*/\n        /*length-1是目前线性表中，在数组中的最大的下标*/\n        /*线性表的第i个位置，也就是数组下标是i-1的位置*/\n        for (k= L->length -1; k>=i-1; k--) {\n            L->data[k+1] = L->data[k];\n        }\n    }\n    /*将新元素插入*/\n    L->data[i-1]=e;\n    L->length++;\n    return OK;\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 删除操作\n\n这个删除方法中，也就是从一个线性表L中，删除第i个数据元素。\n\n删除算法的思路：\n\n * 如果删除位置不合理，抛出异常；\n * 取出删除元素；\n * 从删除元素位置开始遍历到最后一个元素位置，分别将它们都向前移动一个位置。\n * 表长度减去1\n\n参考的代码如下：\n\n# define MAXSIZE 20  /*存储空间初始分配量*/\n# define OK 1\n# define ERROR 0\n# define TRUE 1\n# define FALSE 0\ntypedef int Status;\n/*这里把Status也是一个int类型的数据类型*/\n/*Status是函数的类型，其值是函数结果状态代码，如OK等*/\n/*初始条件：顺序线性表L已存在，1<=i<=ListLength(L)*/\n/*操作结果：用*e返回L中第i个数据元素的值*/\n\ntypedef int ElemType;  /*ElemType类型根据实际情况而定，这里假设为int*/\n\ntypedef struct\n{\n    ElemType data[MAXSIZE];  /*数组存储数据元素，最大值为MAXSIZE*/\n    int length;          /*线性表当前长度*/\n} SqList;\n\n/*初始条件：顺序线性表L已存在，1<= i <= ListLength(L) */\n/*操作结果：删除L的第i个数据元素，并用e返回其值，L的长度减1*/\nStatus ListDelete (SqList *L, int i, ElemType *e) {\n    int k;\n    /*线性表为空*/\n    if (L->length == 0) {\n        return ERROR;\n    }\n    /*删除位置不正确*/\n    if (i<1 || i>L->length) {\n        return ERROR;\n    }\n    *e = L->data[i-1];\n    /*如果删除不是最后的位置*/\n    if (i< L->length) {\n        /*将删除位置后继元素前移*/\n        for (k = i; k< L->length; k++) {\n            L->data[k-1] = L->data[k];\n        }\n    }\n    L->length--;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n\n# D34(2020/10/23)\n\n今天主要是要复习的是链表的各种算法代码。\n\n\n# typedef与struct\n\nstruct是用来定义结构体的，typedef是用来用于自定义类型的。\n\n参考网上的文档。\n\n> https://blog.csdn.net/weixin_41262453/article/details/88120561\n\n\n# 定义结构体/无变量/无类型\n\n这里定义的是最初始的结构体，没有基于定义结构体的时候，定义了该结构体的变量，也没有给这个结构体定义一个自定义的类型，这个结构体中也没有结构体指针。\n\nstruct node {\n    // 一些基本的数据结构或自定义的数据类型\n};\n\n\n1\n2\n3\n\n\n这个时候如果要定义这种类型的结构体的变量时，要这么写struct node n;\n\n\n# 定义结构体(包含结构体变量)\n\n这个时候定义的结构体中，顺带定义了结构体类型的变量，而且这种变量和普通变量一样。这个时候，结构体理解为是一种数据类型。包含结构体类型的变量，和指向这种结构体类型内存地址指针的变量。\n\nstruct studentInfo {\n\tint id;\n\tchar gender; //\'F\' or \'M\'\n\tchar name[20];\n\tchar major[20];\n} Alice, Bob, stu[1000], *p;\n\n\n1\n2\n3\n4\n5\n6\n\n\n在这个结构体定义的例子中，studentInfo是结构体的名字，Alice和Bob是代表着两个结构体变量，stu[1000]代表的是这种结构体的数组，而*p代表的是这种结构体类型的指针变量p，这个变量p存储着一个这种结构体数据类型的内存地址。\n\n\n# 定义结构体(typedef定义了这种结构体的别名)\n\n在这个案例中，会使用到了typedef这个关键字了，定义了这种结构体数据类型的一个别名。\n\ntypedef struct node \n{\n    int no;\n    char name[10];\n} stu, student;\n\n\n1\n2\n3\n4\n5\n\n\n后面的stu和student都是这种结构体类型的别名。我们在声明这种结构体类型的变量的时候，可以这么写了。\n\nstu a1; 或者是student a2;\n\n\n# typedef定义符合类型(指针或数组)\n\n这个理解起来，有些绕口，但是还是会有这样的代码出现。\n\ntypedef还可以用来掩饰复合类型，如指针和数组。\n\n定义一个typedef，每当要用到相同类型和大小的数组时，可以这样书写：\n\ntypedef char Line[81];\n\n此时Line类型即代表了具有81个元素的字符数组(也就是char[81]的意思)，使用方法，例如Line text.\n\n再比如下面，定义了下面，利用typedef定义结构体别名，单独来定义了这个结构体类型的指针变量。\n\n/*线性表的单链表存储结构*/\ntypedef struct Node\n{\n    ElemType data;\n    struct Node *next;\n} Node;\ntypedef struct Node *LinkList;  /*定义LinkList*/\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n其中typedef struct Node *LinkList中的struct可以省略。\n\nLinkList就Node类型的指针变量。\n\n\n# 定义结构体(内含同类型结构体指针变量)\n\n这个结构体Node里面又嵌套了一个Node的数据类型的指针变量，这个指针变量中存放的内存地址，是下个结点的内存地址。\n\nstruct node \n{\n    int age;\n    struct node *next;\n};\n\n\n1\n2\n3\n4\n5\n\n\n通过测试，不管是在C还是C++中，结构体里面的那个struct也是可以去掉的。\n\n\n# 定义结构体(typdef与内含结构体指针的结合)\n\n先看下面的例子是错误的写法。\n\ntypedef struct \n{\n\tint age;\n\tStudent1 *next; \n}Student1,*StudentPtr;\n\n\n1\n2\n3\n4\n5\n\n\n这是由于Student1是在结构体的末尾定义的，在结构体的内部是无法识别这个被typedef定义为别名Student1，这个类型的。\n\n应该改为如下的写法：\n\ntypedef struct Student1\n{\n\tint age;\n\tStudent1 *next; \n}Student1,*StudentPtr;\n\n\n1\n2\n3\n4\n5\n\n\n里面的*next前面的Student1实际上是一开始typedef struct后面的名字，这样里面的结构体指针定义的时候，才能正确识别。\n\n\n# 结构体指针初始化\n\n\n# 单链表\n\n下面是单链表的存储结构：\n\n/*线性表的单链表存储结构*/\ntypedef struct Node\n{\n    ElemType data;\n    struct Node *next;\n} Node;\ntypedef struct Node *LinkList;  /*定义LinkList*/\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n从这个单链表的存储结构体来看，这个结点由存放数据元素的数据域和存放后继结点地址的指针域组成。\n\n定义了一个Node类型的指针变量，用来存储Node类型的内存地址，该变量的名称叫做LinkList.\n\n假设p是指向该单链表的第i个元素的指针，结点$a_i$的数据域，可以用p->data来表示，也就是p->data = $a_i$ , 同样p->next指向的是$a_i$的下一个元素，也就是指向第i+1个元素，即指向$a_{i+1}$的指针，那么p->next->data = $a_{i+1}$ .\n\n\n# 单链表的读取1(大话系列)\n\n和数组有些类似，同样要实现的是读取线性表的第i个元素。该操作的方法，同样定义为Status GetElem (LinkList L, int i, ElemType *e)\n\n获得链表第i个数据的算法的思路：\n\n 1. 声明一个指针p指向链表第一个结点，初始化j从1开始；\n 2. 当j<i时，就遍历链表，让p的指针向后移动，不断指向下一个结点，j累加1；\n 3. 若到链表末尾p为空，则说明第i个元素不存在；\n 4. 否则查找成功，返回结点p的数据。\n\n代码如下：\n\n# define OK 1\n# define ERROR 0\n# define TRUE 1\n# define FALSE 0\n/*这里把Status也是一个int类型的数据类型*/\n/*Status是函数的类型，其值是函数结果状态代码，如OK等*/\ntypedef int Status;\n\n/*ElemType类型根据实际情况而定，这里假设为int*/\ntypedef int ElemType;  \n\n/*线性表的单链表存储结构*/\ntypedef struct Node\n{\n    ElemType data;\n    struct Node *next;\n} Node;\ntypedef struct Node *LinkList;  /*定义LinkList*/\n\n\n/*初始条件：顺序线性表L已存在，1<= i <= ListLength(L)*/\n/*操作结果：用e返回L中第i个数据元素的值*/\nStatus GetElem (LinkList L, int i, ElemType *e) \n{\n    int j;\n    /*声明了指针p*/\n    LinkList p;\n    /*让p指向链表L的第一个结点*/\n    /*也可以改写为p=Head->next;这里的L可以理解为头指针*/\n    p = L->next;\n    /*j为计数器*/\n    j = 1;\n    /*p不为空或计数器j还没有等于i的时候，循环继续*/\n    while (p && j<i) {\n        /*让p指向下一个结点*/\n        p = p->next;\n        ++j;\n    }\n    /*第i个元素不存在*/\n    /*定位位置不合理：空表或i小于0或i大于表长*/\n    if (!p || j>i) {\n        return ERROR;\n    }\n    /*取第i个元素的数据*/\n    *e = p->data;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 单链表(按位序查找)\n\n上面的代码的实际情况，根据提供需要查找的线性表(链表)的第几个位置，来进行返回。\n\n//按位置来查找链表元素\nint GetElem(int i) \n{\n    // 获取第i个数据元素的值\n    Node *p;\n    p = Head->next;\n    int j = 1;\n    while (p && j<i) {\n        p=p->next;\n        j++;\n    }\n    // 定位位置不合理：空表或i小于0或i大于表长\n    if (!p || j>i) {\n        cout << "位置异常";\n        return -1;\n    } else {\n        return p->data;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 单链表查询(按值查找)\n\n按照来查找的操作是，在链表中来查找是否有结点值等于给定值key的结点，若有，则返回首次找到的值为key的结点的存储位置；否则返回NULL。查找过从开始结点出发，顺着链表逐个将结点的值和给定值key做比较。\n\n//按值来查找，匹配值的链表的位置\nint LocateElem(int e)\n{\n    int j=1;\n    Node *p;\n    p = Head->next;\n    while (p && p->data!=e) {\n        p = p->next;\n        j++;\n    }\n    if (p == NULL) {\n        return 0; //0表示不存在，而非0，则表示存在，返回位置\n    } else {\n        return j;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 单链表的插入\n\n已知一个单链表，在这个单链表的第i个结点位置上，插入一个新的结点，这个新结点中有新插入的结点元素的值。\n\n单链表的第i个数据插入结点的算法思路：\n\n 1. 声明一结点p指向链表的第一个结点，初始化j从1开始；\n 2. 当j<i的时候，就遍历链表，让p的指针向后移动，不断指向下一个结点，j累加1；\n 3. 若到链表末尾p为空，则说明第i个元素不存在；\n 4. 否则查找成功，在系统中生成一个空结点s；\n 5. 将数据元素e赋值给s->data;\n 6. 单链表的插入标准语句s->next = p->next； p->next=s;\n 7. 返回成功。\n\n具体的代码如下，重点在于，p结点是要插入位置的前面一个结点，先将原有的p->next值 ，赋值给结点s->next；然后将新增的s结点的内存地址赋值给p->next.\n\n/*初始条件：顺序线性表L已存在，1<= i <= ListLength(L)*/\n/*操作结果：在L中第i个位置之前插入新的数据元素e，L的长度加1*/\nStatus ListInsert(LinkList *L, int i, ElemType e) \n{\n    int j;\n    LinkList p,s;\n    p = *L;\n    j = 1;\n    //寻找第i个结点\n    while (p && j < i) {\n        p = p->next;\n        ++j;\n    }\n    // 第i 个元素不存在\n    if (!p || j>1) {\n        return ERROR;\n    }\n    // 生成新结点, malloc是C语言的标准函数\n    s = (LinkList) malloc(sizeof(Node));\n    s->data = e;\n    // 将p的后继结点赋值给s的后继\n    s->next = p->next;\n    // 将s赋值给p的后继\n    p->next = s;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n简化后的C++的单链表插入的代码如下：\n\nvoid ListInsert(int i, int e)\n{\n    int j = 0;\n    Node *p;\n    p = Head;\n    // 定位到插入点之前\n    while (p && j < i-1) {\n        p = p->next;\n        j++;\n    }\n    // 插入位置不合理，i<0或者i>表长\n    if (!p || j > i-1) {\n        cout << "位置异常，结点插入失败！";\n        return;\n    } else {\n        Node *s;\n        s = new Node;\n        s->data = e;\n        s->next = p->next;\n        p->next = s;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n单链表的插入数据元素，无需像顺序表那样移动其后续数据元素，算法的时间主要耗费在查找正常的插入位置，最好O(1)，最坏O(n)，平均O(n).\n\n\n# 单链表的创建\n\n对于每个链表来说，它所占用空间的大小和位置是不需要预先分配划定的，可以根据系统的情况和实际的需求即时生成。\n\n创建单链表的过程就是一个动态生成链表的过程。即从"空表"的初始状态起，依次建立各元素结点，并逐个插入链表。\n\nvoid CreateListHead(LinkList *L, int n) 的单链表的整表创建的算分思路：\n\n 1. 声明一结点p和计数器变量i;\n 2. 初始化一空链表L;\n 3. 让L的头结点的指针指向NULL，即建立一个带头结点的单链表；\n 4. 循环：\n    * 生成一新结点赋值给p；\n    * 随机生成一数组赋值给p的数据域p->data;\n    * 将p插入到头结点与前一新结点之间。\n\n# 头插法\n\n顾名思义，就是始终让新结点在第一的位置。我们可以把这种算法简称为头插法。\n\nC语言的代码示例如下：\n\n/*随机产生n个元素的值，建立带表头结点的单链线性表L(头插法)*/\nvoid CreateListHead(LinkList *L, int n) \n{   \n    //这里的入参中的  LinkList *L  是否可以改为LinkList L\n    // 结点p是要插入的新结点\n    // 结点L的指针指向的地址，可以理解为头指针\n    LinkList p;\n    int i;\n    /*初始化随机数种子*/\n    srand(time(0));\n    *L = (LinkList) malloc(sizeof(Node));\n    /*先建立一个带头结点的单链表*/\n    (*L)->next = NULL;\n    for (i=0; i<n; i++) {\n        /*生成新结点*/\n        p = (LinkList) malloc(sizeof(Node));\n        /*随机生成100以内的数字*/\n        p->data = rand()%100+1;\n        p->next = (*L)->next;\n        /*插入到表头*/\n        (*L)->next = p;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n重点代码是，一开始创建了一个带有头结点的单链表，这个结点中没有存放任何的数据，只是这个头结点的后继指针指向NULL，指向这个头结点的指针(L)理解为head指针。\n\n关键代码：p->next = (*L)->next;和(*L)->next = p;\n\n用C++的代码来描述：\n\nvoid CreateList1(int n)\n{\n    //头插法创建线性表\n    // p指向头结点的头指针，s是新增的结点\n    Node *p, *s;\n    p = Head;\n    cout << "请依次输入" <<n<< "个数据元素值：" <<endl;\n    for (int i=1; i<=n; i++) {\n        // 新建结点\n        s = new Node;\n        cin >> s->data;\n        // 新结点插入表头\n        s->next = p->next;\n        // 将指针变量s的值赋值给p->next\n        p->next = s;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\nC++的代码相对来说比较清晰，创建了Node 类型的 结点p和结点s这两个指针变量，其中变量p存放的理解为是头结点上的头指针，而变量s存放的理解为是要新增结点上面的指针变量的内存地址。\n\n# 尾插法\n\n头插法虽然算法简单，如果按$a_1$，$a_2$, ... , $a_n$的顺序插入结点元素，头插法插入后展示的链表的各个元素的情况，正好和输入的顺序相反。即首先被插入的结点是线性表的最后一个数据元素$a_n$，最后被插入的结点是线性表的第一个数据元素$a_1$。\n\n为了实现创建链表过程中结点的输入顺序与结点实际的逻辑次序相同，这里可以采用尾插入法。尾插入法每次讲新生成的结点插入到当前链表的表尾上。这里需要增加一个尾指针，始终指向当前链表的尾结点。\n\n如下是C语言的代码：\n\n/*随机产生n个元素的值，建立带表头结点的单链线性表L(尾插法)*/\nvoid CreateListTail(LinkList *L, int n) \n{\n    //p是新增的结点，r是尾部指针\n    LinkList p,r;\n    int i;\n    /*初始化随机数种子*/\n    srand (time(0));\n    /*为整个线性表*/\n    *L = (LinkList) malloc(sizeof(Node));\n    /*r为指向尾部的结点*/\n    r = *L;\n    for (i = 0; i < n; i++) {\n        /*生成新结点*/\n        p = (Node *) malloc(sizeof(Node));\n        /*随机生成100以内的数字*/\n        p->data = rand()%100+1;\n        /*将表尾终端结点的指针指向新结点*/\n        r->next = p;\n        /*将当前的新结点定义为表尾终端结点*/\n        r = p;\n    }\n    /*表示当前链表结束*/\n    r->next = NULL;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n重点在于两段代码r->next = p; r = p;, 在这里L与r的关系，L是指整个单链表，而r是指向的是尾结点的变量，r会随着循环不断地变化结点，而L则是随着循环增长为一个多结点的链表。\n\n如下是C++的代码示例：\n\nvoid CreateList2(int n)\n{\n    //尾插法创建线性表\n    // p结点是尾巴指针变量\n    // s结点是新插入的结点的指针变量\n    Node *p,*s;\n    p = Head;\n    cout << "请依次输入："<< n <<"个数据元素值:" <<endl;\n    for (int i = 1; i <= n; i++) {\n        // 新建结点\n        s = new Node;\n        cin >> s->data;\n        // 新结点插入表尾\n        p->next = s;\n        p = s;\n    } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 单链表的单个元素删除\n\n这里所指的是将这个单链表中的第i个结点删去，也就是改变$a_{i-1}$ 、$a_i$与$a_{i+1}$之间的链接关系。\n\n因为在单链表中结点$a_i$的存储地址是在其前驱结点$a_{i-1}$的指针域next中。\n\n 1. 所以必须首先找到$a_{i-1}$的存储位置p。\n 2. 然后令p->next指向$a_i$的后继结点，即将$a_i$从链表上摘下。\n 3. 最后释放结点$a_i $的空间。\n\n单链表删除第i个数据结点的步骤如下：(p为ai-1 结点，q为ai结点)\n\n 1. 声明一结点p指向链表第一个结点，初始化j从1开始\n 2. 当j<i的时候，就遍历链表，让p的指针向后移动，不断指向下一个结点，j累加1；\n 3. 若到链表末尾p为空，则说明第i个元素不存在；\n 4. 否则查找成功，将欲删除的结点p->next赋值给q；\n 5. 单链表的删除标准语句p->next = q->next\n 6. 将q结点中的数据赋值给e，作为返回；\n 7. 释放q结点；\n 8. 返回成功。\n\n如下是C语言的代码：\n\n/*初始条件：顺序线性表L已存在，1<=i<=ListLength(L)*/\n/*操作结果：删除L的第i个数据元素，并用e返回其值，L的长度减1*/\nStatus ListDelete (LinkList *L, int i, ElemType *e) \n{\n    int j;\n    LinkList p, q;\n    p = *L;\n    j = 1;\n    /*遍历寻找第i个元素*/\n    while (p->next && j < i) {\n        p = p->next;\n        ++j;\n    }\n    if (!(p->next) || j>i) {\n        /*第i个元素不存在*/\n        return ERROR;\n    }\n    q = p->next;\n    /*将q的后继赋值给p的后继*/\n    p->next = q->next;\n    /*将q结点中的数据给e*/\n    *e = q->data;\n    /*让系统回收此结点，释放内存*/\n    free(q);\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n相对C++的代码如下所示：\n\n// i 是第i个位置，删除数据元素e\nNode* ListDelete(int i, int e)\n{\n    int j = 0;\n    Node *p;\n    p =  Head;\n    // 定位到删除点之前\n    while (p && j < i-1)\n    {\n        p = p->next;\n        j++;\n    }\n    if (!p || j > i-1)\n    [\n        cout<<"位置异常，结点插入失败!";\n        return ;\n    ]\n    // 插入位置不合理，i<0或i>表长\n    else {\n        Node *s;\n        s = p;\n        p->next = p->next->next;\n        return s;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 单链表整体删除\n\n当我们不打算使用这个单链表的时候，我们就需要将它销毁，其实也就是在内存中将它释放掉。\n\n单链表整表删除的算法思路如下：\n\n 1. 声明一个结点p和q\n 2. 将第一个结点赋值给p；\n 3. 循环：\n    * 将下一个结点赋值给q；\n    * 释放p；\n    * 将q赋值给p\n\nC语言中实现的代码如下：\n\n/*初始条件：顺序线性表L已经存在，操作结果：将L重置为空表*/\nStatus ClearList(LinkList *L)\n{\n    LinkList p,q;\n    /*p指向第一个结点*/\n    p = (*L)->next;\n    /*没到表尾*/\n    while (p)\n    {\n        q = p->next;\n        free(p);\n        p=q;\n    }\n    /*头结点指针域为空*/\n    (*L)->next = NULL;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n这里需要注意的是，q变量的存在还是很有意义的。在循环体内，不能直接写free (p) ; p = p->next，如果直接free(p)的话，那么就删除了p的整个结点，包含 p的数据域和指针域，就找不到后面的一个链表元素了。这里用q=p->next的目的，就是在于在删除前，先保留住p结点后面的结点的指针地址。\n\n\n# D35(2020/10/26)\n\n今天需要总结的是静态链表的相关知识点。\n\n\n# 静态链表的概念\n\n静态链表是指用一维数组表示的单链表。在静态链表中，用数据元素在数组中的下标作为单链表。\n\n静态链表的特点如下：\n\n 1. 静态的含义是指静态链表采用一维数组表示，表的容量是一定的，因此称为静态。\n 2. 静态链表中结点的指针域next存放的是其后继结点在数组中的位置(即数组下标)\n\n\n# 静态链表的定义\n\n下面的代码就是实现了一个静态链表的结构，实际上是一个结构体数组。\n\n/*线性表的静态链表存储结构*/\n/*假设链表的最大长度是1000*/\n# define MAXSIZE 1000\ntypedef struct \n{\n    ElemType data;\n    /*游标Curosr，为0的时候表示无指向*/\n    /*cur相当于单链表中的next指针*/\n    /*存放该元素的后继在数组中的下标*/\n    int cur;\n} Component, StaticLinkList[MAXSIZE];\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n一般来说对于数组的第一个和最后一个元素作为特殊元素处理，不存数据。\n\n例如数组a[0]作为备用链表表头，备用链表的作用是回收数组中未使用或之前使用过(目前未使用)的存储空间，留待后期使用。也就是说备用链表的表头位于数组下标为0 (a[0])的位置。\n\n数据链表的表头位于数组下标为1 (a[1])的位置。\n\n下面是C++中的代码，定义的静态列表的结构：\n\n\n# 初始化静态列表\n\n下面是用C写的静态列表的初始化，在初始化的时候，我们只是在这里定义了 每个结构体数组中的cur的值为下一个元素在这个结构体数组中存储的下标，相当于单链表中的next 指针。\n\n/*将一维数组space中各分量链成一备用链表*/\n/*space[0].cur为头指针，"0"表示空指针*/\nStatus InitList(StaticLinkList space)\n{\n    int i;\n    for (i=0; i<MAXSIZE-1; i++) {\n        space[i].cur = i+1;\n    }\n    /*目前静态链表为空，最后一个元素的cur为0ss*/\n    space[MAXSIZE-1].cur = 0;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 静态链表的插入操作\n\n定义了一个分配节点malloc的函数。\n\n/*若备用空间链表非空，则返回分配的结点下标，否则返回0*/\nint Malloc_SLL(StaticLinkList space)\n{\n    /*当前数组第一个元素的cur存的值*/\n    /*就是要返回的第一个备用空闲的下标*/\n    int i = space[0].cur;\n    \n    if (space[0].cur) {\n        /*由于要拿出一个分量来使用了，所以我们*/\n        /*就得把它的下一个分量用来做备用*/\n        space[0].cur = space[i].cur;\n    }\n    return i;\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n下面是在一个静态的链表中，插入一个元素。这个时候是不需要移动数据的。例如这个时候需要在第i的位置上插入一个元素e，那就先让第i-1位置上的元素的cur下标指向先插入的元素e，然后将新的元素e的下标指向原来第i 位置所在的下标就可以了。\n\n/*在L中第i个元素之前插入新的数据元素e*/\nStatus ListInsert(StaticLinkList L, int i, ElemType e)\n{\n    int j, k, l;\n    /*注意k首先是最后一个元素的下标*/\n    k = MAX_SIZE -1;\n\n    if (i < 1 || i > ListLength(L) + 1) {\n        return ERROR;\n    }\n    /*获得空闲分量的下标*/\n    j = Malloc_SLL(L);\n    \n    if (j) {\n        /*将数据赋值给此分量的data*/\n        L(j).data = e;\n        // 找到第i个元素之前的位置\n        for (l = 1; l <= i -1; l++) {\n            k = L[k].cur;\n        }\n        // 把第i个元素之前的cur赋值给新元素的cur\n        L[j].cur = L[k].cur;\n        // 把新元素的下标赋值给第i个元素之前元素的cur\n        L[k].cur = j;\n        return OK;\n    }\n    return ERROR;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n下面是C++的实现代码：\n\n// 插入数据元素\nint SLinkList::SLinkListMalloc()\n{\n    // 若链表非空，则返回分配的结点下标，否则返回0\n    int i;\n    i = space[0].next;\n    if (space[0].next) {\n        space[0].next = space[i].next;\n    }\n    return i;\n}\n\nvoid SLinkList::SLinkListInsert(int i, int e)\n{\n    // 在静态链表中第i个位置插入数据元素\n    int j =1, m;\n    while (j<i-1) {\n        j = space[j].next;\n    }\n    m = SLinkListMalloc();\n    space[m].data = e;\n    space[m].next = space[j].next;\n    space[j].next = m;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 静态链表的删除操作\n\nC语言中实现的释放结点的函数free()。\n\n/*删除在L中第i个数据元素e*/\nStatus ListDelete (StaticLinkList L, int i)\n{\n    int j, k;\n    if (i < 1 || i > ListLength(L)) {\n        return ERROR;\n    }\n    k = MAX_SIZE -1;\n    for (j = 1; j <= i -1; j++) {\n        k = L[k].cur;\n    }\n    j = L[k].cur;\n    L[k].cur = L[j].cur;\n    Free_SSL(L, j);\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n最终实现的删除代码如下：\n\n/*将下标为k的空想结点回收到备用链表*/\nvoid Free_SSL(StaticLinkList space, int k) {\n    // 把第一个元素cur值赋给要删除的分量cur\n    space[k].cur = space[0].cur;\n    // 把要删除的分量下标赋值给第一个元素的cur\n    space[0].cur = k;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n下面是C++的代码实现：\n\n//删除数据元素\nint SLinkList::SLinkListDelete(int i)\n{\n    //  删除表中第i个数据元素，并用e返回其值\n    int j = 1, k=Maxsize-1;\n    int e;\n    if (i<1 || i>length) {\n        cout<<"位置不合法";\n        return -1;\n    }\n    for (j=1; j<i; j++) {\n        k = space[k].next;\n    }\n    j = space[k].next;\n    space[k].next = space[j].next;\n    e = space[j].data;\n    free(j);\n    return e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# D36(2020/10/27)\n\n今天主要是学习和复习两块的知识点，循环链表和双向链表，以及两者的结合，双向循环链表。\n\n增加学习栈的内容。\n\n\n# 循环链表\n\n将单链表中终端结点的指针端由空指针改为指向头结点(或理解为第一个元素结点)，就使得整个单链表形成一个环，这种头尾相接的单链表称为单循环链表，简称为循环链表。\n\n循环链表解决了一个很麻烦的问题，就是如何从当中一个结点出发，访问到链表的全部结点。\n\n在循环链表的遍历操作中，单循环链表的终止条件不再像非循环链表那样判断某个指针是否为空，而是判断该指针是否等于某一指定指针(如头指针或尾指针)。\n\n循环链表的类定义与单链表一样，只是使用时间尾结点的指针域由空改为指向头结点。\n\n\n# 双向链表+双向循环链表\n\n双向链表是在单链表的每个结点中，再设置一个指向其前驱结点的指针域。所以在双向链表中的结点都有两个指针域，一个指向直接后继，另一个指向直接前驱。\n\n我们更多的是将双向链表和循环链表组合而成，双向循环链表。\n\n\n# 双向链表的结构\n\n下面使用C语言的结构体来定义的双向链表的存储结构。\n\n// 线性表的双向链表存储结构\ntype struct DulNode\n{\n    ElemType data;\n    // 直接前驱指针\n    struct DulNode *prior;\n    // 直接后继指针\n    struct DulNode *next;\n} DulNode, *DuLinkList;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 双向循环链表带头结点的空链表\n\n如下图示是，双向链表的循环带头结点的空链表。\n\n\n\n\n# 插入一个结点\n\ns是存储元素e的，要插入的结点。我们需要在P结点后面插入s结点。\n\n整体的思路是：\n\n * 先搞定s的前驱和后继；\n * 再搞定"后结点"的前驱；\n * 最后解决"前结点"的后继\n\n\n\n插入的代码示例如下：\n\n//如图中的1，把p赋值给s的前驱\ns->prior = p;\n//如图中的2，把p->next赋值给s的后继\ns->next=p->next;\n//如图中的3，把s赋值给p->next的前驱\np->next->prior = s;\n//如图中的4，把s赋值给p的后继\np->next = s;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 删除一个结点\n\n在循环双向链表中，删除一个结点的话，需要下面两个步骤：\n\n这两个语句的先后顺序是可以颠倒的，图示中，表明的是如何删除一个结点p.\n\n从结点p这个角度出发，可以顺序修改结点p的后继，结点p的前驱。\n\n\n\n// 把p->next赋值给p->prior的后继，如图示中的1\np->prior->next = p->next;\n// 把p->prior赋值给p->next的前驱，如图示中的2\np->next->prior = p->prior;\n// 释放结点\nfree(p);\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 栈\n\n栈是限定仅在表尾进行插入和删除操作的线性表。\n\n允许插入和删除的一端称为栈顶(top)，如果顺序栈，那就是表尾；另外一端称为栈底。\n\n栈的插入操作，叫做进栈，也称为压栈、入栈。push\n\n栈的删除操作，叫做出栈，也有的叫做弹栈。pop\n\n用一个栈顶指针top来指示栈顶，栈底指针base来指示栈底。\n\n\n# 栈的抽象数据类型\n\n我们把栈的插入操作叫做push，把栈的删除操作叫做pop。\n\nADT 栈(stack)\nData \n    同线性表。元素具有相同的类型，相邻的元素具有前驱和后继关系。\nOperation\n    InitStack(*S): 初始化操作，建立一个空栈S。\n    DestoryStack(*S): 若栈存在，则销毁它。\n    ClearStack(*S): 将栈清空\n    StackEmpty(S): 若栈为空，返回true，否则返回false.\n    GetTop(S,*e): 若栈存在且非空，用e返回S的栈顶元素.\n    Push(*S, *e): 若栈S存在，插入新元素e到栈S中并成为栈顶元素\n    Pop(*S, *e): 删除栈S中栈顶元素，并用e返回其值\n    StackLength(S): 返回栈S的元素个数。\nendADT\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 顺序栈的概念\n\n顺序栈，即用一组地址连续的存储单元一次存放自栈底到栈顶的数据元素。top指针来指示栈顶元素在顺序栈中的位置。\n\n顺序栈的栈顶是数组的末尾(线性表的末尾)，栈底是数组的开头(线性表的开头)。\n\n如果用top=0来表示空栈的话，由于数组的下标一般约定从0开始，如此设定会有些不便。\n\n用如下的方法对栈进行初始化，其中StackSize表示栈当前用于存储数据元素的数组长度。\n\n顺序栈中，栈底指针base始终指向栈底的位置，所以若base=NULL，则表明栈结构不存在。\n\n\n\n(1) 栈空的时候，栈顶指针top=base.\n\n(2) 入栈的时候，栈顶指针top=top+1.\n\n(3) 出栈的时候，栈顶指针top=top-1.\n\n(4) 栈满的时候，栈顶指针top=StackSize-1.\n\n\n# 顺序栈的类定义\n\n栈的抽象数据类型的类定义在顺序栈存储结构下用C++实现。\n\nclass SqStack\n{\n    private:\n    // 栈底指针\n      int *base;\n    // 栈顶\n      int top;\n    // 栈容量\n      int stacksize;\n    public:\n    // 构建一个长度为m的栈\n      SqStack(int m);\n    // 销毁栈\n      ~SqStack() {delete [] base; top=-1; stacksize=0};\n    // 入栈\n      void Push(int e);\n    // 出栈\n      int Pop();\n    // 获取栈顶元素\n      int GetTop();\n    // 测栈空\n      int StackEmpty();\n    // 显示栈中元素\n      void StackTranerse();\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 顺序栈入栈算法\n\n顺序栈入栈算法的操作步骤如下。\n\n第一步：如果栈满，则追加存储空间；否则直接执行下一步。\n\n第二步：将新元素插入栈顶位置。\n\n第三步：栈顶指针增加1.\n\n下面是C++的算法实现：\n\nvoid Push(int e)\n{\n    if (top == stacksize -1) {\n        cout<<"栈满，无法入栈";\n        return;\n    }\n    top++;\n    base[top] = e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 顺序栈出栈算法\n\n顺序栈的出栈算法的操作步骤如下：\n\n第一步：如果栈空，则返回错误信息，操作结束；否则执行下一步\n\n第二步：取出栈顶元素赋值给e\n\n第三步：栈顶指针减去1，并返回e\n\n下面是通过代码给出具体的出栈算法:\n\nint Pop() \n{\n    int e;\n    if (top==-1) {\n        cout<<"栈空，不能出栈";\n        return -1;\n    }\n    //base是个int类型的指针，这种写法对吗？\n    e = base[top--];\n    return e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 链式栈的概念\n\n如果栈采用的是链式存储，那么我们就把这种栈叫做链栈。通常链栈采用单链表表示，链栈的插入和删除操作只能在表头进行(链表头)，这一点倒是和数组结构的线性表有些区别的，线性栈是在数组尾巴(表尾)进行的。\n\n为什么这么做呢？\n\n * 数组从尾巴插入删除方便，算法的时间复杂度是O(1)，如果从数组头部插入删除，那么每次操作的时间复杂度就是O(n).\n * 链式表有头插法和尾插法两种，如果考虑的头插法，那么就是从表头来插入和删除的。\n\n1). 入栈的时候，将新创建的结点s加入到链表表头，并将栈顶指针top指向s.\n\n2). 出栈时，栈顶指针top指向链表第一个结点的下一个结点。\n\nQ：链式栈中，为什么不考虑链的尾插法，而是采用头插法呢？\n\n\n# 链栈的类定义\n\n链栈的结点结构与单链表的结点结构相同，因此链栈类定义的C++描述如下：\n\nStruct Node {\n    int data;\n    Node *next;\n};\nclass LinkStack {\n    private:\n    // 栈顶指针即链栈的头指针\n        Node *top;\n    public:\n    // 构造函数，置空链栈\n        LinkStack() {top=NULL};\n    // 析构函数，释放链栈中各结点的存储空间\n        ~LinkStack();\n    // 将元素e入栈\n        void Push(int e);\n    // 将栈顶元素出栈\n        int Pop();\n    // 取栈顶元素(并不删除)\n        int GetTop() {if (top!=NULL) retur top->data;}\n    // 判断链栈是否为空栈\n        bool Empty() {top==Null? return 1; return 0;}\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 链栈入栈算法\n\n链栈入栈即在链表表头插入新结点，且栈顶指针指向该结点。\n\n第一步：创建一个新结点s，将s的值设为入栈元素值。\n\n第二步：将新结点s插入表头。\n\n第三步：栈顶指针指向s。\n\n用C++实现的代码如下：\n\nvoid LinkStack::Push(int e)\n{\n    s = new Node;\n    if (!s) {\n        cout<< "内存分配失败";\n        return ;\n    }\n    //申请一个数据域为e的结点s\n    s->data = e;\n    // 将结点s插在栈顶\n    s->next = top; \n    top = s;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 链栈出栈算法\n\n链栈出栈即删除链表的首元素结点，具体操作步骤如下。\n\n第一步：如果栈空，则返回错误信息，操作结束；否则继续执行下一步。\n\n第二步：取出栈顶元素赋值给e。\n\n第三步：栈顶指针后移一位，并删除原栈顶结点，返回e。\n\n下面是链栈出栈的C++代码：\n\nint LinkStack::Pop()\n{\n    if (top==NULL) {\n        cout<< "溢出";\n        return -1;\n    }\n    //暂存栈顶元素\n    x = top->data;\n    // 将栈顶指针指向后移\n    p = top;\n    top = top->next;\n    delete p;\n    return x;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# D37(2020/10/28)\n\n主要进行基于队列的代码的整理，包括顺序队列，循环队列，链式队列。\n\n\n# 队列的概念\n\n队列是另一种限定存取位置的线性表。它只允许在表的一端插入，在另一端删除，其中允许插入的一端称为队尾(Rear)，允许删除的一端称为队头(Front)。从队尾插入元素的操作称为入队；从队头删除元素的操作称为出队。\n\n\n# 队列的抽象数据类型\n\n队列的操作与栈类似，不同的是队列的删除操作是在表的头部(队头)进行的。下面给出队列的抽象数据类型定义：\n\nADT Queue {\nData \n    同线性表。元素具有相同的类型，相邻元素具有前驱和后继关系。\nOperation\n    InitQueue(*Q): 初始化操作，建立一个空队列Q。\n    DestroyQueue(*Q): 若队列Q存在，则销毁它。\n    ClearQueue(*Q): 将队列Q清空\n    QueueEmpty(Q): 若队列Q为空，返回true，否则返回false.\n    GetHead(Q, *e): 若队列Q存在且非空，用e返回队列Q的队头元素\n    EnQueue(*Q, e): 若队列Q存在，插入新元素e到队列Q中并成为队尾元素。\n    DeQueue(*Q, *e): 删除队列Q中队头元素，并用e返回其值\n    QueueLength(Q): 返回队列Q的元素个数\n}\nEndADT\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 顺序队列\n\n在研究队列的时候，实际上有两个大方向的问题，需要研究和讨论。队列是否采用数组还是链表，队列是否需要循环。\n\n下面会从几个简单的实例来实践一下，如果用数组的方式来实现，需要对该数组的队列方式进行改造，并且明确的指出非循环的队列，肯定会存在问题，所以说到队列，一定会去使用循环队列。\n\n\n# 顺序队列的存储不足\n\n假设这里有一个队列有n个元素是用数组实现的顺序队列，数组下标为0的一端是队头。\n\n入队列的操作，其实就是在队尾追加一个元素，这个时候不需要移动任何元素，因此时间复杂度是O(1)。\n\n但是出队列的时候，是从队头(数组下标为0的位置)删除元素数据的，这样的话，由于数组顺序存储的连续性的要求，后续的数组元素都要依次往前移动，这样的话算法的时间复杂度就是O(n)。\n\n# 解决队头删除元素O(n)的问题\n\n为什么出队列的时候一定要全部移动呢？如果不去限制队列的元素必须存储在数组的前n个单元这一条件，出队的性能就会大大增加。 --\x3e 队头不需要一定在下标为0的位置。\n\n那么就需要一个队头指针和一个队尾指针，入队列一个，队尾指针往后移动一个。出队列一个，队头指针往后移动一个。front指针指向队头元素，rear指针指向队尾元素的下一个位置，当front=rear的以后，此时队列不是还是剩下一个元素，而是空队列。\n\n\n\n提出新问题？"假溢出"\n\n由于front指针随着出队列，不断的往后移动。rear直至随着入队列也不断的往后移动，很有可能会出现rear指针移动到了数组之外，而这个时候数组是有位置的。这种现象就叫做"假溢出"。\n\n\n\n\n# 循环队列定义\n\n为了解决上面的假溢出的问题，就出现了循环队列。解决的方法就是，把一个单一的队列组成一个首尾相连的队列。我们把队列的这种头尾相接的顺序存储结构称为循环队列。\n\n# 循环队列判断队空和队满\n\n之前我们定义的队空的时候是，front == rear，但是如果是循环队列的时候，如果这个时候队满了，也会出现front == rear了，这就会存在问题。\n\n解决办法1：增加设置一个标志位置变量flag，当front == rear，且flag = 0的时候为队列空，当front == rear，且flag = 1的时候为队列满。\n\n解决办法2：当队列空的时候，条件就是front == rear，当队列满的时候，我们修改其条件，保留一个元素空间。也就是说，队列满的时候，数组中还有一个空闲单元了。\n\n\n\n# 保留一个元素空间的方法来判断队满\n\n如果我们采用了上面的第二种方法的话，由于rear可能比front大，也可能比front小，所以尽管它们只相差一个位置时就是满的情况，但也可能是相差整整一圈。\n\n所以若队列的最大尺寸是QueueSize，那么队列满的条件是(rear+1)% Queuesize == front(取模%的目的就是为了整合rear与front大小为一个问题)。\n\n\n\n# 保留一个元素空间的方法来计算队列长度\n\n当采用上面第二中方法，通过留下一个空闲的位置，来判断队列满的情况的话。\n\n有时候，我们front指针会小于rear，也就是rear还没转圈的时候，此时队列的长度为rear - front；有时候，当rear转圈后，就会发现rear < front了，这个时候的队列的长度分为两段了，一段是QueueSize - front，另一端是0 + rear，加在一起，队列长度为rear - front + QueueSSize。\n\n因此通用的计算队列长度公式为：(rear - front + QueueSize)% QueueSize\n\n\n# 循环队列的顺序存储结构\n\n下面是用C定义的循环队列中的顺序存储结构的代码。\n\n// QElemType类型根据实际情况而定，这里设置为int\ntypedef int QElemType;\n// 循环队列的顺序存储结构\ntypedef struct \n{\n    QElemType data[MAXSIZE];\n    // 头指针\n    int front;\n    // 尾指针，若队列不空，指向队尾元素的下一个位置\n    int rear;\n} SqQueue;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 初始化一个空队列\n\n这里用C的代码来实现了初始化一个空队列的目的。\n\n// 初始化一个空队列Q\nStatus InitQueue(SqQueue *Q)\n{\n    Q->front = 0;\n    Q->rear = 0;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 返回队列的当前长度\n\n下面用C的代码实现的是，返回当前队列的长度。\n\n// 返回Q的元素个数，也就是队列的当前长度\nint QueueLength (SqQueue Q)\n{\n    return (Q.rear - Q.front + MAXSIZE )%MAXSIZE;\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 循环队列的入队列\n\n下面是用C++代码实现的，循环队列中入队列的代码。\n\n// 循环队列的入队列操作代码如下：\nStatus EnQueue(SqQueue *Q, QElemType e) \n{\n    // 队列满的判断\n    if ((Q->rear + 1)%MAXSIZE == Q->front) {\n        return ERROR;\n    }\n    // 将元素e赋值给队尾\n    Q->data[Q->rear] = e;\n    // rear直至向后移动一个位置\n    // 若到了最后则转到数组头部\n    Q->rear = (Q->rear + 1)% MAXSIZE;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n下面使用C++代码实现的循环队列入队列的情况。\n\n// 如下使用循环队列中的入队代码\nvoid EnQueue(int e)\n{\n    if ((rear + 1)% queuesize == front) {\n        cout<<"上溢，无法入队";\n        return;\n    }\n    base[rear] = e;\n    rear = (rear+1)% queuesize;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 循环队列的出队列\n\n下面是用C代码实现的，循环队列中出队列的代码。\n\n// 循环队列的出队列的操作代码如下\n// 若队列不空，则删除Q中队头元素，用e返回其值\nStatus DeQueue(SqQueue *Q, QElemType *e)\n{\n    // 队列空的判断\n    if (Q->front == Q->rear) {\n        return ERROR;\n    }\n    // 将队头元素赋值给e\n    *e = Q->data[Q->front];\n    // front指针向后移动一个位置\n    // 若到最后则转到数组头部\n    Q->front = (Q->front + 1)% MAXSIZE;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n下面是用C++代码实现的，在循环队列中出队列的代码情况：\n\n// 如下使用的是循环队列中的出队代码\nint DeQueue() \n{\n    int e;\n    if (front == rear) {\n        cout<<"下溢，不能出队";\n        return -1;\n    }\n    e = base[front];\n    return e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 链式队列\n\n队列的链式存储结构，其实就是线性表的单链表，只不过它只能尾进头出而已，我们把它简称为链队列。\n\n为了操作上的方便，我们将队头指针指向链队列的头结点，而队尾指针指向终端结点。(这里同样引入了头结点，头结点不存储任何元素，只是为了保证为当这个链队列为空的时候，和非空的队列的操作是一致的)。\n\n这个是和上面的顺序队列不一样了，在顺序队列中，头指针指向的是数组下标为0的位置，链尾指针指向的是数组中队尾元素的下一个位置。\n\n\n\n空队列的时候，front和rear都指向头结点。\n\n\n\n\n# 链队列的结构\n\n下面的C语言代码展示的是，链队列的结构。\n\n// QElemType类型根据实际情况而定，这里可以假设为int\ntypedef int QElemType;\n\n// 结点结构\ntypedef struct QNode\n{\n    QElemType data;\n    struct QNode *next;\n} QNode, *QueuePtr;\n\n// 队列的链表结构\ntypedef struct\n{\n    // 队头、队尾指针\n    QueuePtr front,rear;\n} LinkQueue;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 链队列的入队操作\n\n在链队列中，入队的时候，实际上就是在链表的尾部插入结点。\n\n链队列的入队操作中，相比与上面的顺序队列的入队操作来说，不需要去判断这个队列是否满了，也就是不需要去判断数组是否溢出了。\n\n下面是用C语言代码实现的操作：\n\n// 链队列入队的操作\n// 插入元素e为Q的新的队列元素\nStatus EnQueue(LinkQueue *Q, QElemType e)\n{\n    QueuePtr s = (QueuePtr) malloc(sizeof(QNode));\n    // 存储分配失败\n    if (!s) {\n        exit (OVERFLOW);\n    }\n    s->data = e;\n    s->next = NULL;\n    // 把拥有元素e新结点s赋值给原队尾结点的后继\n    Q->rear->next = s;\n    // 把当前的s 设置为队尾结点，rear指向s\n    Q->rear = s;\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n下面是用C++语法实现的链队列入队的代码：\n\n// 链队列入队算法\nvoid LinkQueue::EnQueue(int e)\n{\n    Node *s;\n    s = new Node;\n    s->data = e;\n    // 下面的代码可以直接改写为s->next = NULL\n    s->next = rear->next;\n    //将结点s的地址赋值给了原来的队尾指针\n    //也就是说原来的最后一个元素不是最后一个了，\n    //这个结点的next不是null，而是S结点了\n    rear->next = s;\n    //最后将 s结点的地址赋值给rear指针了\n    //也就是说 rear指针指向的是S结点了\n    rear = s;\n    //如果原来的队列是个空队列\n    //那么就将front指针指向的是新加入的结点S\n    if (front->next == NULL) {\n        front->next = s;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 链队列的出队操作\n\n出队操作时，也就是头结点的后继结点出队，将头结点的后继改为它后面的结点(后继的后继)，若链表除头结点外只剩下一个元素的时候，则需要将rear指向头结点。\n\n\n\n下面是用C语言实现的链队列出队的代码.\n\n// 链队列出队的操作\n// 若队列不空，删除Q的队头元素，用e返回其值，并返回OK，否则返回ERROR\nStatus DeQueue(LinkQueue *Q, QElemType *e)\n{\n    QueuePtr p;\n    if (Q->front == Q->rear) {\n        return ERROR;\n    }\n    // 将要删除的队头结点暂存给p\n    p = Q->front->next;\n    // 将预删除的队头结点的值赋值给e\n    *e = p->data;\n    // 将原队头结点后继p->next赋值给头结点后继\n    Q->front->next = p->next;\n    // 若队头是队尾，则删除后将rear指向头结点\n    if (Q->rear == p) {\n        Q->rear = Q->front;\n    }\n    free(p);\n    return OK;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n下面是C++实现的链队列出队的算法。\n\n// 链队列出队算法\n// front是头结点\nint LinkQueue::DeQueue()\n{\n    int e;\n    Node *p;\n    // 队空，则下溢\n    if (rear == front) {\n        cout<< "下溢";\n        return -1;\n    }\n    // 先将第一个结点的内存地址保存到指针p中\n    p = front->next;\n    // 将第一个结点的data值，保存到变量e中\n    e = p->data;\n    // 将第一个结点的后继赋值给  头结点的后继\n    front->next = p->next;\n    // 如果要删除的这个结点是唯一的结点元素的话\n    if (p->next == NULL) {\n    // 则将rear的末尾结点的指针指向了头结点\n        rear = front;\n    }\n    delete p;\n    return e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# D38(2020/10/29)\n\n今天继续要学习下排序的知识点。\n\n上一节我们学习的冒泡排序、插入排序、选择排序这三种排序算法，它们的时间复杂度都是O($n^2$)，比较高，适合小规模数据的排序。\n\n今天，要学习的两种时间复杂度为O(nlogn)的排序算法，归并排序和快速排序。这两种排序算法适合大规模的数据排序，比上一节讲的那三种排序算法要更常用。\n\n归并排序和快速排序都用到了分治思想，非常巧妙。我们可以借鉴这种思想，来解决非排序的问题，比如：如何在O(n)的时间复杂度内查找一个无序数组中的第K大元素？\n\n\n\n\n# 归并排序的原理\n\n\n# 归并排序(Merge Sort)\n\n我们如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。\n\n归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。\n\n分治思想就是我们之前所讲的递归思想很像。分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。\n\n\n# 递归代码来实现归并排序\n\n之前我们所学习到的书写递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，先得写出归并排序的递推公式。\n\n递推公式：\nmerge_sort(p...r) = merge(merge_sort(p..q), merge_sort(q+1 .... r))\n终止条件：\np >= r 不用再继续分解\n\n\n1\n2\n3\n4\n\n\n理解上面的递推公式。\n\nmerge_sort(p...r)表示，给下标从p到r之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p...q)和merge_sort(q+1 ... r)，其中下标q等于p和r的中间位置，也就是(p+r)/2。当下标从p到q和从q+1到r这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从p到r之间的数据就也排好序了。\n\n有了递推公式，我们就可以转化为代码了，这样就实现了归并排序的算法。\n\n// 归并排序算法，A是数组，n表示数组大小\nmerge_sort(A, n) {\n    merge_sort_c(A, 0, n-1)\n}\n// 递归调用函数\nmerge_sort_c(A,p,r) {\n    // 递归终止条件\n    if p >= r then return\n    // 取p到r之间的中间位置q\n    q = (p+r)/2\n    // 分治递归\n    merge_sort_c(A, p, q)\n    merge_sort_c(A, q+1, r)\n    // 将A[p...q]和A[q+1...r]合并为A[p...r]\n    merge(A[p...r],A[p...q],A[q+1...r])\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n这里，我们可以发现了，merge(A[p...r],A[p...q],A[q+1...r])这个函数的作用就是，将已经有序的A[p...q]和A[q+1 ... r]合并成一个有序的数组，并且放入A[p...r]。那个具体的合并的操作该如何实现呢？\n\n如图所示，我们申请一个临时数组tmp，大小与A[p...r]相同。我们用啊领个游标i和j，分别指向A[p...q]和A[q+1 ... r]的第一个元素。比较这两个元素A[i]和A[j]，如果A[i]<=A[j]，我们就把A[i]放入到临时数组tmp，并且i后移一位，否则将A[j]放入到数组tmp，j后移一位。\n\n继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组tmp中的数据拷贝到原数组A[p...r]中。\n\n\n\n我们把merge()函数写成伪代码，就是下面的样子：\n\nmerge(A[p...r], A[p...q], A[q+1 ... r]) {\n    // 初始化变量i,j,k\n    var i:=p, j:=q+1, k:=0\n    // 申请一个大小跟A[p...r]一样的临时数组\n    var tmp:= new arrray[0 ... r-p]\n    while i<=q AND j<=r do {\n        if A[i] <= A[j] {\n        // i++等于i:=i+1\n            tmp[k++] = A[i++]\n        } else {\n            tmp[k++] = A[j++]\n        }\n    }\n}\n// 判断哪个子数组中有剩余的数据\nvar start:= i, end:=q\nif j<=r then start :=j, end:=r\n// 将剩余的数据拷贝到临时数组tmp\nwhile start<= end do  {\n    tmp[k++] = A[start++]\n}\n// 将tmp中的数组拷贝会A[p...r]\nfor i:=0 to r-p do {\n    A[p+i] = tmp[i]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 归并排序的性能分析\n\n下面来分析一下归并排序的三个问题。\n\n\n# 第一，归并排序是稳定的排序算法吗？\n\n结合前面画的那张图和归并排序的伪代码，我们应该能发现，归并排序稳不稳定关键要看merge()函数，也就是两个有序子数组合并成一个有序数组的那部分代码。\n\n在合并的过程中，如果A[p...q]和A[q+1...r]之间有值相同的元素，那我们可以像伪代码中的那样，先把A[p...q]中的元素放入tmp数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。\n\n\n# 第二，归并排序的时间复杂度是多少？\n\n归并排序涉及递归，时间复杂度的分析稍微有点复杂。我们可以借此来分析一下递归代码的时间复杂度。\n\n在递归那一节，我们了解到，递归适用的场景是，一个问题a可以分解为多个子问题b、c，那求解问题a就可以分解为求解问题b、c。问题b、c解决之后，我们再把b、c 的结果合并成a的结果。\n\n如果我们定义求解问题a的时间T(a)，求解问题b、c的时间分别是T(b)和T(c)，那我们就可以得到这样的递推关系式：\n\nT(a) = T(b) + T(c) +K\n\n其中K等于将两个子问题b、c的结果合并成问题a的结果所消耗的时间。\n\n从刚才的分析，可以看出：不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。\n\n套用这个公式，我们来分析一下归并排序的时间复杂度。\n\n我们假设对n个元素进行归并排序需要的时间是T(n)，那分解成两个子数组排序的时间都是T(n/2)。我们知道，merge()函数合并两个有序子数组的时间复杂度是O(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是：\n\nT(1) = C; n=1时，只需要常量级的执行时间，所以表示为C\nT(n) = 2* T(n/2) + n; n>1\n\n\n1\n2\n\n\n通过这个公式，如何来求解T(n)呢？我们可以再进一步分解计算过程。\n\nT(n) = 2*T(n/2) + n\n     = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n\n     = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n\n     = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n\n     ......\n     = 2^k * T(n/2^k) + k * n\n     ......\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n通过这样一步一步分解推导，我们可以得到T(n)= 2^k * T(n/2^k) + k * n。当T(n/2^k) = T(1)时，也就是n/2^k = 1，我们得到k=$log_2{n}$ . 我们将k值代入上面的公式，得到T(n)=Cn + n$log_2{n}$。如果我们用大O标记法来表示的话，T(n)就等于O(nlogn)。\n\n从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度非常稳定，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是O(nlogn).\n\n\n# 第三，归并排序的空间复杂度是多少？\n\n归并排序的时间复杂度任何情况下都是O(nlogn)，看起来非常优秀。(和下面的快速排序相比，最坏情况天，时间复杂度也是O($n^2$))。但是，归并排序并没有像快排那样，应用广泛，这是为什么呢？\n\n这是因为它有一个致命的"弱点"，那就是归并排序不是原地排序算法。\n\n这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组的时候，需要借助额外的存储空间。但是，归并排序的空间复杂度到底是多少呢？\n\n如果我们继续按照分析递归时间复杂度的方法，通过递推公式来求解，那整个归并过程需要的空间复杂度就是O(nlogn)。不过，类似分析时间复杂度那样来分析空间复杂度，这个思路是不对的。\n\n实际上，递归代码的空间复杂度并不能像时间复杂度那样累加。刚刚我们忘记了最重要的一点，那就是，尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间也就释放掉了。在任意时刻，CPU只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过n个数据的大小，所以空间复杂度是O(N).\n\n\n# 快速排序的原理\n\nQuickSork，习惯性地叫做"快排"。快排利用的也是分治思想。\n\n快排的思想是这样：如果要排序数组中下标从p到r之间的一组数据，我们选择p从r之间的任意一个数据作为pivot (分区点)。\n\n我们遍历p到r之间的数据，将小于pivot的放到左边，将大于pivot的放到右边，将pivot放到中间。经过这一步骤之后，数组p到r之间的数据就被分成了三个部分，前面p到q-1之间都是小于pivot的，中间是pivot，后面的q+1到r之间是大于pivot的。\n\n\n\n根据分治、递归的处理思想，我们可以用递归排序下标从p到q-1之间的数据和下标从q+1到r之间的数据，直到区间缩小为1，就说明所有的数据都有序了。\n\n如果我们用递推公式来将上面的过程写出来的话，就是这样：\n\n递推公式：\nquick_sort(p...r) = quick_sort(p...q-1) + quick_sort(q+1 ... r)\n终止条件：\np >= r\n\n\n1\n2\n3\n4\n\n\n我们将递推公式转化成递归代码。跟归并排序一样，下面用伪代码来实现下。\n\n// 快速排序，A是数组，n表示数组的大小\nquick_sort(A, n) {\n    quick_sort_c(A, 0, n-1)\n}\n// 快速排序递归函数，p，r为下标\nquick_sort_c(A, p, r) {\n    if p >= r then return\n    // 获取分区点\n    q = partition(A, p, r)\n    quick_sort_c(A, p, q-1)\n    quick_sort_c(A, q+1, r)\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n归并排序中有一个merge()合并函数，我们这里有一个partition()分区函数。partition()分区函数实际上我们前面已经了解了，就是随机选择一个元素作为pivot(一般情况下，可以选择p到r区间的最后一个元素)，然后对A[p...r]分区，函数返回pivot的下标。\n\n如果我们不考虑空间消耗的话，partition()分区函数可以写得非常简单。我们申请两个临时数组X和Y，遍历A[p...r]，将小于pivot的元素都拷贝到临时数组X，将大于pivot的元素都拷贝到临时数组Y，最后再将数组X和数组Y中数据顺序拷贝到A[p...r].\n\n\n\n但是，如果按照这种思路实现的话，partition()函数就需要很多额外的内存空间，所以快排就不是原地排序算法了。如果我们希望快排是原地排序算法，那它的空间复杂度就得是O(1)，那partition()分区函数就不能占用太多额外的内存空间，我们就需要在A[p...r]的原地完成分区操作。\n\n原地分区函数的实现思路非常巧妙，如下面的伪代码.\n\npartition(A, p, r) {\n    privot:= A[r]\n    i := p\n    for j := p to r-1 do {\n        swap A[i] with A[j]\n        i := i+1\n    }\n}\nswap A[i] with A[r]\nreturn i\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n这里的处理有点类似选择排序。我们通过游标i把A[p...r-1]分成两部分。A[p...i-1]的元素都是小于pivot的，我们暂且叫它"已处理区间"，A[i...r-1]是"未处理区间"。我们每次都从未处理的区间A[i...r-1]中取一个元素A[j]，与pivot对比，如果小于pivot，则将其加入到已处理区间的尾部，也就是A[i]的位置。\n\n在数组的插入操作中，在数组某个位置插入元素，需要搬移数据，非常耗时。当时我们也提及到一种处理技巧，就是交换，在O(1)的时间复杂度内完成插入操作。这里我们也借助这个思想，只需要将A[i]和A[j]交换，就可以在O(1)的时间复杂度内将A[j]放到下标为i的位置。\n\n如下图所示的，就是一个快速排序中的图示。\n\n\n\n因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个6的相对先后顺序就会改变。所以，快速排序并不是一个稳定的排序算法。\n\n到此，快速排序的原理也了解了。现在，我们提出一个问题：快排和归并用的都是分治思想，递推公式和递归代码也非常相似，它们的区别在哪里？\n\n\n\n可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。\n\n归并排序虽然是稳定的、时间复杂度为O(nlogn)的排序算法，但是它是非原地排序算法。前面提到过，归并排序之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。\n\n\n# 快速排序的性能分析\n\n接下来我们分析一下快速排序的性能。在讲解快速排序的实现原理的时候，已经分析了稳定性和空间复杂度。快排是一种原地、不稳定的排序算法。\n\n快排也是用递归来实现的。对于递归代码的时间复杂度，前面提及的总结的公式，这里还是适用的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是O(nlogn).\n\nT(1) = C;  n=1时，只需要常量级的执行时间，所以表示C\nT(n) = 2*T(n/2) + n; n>1\n\n\n1\n2\n\n\n但是，公式成立的前提是每次分区操作，我们选择的pivot都很合适，正好能将大区间对等地一分为二。\n\n我们举一个比较极端的例子。如果数组中的数据原来就已经是有序的了，比如1，3，5，6，8。如果我们每次选择最后一个元素作为pivot，那每次分区得到的两个区间都是不均等的。我们需要进行大约n次分区操作，才能完成快排的整个过程。每次分区我们平均要扫描大约n/2 个元素，这种情况下，快排的时间复杂度就从O(nlogn) 退化成了O($n^2$) .\n\n刚才看到的是两个极端情况下的时间复杂度，一个是分区极其均衡，一个是分区极其不均衡。它们分别对应快排的最好情况时间复杂度和最坏情况时间复杂度。那快排的平均情况时间复杂度是多少呢？\n\n我们假设每次分区操作都将区间分成大小为9:1的两个小区间。我们继续套用递归时间复杂度的递推公式，就会变成这样：\n\nT(1) = C; n=1时，只需要常量级的执行时间，所以表示为C。\nT(n) = T(n/10) + T(9*n/10) +n; n>1\n\n\n1\n2\n\n\n这个公式的递推求解的过程非常复杂，虽然可以求解，但是我们不推荐用这种方法。实际上，递归的时间复杂度的求解方法除了递推公式之外，还有递归树。这里先记住结论：T(n)在大部分的情况下的时间复杂度都可以做到O(nlogn)，只有在极端的情况下，才会退化到O($n^2$)。\n\n\n# 解答开篇\n\n快排的核心思想就是分治和分区，我们可以利用分区的思想，来解答开篇的问题：O(n)时间复杂度内求无序数组中的第K大元素。比如，4,2,5,12,3这样一组数据，第3大元素就是4。\n\n我们选择数组区间A[0...n-1]的最后一个元素A[n-1]作为pivot，对数组A[0...n-1]原地分区，这样数组就分成了三部分，A[0...p-1]、A[p]、A[p+1 ... n-1].\n\n如果p+1 =K，那么A[p]就是要求解的元素；如果K>p+1，说明第K大元素出现在A[p+1...n-1]区间，我们再按照上面的思路递归第在A[p+1...n-1]这个区间内查找。同理，如果K<p+1，那我们就在A[0...p-1]区间查找。\n\n\n\n我们再来看，为什么上述解决思路的时间复杂度是O(n)？\n\n第一次分区查找，我们需要对大小为n的数组执行分区操作，需要遍历n个元素。第二次分区查找，我们只需要对大小为n/2的数组执行分区操作，需要遍历n/2个元素。依次类推，分区遍历元素的个数分别为n/2、n/4、n/8、n/16 ....直到区间缩小为1.\n\n如果我们把每次分区遍历的元素个数加起来，就是：n+n/2 +n/4 +n/8 +...+1. 这是一个等比数列求和，最后的和是2n-1、所以，上述解决思路的时间复杂度是O(n).\n\n\n# 内容小结\n\n归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归俩实现，过程非常相似。理解归并排序的重点是理解递推公式和merge()合并函数。同理，理解快排的重点也是理解递推公式，还有partition()分区函数。\n\n归并排序算法是一种在任何情况下时间复杂度都比较稳定的排序算法，这也使它存在致命的缺点，即归并排序不是原地排序算法，空间复杂度比较高，是O(n)。正因为此，它也没有快排应用广泛。\n\n快速排序算法虽然最坏情况下的时间复杂度是O($n^2$)，但是平均情况下时间复杂度都是O(nlogn)。不仅如此，快速排序算法时间复杂度退化到O($n^2$)的概率非常小，我们可以通过合理地选择pivot来避免这种情况。\n\n\n# D39(2020/11/02)\n\n今天主要是再次梳理一下归并排序和快速排序的内容，熟悉书写的代码和排序的原理。\n\n\n# 归并排序\n\n归并排序(Merge Sort)是一类借助"归并"进行排序的方法。\n\n归并的含义是将两个或两个以上的有序序列归并为一个有序序列的过程。归并排序按所合并的表的个数可分为二路归并排序和多路归并排序。\n\n二路归并排序(2-way Merge Sort)的基本思想：将待排序的n个元素看成是n个有序的子序列，每个子序列的长度是1，然后两两归并，得到[$\\frac{n}{2}$] 向下取整，个长度为2或1(最后一个有序序列的长度可能是1)的有序子序列；再两两归并，得到[$\\frac{n}{4}$] 向下取整，个长度为4或小于4(最后一个有序序列的长度可能小于4)的有序子序列；再两两归并，.... 直至得到一个长度为n的有序序列。\n\n\n# 二路归并排序的操作步骤如下\n\nStep1: 将待排序的列划分为两个长度相当的子序列。\n\nStep2: 若子序列长度大于1，则对子序列执行一次归并排序。\n\nStep3: 执行下列步骤对子序列两两合并成有序序列。\n\n 1. 创建一个辅助数组temp[]. 假设两个子列的长度分别为u、v，两个子列的下标为0~u，u+1 ~ v+u+1。设置两个子表的起始下标和辅助数组的起始下标：i=0; j=u+1; k=0\n 2. 若i>u或j>v+u+1，说明其中一个子表已经合并完毕，执行第4步。\n 3. 选取r[i]和r[j]中关键字较小的存入辅助数组temp[]; 若r[i].key < r[j].key，则temp[k]=r[i]; i++;k++; 否则temp[k]=r[j]; j++; k++。返回第2步。\n 4. 将尚未处理完的子表元素依次存入temp[]，结束合并。\n\n\n# 快速排序\n\n\n# 快速排序的基本思想\n\n快速排序是通过对关键字的比较和交换，以待排序列中的某个数据为支点，将待排序列分成两部分，其中左半部分数据小于等于支点，右半部分数据大于等于支点。然后，对左右两部分分别进行快速排序的递归处理，直到整个序列按关键字有序为止。\n\n\n# 快速排序与冒泡排序\n\n在冒泡排序中，元素的比较和移动是在相邻位置进行的，元素的每次交换只能前移或后移一个位置，因而总的比较次数和移动次数较多。\n\n而在快速排序中，元素的比较和移动是从两端向中间进行的，关键字较大的记录一次就能从前面移动到后面，关键字较小的记录一次就能从后面移动到前面，记录移动的距离较远，从而减少了总的比较次数和移动次数。\n\n因此，可以将快速排序视为对冒泡排序的一次改进。\n\n\n# 快速排序的流程图\n\n下面是一个快速排序的完整的流程图，其中low是序列中的第一个结点，high指向的是序列中的最后一个结点，且令r[low]为支点。\n\n\n\n\n# 快速排序的基本步骤\n\nStep1: 如果待排子序列中元素的个数等于1，则排序结束；否则以r[low]为支点，按如下方式进行一次划分；\n\n 1. 设置两个搜索指针：low是向后搜索指针，初始指向序列第一个结点；high是向前搜索指针，初始指向最后一个结点；取第一个记录为支点，low位暂时取值为支点privotkey = r[low].key\n 2. 若low=high，一次划分结束\n 3. 若low<high 且r[high].key >= privotkey, 则从high 所指定的位置向前搜索：high =high -1，重新执行1.3；否则若有low<high并且有r[high].key < privotkey，交换r[high].key和r[low].key,然后令low=low +1，执行下面第4步；若有low>=high，则指向上面第2步；\n 4. 若low<high且r[high].key<= privotkey。则从low所指的位置开始往后搜索：low = low +1，然后再重新执行第4步；否则若有low < high并且有r[low].key > pivotkey，则交换r[high].key和r[low].key，然后令high = high -1，执行上面第3步；若有low >= high，则执行第2步。\n\nStep2: 对支点左半子序列重复Step1\n\nStep3: 对支点右半子序列重复Step1.\n\n\n# 快速排序代码\n\n分为分区函数和主函数两块的代码。\n\n# include <stdio.h>\n\n// 对序列进行一次划分\nint Partition (SqList &L, int low, int high)\n{\n    int pivotkey;\n    // 关键字\n    privotkey = L.r[low];\n\n    // 从表的两端交替向中间扫描\n    while (low < high) \n    {\n        while (low<high && L.r[high] >=privotkey) {\n            --high;\n        }\n        // 交换位置\n        L.r[low] = L.r[high];\n        while (low<high && L.r[low] <=pivotkey) {\n            ++low;\n        }\n        // 交换位置\n        L.r[high] = L.r[low];\n    }\n    // 返回中间的位置\n    return low;\n}\n\n// 主程序，按分区对子程序进行调用\nvoid QuickSort1 (SqList &L, int low, int high)\n{\n    int mid;\n    if (low < high) {\n        mid = Partition(L, low, high);\n        //对低子表进行排序\n        QuickSort1 (L, low, mid-1);\n        //对高子表进行排序\n        QuickSort1 (L, mid+1, high);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# D40(2020/11/03)\n\n今天主要是要学习的桶排序、计数排序，还有基数排序中的内容。\n\n上两节中，我们学习了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。今天，我们会重点学习三种时间复杂度是O(n)的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫做线性排序(Linear sort)。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。\n\n这几种排序算法理解起来都不难，时间、空间复杂度分析起来也和简单，但是对要排序的数据要求很苛刻，所以学习的重点是掌握这些排序算法的适用场景。\n\n\n# 桶排序(Bucket sort)\n\n首先，我们来看桶排序。桶排序，顾名思义，会用到"桶"，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。\n\n\n\n桶排序的时间复杂度为什么是O(n)呢？\n\n如果要排序的数据有n个，我们把它们均匀地划分到m个桶内，每个桶里就有k= n/m 个元素。每个桶内部使用快速排序，时间复杂度为O(k* logk)。m个桶排序的时间复杂度就是O(m*k* logk)，因为k=n/m，所以整个桶排序的时间复杂度就是O(n* log(n/m))。当桶的个数m接近数据个数n的时候，log(n/m)就是一个非常小的常量，这个时候桶排序的时间复杂度接近O(n).\n\n\n# 桶排序看起来很优秀，是不是可以替代之前讲的排序算法？\n\n答案是否定的。上面的例子是为了轻松理解桶排序的核心思想，做了很多的假设。实际上，桶排序对要排序数据的要求是非常苛刻的。\n\n首先，要排序的数据需要很容易就能划分成m个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。\n\n其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不均匀，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为O(nlogn)的排序算法了。\n\n桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。\n\n比如说我们有10GB的订单数据，我们希望按订单金额(假设金额都是正整数)进行排序，但是我们的内存有限，只有几百MB，没办法一次性把10GB的数据都加载到内存中。这个时候该怎么办呢？\n\n现在可以来思考一下，如何借助桶排序的处理思想来解决这个问题。\n\n我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是1元，最大是10万元。我们将所有订单根据金额划分到100个桶里，第一个桶我们存储金额在1元到1000元之内的订单，第二桶存储金额在1001到2000元之间的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名(00, 01, 02... 99)。\n\n理想的情况下，如果订单金额在1到10万元之间均匀分布，那订单会被均匀划分到100个文件中，每个小文件中存储大约100MB的订单数据，我们就可以将这100个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。\n\n不过，我们也可能会发现，订单按照金额在1元到10万元之间并不一定是均匀分布的，所以10GB订单数据是无法均匀地被划分到100个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该如何？\n\n针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在1元到1000元之间的比较多，我们就将这个区间继续划分为10个小区间，1元到100元，101元到200元，201元到300元 ... 901元到1000元。如果划分之后，101元到200元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。\n\n\n# 计数排序(Counting sort)\n\n个人认为，计数排序其实是桶排序的一种特殊情况。当要排序的n个数据，所处的范围并不大的时候，比如最大值是k，我们就可以把数据划分成k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。\n\n如果所在的省有50万考生，如何通过成绩快速排序得出名次呢？\n\n考生的满分是900分，最小是0分买这个数据的范围很小，所以我们可以分成901个桶，对应分数从0分到900分。\n\n根据考生的成绩，我们将这50W考生划分到这901个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要一次扫描每个桶，将桶内的考试依次输出到一个数组中，就实现了50万考试的排序。因为只涉及扫描遍历操作，所以时间复杂度是O(n).\n\n计数排序的算法思想，跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个排序算法叫"计数"排序呢？\n\n我们还是用考生的例子，对数据规模做了简化。假设只有8个考生，分数在0到5分之间。这8个考生的成绩我们放在一个数组A[8]中，它们分别是：2，5，3，0，2，3，0，3。\n\n考生的成绩从0到5分，我们使用大小为6的数组C[6]表示桶，其中下标对应分数。不过，C[6]内存储的并不是考生，而是对应的考生个数。以刚才的例子，我们只需要遍历一遍考试分数，就可以得到C[6]的值。\n\n\n\n从图中可以看出，分数为3分的考生有3个，小于3分的考试有4个，所以，成绩为3分的考试在排序之后的有序数组R[8]中，会保存下标4，5，6的位置。\n\n\n\n那我们如何快速计算出，每个分数的考试在有序数组中对应的存储位置呢？这个处理方法非常巧妙。\n\n思路是这样的：我们对C[6]数组顺序求和，C[6]存储的数据就变成了下面的样子。C[k]里存储小于等于分数k的考生个数。\n\n\n\n我们从后到前依次扫描数组A。比如，当扫描到3的时候，我们可以从数组C中取出下标为3的值7，也就是说，到目前为止，包括自己在内，分数小于等于3的考生有7个，也就是说3是数组R中的第7个元素(也就是数组R中下标为6的位置)。当3放入到数组R中后，小于等于3的元素就只剩下6个了，所以相应的C[3]要减1，变成6。\n\n> 理解：从后到前的方向来扫描数组A，扫描到一个元素值的时候，到C这个数组下面，根据A中扫描的那个值的作为C的下标，得到C中对应的下标的元素的值，我们从C中得到值X，减去1，放入到R中(x-1)的数组下标下面。完成一次后，将C中对应的下标中的值x也减去1。\n\n以此类推，当我们扫描到第2个分数为3的考生的时候，就会把它放入数组R中的第6个元素的位置(也就是下标为5的位置)。当我们扫描完整个数组A后，数组R内的数据就按照分数从小到大有序排列的了。\n\n计数排序只能用在数据范围不大的场景中，如果数据范围k比要排序的数据n大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变大小的情况下，转化为非负整数。\n\n比如，还是考生的例子。如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以10，转化成整数，然后再放到9010个桶内。再比如，如果要排序的数据中有负数，数据的范围是[-1000,1000]，那我们就需要先对每个数据都加1000，转化成非负整数。\n\n\n# 基数排序(Radix sort)\n\n再看一个排序问题。假设我们有10万个手机号码，希望将这10万个手机号码从小到大排序。\n\n利用之前讲的快排，时间复杂度可用做到O(nlogn)，是否还有更加高效的排序算法？桶排序、计数排序能排上用场吗？\n\n由于手机号码有11位，范围太大了，显然不适合用桶排序或者计数排序。这里，我们可以用到新的排序算法，基数排序。\n\n刚刚这个问题里有这样的规律：假设要比较两个手机号码a，b的大小，如果在前面几位中，a手机号码已经比b手机号码大了，那后面的几位就不用看了。\n\n借助稳定排序算法，这里有一个巧妙的实现思路。我们这里先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过11次排序之后，手机号码就都有序了。\n\n\n\n这里按照每位来排序的排序算法要是稳定的。因为如果是非稳定的排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，这样的话，低位的排序就完全没有意义了。\n\n根据每一位来排序，我们可以用刚讲过的桶排序或计数排序，它们的时间复杂度可以做到O(n)。如果要排序的数据有k位，那我们就需要k次桶排序或计数排序，总的时间复杂度是O(k*n)。当k不大的时候，比如手机号码排序的例子，k最大就是11，所以基数排序的时间复杂度就近视于O(n).\n\n实际上，有时候要排序的数据并不都是等长的，例如英文单词，最短的只有1个字母，最长的有45个字母。对于这种不等长的数据，基数排序如何去做呢？\n\n我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补"0"。因为根据ASCII值，所有字母都大于"0"，所以补"0"不会影响到原有的大小顺序。这样就可以继续使用基数排序了。\n\n总结来说，基数排序对要排序的数据是有要求的，需要可以分割出独立的"位"来比较，而且位之间有递进的关系，如果a数据的高位比b数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到O(n)了。\n\n\n# 解答开篇\n\n再来看看开篇的思考题：如何根据年龄给100W用户排序？\n\n实际上，根据年龄给100W用户排序，就类似按照成绩给50W考生排序。我们假设年龄的范围最小1岁，最大不超过120岁。我们可以遍历这100万用户，根据年龄将其划分到这120个桶里，然后依次顺序遍历这120个桶内的元素。这样就得到了按照年龄排序的100万用户的数据。\n\n\n# 内容小结\n\n今天，学习了3种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到O(n).\n\n桶排序和计数排序的排序思想非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。\n\n基数排序要求数据可以划分为高低位，位之间有递进的关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一位的排序工作。\n\n\n# D41(2020/11/04)\n\n今天主要是要学习的是，对各个排序算法的一种大概的总结。\n\n几乎所有的编程语言都会提供排序函数，比如C语言中qsort()，C++ STL中的sort()、stable_sort()，还有Java语言中的Collections.sort()。在平时的开发中，我们也都是直接使用这些现成的函数来实现业务逻辑中的排序功能。\n\n\n# 如何选择合适的排序函数？\n\n如果要实现一个通用的、高效率的排序函数，我们应该选择哪种排序算法呢？\n\n\n\n\n# 冒泡排序\n\n# 冒泡排序的思想\n\n对于冒泡排序来说，会有两个for循环，外层的for循环，控制的是排序的趟数，内层的for循环，是在某一趟内，对相邻的两个元素进行比较和交换。\n\n# 冒泡排序的优化\n\n这里会涉及到一个对冒泡排序的优化，在外层for循环排序那，设置一个flag标签，外层flag的初始值是false。在实际的冒泡排序中，会存在这么一种场景，当内层的for循环在执行的过程中，是对前后相邻的两个数进行比较，但是如果这个时候发现相邻的两个数没有交换(已经有序)了，那么后面的数也是有序的了。所以来说，只有当内存for循环执行了交换，才会去修改flag的值为true，否则如果flag依旧还是为false的话，那么就直接跳出\n\n# 冒泡排序的代码\n\n代码示例如下：\n\nint main()\n{\n    int n;\n    int arry[10];\n    for (int i = 0; i < n - 1; i++)\n    {\n        bool flag = false;\n        for (int j = 0; j < n - i - 1; j++)\n        {\n            if (arry[j] > arry[j + 1])\n            {\n                int tmp = arry[j];\n                arry[j] = arry[j + 1];\n                arry[j + 1] = tmp;\n                flag = true;\n            }\n        }\n\n        if (!flag)\n        {\n            break;\n        }\n    }\n\n    for (int i = 0; i < n; i++)\n    {\n        printf("%d\\n", arry[i]);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n书写冒泡排序有两个重要点：\n\n 1. 要注意两个for循环的i的范围和j的范围，i的范围从[0, n-1]，j的范围是从[0,n-i-1]。\n 2. 注意flag的位置，初始设置flag=false的位于最外层for循环开始的位置，而flag = true则设置在内层循环的交换完毕后，设置的值，具体的位置应该是在if (a[j]>a[j+1]) 里面的方法体内，具体位置，在交换位置的代码的最后。确保只有发生了交换，才会去设置这个flag 为true\n\n# 冒泡排序算法分析\n\n 1. 冒泡排序是原地的排序算法。在整个的排序的过程中，只是涉及到相邻的数据的交换的操作，只需要引入一个临时的数组元素变量，不需要申请其他的内存空间了。所以冒泡排序的空间复杂度是O(1)，是一个原地排序算法。\n\n 2. 冒泡排序是稳定的排序算法。在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换。这样的话，相同大小的数据在排序前后不会改变顺序。\n\n 3. 冒泡排序的时间复杂度。\n    \n    * 最好情况时间复杂度O(n)。当要排序的数据已经是有序的了，我们只需要最外层的一个for循环就可以了(表示的要比较的趟数)，里面的for循环(初始的去判断一次，由于不满足条件，没有去交换元素，直接将flag设置为了true)，从而直接跳出了外层的for循环。整体的最好情况时间复杂度是O(n).\n    \n    * 最坏情况时间复杂度O($n^2$)。最外层的for循环需要n趟，里层的for循环也要有n次，所以最坏情况下的时间复杂度是O($n^2$)。\n\n\n# 插入排序\n\n# 插入排序思想\n\n插入排序整体思想是，将数组中的数据分为两个区间，已排序区间和未排序区间。\n\n初始的已排序的区间，我们设置为只有1个元素，这个元素就是数组中的第一个元素。\n\n核心思想就是，取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。\n\n# 有序度/逆序度/满有序度\n\n 1. 有序度是指，数组中具有有序关系的元素对的个数。\n    \n    2,4,3,1,5,6这组数据的有序度是11，因为其有序元素对为11个。分别是：\n    (2,4) (2,3) (2,5) (2,6) \n    (4,5) (4,6) (3,5) (3,6)\n    (1,5) (1,6) (5,6)\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 满有序度是指，数组中完全有序的数组的有序度，也就是完全有序的有序关系的元素对的个数。\n    \n    对于一个完全有序的数组，比如1,2,3,4,5,6，\n    有序度就是n*(n-1)/2，也就是15\n    \n    \n    1\n    2\n    \n\n 3. 逆有序度是指，数组中无序关系的元素对的个数。同时，我们还可以了解到逆序度=满有序度-有序度.\n    \n    对于冒泡排序或者是插入排序，这种包含比较和交换两种操作的排序来说。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是n*(n-1)/2 -初始有序度。\n\n# 插入排序代码\n\n代码示例如下：\n\n#include <stdio.h>\nvoid InsertSort(int *a, int length)\n{\n    int i, j, tmp;\n    // 外层的循环，表示的是需要有n-1个数组元素需要进行插入排序\n    // 默认认定a[0]这个元素是有序的\n    for (i = 1; i < length; i++)\n    {\n        // 将要插入的数a[i]赋值给tmp\n        // 为什么要引入tmp这个中间变量呢？\n        // 后续里面从a[0]到a[i]中的元素，都有可能出现移动，而导致原来a[i]的值出现改变\n        // 或者可以理解为先将a[i]这个值拿出来\n        tmp = a[i];\n        // 从i-1开始，逐个向前递减，循环来比较\n        for (j = i - 1; j >= 0; j--)\n        {\n            // a[j]代表的是有序的数组序列中的一个元素\n            // 如果要插入的元素，比a[j]要小\n            if (tmp < a[j])\n            {\n                // 将a[j]的值赋值给a[j+1]\n                // 这样就实现了a[j]往后移动一个位置的目的\n                // 这里用a[j+1]而不是a[i]\n                // a[j+1]是个动态变化的过程\n                a[j + 1] = a[j];\n            }\n            else\n            {\n                // 如果要插入的元素，大于等于a[j]\n                // 直接跳出内层的循环\n                break;\n            }\n        }\n        // 将要插入的数tmp放入a[j+1]中\n        a[j + 1] = tmp;\n    }\n}\n\nint main()\n{\n    int array[10] = {9, 4, 2, 10, 5, 1, 3, 5, 7, 9};\n    \n    InsertSort(array, sizeof(array) / sizeof(array[0]));\n    int i;\n    for (i = 0; i < sizeof(array) / sizeof(array[0]); i++)\n    {\n        printf("%d ", array[i]);\n    }\n    printf("\\n");\n    return 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n在书写插入排序代码的时候，还是有一定的难度的。\n\n总体来说，插入排序的逻辑还是比较和交换。思想是将数组分为两个部分，有序子序列，和无序的子序列。我们先默认有序子序列只有1个，是数组的第一个元素a[0]，然后依次从无序子序列中取出一个数组元素，加入到有序子序列中，通过将拿出来的那个无序的元素，通过从后往前的一个个比较，找到合适的位置，然后再将其插入，相依的原有的有序子序列往后移动。\n\n代码分为两层for循环，外层for循环是依次从无序的子序列中取出一个元素，外层for循环i从 1开始，小于n，i++，共需要取n-1次。内层的for循环，表示的是要比较的数组元素，j从i-1开始，j>=0，j--。由于内层的for循环都是有序的子序列，所以最多需要比较i-1次，一旦发现有序序列中的某个元素是比 外层拿出来比较的元素要来的小，那就跳出内层for循环。\n\n# 书写代码的注意点\n\n 1. 外层for循环，i从1开始，i<n，i++\n 2. 内层for循环，j从i-1开始，j>=0， j--\n 3. 在外层for循环内，引入一个中间变量tmp，用来临时存储a[i]，这是由于取出a[i]放入有序序列进行比较的过程中，会有各个有序序列的移动，这样的话，要插入的元素a[i]的值就会有变化了，所以先取出来，放入一个中间变量中，待找到合适的位置的时候，再插入进去。\n 4. 内层循环中，判断条件是 if (tmp < a[j])，那么a[j+1] = a[j]。也就是a[j]元素的值，放入到a[j+1]的上面了。\n 5. 内层循环中，如果条件 if (tmp >= a[j])的话，直接跳出内层for循环。\n 6. 注意最后的赋值出现在内层for循环中，a[j+1] = tmp。这个要分两种情况来理解，一种是完整执行完毕内层for循环，跳出了循环，这个时候j=-1了，所以需要j+1。第二种情况，是中途当发现tmp>=a[j]的话的时候，直接跳出循环了，将tmp放入a[j+1]的位置。\n\n# 插入排序代码分析\n\n 1. 插入排序是原地排序算法。在整个代码的实现过程中，可以看出，插入排序算法中，除了引入了一个tmp的临时变量，没有开辟其他的内存空间。所以空间复杂度是O(1)，这是一个原地的排序算法。\n 2. 插入排序是稳定的排序算法。在将取出的无序的子序列的一个元素tmp的时候，会依次与a[j]开始比较，只有当tmp <a[j]的时候，才会去交换外置，等于的时候，不变化位置，这样的话，就可以保证原有的前后顺序不变，所以说插入排序是稳定的排序算法。\n 3. 插入排序的时间复杂度。如果数组的元素都是有序的情况下，只需要一个外层的每个取无序数组元素的一个操作，内层for循环由于不满足tmp < a[j]，直接就跳出了循环，所以最好情况下的时间复杂度是O(n)。如果数组的元素都是无序的，那么除了外层的取各个元素的for 循环操作，内层for循环还需要挨个的比较，交换位置。所以最坏情况时间复杂度是O($n^2$)。平均时间复杂度是O($n^2$)，数组中插入一个数据的评卷时间复杂度是O(n)，循环n次，就是O($n^2$)。\n\n\n# 选择排序\n\n# 选择排序思想\n\n选择排序算法的实现思路也有点类似插入排序，也分为已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\n\n# 选择排序代码\n\n# include <stdio.h>\n\nvoid select_sort(int *a, int len)\n{\n    int i;\n    int j;\n    // 定义最小值\n    int min_value;\n    // 定义最小值的位置\n    int min_pos;\n\n    // n个元素，需要进行n-1趟的排序\n    // 看作是n个位置上逐个从剩余的元素中找最小值\n    // 第一个位置，从n个元素中找\n    // 第二个位置，从n-1个元素中找\n    // 第n-1个位置，剩下的2个元素中来找\n    // 第n个位置，不要找了，就是最后一个元素\n    for (i=0; i< len -1; i++)\n    {\n        // 先定义一个初始的最小值和最小值的位置\n        min_value = a[i];\n        min_pos = i;\n\n        // 内层循环从a[i+1]开始，直到a[len-1]结束的无序数组中来找寻最大值\n        for (j=i+1; j< len; j++)\n        {\n            // 如果无序子序列中的某个元素值，要小于初始定义的最小值\n            if (a[j] < min_value)\n            {\n                //  将a[j]赋值给min_value\n                min_value = a[j];\n                // 将j赋值给min_pos\n                min_pos = j;\n            }\n        }\n        // 将内层的for循环中找到的最小值，和初始的最小值进行比较\n        // 如果内层的无序的for循环中的最小值要比初始定义的最小值，还要小\n        if (min_value < a[i]) \n        {\n            // 将原先定义的最小值放入，后面无序子序列中找到的最小值的位置上\n            // 也就是实现了a[i]和后面找到的a[min_pos]的两个值的交换\n            a[min_pos] =a[i];\n            // 将后面找到的最小值赋值给a[i]\n            a[i] = min_value;\n        }\n    }\n}\n\nint main()\n{\n    int a[9] = {12, 44, 0, 12, 45, 666, 5, 4, 3};\n    select_sort(a, 9);\n    for (int i=0; i<9; i++)\n    {\n        printf("%d ", a[i]);\n    }\n    printf("\\n");\n    return 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n\n\n# 选择排序代码分析\n\n 1. 选择排序是原地排序代码。只是引入了一个int 最小值，和一个最小值所在的位置，空间复杂度是O(1)。所以选择排序是一种原地排序算法。\n 2. 选择排序是不稳定的排序算法。比如5,7,5,2,9这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素2，与第一个5进行了交换位置，那么第一个5和中间的5的顺序就变了，所以就不稳定了。\n 3. 选择排序的时间复杂度。最好情况，最坏情况，和平均情况的时间复杂度都是O($n^2$)。这是由于外层逐个位置的for循环不会少，内层的从无序子序列中找寻最小值的for循环也不会少，只是后面的if代码会有些变化，但是总体的时间复杂度还是O($n^2$) .\n\n\n# 归并排序\n\n# 归并排序思想\n\n基本思想就是：将两个或两个以上的有序子序列"归并"为一个有序序列。如果是两个有序子序列的话，就是2-路归并排序。\n\n即将两个位置相邻的有序子序列R[l...m]和R[m+1...n]归并为一个有序序列R[l...n].\n\n# 归并排序代码\n\n#include <iostream>\nusing namespace std;\n\n// L是指arr数组中最开始的位置，数组下标\n// R是指arr数组中最右边的位置，数组下标\n// M是(L+R)/2后得到的中间位置的数组下标\n// arr是一个数组，这个数组有个特点，前半部分有序，后半部分也有序\nvoid merge(int *arr, int L, int M, int R){\n\tint left_size = M-L;\n\tint right_size = R-M+1;\n\tint *L_arr = new int[left_size];\n\tint *R_arr = new int[right_size];\n\n\t// 1. fill in the left sub array\n\tfor(int i=L; i<M; i++)\n\t\tL_arr[i-L] = arr[i];\n\t// 2. fill in the right sub array\n\tfor(int i=M; i<=R; i++)\n\t\tR_arr[i-M] = arr[i];\n\t\n\t// i表示的是left_size数组中一开始的下标\n\t// j表示的是right_size数组中一开始的下标\n\t// 定义k一开始指向原数组最开始的位置,也就是L，由于是反复的嵌套调用，所以需要是L\n\tint i = 0, j = 0, k = L;\n\t// i和j都不碰到各自s数组的最大的限制\n\twhile(i < left_size && j < right_size)\n\t    //判断两个数组中，下标为i和下标为j 的这两个谁大\n\t\tif(L_arr[i]<R_arr[j])\n\t\t    // 可以改写为arr[k]=left[i]; i++; k++;\n\t\t\tarr[k++] = L_arr[i++];\n\t\telse\n\t\t   // 可以改写为arr[k]=right[j]; j++; k++;\n\t\t\tarr[k++] = R_arr[j++];\n\t// 上面那个while结束后，如果i还小于left_size的话\n\t// 就将left数组中的剩余的元素都挨个，赋值到arr数组中\n\twhile(i < left_size)\n\t    // 下面代码可以改写为arr[k]=left[i];i++;k++;\n\t\tarr[k++] = L_arr[i++];\n\t// 上面那个while结束后，如果j还小于right_size的话\n\t// 就将right数组中的剩余的元素都挨个，赋值到arr数组中\n\twhile(j < right_size)\n\t    // 下面代码可以改写为arr[k]=right[j];j++;k++;\n\t\tarr[k++] = R_arr[j++];\n}\n\nvoid merge_sort(int *arr, int L, int R){\n\t// 左边的下标和右边下标相等的话，\n\t// 就认为是已经能拍好顺序了\n\t// 递归的终止条件\n\tif(L == R)\n\t\treturn;\n\telse\n\t{\n\t\tint M = (L+R)/2;\n\t\tmerge_sort(arr, L, M);\n\t\tmerge_sort(arr, M+1, R);\n\t\tmerge(arr, L, M+1, R);\n\t}\n}\n\nvoid print_sort(int *arr, int num)\n{\n\tfor(int i=0; i < num; i++)\n\t\tcout << arr[i] << "   ";\n\tcout << endl;\n}\n\nint main()\n{\n\tint arr[11] ={6, 3};\n\tmerge_sort(arr, 0, 1);\n\tprint_sort(arr, 2);\n\treturn 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n\n\n# 归并排序代码分析\n\n 1. 归并排序不是原地的排序算法，需要额外申请一个和原数组大小一样的数组来临时存放数据。空间复杂度是O(n).\n 2. 归并排序是稳定的排序算法。在比较两个有序子序列的时候，我们可以将两个相等元素，不交换位置。\n 3. 归并排序的最好，最坏的时间复杂度都是O(nlogn).\n\n\n# 快速排序\n\n# 排序思想\n\n快排是以待排序列中的某个数据为支点，将待排序的序列分为两个部分，其中左半部分的数据要小于等于支点，而右半部分的数据要大于等于支点。\n\n分别三个下标，privotkey是我们选择的支点，可以选择是第一个或最后一个元素。low是我们选择的从数组最开始往后搜索的指针，而high是从数组中最末尾向前搜索的指针。\n\n如果privotkey是第一个元素，那么先从high 与 privotkey比较，如果high > privotkey则继续high向前移动，直到小于privotkey的时候，与low交换位置。交换完后，low开始向后比较，直到low所在的元素比pri大的时候，与high 交换位置。最后当high与low重合的时候，该趟停止。\n\n然后将在pirvotkey的两边，分成两个子序列，循序继续上面的操作。直到所有都有序。\n\n# 快速排序的代码\n\n# include <stdio.h>\n\n// 对序列进行一次划分\nint Partition (SqList &L, int low, int high)\n{\n    int pivotkey;\n    // 关键字\n    privotkey = L.r[low];\n\n    // 从表的两端交替向中间扫描\n    while (low < high) \n    {\n        while (low<high && L.r[high] >=privotkey) {\n            --high;\n        }\n        // 交换位置\n        L.r[low] = L.r[high];\n        while (low<high && L.r[low] <=pivotkey) {\n            ++low;\n        }\n        // 交换位置\n        L.r[high] = L.r[low];\n    }\n    // 返回中间的位置\n    return low;\n}\n\n// 主程序，按分区对子程序进行调用\nvoid QuickSort1 (SqList &L, int low, int high)\n{\n    int mid;\n    if (low < high) {\n        mid = Partition(L, low, high);\n        //对低子表进行排序\n        QuickSort1 (L, low, mid-1);\n        //对高子表进行排序\n        QuickSort1 (L, mid+1, high);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 分析快速排序的代码\n\n 1. 快速排序是不稳定的排序算法。无法保证两个相等的元素，排序前后的位置是一致的。\n 2. 快速排序是原地的排序算法，不需要一看如额外的内存空间。\n 3. 快排的时间复杂度，快排的大部分时候的时间复杂度是O(nlogn)，但是当分区均衡的时候，是O(nlogn)。而当分区及其不均衡的时候，是指已经是一个有序的数组了，这种情况下分区是及其不均衡的，这个时候的时间复杂度会退化到O($n^2$) .\n\n\n# 桶排序\n\n# 桶排序的排序思想\n\n首先对于使用桶排序，还是对数组序列有一定的要求的。这些要排序的数据需要很容易的划分成m个桶。\n\n桶与桶之间有着天然的大小顺序，这个每个桶内的数据都排好序后，桶与桶之间的额数据不需要再进行排序了。另外桶的范围不能太大了。\n\n 1. 首先将要排序的数据分到几个有序的桶里。\n 2. 每个桶里的数据再单独进行排序。\n 3. 桶内排完序后，再把每个桶内的数据按照顺序依次取出。\n 4. 最后组成的序列就是有序的了。\n\n# 分析桶排序代码\n\n 1. 桶排序是稳定的排序算法，原数组扫描的时候，如果从头开始扫描依次放入各个桶内，然后从桶内读取数据，也依次读出，可以保证有序性。\n 2. 桶排序是非原地的排序算法。需要引入额外的内存空间，桶。\n 3. 桶排序的时间复杂度。\n    * 最好情况下，n个数据，均匀地分到m个桶内，每个桶内就是k=n/m个数据。每个桶内使用快排，时间复杂度就是O(k*logk)。m个桶排序的时间复杂度就是O(m*k*logk)，因为k=n/m，所以整个桶的排序就是O(n*log(n/m)).当桶的个数m接近数据个数n的时候，log(n/m)就是一个非常小的常量。这个时候的桶排序时间复杂度接近O(n).\n    * 最坏情况下，同上面的具体情况，如果极度不均匀，就只有一个桶的时候，一个桶的排序就是一个快排了O(nlogn).\n\n\n# 计数排序\n\n# 计数排序的算法思想\n\n普通思想：\n\nn个数据，所处的范围并不大的时候。比如最大值为k，我们就设定每个值是1个桶，就把数据划分成了k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。最后依次扫描各个桶，形成排好序的完整数组。\n\n稳定的算法思想：\n\n存储每个值的桶内的原始值，为数组前后的相加。这样每个桶位置上的数值，就是小于等于该桶下标的值的数量。我们先从尾到头遍历一下原来要排序的数组A，找到A中的一个元素值X，利用这个值，到C桶数组中，找到C桶中对应的这个X值的下标的值是Y。将A中的这个元素值，写入到完整排好序的数组R[Y-1]的位置。继续从为到头遍历数组A，这样继续从尾巴到头遍历数组A，下来，数组中相等元素的位置就是稳定的了。\n\n# 计数排序的特殊要求\n\n计数排序只能是针对大量重复数据，在一个小范围内的排序，如果数据范围k比要排序的数据n大得多，就不适合用计数排序了。\n\n计数排序其实是桶排序的一种特殊情况。\n\n计数排序中的桶大小粒度，与桶排序中的粒度是不一样的。桶排序中，考虑的是每个桶内要均匀，而在计数排序中的桶，考虑的是涵盖一个小的范围，每个值就是一个桶。\n\n计数排序只能给非负整数排序，如果要排序的数据是其他类型，要将其在不改变相对大小的情况下，转化为非负整数。\n\n# 计数排序的算法分析\n\n 1. 计数排序可以做到稳定的排序算法。\n 2. 计算排序不是原地排序算法，同样，在排序的过程中，需要额外申请一个桶C的数组k，还有一个最后拍好序的数组n.\n 3. 计数排序的时间复杂度。由于计数排序的要求是特殊场景下，才能实现的，只要满足这种场景的情况下使用的计数排序，都是O(n).\n\n\n# 基数排序\n\n# 基数排序的特殊要求\n\n 1. 要排序的数据，可以分割出独立的"位"来比较，位数不够的，需要补足位数。补足的位数，不要影响原有的大小顺序。数据可以划分"高低"位。\n 2. 位与位之间有递进的关系，如果a数据的高位比b数据大，那么剩下的低位就不用比较了。\n 3. 每一位的数据范围不能太大，要可以使用线性排序算法。由于要对每一位上的数据范围，进行桶排序或者是计数排序。\n\n# 基数排序的排序思想\n\n 1. 所有数据先按最后一位来排序，可以使用桶排序或计数排序。\n 2. 接下来是倒数第二位排序。\n 3. 直到最后对第一位排序。\n\n# 基数排序算法分析\n\n 1. 基数排序是稳定的排序算法，在对每一位进行桶或计数排序的时候，可以保证整个数据的前后顺序的一致性。\n 2. 基数排序不是原地的排序算法。由于每一位的排序中，用到了桶排序或计数排序，而桶排序需要一个C数组和一个新牌号序的数组R，计数排序也是同样需要的。\n 3. 基数排序算法的时间复杂度。道理同上面的计数排序，基数排序还是非常特殊的排序，用在特殊的场景下，如果这个时候要排序的数据有k位，需要使用k次桶排序或计数排序，总的时间复杂度就是O(k*n)。当k不大的时候，可以将基数排序时间复杂度近似于O(n)。\n\n\n# 整体分析\n\n线性排序算法(桶排序、计数排序、基数排序)的时间复杂度比较低，使用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。\n\n如果对小规模数据进行排序，可以选择时间复杂度是O($n^2$)的算法；如果Udine大规模数据进行排序，时间复杂度是O(nlogn)的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是O(nlogn)的排序算法来实现排序函数。\n\n时间复杂度是O(nlogn)的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面还会有堆排序。堆排序和快速排序都有比较多的应用，比如Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。\n\n实际过程中，使用归并排序的情况其实并不多。我们知道，快排在最坏情况下的时间复杂度是O($n^2$) ，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是O(nlogn)，从这点上看起来很诱人，那为何归并排序使用不多呢？\n\n这是由于归并排序并不是原地排序算法，空间复杂度是O(n)。\n\n前面提及了，快速排序比较适合来实现排序函数，但是，我们也知道，快速排序子最坏情况下的时间复杂度是O($n^2$)，如何来解决这个问题呢？\n\n\n# 如何优化快速排序？\n\n快排算法，为什么在最坏情况下的时间复杂度是O($n^2$)呢？如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算就会变得非常糟糕，时间复杂度就会退化为O($n^2$)。实际上，这种O($n^2$) 时间复杂度出现的这主要原因还是因为我们分区点选得不够合理。\n\n那如何来选择好的分区点呢？\n\n最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。\n\n如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现之前的情况。在某些情况下，排序的最坏情况时间复杂度是O($n^2$)。\n\n\n# 三数取中法\n\n我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。\n\n\n# 随机法\n\n随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选得很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的 O(n2) 的情况，出现的可能性不大。\n\n我们知道，快速排序是用递归来实现的。我们在递归那一节讲过，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。\n\n\n# 举例分析排序函数\n\nGlibc 中的 qsort() 函数举例说明一下。虽说 qsort() 从名字上看，很像是基于快速排序算法实现的，实际上它并不仅仅用了快排这一种算法。\n\n如果你去看源码，你就会发现，qsort() 会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是 O(n)，所以对于小数据量的排序，比如 1KB、2KB 等，归并排序额外需要 1KB、2KB 的内存空间，这个问题不大。现在计算机的内存都挺大的，我们很多时候追求的是速度。还记得我们前面讲过的用空间换时间的技巧吗？这就是一个典型的应用。\n\n但如果数据量太大，就跟我们前面提到的，排序 100MB 的数据，这个时候我们再用归并排序就不合适了。所以，要排序的数据量比较大的时候，qsort() 会改为用快速排序算法来排序。\n\nqsort() 选择分区点的方法就是“三数取中法”。还有我们前面提到的递归太深会导致堆栈溢出的问题，qsort() 是通过自己实现一个堆上的栈，手动模拟递归来解决的。\n\n实际上，qsort() 并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于 4 时，qsort() 就退化为插入排序，不再继续用递归来做快速排序，因为我们前面也讲过，在小规模数据面前，O(n2) 时间复杂度的算法并不一定比 O(nlogn) 的算法执行时间长。\n\n我们在讲复杂度分析的时候讲过，算法的性能可以通过时间复杂度来分析，但是，这种复杂度分析是比较偏理论的，如果我们深究的话，实际上时间复杂度并不等于代码实际的运行时间。\n\n时间复杂度代表的是一个增长趋势，如果画成增长曲线图，你会发现 O(n2) 比 O(nlogn) 要陡峭，也就是说增长趋势要更猛一些。但是，我们前面讲过，在大 O 复杂度表示法中，我们会省略低阶、系数和常数，也就是说，O(nlogn) 在没有省略低阶、系数、常数之前可能是 O(knlogn + c)，而且 k 和 c 有可能还是一个比较大的数。\n\n假设 k=1000，c=200，当我们对小规模数据（比如 n=100）排序时，n2的值实际上比 knlogn+c 还要小。\n\n所以，对于小规模数据的排序，O(n2) 的排序算法并不一定比 O(nlogn) 排序算法执行的时间长。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法。\n\n\n# D42(2020/11/10)\n\n今天要学习的是二分查找的第一部分。\n\n今天要学习的是一种针对有序数据集合的查找算法：二分查找算法，也叫折半查找算法。二分查找的思想非常简单，但是看似简单的东西往往越掌握好，想要灵活应用就更加困难。\n\n\n# 无处不在的二分思想\n\n假设有10个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98。我们如何利用二分查找的思想，查找到是否存在金额等于19元的订单。如果存在，则返回订单数据，如果不存在则返回null.\n\n利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。为了更加直观，画了一张查找过程的图。其中，low和high表示待查找区间的下标，mid表示待查找区间的中间元素下标。\n\n\n\n二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，知道找到要查找的元素，或者区间被缩小为0。\n\n\n# O(logn)惊人的查找速度\n\n我们假设数据大小是n，每次查找后数据都会缩小为原来的一半，也就是会除以2。最坏情况下，直到查找区间被缩小为空，才停止。\n\n\n\n可以看出来，这是一个等比数列。其中n/ $2^k$ = 1时，k的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了k次区间缩小操作，时间复杂度就是O(k)。通过n/ ($2^k$) = 1, 我们可以求得k= log2 n ,所以时间复杂度就是O(logn)。\n\n二分查找是我们目前为止遇到的第一个时间复杂度是O(logn)的算法。后面章节还会讲到堆、二叉树的操作等等，它们的时间复杂度也是O(logn)。\n\nO(logn)这种对数时间复杂度，是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级O(1)的算法还要高效。\n\n因为logn是一个非常"恐怖"的数量级，即便n非常非常大，对应的logn也很小。比如n等于2的32次方，这个数大约是42亿。也就是说，如果我们在42亿个数据中用二分查找一个数据，最多需要比较32次。\n\n我们前面讲过，用大O标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1)有可能表示的是一个非常大的常量值，比如O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有O(logn)的算法执行效率高。\n\n\n# 二分查找的递归与非递归实现\n\n简单的二分查找，并不难写，如下所示。\n\n最简单的情况就是有序数组中不存在重复元素，我们在其中用二分查找值等于给定值的数据。如下面的Java代码：\n\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n\n  while (low <= high) {\n    int mid = (low + high) / 2;\n    if (a[mid] == value) {\n      return mid;\n    } else if (a[mid] < value) {\n      low = mid + 1;\n    } else {\n      high = mid - 1;\n    }\n  }\n\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n在这个代码中，low、high、mid都是指数组下标，其中low和high表示当前查找的区间范围，初始low=0，high=n-1。mid表示[low, high]的中间位置。我们通过对比a[mid]与value的大小，来更新接下来要查找的区间范围，直到找到或者区间缩小为0，就退出。这里有三个容易出错的地方.\n\n 1. 循环退出条件\n    \n    注意是low <= high，而不是low < high.\n\n 2. mid的取值\n    \n    实际上，mid=(low+high)/2这种写法是由问题的。因为如果low和high比较大的话，两者之和就有可能会溢出。改进的方法是将mid的计算方式写成low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以2操作转化成位运算low+((high-low)>>1)。因为相比除法运算来说，计算机处理位运算要快得多。\n\n 3. low和high的更新\n    \n    low = mid +1, high=mid -1。注意这里的+1和-1，如果直接写成low=mid 或者high=mid，就可能会发生死循环。比如，当high=3, low=3时，如果a[3]不等于value，就会导致一直循环不退出。\n\n实际上，二分查找除了用循环来实现，还可以用递归来实现，过程也非常简单。\n\n如下面的Java代码所示：\n\n// 二分查找的递归实现\npublic int bsearch(int[] a, int n, int val) {\n  return bsearchInternally(a, 0, n - 1, val);\n}\n\nprivate int bsearchInternally(int[] a, int low, int high, int value) {\n  if (low > high) return -1;\n\n  int mid =  low + ((high - low) >> 1);\n  if (a[mid] == value) {\n    return mid;\n  } else if (a[mid] < value) {\n    return bsearchInternally(a, mid+1, high, value);\n  } else {\n    return bsearchInternally(a, low, mid-1, value);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 二分查找应用场景的局限性\n\n二分查找的时间复杂度是O(logn)，查找数据的效率非常高。不过，并不是什么情况下都可以用二分查找，它的应用场景是有很大局限性的。那什么情况下适合用二分查找，什么情况下不适合用呢？\n\n首先，二分查找依赖的是顺序表结构，简单来说就是数组。\n\n如果我们对链表是否能使用二分查找呢？答案是不可以的，主要原因是二分查找算法需要按照下标随机访问元素。数组按照下标随机访问数据的时间复杂度是O(1)，而链表随机访问的时间复杂度是O(n)。所以，如果数据使用链表来存储的话，二分查找的时间复杂度就会变得很高。\n\n二分查找只能用在数据是通过顺序表来存储的数据结构上。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。\n\n其次，二分查找针对的是有序数据。\n\n二分查找对这一点的要求比较苛刻，数据必须是有序的。如果数据没有序，我们需要先排序。前面提及，排序的时间复杂度最低是O(nlogn)。所以，如果我们针对的是一组静态的数据，没有频繁地插入、删除，我们可以进行一次排序，多次二分查找。这样排序的成本可被均摊，二分查找的边际成本就会比较低。\n\n但是，如果我们的数据集合有频繁的插入和删除操作，要想用二分查找，要么每次插入、删除操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，无论哪种方法，维护有序的成本都是很高的。\n\n所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用。那针对动态数据集合，会考虑使用二叉树。\n\n再次，数据量太小的不适合二分查找。\n\n如果要处理的数据量很小，完全没有必要使用二分查找，顺序遍历就足够了。比如我们在一个大小为10的数组中查找一个元素，不管用二分查找还是顺序遍历，查找速度都差不多。只有数据量比较大的时候，二分查找的优势才会比较明显。\n\n不过，这里也有一个例外。如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找。比如，数组中存储的都是长度超过300的字符串，如此长的两个字符串之间比对大小，就会非常耗时。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。\n\n最后，数据量太大也不适合二分查找。\n\n二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有1GB大小的数据，如果希望用数组来存储，那就需要连续的，连续的，连续的1GB的内存空间。\n\n\n# 解答开篇\n\n虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决。但是，我们后面会了解到，不管是散列表还是二叉树，都会需要比较多的额外的内存空间。\n\n如果用散列表或者二叉树来存储这1000万的数据，用100MB的内存肯定是存不下的。二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式。\n\n\n# D43(2020/11/11)\n\n今天要学习二分查找的第二部分。如何快速定位IP对应的额省份地址？\n\n我们把要查询202.102.133.13这个IP地址的归属地的问题，转换为查找这个IP地址是否落在了[202.102.133.0,202.102.133.255]这个地址范围内，我们利用这个地址范围来给出这个IP地址的归属地了。\n\n我们的问题是，在一个庞大的地址库中逐一比对IP地址所在的区间，是非常耗时的。假设我们有12万条这样的IP区间与归属地的对应关系，如何快速定位出一个IP地址的归属地呢？\n\n上一节所学的二分查找的代码实现并不难写，那是因为上一节讲的只是二分查找中最简单的一种情况，在不存在重复元素的有序数组中，查找值等于给定值的元素。最简单的二分查找写起来确实不难，但是，二分查找的变形问题就没那么好写了。\n\n下面要描述的是4种二分查找的变形问题，分别是查找第一个值等于给定值的元素；查找最后一个值等于给定值的元素；查找第一个大于等于给定值的元素；查找最后一个小于等于给定值的元素。\n\n\n# 变体一：查找第一个值等于给定值的元素\n\n上一节的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。如果我们将这个问题稍微修改一下，有序数据集合中存在重复的数据，我们希望找到第一个值等于给定值的数据，这样之前的二分查找代码还能继续工作吗？\n\n比如下面这样一个有序数组，其中，a[5]，a[6]，a[7]的值都等于8，是重复的数据。我们希望查找第一个等于8的数据，也就是下标是5的数据。\n\n\n\n如果我们用上一节课讲的二分查找的代码实现，首先拿8与区间的中间值a[4]比较，8比6大，于是在下标5到9之间继续查找。下标5和9的中间位置是下标7，a[7]正好等于8，所以代码就返回了。\n\n尽管a[7]也等于8，但它并不是我们想要找的第一个等于8的元素，因为第一个值等于8的元素是数组下标为5的元素。我们上一节讲的二分查找代码就无法处理这种情况了。所以，针对这个变形问题，我们可以改造上一节的代码。\n\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n  while (low <= high) {\n    int mid =  low + ((high - low) >> 1);\n    if (a[mid] > value) {\n      high = mid - 1;\n    } else if (a[mid] < value) {\n      low = mid + 1;\n    } else {\n      if ((mid == 0) || (a[mid - 1] != value)) return mid;\n      else high = mid - 1;\n    }\n  }\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\na[mid]跟要查找的value的大小关系有三种情况：大于、小于、等于。对于a[mid]>value的情况，我们需要更新high=mid -1; 对于a[mid] < value的情况，我们需要更新low = mid+1。这两点很好理解。那当a[mid] = value的时候应该如何处理呢？\n\n如果我们查找的是任意一个值等于给定值的元素，当a[mid]等于要查找的值的时候，a[mid]就是我们要找的元素。但是，这里我们要求解的是第一个值等于给定值的元素，当a[mid]等于要查找的值时，我们就需要确认一下这个a[mid]是不是第一个值等于给定值的元素。\n\n我们来看第11行代码。\n\nif ((mid == 0) || (a[mid - 1] != value)) return mid;      \nelse high = mid - 1;\n\n\n1\n2\n\n\n如果mid等于0，那这个元素已经是数组的第一个元素，那它肯定是我们要找的；如果mid不等于0，但a[mid]的前一个元素a[mid-1]不等于value，那也说明a[mid]就是我们要找的第一个值等于给定值的元素。\n\n如果经过检查之后发现a[mid]前面的一个元素a[mid-1]也等于value，那说明此时的a[mid]肯定不是我们要查找的第一个值等于给定值的元素。那我们就更新high=mid -1，因为要找额元素肯定出现在[low, mid-1]之间。\n\n\n# 变体二：查找最后一个值等于给定值的元素\n\n如果这里要查找的是最后一个值等于给定值的元素，该如何写代码。\n\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n  while (low <= high) {\n    int mid =  low + ((high - low) >> 1);\n    if (a[mid] > value) {\n      high = mid - 1;\n    } else if (a[mid] < value) {\n      low = mid + 1;\n    } else {\n      if ((mid == n - 1) || (a[mid + 1] != value)) return mid;\n      else low = mid + 1;\n    }\n  }\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n上面的代码是根据第一种问题的改写，得来的。\n\n重点还是在第11行代码，如果a[mid]这个元素已经是数组中的最后一个元素了，那它肯定是我们要找的；如果a[mid]的后一个元素a[mid+1]不等于value，那也说明a[mid]就是我们要找的最后一个值等于给定值的元素。\n\n否则的话(这个时候已经找到了mid是给定的值，但是不是最后一个，后一个元素还是等于给定值)，那么就将low = mid+1，也就是说将low下标往后移动一格，继续去执行while循环。直到满足下面的条件，才返回。\n\nif ((mid == n - 1) || (a[mid + 1] != value)) return mid;      \nelse low = mid + 1;\n\n\n1\n2\n\n\n如果我们经过检查之后，发现a[mid]后面的一个元素a[mid+1]也等于value，那说明当前的这个a[mid]并不是最后一个值等于给定值的元素。我们就更新low=mid+1，因为要找的元素肯定出现在[mid+1,high]之间。\n\n\n# 变体三：查找第一个大于等于给定值的元素\n\n现在我们再来看另外一个变形问题。在有序数组中，查找第一个大于等于给定值的元素。比如，数组中存储的这样一个序列：3，4，6，7，10。如果查找第一个大于等于5的元素，那就是6。\n\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n  while (low <= high) {\n    int mid =  low + ((high - low) >> 1);\n    if (a[mid] >= value) {\n      if ((mid == 0) || (a[mid - 1] < value)) return mid;\n      else high = mid - 1;\n    } else {\n      low = mid + 1;\n    }\n  }\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n如果a[mid]小于要查找的值value，那要查找的值肯定在[mid+1, high]之间，所以，我们更新low=mid+1。\n\n对于a[mid]大于等于给定值的value的情况，我们要先看下这个a[mid]是不是我们要找的第一个值大于等于给定值的元素。如果a[mid]前面已经没有元素，或者前面一个元素小于要查找的值value，那a[mid]就是我们要找的元素。这段逻辑对应的代码是第7行。\n\nif (a[mid] >= value) {      \n    if ((mid == 0) || (a[mid - 1] < value)) return mid;      \n    else high = mid - 1;    \n} else {      \n    low = mid + 1;    \n    }  \n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n如果a[mid-1]也大于等于要查找的值value，那说明要查找的元素在[low, mid-1]之间，所以，我们将high更新为mid-1。\n\n\n# 变体四：查找最后一个小于等于给定值的元素\n\n现在，我们来看最后一种二分查找的变形问题，查找最后一个小于等于给定值的元素。比如，数组中存储了这样一组数据：3，5，6，8，9，10。最后一个小于等于7的元素就是6。\n\npublic int bsearch7(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n  while (low <= high) {\n    int mid =  low + ((high - low) >> 1);\n    if (a[mid] > value) {\n      high = mid - 1;\n    } else {\n      if ((mid == n - 1) || (a[mid + 1] > value)) return mid;\n      else low = mid + 1;\n    }\n  }\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 解答开篇\n\n如何快速定位出一个IP地址的归属地？\n\n如果IP区间与归属地的对应关系不经常更新，我们可以先预处理这12万条数据，让其按照起始IP从小到大排序。我们知道，IP地址可以转化为32位的整型数。所以，我们可以将起始地址，按照对应的整型值的大小关系，从小到大进行排序。\n\n然后，这个问题就可以转化为第四种变形问题 "在有序数组中，查找最后一个小于等于某个给定值的元素"了。\n\n当我们要查询某个IP归属地时，我们可以通过二分查找，找到最后一个起始IP小于等于这个IP的IP区间，然后，检查这个IP是否在这个IP区间内，如果在，我们就取出对应的归属地显示；如果不在，就返回未查找到。\n\n\n# 内容小结\n\n凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。\n\n实际上，上一节我们学习的"值等于给定值"的二分查找确实不怎么会被用到，二分查找更适合用在"近似"查找问题，在这类问题上，二分查找的优势就更加明显了。比如，今天学习的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。\n\n\n# D44(2020/11/12)\n\n今天要学习的是跳表的数据结构。\n\n上两节我们讲解了二分查找算法。当时我们讲到，因为二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，就真的没有办法来用二分查找算法了吗？\n\n实际上，我们只需要对链表稍加改造，就可以支持类似"二分"的查找算法。我们把改造之后的数据结构叫做跳表 (skip list)。\n\n跳表这种数据结构，是一种各方面性能都比较优秀的动态数据结构。可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树。\n\nRedis 中的有序集合(sorted Set)就是用跳表来实现的。我们也知道红黑树也是可以实现快速地插入、删除和查找操作。\n\n那Redis 为什么会选择用跳表来实现有序集合呢？为什么不用红黑树呢？\n\n\n# 如何理解跳表\n\n对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是O(n)。\n\n\n\n那么如何来提高查找效率呢？如果像图中那样，对链表建立一级"索引"，查找起来是不是就会更快一些呢？每两个结点提取一个结点到上一级，我们把抽出来的那一级叫做索引或索引层。如下图所示，down表示down指针，指向下一级结点。\n\n\n\n如果我们现在要查找某个结点，比如16。我们可以先在索引层遍历，当遍历到索引层中值为13的结点时，我们发现下一个结点是17，那要查找的结点16肯定就在这两个结点之间。然后我们通过索引层结点的down指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历2个结点，就可以找到值等于16的这个结点了。这样，原来如果要查找16，需要遍历10个结点，现在只需要遍历7个结点。\n\n从这个例子来看，加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就说查找效率提高了。那如果我们再加一级索引呢？效率会不会提升更多呢？\n\n跟前面建立第一级索引的方式相似，我们在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在我们再来查找16，只需要遍历6个结点了，需要遍历的结点数量又减少了。\n\n\n\n下面的例子中，我画了一个包含64个结点的链表，按照前面讲的思路，建立了五级索引。\n\n\n\n从图中我们可以看出，原来没有索引的时候，查找62需要遍历62个结点，现在只需要遍历11个节点，速度提高了很多。所以，当链表的长度n比较大的时候，比如1000、10000的时候，在构建索引之后，查找效率的提升就会非常明显。\n\n前面讲的这种链表加多级索引的结构，就是跳表。我们通过例子展示了跳表是如何减少查询次数的，由此来提高查询效率。接下来，我们会定量地分析一下，用跳表查询到底有多快呢？\n\n\n# 用跳表查询到底有多快？\n\n前面我们讲解过，算法的执行效率可以通过时间复杂度来度量的。我们知道，在一个单链表中查询某个数据的时间复杂度是O(n)。那在一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢？\n\n这个时间复杂度的分析方法比较难以想到。我们这里把问题分解一下，先来看这样一个问题，如果链表里有n个结点，会有多少级索引呢？\n\n按照我们刚才讲的，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是n/8，依次类推，也就是说，第k级索引的结点个数是第k-1级索引的结点个数的1/2，那第k级索引结点的个数就是 n/(2^k)。\n\n假设索引有h级，最高级的索引有2个结点。通过上面的公式，我们可以得到n/(2^k) = 2, 从而求得h= $\\log_2 k$ -1 。如果包含原始链表这一层，那么整个跳表的高度就是 $log_2 n$。我们在跳表中查询某个数据的时候，如果每一层都要遍历m个结点，那么在跳表中查询一个数据的时候的时间复杂度就是O(m*$log_2 n$) .\n\n那么这个m的值到底是多少来着呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历3个结点，也就是说m=3，为什么呢？\n\n假设我们要查找的数据是x，在第k级索引中，我们遍历到y结点之后，发现x大于y，小于后面的结点z，所以我们通过y的down指针，从第k级索引下降到第k-1级索引。在第k-1级索引中，y和z之间只有3个节点(包含y和z)，所以，我们在K-1级索引中最多只需要遍历3个结点，依次类推，每一级索引都最多只需要遍历3个结点。\n\n\n\n通过上面的分析，我们得到m=3，所以在跳表中查询任意数据的时间复杂度就是O($log n$). 这个查找的时间复杂度跟二分查找是一样的。换句话说，我们其实是基于单链表实现了二分查找。\n\n但是这种查询效率的提升，前提是建立了很多级的索引，也就是我们所说的用空间来换时间的设计思路。\n\n\n# 跳表是不是很浪费内存\n\n比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。那到底需要消耗多少额外的存储空间呢？ 我们来分析一下跳表的空间复杂度。\n\n跳表的空间复杂度分析并不难，在前面所说的，假设原始链表大小为n，那第一级索引大约有n/2个结点，第二级索引大约有n/4个结点，以此类推，每上升一级就减少一半，直到剩下2个结点。如果我们把每层索引的结点数都写出来，就是一个等比数列。\n\n\n\n这几级索引的结点总和就是n/2 + n/4 + n/8 ... +8 +4+2 =n-2。所以，跳表的空间复杂度是 O(n)。也就是说，如果将包含n个结点的单链表构造成跳表，我们需要额外再用接近n个结点的存储空间。那我们有没有办法降低索引上占用的内存空间呢？\n\n我们前面都是每两个结点抽一个结点到上级索引，如果我们每三个结点或五个结点，抽一个结点到上级索引，是不是就不用那么多索引结点了呢？下面是一个每三个结点抽一个的示意图，我们可以看下。\n\n\n\n从图中可以看出，第一级索引需要大约n/3个结点，第二级索引需要大约n/9个结点。每往上一级，索引结点个数都除以3。为了方便计算，我们假设最高一级的索引结点个数是1。我们把每级索引的结点个数都写下来，也是一个等比数列。\n\n\n\n通过等比数列求和公式，总的索引结点大约就是n/3 + n/9 + n/27 + ...+9+3+1 = n/2。尽管空间复杂度还是O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结构存储空间。\n\n实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性把要处理的数据看成是整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。\n\n\n# 高效的动态插入和删除\n\n上面描述了跳表的查找操作。实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是O(logn)。\n\n\n# 动态插入\n\n下面来看下，如何在跳表中插入一个数据，以及它是如何做到O(logn)的时间复杂度。\n\n我们知道，在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。\n\n对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的时间复杂度是O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是O(logn)。\n\n\n# 动态删除\n\n如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。\n\n\n# 跳表索引动态更新\n\n当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某2个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。\n\n\n\n作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点过多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。\n\n我们了解到红黑树、AVL树这样平衡二叉树，它们是通过左右旋的方式保持左右子树的大小平衡，而跳表是通过随机函数来维护前面提到的"平衡性"。\n\n当我们往跳表中插入数据的嘶吼，我们可以选择同时将这个数据插入到部分索引层中。如何选择加入哪些索引层呢？\n\n我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值K，那我们将这个结点添加到第一级到第K级索引中。\n\n\n\n随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。\n\n\n# 解答开篇\n\n我们来讲解一下开篇的思考题：为什么Redis要用跳表来实现有序集合，而不是红黑树。\n\nRedis中的有序集合是通过跳表来实现的，严格来说，其实还用到了散列表。不如果我们去查看Redis 的开发手册，就会发现，Redis中的有序集合支持的核心操作主要有下面这几个：\n\n * 插入一个数据；\n * 删除一个数据；\n * 查找一个数据；\n * 按照区间查找数据(比如查找值在[100, 356]之间的数据)；\n * 迭代输出有序序列。\n\n其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表一样。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。\n\n对于按照区间查找数据这个操作，跳表可以做到O(logn)的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。\n\n当然，Redis 之所以用跳表来实现有序集合，还有其他原因，比如，跳表更容易代码实现。虽然跳表的实现也不简单，但比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。\n\n不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的Map类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果我们想要使用跳表，必须要自己实现。\n\n\n# 内容小结\n\n今天我们学习了跳表这种数据结构。跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的"二分查找"。跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是O(logn).\n\n跳表的空间复杂度是O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向于跳表。\n\n\n# D45(2020/11/19)\n\n今天要学习的是散列表中的第一部分的内容。\n\n\n# 散列思想\n\n散列表的英文叫"Hash Table"，我们平时也叫它"哈希表"或者"Hash表"。\n\n散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就会数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。\n\n这里用一个例子来解释一下。假如我们有89名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。这89名选手的编号依次是1到89。现在我们希望编程实现这样一个功能，通过编号快速找到对应的选手信息。\n\n我们可以把这89名选手的信息放在数组里。编号为1的选手，我们放到数组中下标为1的位置；编号为2的选手，我们放到数组中下标为2的位置。以此类推，编号为K的选手放到数组中下标为K的位置。\n\n因为参赛编号跟数组下标一一对应，当我们需要查询参赛编号为x的选手的时候，我们只需要将下标为x的数组元素取出来就可以了，时间复杂度就是O(1)。这样按照编号查找选手信息。\n\n实际上，这个例子已经用到了散列的思想。在这个例子里，参赛编号是自然数，并且与数组的下标形成一一映射，所以利用数组支持根据下标随机访问的时候，时间复杂度是O(1)这一特性，就可以实现快速查找编号对应的选手的信息。\n\n假设校长说，参赛编号不能设置得这么简单，要加上年级、班级这些更详细的信息，所以我们把编号的规则稍微修改了一下，用6位数字来表示。比如051167，其中，前两位05表示年级，中间两位11表示班级，最后两位还是原来的编号1到89。这个时候我们该如何存储选手信息，才能够支持通过编号来快速查找选手信息呢？\n\n思路还是跟前面类似。尽管我们不能直接把编号作为数组下标，但我们可以截取参数编号的后两位作为数组下标，来存取选手信息数据。当通过参数编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。\n\n这就是典型的散列思想。其中，参赛选手的编号我们叫做键(key)或者关键字。我们用它来标识一个选手。我们把参数编号转化为数组下标的映射方法就叫做散列函数(或Hash函数 哈希函数)，而散列函数计算得到的值就叫做散列值。\n\n\n\n通过这个例子，我们可以总结出这样的规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是O(1)的特性。我们通过散列函数把元素的键值映射为下标，然后量数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。\n\n\n# 散列函数\n\n散列函数在散列表中起着非常关键的作用。\n\n散列函数，顾名思义，它是一个函数。我们可以把它定义为hash(key)，其中key表示元素的键值，hash(key)的值表示经过散列函数计算得到的散列值。\n\n那第一个例子中，编号就是数组下标，所以hash(key)就等于key。改造后的例子，写成散列函数稍微有点复杂。下面用伪代码将它写成函数如下：\n\nint hash(String key) {\n  // 获取后两位字符\n  string lastTwoChars = key.substr(length-2, length);\n  // 将后两位字符转换为整数\n  int hashValue = convert lastTwoChas to int-type;\n  return hashValue;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n刚刚举的学校运动会的例子，散列函数比较简单，也比较容易想到。但是，如果参数选手的编号是随机生成的6位数字，又或者用的是a到z之间的字符串，该如何构造散列函数呢？我总结了三点散列函数设计的基本要求：\n\n 1. 散列函数计算得到的散列值是一个非负整数；\n 2. 如果key1 = key2，那hash(key1) == hash(key2);\n 3. 如果key1 != key2，那hash(key1) != hash(key2)。\n\n其中，第一点理解起来应该没有任何问题。因为数组下标是从0开始的，所以散列函数生成的散列值也要是非负整数。第二点也很好理解。相同的key，经过散列函数得到的散列值也应该是相同的。\n\n第三点理解起来可能会有问题。这个要求看起来合情合理，但是在真实的情况下，要想找到一个不同的key对应的散列值都不一样的散列函数，几乎是不可能的。即便是业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。\n\n所以我们几乎无法找到一个完美的无冲突的散列函数，即便能够找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，我们需要通过其他途径 解决。\n\n\n# 散列冲突\n\n再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法和链表法。\n\n\n# 开放寻址法\n\n开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法，线性探测。\n\n当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，知道找到为止。\n\n如下面图示来看，这里面黄色的色块表示空闲位置，橙色的色块表示已经存储了数据。\n\n\n\n从图中可以看出，散列表的大小为10，在元素x插入散列表之前，已经6个元素插入到散列表中。x经过Hash算法之后，被散列到位置下标为7的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置2，于是将其插入到这个位置。\n\n在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。\n\n\n\n散列表跟数组一样，不仅支持插入、查找操作，还支持删除操作。对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。\n\n在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。\n\n我们可以将删除的元素，特殊标记为deleted。当线性探测查找的时候，遇到标记为deleted的空间，并不是停下来，而是继续往下探测。\n\n\n\n我们可能已经发现了，线性探测法其实存在很大的问题。当散列表中插入的数据越来越多的时候，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下时间复杂度是O(n)。同理，在删除和查找的时候，也有可能会线性探测整张散列表，才能找到要查找或删除的数据。\n\n对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测和双重散列。\n\n所谓二次探测，跟线性探测很像，线性探测的每次探测的步长是1，那它探测的下标序列就是hash(key)+0，hash(key)+1，hash(key)+2 .... 而二次探测探测的步长就变成了原来的"二次方"，也就是说，它探测的下标序列就是hash(key)+0，hash(key)+1^2 , hash(key)+2^2 ....\n\n所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数hash1(key)，hash2(key)，hash3(key) ....我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，知道找到空闲的存储位置。\n\n不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子来表示空位的多少。\n\n装载因子的计算公式是：\n\n散列表的装载因子 = 填入表中的元素个数/散列表的长度\n\n装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。\n\n\n# 链表法\n\n链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。我们来看下面的图示，在散列表中，每个"桶(bucket)"或者"槽(slot)"会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。\n\n\n\n当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是O(1)。当查找、删除一个元素的时候，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或删除。\n\n实际上，这两个操作的时间复杂度跟链表的长度k成正比，也就是O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中n表示散列中数据的个数，m表示散列表中"槽"的个数。\n\n\n# 解答开篇\n\nWord文档中单词拼写检查功能是如何实现的？\n\n常用的英文单词有20万个左右，假设单词的平均长度是10个字母，平均一个单词占用10个字节的内存空间，那20W英文单词大约占2MB的存储空间，就算放大10倍也就是20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。\n\n当用户输入某个英文单词的时候，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。\n\n\n# 内容小结\n\n散列表来源于数组，它借助散列山水对数组这种数据结构进行扩展，利用的是数组支持按照下标随机访问元素的特性。散列表两个核心问题是散列函数设计和散列冲突解决。散列冲突有两种常用的解决方法，开放寻址法和链表法。散列函数的设计好坏决定了散列冲突的概率，也就决定散列表的性能。\n\n\n# D46(2020/11/23)\n\n今天要学习的是散列表中的第二部分的内容。\n\n通过上一节的学习，我们知道，散列表的查询效率并不能笼统地说成是O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。\n\n在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果我们使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从O(1)急速退化为O(n)。\n\n如果散列表中有10万个数据，退化后的散列表查询的效率就下降了10万倍。更直接点说，如果之前运行100次查询只需要0.1秒，那现在就需要1万秒。这样就有可能因为查询操作消耗大量CPU或线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击(DoS)的目的。这也就是散列表碰撞攻击的基本原理。\n\n今天，我们就来学习一下，如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？\n\n\n# 如何设计散列函数？\n\n散列函数设计的好坏，决定了散列表冲突的概率大小，也直接决定了散列表的性能。那什么才是好的散列函数呢？\n\n首先，散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接影响到散列表的性能。\n\n其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。\n\n实际工作中，我们还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。散列函数各式各样，我举几个常用的、简单的散列函数的设计方法，让我们有个直观的感受。\n\n第一个例子就是我们上一节的学生运动会的例子，我们通过分析参赛编号的特征，把编号中的后两位作为散列值。我们还可以用类似的散列函数处理手机号码，因为手机号码前几位重复的可能性很大，但是后面几位就比较随机，我们可以取手机号的后四位作为散列值。这种散列函数的设计方法，我们一般叫做"数据分析法"。\n\n第二个例子就是上一节的开篇思考题，如何实现Word拼写检查功能。这里面的散列函数，我们就可以这样设计：将单词中每个字母的ASCII码值"进位"相加，然后再跟散列表的大小求余、取模，作为散列值。\n\nhash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978\n\n\n1\n\n\n实际上，散列函数的设计方法还有很多，比如直接寻址法、平方取中法、折叠法、随机数法等。\n\n\n# 装载因子过大了怎么办？\n\n我们上一节讲到散列表的装载因子的时候说过，转载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。\n\n对于没有频繁插入和删除的静态数据集合来说，我们很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数，因为毕竟之前数据都是已知的。\n\n对于动态散列表来说，数据集合是频繁变动的，我们事先无法预估将要加入的数据个数，所以我们也无法事先申请一个足够大的散列表。随着数据慢慢加入，装载因子就会慢慢变大。当装载因子大到一定程度之后，散列冲突就会变得不可接受。这个时候，我们该如何处理呢？\n\n我们可以参照之前学习的"动态扩容"的知识点，我们在数组、栈、队列的时候是如何实现动态扩容的。\n\n针对散列表，当装载因子过大的时候，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了0.4。\n\n针对数据的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。\n\n可以看到图里这个例子。在原来的散列表中，21这个元素原来存储在下标Wie0的位置，搬移到新的散列表中，存储在下标为7的位置。\n\n\n\n对于支持动态扩容的散列表，插入操作的时间复杂度是多少呢？\n\n插入一个数据，最好情况下，不需要扩容，最好时间复杂度是O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是O(1).\n\n实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在转载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。\n\n我们前面讲到，当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。\n\n装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于1。\n\n\n# 如何避免低效的扩容？\n\n我们刚刚分析得到，大部分情况下，动态扩容的散列表插入一个数据都很快，但是在特殊情况下，当装载因子已经达到阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢，甚至会无法接受。\n\n这里有个极端的例子，如果散列表当前大小为1GB，要想扩容为原来的两倍大小，那就需要对1GB的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表，听起来就很耗时。\n\n如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，"一次性"扩容的机制就不合适了。\n\n为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。\n\n当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表中。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。\n\n\n\n这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。\n\n通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是O(1)。\n\n\n# 如何选择冲突解决方法？\n\n上一节中我们讲解了两种主要的散列冲突的解决办法，开放寻址法和链表法。这两种冲突解决办法在实际的软件开发中都非常常用。比如，Java中LinkedHashMap就采用了链表法解决冲突，ThreadLocalMap是通过线性探测的开放寻址法来解决冲突。\n\n\n# 开放寻址法\n\n开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用CPU缓存来加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没有那么容易。序列化是很常用的场景。\n\n开放寻址法又有哪些缺点呢？\n\n用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。\n\n我总结一下，当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。\n\n\n# 链表法\n\n首先，链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。\n\n链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于1的情况。接近1时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。\n\n我们在讲解链表的时候，提及。链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对CPU缓存是不友好的，这方面对于执行效率也有一定的影响。\n\n当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小(4个字节或8个字节)，那链表中指针的内存消耗在大对象面前就可以忽略了。\n\n实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。\n\n\n\n所以，我总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。\n\n\n# 工业级散列表举例分析\n\n现在，我们就以Java中的HashMap为例，来介绍工业级的散列表。\n\n\n# 初始大小\n\nHashMap默认的初始大小是16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高HashMap的性能。\n\n\n# 装载因子和动态扩容\n\n最大装载因子默认是0.75，当HashMap中元素个数超过0.75*capacity(capacity表示散列报的容量)的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。\n\n\n# 散列冲突解决办法\n\nHashMap底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。\n\n于是，在JDK1.8版本中，为了对HashMap做进一步优化，我们引入了红黑树。而当链表长度太长(默认超过8)时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高HashMap的性能。当红黑树结点个数少于8个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。\n\n\n# 散列函数\n\n散列函数的设计并不复杂，追求的是简单高效、分布均匀。如下所示：\n\nint hash(Object key) {\n    int h = key.hashCode()；\n    return (h ^ (h >>> 16)) & (capicity -1); //capicity表示散列表的大小\n}\n\n\n1\n2\n3\n4\n\n\n其中，hashCode()返回的是Java对象的hash code。比如String类型的对象的hashCode()就是下面：\n\npublic int hashCode() {\n  int var1 = this.hash;\n  if(var1 == 0 && this.value.length > 0) {\n    char[] var2 = this.value;\n    for(int var3 = 0; var3 < this.value.length; ++var3) {\n      var1 = 31 * var1 + var2[var3];\n    }\n    this.hash = var1;\n  }\n  return var1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 解答开篇\n\n现在，我们来分析一下开篇的问题：如何设计一个工业级的散列函数？如果这是一道面试题或摆在我们面前的实际开发问题，我们会从哪几个方面思考呢？\n\n首先，我们会思考，什么是一个工业级的散列表？工业级的散列表应该具有哪些特性？\n\n结合已经学习过的散列知识，我们觉得应该有这样几点要求：\n\n * 支持快速地查询、插入、删除操作；\n * 内存占用合理，不能浪费过多的内存空间；\n * 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。\n\n如何实现这样一个散列表呢？根据前面讲到的知识，我们会从这三个方面来考虑设计思路：\n\n * 设计一个合适的散列函数；\n * 定义装载因子阈值，并且设计动态扩容策略；\n * 选择合适的散列冲突解决方法。\n\n\n# 内容小结\n\n今天的内容偏向于实战。主要讲解了如何设计一个工业级的散列表，以及如何应对各种异常情况，防止在极端情况下，散列表的性能退化过于严重。我们分了三部分来讲解这些内容：分别是：如何设计散列函数，如何根据转载因子动态扩容，以及如何选择散列冲突解决方法。\n\n关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，这样会尽可能地减少散列冲突，即便冲突之后，分配到每个槽内的数据也比较均匀。除此之外，散列函数的设计也不能太复杂，太复杂就会太耗时间，也会影响散列表的性能。\n\n关于散列冲突解决方法的选择，我对比了开放寻址法和链表法两种方法的优劣和适应的场景。大部分情况下，链表法更加普适。而且，我们还可以通过将链表法中的链表改造成其他动态查找数据结构，比如红黑树，来避免散列表时间复杂度退化成O(n)，抵御散列碰撞攻击。但是，对于小规模数据、装载因子不高的散列表，比较适合用开放寻址法。\n\n对于动态散列表来说，不管我们如何设计散列函数，选择什么样的散列冲突解决方法。随着数据的不断增加，散列表总会出现装载因子过高的情况。这个时候，我们就需要启动动态扩容。\n\n\n# D47(2020/11/24)\n\n今天要学习的是散列表的第三篇，重点考虑散列表和链表的结合。\n\n在链表那一节，我们讲到如何用链表来实现LRU缓存淘汰算法，但是链表实现的LRU缓存淘汰算法的时间复杂度是O(n)，我们可以通过散列表将这个时间复杂度降低到O(1)。\n\n在跳表那一节，我们提到Redis的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表。当时我们也提到，Redis有序集合不仅使用了跳表，还用到了散列表。\n\n除此之外，我们会发现在Java编程语言中，LinkedHashMap这样一个常用的容器，也用到了散列表和链表两种数据结构。\n\n这里我们就来讨论，散列表和链表都是如何组合起来使用的，以及为什么散列表和链表会经常放到一块使用。\n\n\n# LRU缓存淘汰算法\n\n在链表那一节中，我们提到，借助于散列表，我们可以把LRU缓存淘汰算法的时间复杂度降低为O(1)。\n\nLRU也就是Least Recently Used，也就是最近最少使用，选择最近最久未使用的页面予以淘汰。\n\n下面是我们之前利用链表来实现LRU缓存淘汰算法的。\n\n我们需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据，我们就直接将链表头部的结点删除。\n\n当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放入到链表的头部；如果找到了，我们就把它移动到链表的头部。因为查找数据需要遍历链表，所以单纯用链表实现的LRU缓存淘汰算法的时间复杂度很高，是O(n)。\n\n实际上，我们总结一下，一个缓存(cache)系统主要包含下面这几个操作：\n\n * 往缓存中添加一个数据；\n * 从缓存中删除一个数据；\n * 在缓存中查找一个数据。\n\n这三个操作都涉及"查找"操作，如果单纯地采用链表的话，时间复杂度只能是O(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到O(1)。\n\n我们使用双向链表存储数据，链表中的每个结点处理存储数据(data)、前驱指针(prev)、后继指针(next)之外，还新增了一个特殊的字段hnext。这个hnext有什么用呢？\n\n因为我们的散列表示通过链表法来解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext指针是为了将结点串在散列表的拉链中。\n\n了解了这个散列表和双向链表的组合存储结构之后，我们再来看，前面讲到的缓存的三个操作，是如何做到时间复杂度是O(1)的？\n\n 1. 每个页面的hash后的值确定的 数组下标位置，后面用链表结点来存。\n 2. 不同的页面hash后可以有相同的值，相同的数组下标后面用，各个链表结点连接起来。其中各个结点连接起来的指针就是这里的hnext。\n 3. 各个散列表槽位中的各个链表结点之间，还有一个prev和next指针，这个是用来维护的是页面的访问的时间的。\n 4. 这个双向链表中，第一个结点是访问最早的时间。最后一个结点是访问最后的时间。\n 5. 要访问新页面的时候，先计算该页面的hash值，到对应的槽位（数组下标后面一串链表结点中），去找一找是否有已有的页面。\n 6. 如果没有找到，那么就在刚才找的数组下标后面的一串链表结点末尾增加这个结点，并且让双向链表的末尾结点指向它。\n 7. 如果这个时候，缓存要满了，那就先删除双向链表的一个结点，再去增加双向链表的末尾结点。\n 8. 如果找到了，那么就调整双向链表中这个结点的顺序，这个结点塞到双向链表的最后。\n\n首先，我们来看如何查找一个数据。我们前面讲过，散列表中查找数据的时间复杂度接近O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。\n\n其次，我们来看如何删除一个数据。我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在O(1)时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针O(1)时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要O(1)的时间复杂度。\n\n最后，我们来看如何添加一个数据。添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。\n\n这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在O(1)的时间复杂度内完成。所以，这三个操作的时间复杂度都是O(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持LRU缓存淘汰算法的缓存系统原型。\n\n\n# Redis有序集合\n\n在跳表那一节，讲到有序集合的操作时，我们稍稍做了些简化。实际上，在有序集合中，每个成员对象有两个重要的属性，key(键值)和score(分值)。我们不仅会通过score来查找数据，还会通过key来查找数据。\n\n举个例子，比如用户积分排行榜有这样一个功能：我们可以通过用户的ID来查找积分信息，也可以通过积分区间来查找用户ID或姓名信息。这里包含ID、姓名和积分的用户信息，就是成员对象，用户ID就是key，积分就是score。\n\n所以，如果我们细化一下Redis有序集合的操作，那就是下面这样：\n\n * 添加一个成员对象；\n * 按照键值来删除一个成员对象；\n * 按照键值来查找一个成员对象；\n * 按照分值区间来查找数据，比如积分在[100,356]之间的成员对象；\n * 按照分值从小到大排序成员变量；\n\n如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢，解决方法与LRU缓存淘汰算法的聚集方法类似。我们可以再按照键值构建一个散列表，这样按照key来删除、查找一个成员对象的时间复杂度就变成了O(1)。同时，借助跳表结构，其他操作也非常高效。\n\n实际上，Redis有序集合的操作还有另外一类，也就是查找成员对象的排名(Rank)或者根据排名区间查找成员对象。\n\n\n# Java LinkedHashMap\n\n前面我们学习了两个散列表和链表结合的例子，这里再看一个Java中的LinkedHashMap这种容器。\n\n在Java中的HashMap底层就是通过散列表这种数据结构实现的。而LinkedHashMap前面比HashMap多了一个 "Linked"。\n\n实际上，LinkedHashMap并没有这么简单，其中的"Linked"也并不仅仅代表它是通过链表法解决散列冲突的。\n\nLinedHashMap也是通过散列表和链表组合在一起实现的。实际上，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。\n\n\n# 按照插入的顺序打印\n\n看下面的一段hashmap的代码，我们依次插入(3,11)，(1,12), (5,23), (2,22)。接下里打印出来，发现打印出来的顺序就是3,1,5,2。\n\nHashMap<Integer, Integer> m = new LinkedHashMap<>();\nm.put(3, 11);\nm.put(1, 12);\nm.put(5, 23);\nm.put(2, 22);\n\nfor (Map.Entry e : m.entrySet()) {\n  System.out.println(e.getKey());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n这个问题，实际上是比较奇怪的，按照我们对HashMap的字面意思，散列表中的数据不是经过散列函数打乱之后无规则存储的么，这里怎么就按照数据的插入顺序来遍历打印了呢？\n\n实际上，我们已经可以猜测出LinkedHashMap也是通过散列表和链表组合在一起实现。\n\n实际上，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。\n\n\n# 按照访问顺序来遍历\n\n看如下的HashMap的代码\n\n// 10是初始大小，0.75是装载因子，true是表示按照访问时间排序\nHashMap<Integer, Integer> m = new LinkedHashMap<>(10, 0.75f, true);\nm.put(3, 11);\nm.put(1, 12);\nm.put(5, 23);\nm.put(2, 22);\n\nm.put(3, 26);\nm.get(5);\n\nfor (Map.Entry e![hash_table_10](D:\\tupian\\hash_table_10.jpg) : m.entrySet()) {\n  System.out.println(e.getKey());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n我们先是不断的put了3,1,5,2 四个 key/value对。然后再put 了 (3,26)，接着再get(5)。\n\n这段代码打印的结果是1，2，3，5。接下来我们具体分析一下，为什么这段代码会按照这样的顺序来打印。\n\n每次调用put()函数，往LinkedHashMap中添加数据的时候，都会将数据添加到链表的尾部，所以，在前四个从操作完成之后，链表中的数据是下面这样：\n\n\n\n在第8行代码中，再次将键值为3的数据放入到LinkedHashMap的时候，会先查找这个键值是否已经有了，然后，再将已经存在的(3,11)删除，并且将新的(3,26)放到链表的尾部。所以，这个时候链表中的数据就是下面这样：\n\n\n\n当第9行代码访问到key为5的数据的时候，我们将被访问的数据移动到链表的尾部。所以，第8行代码之后，链表中的数据是下面这样：\n\n\n\n所以，最后打印出来的数据是1,2,3,5。从上面的分析，我可以返现，按照访问时间排序的LinkedHashMap本身就是一个支持LRU缓存淘汰策略的缓存系统。\n\n现在总结一下，实际上，LinkedHashMap是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap中的"Linked"实际上是指的是双向链表，并非指用链表法解决散列冲突。\n\n\n# 解答开篇&内容小结\n\n为什么散列表和链表经常一块使用呢？\n\n散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就是说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。\n\n因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按照顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表(或者跳表)结合在一起使用。\n\n用链表或跳表的数据结构，来实现对散列表中的各个数据有序的排序。\n\n\n# D48(2020/11/25)\n\n今天我们要学习的哈希算法的第一篇。"如何防止数据库中的用户信息被脱库？"\n\n\n# 什么是哈希算法？\n\n哈希算法的定义和原理非常简单，将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。\n\n但是，要想设计一个优秀的哈希算法并不容易，可以参照如下几点要求：\n\n * 从哈希值不能反向推导出原始数据(所以哈希算法也叫单向哈希算法)；\n * 对输入数据非常敏感，哪怕原始数据只修改了一个bit，最后得到的哈希值也大不相同；\n * 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；\n * 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。\n\n这里拿MD5这种哈希算法来具体说明一下。\n\n我们分别对"今天我来讲哈希算法"和"jiajia"这两个文本，计算MD5哈希值，得到两串看起来毫无规律的字符串(MD5的哈希值是128位的Bit长度，为了方便表示，我们把它们转化成了16进制编码)。可以看出来，无论要哈希的文本有多长、多短，通过MD5哈希之后，得到的哈希值的长度都是相同的，而且得到的哈希值看起来像一堆随机数，完全没有规律。\n\n我们再来看两个非常相似的文本，"我今天讲哈希算法!"和"我今天讲哈希算法"。这两个文本只有一个感叹号的区别。如果用MD5的哈希算法分别计算它们的哈希值，我们会发现，尽管只有一字之差，得到的哈希值也是完全不同的。\n\n我们在前面也讲解过，通过哈希算法得到的哈希值，很难反向推导出原始数据。比如上面的例子中，我们很难通过哈希值反推出对应的文本。\n\n哈希算法要处理的文本可能是各种各样的。比如，对于非常长的文本，如果哈希算法的计算时间很长，那就只能停留在理论研究的层面，很难应用到实际的软件开发中。\n\n哈希算法的应用非常多，最常见的有七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。\n\n\n# 应用一：安全加密\n\n说到哈希算法的应用，最先想到的应该就是安全加密。最常用于加密的哈希算法是MD5和SHA。\n\n除了这两个之外，当然还有很多其他加密算法，比如DES、AES。\n\n前面讲解的哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。\n\n第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。所以我们着重看下第二点。实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。\n\n这里就基于组合数学中一个非常基础的理论，鸽巢原理(也叫抽屉原理)。这个原理本身很简单，它是说，如果有10个鸽巢，有11个鸽子，那肯定有1个鸽巢中的鸽子数量多于1个，换句话说就是，肯定有2只鸽子在1个鸽巢内。\n\n有了鸽巢原理的铺垫之后，我们再来看，为什么哈希算法无法做到零冲突？\n\n我们知道，哈希算法产生的哈希值的长度是固定且有限的。比如上面举的MD5的例子，哈希值是固定的128位二进制串，能表示的数据是有限的，最多能表示2^128个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对2^128+1个数据求哈希值，就必须会存在哈希值相同的情况。这里我们应该可以想到，一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。\n\n不过，即便哈希算法存在散列冲突的情况，但是因为哈希值的范围很大，冲突的概率极低，所以相对来说还是很难破解的。像MD5，有2^128个不同的哈希值，这个数据已经是一个天文数字。所以，即便哈希算法存在冲突，但是在有限的时间和资源下，哈希算还是很难被破解的。\n\n除此之外，没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长。比如SHA-256比SHA-1要更复杂、更安全，相应的计算时间就会比较长。密码学界也一致致力于找到一种快速并且很难被破解的哈希算法。我们在实际的开发过程中，也需要权衡破解难度和计算时间，来决定究竟使用哪种加密算法。\n\n\n# 应用二：唯一标识\n\n我来先举一个例子。如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息(比如图片名称)来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那我们该如何搜索呢？\n\n我们知道，任何文件在计算中都可以表示成二进制码串，所以，比较笨的方法就是，拿要查找的图片的二进制码串与图库中的所有图片的二进制码串一一比对。如果相同，则说明图片在图库中存在。但是，每个图片小则几十KB、大则几MB，转化成二进制是一个非常长的串，比对起来非常耗时。有没有比较快的方法呢？\n\n我们可以给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取100个字节，从中间取100个字节，从最后再取100个字节，然后将这300个字节放到一块，通过哈希算法(比如MD5)，得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。\n\n如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。\n\n如果不存在，那就说明这个图片不再图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。\n\n\n# 应用三：数据校验\n\n电驴这样的BT下载软件的原理是基于P2P协议的。我们从多个机器上并行下载一个2GB的电影，这个电影文件可能会被分割成很多文件块(比如可用分成100块，每块大约20MB)。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。\n\n我们知道，网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或是下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？\n\n具体的BT协议很复杂，校验方法也有很多，下面是其中的一种思路。\n\n我们通过哈希算法，对100个文件块分别去哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的额文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或被篡改了，需要再重新从其他宿主机上下载该文件块。\n\n\n# 应用四：散列函数\n\n散列函数也是哈希算法的一种应用。\n\n前面提及到，散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法来解决。\n\n不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关系。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。\n\n\n# 内容小结\n\n今天主要讲解了哈希算法的四个应用场景。\n\n第一个应用是唯一标识，哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。\n\n第二个应用是用于校验数据的完整性和正确性。\n\n第三个应用是安全加密，我们讲到任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。\n\n第四个应用是散列函数，在散列表中，对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率。\n\n\n# D49(2020/11/26)\n\n上一节，我们讲了哈希算法的四个应用，它们分别是：安全加密、数据校验、唯一标识、散列函数。今天，我们再来看剩余三种应用：负载均衡、数据分片、分布式存储。\n\n这三个应用都跟分布式系统有关。今天就是来研究学习下，哈希算法是如何解决这些分布式问题的。\n\n\n# 应用五：负载均衡\n\n我们知道，负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞(session sticky)的负载均衡算法呢？也就是说，我们需要在同一个客户端上，上一次会话中的所有请求都路由到同一个服务器上。\n\n最直接的方法就是，维护一张映射关系表，这张表的内容是客户端IP地址或会话ID与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：\n\n * 如果客户端很多，映射表可能会很大，比较浪费内存空间；\n * 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大。\n\n如果借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端IP地址或会话ID计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。这样，我们就可以把同一个IP过来的所有请求，都路由到同一个后端服务器上。\n\n\n# 应用六：数据分片\n\n哈希算法还可以用于数据的分片。这里有两个例子。\n\n\n# 如何统计"搜索关键词"出现的次数\n\n假如我们有1T的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？\n\n我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。\n\n针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用n台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号。\n\n这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。\n\n实际上，这里的处理过程也是MapReduce的基本设计思想。\n\n\n# 如何快速判断图片是否在图库中\n\n如何快速判断图片是否在图库中？上一节我们讲过这个例子，当时我们介绍了一种方法，就是给每个图片取唯一标识(或者信息摘要)，然后构建散列表。\n\n假设现在我们的图库中有1亿图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存是有限的，而1亿张图片构建散列表显然远远超过了单台机器的内存上限。\n\n我们同样可以对数据进行分片，然后采用多机处理。我们准备n台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数n 求余取模，得到的值就是要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。\n\n当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数n求余取模。假设得到的值是k，那就去编号k的机器构建的散列表中查找。\n\n现在，我们来估算一下，给这个1亿图片构建散列表大约需要多少台机器。\n\n散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设我们通过MD5来计算哈希值，那长度就是128比特，也就是16字节。文件路径长度的上限是256字节，我们可以假设平均长度是128字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用8字节。所以，散列表中每个数据单元就占用 152字节。\n\n假设一台机器的内存大小为2GB，散列表的装载因子为0.75，那一台机器可以给大约1000万(2GB*0.75/152)张图片构建散列表。所以，如果要对1亿张图片构建索引，需要大约十几台机器。在工程中，这种估算还是很重要的，能让我们实现对需要投入的资源、资金有个大概的了解，能更好地平均解决方案的可行性。\n\n实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU等资源的限制。\n\n\n# 应用七：分布式存储\n\n现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。\n\n该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。\n\n但是，如果数据增多，原来的10个机器已经无法承受了，我们就需要扩容了，比如扩到11个机器，这个时候麻烦就来了。因为，这里并不是简单地加个机器就可以了。\n\n原来的数据是通过与10来取模的。比如13这个数据，存储在编号为3这台机器上。但是新加了一台机器中，我们对数据按照11取模，原来13这个数据就被分配到2号这台机器上了。\n\n\n\n因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。\n\n所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要可以用到了。\n\n假设我们有k个机器，数据的哈希值的范围是[0,MAX]。我们将整个范围划分成m个小区间(m远大于k)，每个机器负责m/k个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。\n\n一致性哈希算法的基本思想就是这样的。除此之外，它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来。\n\n\n# 解答开篇&内容小结\n\n今天我们讲解了三种哈希算法在分布式系统中的应用，它们分别是：复杂均衡、数据分片、分布式存储。\n\n在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。\n\n\n# D50(2020/11/28)(二叉树)\n\n今天学习二叉树基础的第一篇。\n\n前面我们讲的都是线性表结构，栈、队列等等。今天我们讲一种非线性表结构，树。树这种数据结构比线性表的数据结构要复杂得多，内容也比较多，会分四节来讲解。\n\n第一节，我们将讲解树和二叉树；第二节，我们将讲解二叉查找树；第三节，平衡二叉查找树、红黑树；第四节，递归树。\n\n今天提出的问题是：二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？\n\n\n# 树(Tree)\n\n下面图示中的是一些树，观察这些树都有什么特征？\n\n\n\n树这种数据结构很像我们现实生活中的"树"，这里面每个元素我们叫做"节点"；用来连接相邻节点之间的关系，我们叫做"父子关系"。\n\n在下面的这幅画中，A节点就是B节点的父节点，B节点是A节点的子节点。B、C、D这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫做根节点，也就是图中的节点E。我们把没有子节点的节点叫做叶子节点或叶节点，比如图中的G、H、I、J、K、L都是叶子节点。\n\n\n\n除此之外，关于"树"，还有三个比较相似的概念：高度(Height)、深度(Depth)、层(Level)。它们的定义是这样的：\n\n\n\n这三个概念的定义比较容易混淆，描述起来也比较空洞。从下图的图示中，来观察这些概念的定义。\n\n\n\n记住这几个概念，这里还有一个小窍门，就是类比"高度"、"深度"、"层"这几个名词在生活中的含义。\n\n在我们的生活中，"高度"这个概念，其实就是从下往上度量，比如我们要度量第10层楼的高度、第13层楼的高度，起点就是地面。所以，树这种数据结构的高度也是一样，从最底层开始计数，并且计数的起点是0。\n\n"深度"这个概念在生活中是从上往下度量的，比如水中鱼的深度，是从水平面开始度量的。所以，树这种数据结构的速度也是类似的，从根结点开始度量，并且计数起点也是0。\n\n"层数"跟深度的计算类似，不过，计数起点是1，也就是说根节点位于第1层。\n\n\n# 二叉树\n\n树结构多种多样，我们最常用的还是二叉树。\n\n二叉树，顾名思义，每个节点最多有两个"叉"，也就是两个子结节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。下面画的都是几个都是二叉树。\n\n\n\n这个图里面，有两个比较特殊的二叉树，分别是编号2和编号3这两个。\n\n其中，编号2的二叉树中，叶子节点全部在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做满二叉树。\n\n编号3的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做完全二叉树。\n\n满二叉树很好理解，也很好识别，但是完全二叉树，有的人就可能分不清了。下面图示中有几个完全二叉树和非完全二叉树的例子，可以对比看。\n\n\n\n你可能会说，满二叉树的特征非常明显，我们把它单独拎出来将，这个可以理解。但是完全二叉树的特征不怎么明显，单从长相上来看，完全二叉树并没有特别特殊的地方啊，更像是"芸芸众树"中的一种。\n\n那我们为什么还要特意把它拎出来讲呢？为什么偏偏把最后一层的叶子节点靠左排列的叫完全二叉树？如果靠右排序就不能叫完全二叉树了吗？这个定义的由来或说目的在哪里？\n\n要理解完全二叉树定义的由来，我们需要先了解，如何表示(或者存储)一棵二叉树？\n\n想要存储一棵二叉树，我们有两种方法，一种是基于指针或引用的二叉链式存储法，一种是基于数组的顺序存储法。\n\n我们先来看比较简单、直观的链式存储法。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。\n\n\n\n我们再来看，基于数组的顺序存储法。我们把根节点存储在下标i=1的位置，那左子节点存储在下标2*i =2的位置，右子节点存储在2*i +1=3的位置。以此类推，B节点的左子节点存储在2* i = 2 * 2 =4的位置，右子节点存储子啊2 * i + 1 = 2 * 2 +1 = 5的位置。\n\n\n\n我们来总结一下，如果节点X存储在数组中下标为i的位置，下标为2 * i的位置存储的就是左子节点，下标为2 * i +1 的位置存储的就是右子节点。反过来，下标为i/2的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置(一般情况下，为了方便计算子节点，根节点会存储在下标为1的位置)，这样就可以通过下标计算，把整棵树都串起来。\n\n上面的一棵完全二叉树的例子中，仅仅"浪费"了一个下标为0的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。可以看下面的例子。\n\n\n\n所以，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。\n\n当我们讲到堆和堆排序的时候，就会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。\n\n\n# 二叉树的遍历\n\n前面讲解了二叉树的基本定义和存储方法，现在来看二叉树中非常重要的操作，二叉树的遍历。\n\n如何将所有节点都遍历打印出来呢？经典的方法有三种，前序遍历、中序遍历和后序遍历。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。\n\n * 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。\n * 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。\n * 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。\n\n\n\n实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。\n\n写递归代码的关键，就是看能不能写出递推公式，而写递推公式的关键就是，如果要解决问题A，就假设子问题B、C已经解决，然后再来看如何利用B、C来解决A。所以，我们可以把前、中、后序遍历的递推公式都写出来。\n\n前序遍历的递推公式：\npreOrder(r) = print r->preOrder(r->left)->preOrder(r->right)\n\n中序遍历的递推公式：\ninOrder(r) = inOrder(r->left)->print r->inOrder(r->right)\n\n后序遍历的递推公式：\npostOrder(r) = postOrder(r->left)->postOrder(r->right)->print r\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n有了递推公式，代码就可以写了。这三种遍历方式的代码，可以见如下的。\n\nvoid preOrder(Node* root) {\n  if (root == null) return;\n  print root // 此处为伪代码，表示打印root节点\n  preOrder(root->left);\n  preOrder(root->right);\n}\n\nvoid inOrder(Node* root) {\n  if (root == null) return;\n  inOrder(root->left);\n  print root // 此处为伪代码，表示打印root节点\n  inOrder(root->right);\n}\n\nvoid postOrder(Node* root) {\n  if (root == null) return;\n  postOrder(root->left);\n  postOrder(root->right);\n  print root // 此处为伪代码，表示打印root节点\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n二叉树的前、中、后序遍历的递归实现如上，二叉树遍历的时间复杂度是多少？\n\n从前面画的前、中、后序遍历的顺序图，可以看出来，每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数n成正比，也就是说二叉树遍历的时间复杂度是O(n)。\n\n\n# 解答开篇&内容小结\n\n今天，我们讲解了一种非线性表数据结构，树。关于苏，有几个比较常用的概念需要掌握，那就是：根节点、叶子节点、父节点、子节点、兄弟节点，还有节点的高度、深度、层数，以及树的高度。\n\n我们平时最常用的树就是二叉树。二叉树的每个节点最多有两个子结点，分别是左子节点和右子节点。二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。满二叉树又是完全二叉树的一种特殊情况。\n\n二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。除此之外，二叉树里非常重要的操作就是前、中、后序遍历操作，遍历的时间复杂度是O(n)，我们需要理解并能用递归代码来实现。\n\n\n# D51(2020/11/29) (二叉查找树)\n\n上一节我们学习了树、二叉树以及二叉树的遍历，今天我们再来学习一种特殊的二叉树，二叉查找树。二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。\n\n我们之前说过，散列表也是支持这些操作的，并且散列表的这些操作比二叉查找树更高效，时间复杂度是O(1)。既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？\n\n\n# 二叉查找树\n\n二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。\n\n这些都依赖于二叉查找树的特殊结构。二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。\n\n\n\n下面来看下二叉查找树的快速查找、插入、删除操作是如何实现的。\n\n\n# 二叉查找树的查找操作\n\n首先，我们看如何在二叉查找树中查找一个节点。我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。\n\n\n\n查看下如下的代码。\n\npublic class BinarySearchTree {\n  private Node tree;\n\n  public Node find(int data) {\n    Node p = tree;\n    while (p != null) {\n      if (data < p.data) p = p.left;\n      else if (data > p.data) p = p.right;\n      else return p;\n    }\n    return null;\n  }\n\n  public static class Node {\n    private int data;\n    private Node left;\n    private Node right;\n\n    public Node(int data) {\n      this.data = data;\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 二叉查找树的插入操作\n\n二叉查找树的插入过程有点类似查找操作。新插入的数据一般是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。\n\n如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。\n\n\n\n具体的代码如下：\n\npublic void insert(int data) {\n  if (tree == null) {\n    tree = new Node(data);\n    return;\n  }\n\n  Node p = tree;\n  while (p != null) {\n    if (data > p.data) {\n      if (p.right == null) {\n        p.right = new Node(data);\n        return;\n      }\n      p = p.right;\n    } else { // data < p.data\n      if (p.left == null) {\n        p.left = new Node(data);\n        return;\n      }\n      p = p.left;\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 二叉查找树的删除操作\n\n二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了。针对要删除的节点的子节点个数的不同，我们需要分三种情况来处理。\n\n第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除的节点的指针置为null。比如图中的删除节点55。\n\n第二种情况是，如果要删除的节点只有一个子节点(只有左子节点或右子节点)，我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点13。\n\n第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除这个最小节点，因为最小节点肯定没有左子节点(如果有左子节点，那就不是最小节点了)，所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点18。\n\n\n\n如下面的删除代码的示例：\n\npublic void delete(int data) {\n  Node p = tree; // p指向要删除的节点，初始化指向根节点\n  Node pp = null; // pp记录的是p的父节点\n  while (p != null && p.data != data) {\n    pp = p;\n    if (data > p.data) p = p.right;\n    else p = p.left;\n  }\n  if (p == null) return; // 没有找到\n\n  // 要删除的节点有两个子节点\n  if (p.left != null && p.right != null) { // 查找右子树中最小节点\n    Node minP = p.right;\n    Node minPP = p; // minPP表示minP的父节点\n    while (minP.left != null) {\n      minPP = minP;\n      minP = minP.left;\n    }\n    p.data = minP.data; // 将minP的数据替换到p中\n    p = minP; // 下面就变成了删除minP了\n    pp = minPP;\n  }\n\n  // 删除节点是叶子节点或者仅有一个子节点\n  Node child; // p的子节点\n  if (p.left != null) child = p.left;\n  else if (p.right != null) child = p.right;\n  else child = null;\n\n  if (pp == null) tree = child; // 删除的是根节点\n  else if (pp.left == p) pp.left = child;\n  else pp.right = child;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n实际上，关于二叉查找树的删除操作，还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为"已删除"，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。而且，这种处理方法也并没有增加插入、查找操作代码实现的难度。\n\n\n# 二叉查找树的其他操作\n\n除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。\n\n二叉查找树除了支持上面几个操作之外，还有一个重要的特性，就是中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是O(n)，非常高效。因此，二叉查找树也叫做二叉排序树。\n\n\n# 支持重复数据的二叉查找树\n\n前面讲二叉查找树的时候，我们默认树中节点存储的都是数字。很多时候，在实际的软件开发中，我们在二叉查找树中存储的，是一个包含很多字段的对象。我们利用对象的某个字段作为键值(key)来构建二叉查找树。我们把对象中的其他字段叫做卫星数据。\n\n前面我们讲的二叉查找树的操作，针对的都是不存在键值相同的情况。那如果存储的两个对象键值相同，这种情况该怎么处理呢？这里有两种解决方法。\n\n第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因为我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。\n\n第二种方法比较不好理解，不过更加优雅。\n\n每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。\n\n\n\n当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。\n\n\n\n对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。\n\n\n\n\n# 二叉查找树的时间复杂度分析\n\n上面介绍了二叉查找树的常用操作的实现方式。现在，我们来分析一下，二叉查找树的插入、删除、查找操作的时间复杂度。\n\n实际上，二叉查找树的形态各式各样。比如下面图中，对于同一组数据，我们构造了三种二叉查找树。它们的查找、插入、删除操作的执行效率都不一样的。图中第一种二叉查找树，根节点的左右子树极度不平衡，已经退化成了链表，所以查找的时间复杂度就变成了O(n)。\n\n\n\n我们刚才其实分析了一种最糟糕的情况，我们现在来分析一个最理想的情况，二叉查找树是一棵完全二叉树(或满二叉树)。这个时候，插入、删除、查找的时间复杂度是多少呢？\n\n从前面的例子、图，还有代码来看，不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是O(height)。既然这样，现在问题就转变成另外一个了，也就是，如何求一棵包含n个节点的完全二叉树的高度？\n\n树的高度就等于最大层数减1，为了方便计算，我们转换成层来表示。从图中可以看出，包含n个节点的完全二叉树中，第一层包含1个节点，第二层中包含2个节点，第三层中包含4个节点，以此类推，下面一层节点个数是上一层的2倍，第k层包含的节点个数就是2^(k-1)。\n\n不过，对于完全二叉树来说，最后一层的节点个数有点不遵守上面的规律了。它包含的节点个数在1个到2^(L-1)个之间(我们假设最大层数是L)。如果我们把每一层的节点个数加起来就是总的节点个数n。也就是说，如果节点的个数是n，那么n满足这样一个关系：\n\nn >= 1+2+4+8+...+2^(L-2)+1\nn <= 1+2+4+8+...+2^(L-2)+2^(L-1)\n\n\n1\n2\n\n\n借助等比数列的求和公式，我们可以计算出，L的范围是[$log_2 (n+1)$, $log_2n +1$]。完全二叉树的层数小于等于$log_2 n +1$，也就是说，完全二叉树的高度小于等于$log_2 n$。\n\n显然，极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。我们需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树，这就是我们下一节可要详细讲的，一种特殊的二叉查找树，平衡二叉查找树。平衡二叉查找树的高度接近logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是O(logn)。\n\n\n# 解答开篇\n\n我们在散列表那节讲过，散列表的插入、删除、查找操作的时间复杂度可以做到常量级的O(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是O(logn)，相对散列表，好像并没有什么优势，那我们为什么还要使用二叉查找树呢？\n\n我认为有下面几个原因：\n\n第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在O(n)的时间复杂度内，输出有序的数据序列。\n\n第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)。\n\n第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比logn小，所以实际的查找速度可能不一定比O(logn)快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。\n\n第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。\n\n最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。\n\n综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。\n\n\n# 内容小结\n\n今天学习了一种特殊的二叉树，二叉查找树。它支持快速地查找、插入、删除操作。\n\n二叉查找树中，每个节点的值都大于左子树节点的值，小于右子树节点的值。不过，这只是针对没有重复数据的情况。对于存在重复数据的二叉查找树，这里介绍了两种构建方法，一种是让每个节点存储多个值相同的数据；另一种是，每个节点中存储一个数据。针对这种情况，我们只需要稍加改造原来的插入、删除、查找操作即可。\n\n在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是O(n)和O(logn)，分别对应二叉树退化成链表的情况和完全二叉树。\n\n为了避免时间复杂度的退化，针对二叉查找树，我们又涉及了一种更加复杂的树，平衡二叉查找树，时间复杂度可以做到稳定的O(logn)。\n\n\n# D52(2020/12/01) 红黑树\n\n今天主要学习的是红黑树的第一篇。\n\n上面两节，我们依次讲了树、二叉树、二叉查找树。二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是O(logn)。\n\n不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于$log_2n$的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到O(n)。上一节所说，要解决这个复杂度退化的问题，我们需要设计一种平衡二叉查找树，也就是今天要讲的这种数据结构。\n\n很多书籍中，但凡讲到平衡二叉查找树，就会拿红黑树作为例子。我们在工程中，很多用到平衡二叉查找树的地方都会用红黑树。为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？\n\n\n# 什么是"平衡二叉查找树"\n\n平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于1。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。\n\n\n\n平衡二叉查找树不仅满足上面平衡二叉树的定义，还满足二叉查找树的特点。最先被发明的平衡二叉查找树是AVL树，它严格符合我们刚讲的平衡二叉查找树的定义，即任何节点的左右子树高度相差不超过1，是一种高度平衡的二叉查找树。\n\n但是很多平衡二叉查找树其实并没有严格符合上面的定义(树中任意一个节点的左右子树的高度相差不能大于1)，比如我们下面要讲的红黑树，它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。\n\n我们学习数据结构和算法是为了应用到实际的开发中的，所以，我觉得没有必要去死抠定义。对于平衡二叉查找树这个概念，我觉得我们要从这个数据结构的由来，去理解"平衡"的意思。\n\n发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。\n\n所以，平衡二叉查找树中"平衡"的意思，其实就是让整棵树左右看起来比较"对称"、比较"平衡"，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。\n\n所以，如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比$log_2 n$大很多(比如树的高度仍然是对数量级的)，尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。\n\n\n# 如何定义一棵"红黑树"\n\n平衡二叉查找树其实有很多，比如，Splay Tree(伸展树)、Treap(树堆)等，但是我们提到平衡二叉查找树，听到的基本都是红黑树。它的出镜率甚至要高于"平衡二叉查找树"这几个字，有时候，我们甚至默认平衡二叉查找树就是红黑树。\n\n红黑树，简称为R-B Tree。它是一种不严格的平衡二叉查找树，前面也提及到了，它的定义是不严格符合平衡二叉查找树的定义的。\n\n顾名思义，红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：\n\n * 根节点是黑色的；\n * 每个叶子节点都是黑色的空节点(NIL)，也就说，叶子节点不存储数据；\n * 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；\n * 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；\n\n这里的第二条要求"叶子节点都是黑色的空节点"，稍微有些奇怪，它主要是为了简化红黑树的代码实现而设置的，下一节我们讲红黑树的实现的时候会江街道。这节我们暂时不考虑这一点，所以，在画图和讲解的时候，我将黑色的、空的叶子节点都省略掉了。\n\n根据上面的定义，画出了两个红黑树的图例，可以参照对照来看。\n\n\n\n\n# 为什么说红黑树是"近似平衡"的？\n\n我们前面也提到，平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，"平衡"的意思可以等价为性能不退化。"近似平衡"就等价为性能不会退化得太严重。\n\n上一节讲解过，二叉查找树很多操作的性能都跟树的高度成正比。一棵极其平衡的二叉树(满二叉树或完全二叉树)的高度大约是$log_2 n$ ，所以如果要证明红黑树是近似平衡的，我们只需要分析，红黑树的高度是否比较稳定地趋近$log_2 n$就好了。\n\n红黑树的高度不是很好分析。\n\n首先，我们来看，如果我们将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？\n\n红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点(父节点的父节点)作为父节点。所以，之前的二叉树就变成了四叉树。\n\n\n\n前面红黑树的定义里有这么一条：从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。\n\n上一节我们提及，完全二叉树的高度近似$log_2 n$，这里的四叉"黑树"的高度要低于完全二叉树，所以去掉红色节点的"黑树"的高度也不会超过$log_2 n$。\n\n我们现在知道只包含黑色节点的"黑树"的高度，那我们现在把红色节点加回去，高度会变成多少呢？\n\n从上面我们画的红黑树的例子和定义看，在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过$log_2n$ ,所以加入红色节点之后，最长路径不会超过2$log_2 n $，也就是说，红黑树的高度近似2$log_2 n $。\n\n所以，红黑树的高度只比高度平衡的AVL树的高度($log_2 n$)仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。\n\n\n# 解答开篇\n\n我们刚刚提到了很多平衡二叉查找树，现在我们来看下，为什么在工程中大家都喜欢用红黑树这种平衡二叉查找树。\n\n前面提到的Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单词操作时间非常敏感的场景来说，它们并不适用。\n\nAVL树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利也有弊，AVL树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用AVL树的代价就有点高了。\n\n红黑树只是做了近似平衡，并不严格的平衡，所以在维护平衡的成本上，要比AVL树要低。\n\n所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。\n\n\n# 内容小结\n\n红黑树很难，的确，它算是最难掌握的一种数据结构。其实红黑树最难的地方是它的实现，今天还没有涉及。\n\n不过呢，我们认为，其实我们不应该把学习的侧重点，放到它的实现上。红黑树，究竟要掌握哪些东西呢？\n\n我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。对于红黑树，也不例外。如果我们能搞懂这几个问题，其实就已经足够了。\n\n红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似$log_2 n$，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是O($logn$)。\n\n因为红黑树是一种性能稳定的二叉查找树，所以，在工程上，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现其阿里比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向于用跳表来替代它。\n\n\n# 数据结构整理\n\n 1. 数组\n    \n    定义：连续的内存空间，支持按下标随机访问O(1)，插入和删除的时候，可能会涉及到数据的搬移，时间复杂度是O(n)。\n    \n    适用场景：数据规模较小，不经常变动的场景。\n    \n    缺点：对于内存连续性要求高，插入删除操作效率低。\n\n 2. 链表\n    \n    定义：查询效率不高O(n)，插入和删除效率O(1)，并且内存申请可以不连续。\n    \n    适用场景：插入和删除多于查询操作。顺序访问数据，数据维护比较频繁的场景。\n    \n    缺点：随机查找效率低，实际上删除之前先要查找，所以实际删除效率也不高。\n\n 3. 散列表\n    \n    定义：利用数组和链表两个基本数据结构设计了一个高效的动态数据结构。利用了数组的随机访问特性，用于满足根据某个属性来随机访问元素。基于key查找效率很高 O(1)。同时借助链表进行散列冲突解决的方法，删除和插入操作效率也可以接近O(1)。\n    \n    适用场景：海量数据随机访问、防止重复、缓存等。\n    \n    缺点：需要设计合理的散列函数，并且要考虑散列冲突和动态扩容。\n\n 4. 跳表\n    \n    定义：尽管散列表效率很高，但是散列表是无序的，跳表效率和散列表类似，并且支持区间序列的输出(因为基于链表)。\n    \n    适用场景：对有序元素的快速查找、插入和删除\n    \n    缺点：比较占用内存。\n\n 5. 红黑树\n    \n    定义：红黑树是平衡二叉查找树的一种近似实现。红黑树和跳表类似，但是实现方式有所差异。红黑树存在的价值是，它可以实现比较高效的查找，删除和插入。虽然相比高度平衡的AVL树效率有所下降，但是红黑树不用耗费太多精力维护平衡。相比跳表，红黑树除了内存占用比较小，其他性能并不比跳表更优。但由于历史的原因，红黑树使用的更加广泛。\n    \n    缺点：实现比较复杂。\n\n\n# D53(2020/12/03) 递归树\n\n今天学习的主题的内容是，如何借助于树来求解递归算法的时间复杂度。\n\n我们都知道，递归代码的时间复杂度分析起来很麻烦。我们在第12节讲述过，如何利用递归公式，来求解归并排序、快速排序的时间复杂度，但是，有些情况，比如快排的平均时间复杂度的分析，用递推公式的话，会涉及非常复杂的数学推导。\n\n除了用递推公式这种比较复杂的分析方法，有没有更简单的方法呢？今天，我们就来学习另外一种方法，借助递归树来分析递归算法的时间复杂度。\n\n\n# 递归树与时间复杂度分析\n\n我们前面讲解过，递归的思想就是，将大问题分解为小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止。\n\n如果我们把这个一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫做递归树。我们这里画了一棵斐波那契数列的递归树。节点里的数字表示数据的规模，一个节点的求解可以分解为左右子节点两个问题的求解。\n\n> 斐波那契数列中，这个数列从第3项开始，每一项都等于前两项之和。$a_n$ = $a_{n-1}$ + $a_{n-2}$\n> \n> 0,1,1,2,3,5,8,13 ....\n\n\n\n通过这个例子，我们可以对递归树的样子应该有个感性的认识了，看其阿里并不复杂。现在，我们就来看，如何用递归树来求解时间复杂度。\n\n归并排序每次会将数据规模一分为二，我们把归并排序画成递归树。\n\n\n\n因为每次分解都是一分为二，所以代价很低，我们把时间上的消耗记作常量1。归并算法中比较耗时的是归并排序，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记住n。\n\n现在，我们只需要知道这棵树的高度h，用高度h乘以每一层的时间消耗n，就可以得到总的时间复杂度O(n*h)。\n\n从归并排序的原理和递归树，可以看出来，归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是$log_2 n$，所以归并排序递归实现的时间复杂度就是O(n$logn$)。这里的时间复杂度都是估算的，对树的高度的计算也没那么精确，但是这并不影响复杂度的计算结果。\n\n利用递归树的时间复杂度分析方法并不难理解，关键还是在实战，所以，接下来我会通过三个实际的递归算法，来实战理解一下递归的复杂度分析。学习完这节课之后，我们才能真正掌握递归代码的复杂度分析。\n\n\n# 实战一： 分析快速排序的时间复杂度\n\n在用递归树推导之前，我们先来回忆一下用递推公式的分析方法。我们可以回想一下，当时，我们为什么说用递推公式来求解平均时间复杂度非常复杂？\n\n快速排序在最好的情况下，每次分区都能一分为二，这个时候用递推公式T(n) = 2T(n/2) +n，很容易就推导出时间复杂度是O(nlogn)。但是，我们并不可能每次分区都这么幸运，正好一分为二。\n\n我们假设平均情况下，每次分区之后，两个分区的大小比例是1:k。当k=9时，如果用递推公式的方法来求解时间复杂度的话，递推公式就写成T(n) = T(n/10) + T(9n/10) + n。\n\n这个公式可以推导出时间复杂度，但是推导过程非常复杂。如果我们用递归树来分析快速排序的平均情况时间复杂度，是不是比较简单。\n\n我们还是取k等于9，也就是说，每次分区都很大平均，一个分区是另一个分区的9倍。如果我们把递归分解的过程画成递归树，就是下面的样子：\n\n\n\n快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是n。我们现在只要求出递归树的高度h，这个快排过程遍历的数据个数就是h*n，也就说，时间复杂度是O(h*n) 。\n\n因为每次分区并不是均匀地一分为二，所以递归树并不是满二叉树。这样一个递归树的高度是多少呢？\n\n我们知道，快速排序结束的条件就是待排序的小区间，大小为1，也就说叶子节点里的数据规模是1。从根节点n到叶子节点1，递归树中最短的一个路径每次都乘以1/10，最长的一个路径每次都乘以9/10。通过计算，我们可以得到，从根节点到叶子节点的最短路径是$log_{10} n$ ,最长的路径是$log{10/9} n$\n\n\n\n所以，遍历数据的个数总和就介于n$log_{10} n$ 和n $log{10/9} n$ 之间。根据复杂度的大O表示法，对数复杂度的底数不管是多少，我们统一写成logn，所以，当分区大小比例是1:9的时候，快速排序的时间复杂度仍然是O(nlogn)。\n\n刚刚我们假设k=9，那如果k=99，也就是说，每次分区及其不平均，两个区间大小是1:99，这个时候的时间复杂度是多少呢？\n\n我们可以类比上面k=9的分析过程。当k=99的时候，树的最短路径就是$log_{100} n$，最长路径是$log_{100/99} n$，所以总遍历数据个数介于n$log_{100} n$和nlog 100/99 n之间。尽管底数变了，但是时间复杂度也仍然是O(nlogn)。\n\n也就是说，对于k等于9，99，甚至是999，9999.....，只要k的值不随n变化，是一个事先确定的常量，那快排的时间复杂度就是O(nlogn)。所以，从概率论的角度来说，快排的平均时间复杂度就是O(nlogn)。\n\n\n# 实战二： 分析斐波那契数列的时间复杂度\n\n在递归那一节中，我们举了一个跨台阶的例子。那个例子实际上就是一个斐波那契数列。它的代码实现贴在如下：\n\nint f(int n) {\n  if (n == 1) return 1;\n  if (n == 2) return 2;\n  return f(n-1) + f(n-2);\n}\n\n\n1\n2\n3\n4\n5\n\n\n这样一段代码的时间复杂度是多少呢？如何利用递归树来分析。\n\n我们先把上面的递归代码画成递归树，就是下面这个样子：\n\n\n\n这棵递归树的高度是多少呢？\n\nf(n)分解为f(n-1)和f(n-2 )，每次数据规模都是-1或-2，叶子节点的数据规模是1或2。所以，从根节点走到叶子节点，每条路径是长短不一的。如果每次都是-1，那最长路径大约就是n；如果每次都是-2，那最短路径大约就是n/2。\n\n每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作1 ，第二层的总时间消耗是2，第三层额总时间消耗就是2^2。依次类推，第k层的时间消耗就是2^(k-1)，那整个算法的总的时间消耗就是每一层时间消耗之和。\n\n如果路径长度都为n，那这个总和就是2^n -1。\n\n\n\n如果路径长度都是n/2，那整个算法的总的时间消耗就是2^(n/2) -1.\n\n\n\n所以，这个算法的时间复杂度就介于O(2^n)和O(2^(n/2))之间。虽然这样得到的结果还不够精确，只是一个范围，但是我们也基本上知道了上面算法的时间复杂度是指数级的，非常高。\n\n\n# 实战三：分析全排列的时间复杂度\n\n前面两个复杂度分析\n\n\n# D54(2020/12/05) 堆排序\n\n我们今天讲另外一种特殊的树，"堆"。堆这种数据结构的应用场景非常多，最经典的莫过于堆排序。堆排序是一种原地的、时间复杂度为O(nlogn)的排序算法。\n\n前面我们学过快速排序，平均情况下，它的时间复杂度是O(nlogn)。尽管这两种排序算法的时间复杂度都是O(nlogn)，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？\n\n\n# 如何理解"堆"？\n\n前面我们提到，堆是一种特殊的树。现在来看，什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。\n\n * 堆是一个完全二叉树。\n * 堆中每一个节点的值都必须大于等于(或小于等于)其子树中每个节点的值。\n\n第一点，堆必须是一个完全二叉树。之前我们对完全二叉树的定义，完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。\n\n第二点，堆中的每个节点的值必须大于等于(或小于等于)其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于(或小于等于)其左右子节点的值。这两种表述是等价的。\n\n对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做"大顶堆"。对于每个节点的值多小于等于子树中每个节点值的堆，我们叫做"小顶堆"。\n\n下面我们来看看，下面的图示中，几个二叉树是不是堆？\n\n\n\n其中第1个和第2个是大顶堆，第3个是小顶堆，第4个不是堆。除此之外，从图中还可以看出来，对于同一组数据，我们可以构建多种不同形态的堆。\n\n\n# 如何实现一个堆？\n\n要实现一个堆，我们先要知道，堆都支持哪些操作以及如何存储一个堆。\n\n我们之前讲过，完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。\n\n下面是画了一个用数组存储堆的例子\n\n\n\n从图中我们可以看到，数组下标为i的节点的左子节点，就是下标为i*2的节点，右子节点就是下标为i*2+1 的节点，父节点就是下标为i/2的节点。\n\n知道了如何存储一个堆，那我们再来看看，堆上的操作有哪些呢？我罗列了几个非常核心的操作，分别是往堆中插入一个元素和删除堆顶元素。\n\n\n# 往堆中插入一个元素\n\n往堆中插入一个元素后，我们需要继续满足堆的两个特性。\n\n如果我们把新插入的元素放到堆的最后，可以看到下面的图，还是不符合堆的特性了。于是，我们就需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做堆化。\n\n堆化实际上有两种，从下往上和从上往下。这里我们先讲从下往上的堆化方法。\n\n\n\n堆化非常简单，就是顺着节点所在的路径，向上或向下，对比，然后交换。\n\n我这里画了一张堆化的过程分解图。我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。\n\n\n\n将上面讲的往堆中插入数据的过程，代码如下。\n\npublic class Heap {\n  private int[] a; // 数组，从下标1开始存储数据\n  private int n;  // 堆可以存储的最大数据个数\n  private int count; // 堆中已经存储的数据个数\n\n  public Heap(int capacity) {\n    a = new int[capacity + 1];\n    n = capacity;\n    count = 0;\n  }\n\n  public void insert(int data) {\n    if (count >= n) return; // 堆满了\n    ++count;\n    a[count] = data;\n    int i = count;\n    while (i/2 > 0 && a[i] > a[i/2]) { // 自下往上堆化\n      swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素\n      i = i/2;\n    }\n  }\n }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 删除堆顶元素\n\n从堆的定义的第二条中，任何节点的值都大于等于(或小于等于)子树节点的值，我们可以发现，堆顶元素存储的就是堆中数据的最大值或最小值。\n\n假设我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。\n\n这里我们也有一个分解图。不过这种方法有点问题，就是最后堆化出来的堆并不满足完全二叉树的特性。\n\n\n\n实际上，我们稍微改变一下思路，就可以解决这个问题。看下面画的这幅图，我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。\n\n因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的"空洞"，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。\n\n\n\n把上面的删除代码如下。\n\npublic void removeMax() {\n  if (count == 0) return -1; // 堆中没有数据\n  a[1] = a[count];\n  --count;\n  heapify(a, count, 1);\n}\n\nprivate void heapify(int[] a, int n, int i) { // 自上往下堆化\n  while (true) {\n    int maxPos = i;\n    if (i*2 <= n && a[i] < a[i*2]) maxPos = i*2;\n    if (i*2+1 <= n && a[maxPos] < a[i*2+1]) maxPos = i*2+1;\n    if (maxPos == i) break;\n    swap(a, i, maxPos);\n    i = maxPos;\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n我们知道，一个包含n个节点的完全二叉树，树的高度不会超过$log_2 n$。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是O(logn)。插入数据和删除堆顶数据的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是O(logn)。\n\n\n# 如何基于堆实现排序？\n\n前面我们讲过好几种排序算法，我们再来回忆一下，有时间复杂度是O($n ^2$)的冒泡排序、插入排序、选择排序，有时间复杂度是O(nlogn)的归并排序、快速排序，还有线性排序。\n\n这里我们借助于堆这种数据结构实现的排序算法，就叫做堆排序。这种排序方法的时间复杂度非常稳定，是O(nlogn)，并且它还是原地排序算法。\n\n我们可以把堆排序的过程大致分解成两个大的步骤，建堆和排序。\n\n\n# 建堆\n\n我们首先将数组原地建成一个堆。所谓"原地"就是，不借助另一个数组，就在原数组上操作。建堆的过程，有两种思路。\n\n第一种是借助我们前面讲的，在堆中插入一个元素的思路。尽管数组中包含n个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为1的数据。然后，我们调用前面讲的插入操作，将下标从2到n的数据依次插入到堆中。这样我们就将包含n个数据的数组，组织成了堆。\n\n第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且每个数据都是从上往下堆化。\n\n我举了一个例子，并且画了一个第二种实现思路的建堆分解步骤图，可以看下。因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从最后一个非叶子节点开始，依次堆化就行了。\n\n\n\n\n\n第二种实现思路翻译成了代码。\n\nprivate static void buildHeap(int[] a, int n) {\n  for (int i = n/2; i >= 1; --i) {\n    heapify(a, n, i);\n  }\n}\n\nprivate static void heapify(int[] a, int n, int i) {\n  while (true) {\n    int maxPos = i;\n    if (i*2 <= n && a[i] < a[i*2]) maxPos = i*2;\n    if (i*2+1 <= n && a[maxPos] < a[i*2+1]) maxPos = i*2+1;\n    if (maxPos == i) break;\n    swap(a, i, maxPos);\n    i = maxPos;\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n我们可能已经发现了，在这段代码中，我们从下标从n/2开始到1的数据进行堆化，下标是n/2 + 1到n的节点是叶子节点，我们不需要堆化。实际上，对于完全二叉树来说，下标从n/2 + 1到n的节点都是叶子节点。\n\n现在我们来看一下，建堆操作的时间复杂度是多少？\n\n每个节点堆化的时间复杂度是O(logn)，那n/2 + 1个节点堆化的总时间复杂度是不是就是O(nlogn)呢？这个答案虽然也没有错，但是这个值不够精准。实际上，堆排序的建堆过程的时间复杂度是O(n)。\n\n因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度k成正比。\n\n我们把每一层的节点个数和对应的高度画出来，我们可以看下。我们只需要将每个节点的高度求和，得出的就是建堆的时间复杂度。\n\n\n\n我们将每个非叶子节点的高度求和，就是下面的这个公式：\n\n\n\n这个公式的求解稍微有点技巧，我们高中就应该学过：把公式左右都乘以2，就得到另一个公式S2。我们将S2错位对齐，并且用S2减去S1，就可以得到S了。\n\n\n\nS的中间部分是一个等比数列，所以最后可以用等比数列的求和公式来计算，最终的结果就是下面图中画的样子。\n\n\n\n因为h=$log_2 n$，代入公式S，就能得到S = O(n)，所以，建堆的时间复杂度就是O(n)。\n\n\n# 排序\n\n建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为n的位置。\n\n这个过程有点类似上面讲的"删除堆顶元素"的操作，当堆顶元素移除之后，我们把下标为n的元素放到堆顶，然后再通过堆化的方法，将剩下的n-1个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是n-1的位置，一直重复这个过程，知道最后堆中只剩下标为1的一个元素，排序工作就完成了。\n\n\n\n堆排序的过程的代码如下。\n\n// n表示数据的个数，数组a中的数据从下标1到n的位置。\npublic static void sort(int[] a, int n) {\n  buildHeap(a, n);\n  int k = n;\n  while (k > 1) {\n    swap(a, 1, k);\n    --k;\n    heapify(a, k, 1);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n现在，我们再来分析一下堆排序的时间复杂度、空间复杂度以及稳定性。\n\n整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是O(n)，排序过程的时间复杂度是O(nlogn)，所以，堆排序整体的时间复杂度是O(nlogn)。\n\n堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。\n\n上面的讲解中，都是假设，堆中的数据是从数组下标为1的位置开始存储。如果从0开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了。\n\n如果节点的下标是i，那左子节点的下标就是2*i +1，右子节点的下标就是2*i + 2，父节点的下标就是 (i-1)/2 。\n\n\n# 解答开篇\n\n现在我们来看下开篇的问题，在实际开发中，为什么快速排序要比堆排序性能好？\n\n我觉得主要有两方面的原因。\n\n第一点，堆排序数据访问的方式没有快速排序友好。\n\n对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。比如，对排序中，最重要的一个操作就是数据的堆化。比如下面的这个例子，对堆顶节点进行堆化，会依次访问数组下标是1，2，4，8的元素，而不是像快速排序那样，局部顺序访问，所以，这样对CPU缓存是不友好的。\n\n\n\n第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。\n\n我们在讲排序的时候，提过两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换(或移动)。快速排序数据交换的次数不会比逆序度多。\n\n但是堆排序的第一步就是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。\n\n\n\n\n# 内容小结\n\n今天我们讲了堆这种数据结构，堆是一种完全二叉树。它最大的特性是：每个节点的值都大于等于(或小于等于)其子树节点的值。因此，堆被分成了两类，大顶堆和小顶堆。\n\n堆中比较重要的两个操作是插入一个数据和删除堆顶元素。这两个操作都要用到堆化。插入一个数据的时候，我们把新插入的数据放到数组的最后，然后从下往上堆化；删除堆顶数据的时候，我们把数组中的最后一个元素放到堆顶，然后从上往下堆化。这两个操作时间复杂度都是O(logn)。\n\n除此之外，我们还讲了堆的一个经典应用，堆排序。堆排序包含两个过程，建堆和排序。我们将下标n/2到1的节点，依次进行从上到下的堆化操作，然后就可以将数组中的数据组织成堆这种数据结构。接下来，我们迭代地将堆顶的元素放到堆的末尾，并将堆的大小减去一，然后再堆化，重复这个过程，直到堆中只剩下一个元素，整个数组中的数据就都有序排列了。\n\n\n# D55(2020/12/09) 堆应用\n\n假设现在我们有一个包含10亿个搜索关键词的日志文件，如何能快速获取到热门榜Top 10的搜索关键词呢？\n\n这个问题就可以用堆来解决，这也是堆这种数据结构一个非常典型的应用。上一节我们讲了堆和堆排序的一些理论知识，今天我们就来讲一讲，堆这种数据结构几个非常重要的应用：优先级队列、求Top K和求中位数。\n\n\n# 堆的应用一：优先级队列\n\n首先，我们来看第一个应用场景：优先级队列。\n\n优先级队列，顾名思义，它首先应该是一个队列。我们前面讲过，队列最大的特性就是先进先出。不过，在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。\n\n如果实现一个优先级队列呢？方法有很多，但是用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。\n\n不要小看这个优先级队列，它的应用场景非常多。我们后面要讲的很多数据结构和算法都要依赖它。比如，赫夫曼编码、图的最短路径、最小生成树算法等。不仅如此，很多语言中，都提供了优先级队列的实现。比如，Java的PriorityQueue，C++的priority_queue等。\n\n只讲这些应用场景比较空泛，现在，我举两个具体的例子，来感受一下优先级队列具体是怎么用的。\n\n\n# 合并有序小文件\n\n假设我们有100个小文件，每个文件的大小是100MB，每个文件中存储的都是有序的字符串。我们希望将这些100个小文件合并成一个有序的大文件。这里就会用到优先级队列。\n\n整体思路有点像归并排序中的合并函数。我们从这100个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。\n\n假设，这个最小的字符串来自于13.txt这个小文件，我们就再从这个小文件取下一个字符串，放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，将它从数组中删除。\n\n这里我们用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。有没有更加高效方法呢？\n\n这里就可以用到优先级队列，也可以说是堆。我们将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将100个小文件中的数据依次放入到大文件中。\n\n我们知道，删除堆顶数据和往堆中插入数据的时间复杂度都是O(logn)，n表示堆中的数据个数，这里就是100。这样的话就比原来数组存储的方式要高效很多了。\n\n\n# 高性能定时器\n\n假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器没过一个很小的单位时间(比如1秒)，就扫描一遍任务，看是否任务到达设定的执行时间。如果到达了，就拿出来执行。\n\n\n\n但是，这样每过1秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。\n\n针对这些问题，我们就可以用优先级队列来解决。我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部(也就是小顶堆的堆顶)存储的是最先执行的任务。\n\n这样，定时器就不需要每隔1秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔T。\n\n这个时间间隔T就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在T秒之后，再来执行任务。从当前时间点到(T-1)秒这段时间里，定时器都不需要做任何事情。\n\n当T秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间与当前时间点的差值，把这个差值作为定时器执行下一个任务需要等待的时间。\n\n这样，定时器既不用间隔1秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。\n\n\n# 堆的应用二：利用堆求Top K\n\n刚刚我们学习了优先级队列，我们现在来看，堆的另外一个非常重要的应用场景，那就是"求Top K问题"。\n\n我们把这种求Top K的问题抽象成两类。一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。\n\n针对静态数据，如何在一个包含n个数据的数组中，查找前K大数据呢？我们可以维护一个大小为K的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不作处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前K大数据了。\n\n遍历数组需要O(n)的时间复杂度，一次堆化操作需要O(logk)的时间复杂度，所以最坏情况下，n个元素都入堆一次，时间复杂度就是O(nlogk)。\n\n针对动态数据求得Top K就是实时Top K。例如，一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前K大数据。\n\n如果每次询问前K大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是O(nlogk)，n表示当前的数据的大小。实际上，我们可以一直都维护一个K大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前K大数据，我们都可以立刻返回给他。\n\n\n# 堆的应用三：利用堆求中位数\n\n如何求动态数据集合中的中位数。\n\n中位数，顾名思义，就是处于中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第n/2 +1个数据就是中位数；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第n/2个和第n/2 +1个数据，这个时候，我们可以随意取一个作为中位数，比如取两个数中靠前的那个，就是第n/2个数据。\n\n\n\n对于一组静态数据，中位数是固定的，我们可以先排序，第n/2个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。所以，尽管排序的代价比较大，但是边际成本会很小。但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。\n\n借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。\n\n我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。\n\n也就是说，如果有n个数据，n是偶数，我们从小到大排序，那前n/2个数据存储在大顶堆中，后n/2个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果n是奇数，情况是类似的，大顶堆就存储了n/2 + 1个数据，小顶堆中就存储n/2个数据。\n\n\n\n我们前面也提到，数据是动态变化的，当新添加一个数据的时候，我们如何调整两个堆，让大顶堆的堆顶元素继续是中位数呢？\n\n如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。\n\n这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果n是偶数，两个堆中的数据个数都是n/2；如果n是奇数，大顶堆有n/2 +1 个数据，小顶堆有n/2个数据。这个时候，我们可以从一个堆中不停地将堆顶元素移到到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。\n\n\n\n于是，我们就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间复杂度变成了O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是O(1)。\n\n实际上，利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。之前我们提到一个问题，"如何快速求接口的99%响应时间？" 我们现在来看下，利用两个堆如何来实现。\n\n在开始这个问题的讲解之前，我们先解释一下，什么是"99%响应时间"。\n\n中位数的概念就是将数据从小到大排列，处于中间位置，就叫中位数，这个数据会大于等于前面50%的数据。99百分位数的概念可以类比中位数，如果将一组数据从小到大排列，这个 99百分位数就是大于前面99%数据的那个数据。\n\n如果这个还是不好接，例如有100个数据，分别是1，2，3，。。。。。100，那99百分位数就是99，因为小于等于99的数占总个数的99%。\n\n\n\n我们再来看99%响应时间。如果有100个接口访问请求，每个接口请求的响应时间都不同，比如55毫秒、100毫秒、23毫秒等，我们把这100个接口的响应时间按照从小到大排列，排在第99的那个数据就是99%响应时间，也叫99百分位响应时间。\n\n我们来总结一下，如果有n个数据，将数据从小到大排列之后，99百分位数大约就是第n*99%个数据，同类，80百分位数大约就是第n*80%个数据。\n\n弄懂了这些，我们再来看如何求99%响应时间。\n\n我们维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是n，大顶堆中保存n*99%个数据，小顶堆中保存n*1%个数据。大顶堆堆顶的数据就是我们要找的99%响应时间。\n\n每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。\n\n但是，为了保持大顶堆中的数据占99%，小顶堆中的数据占1%，在每次新插入数据之后，我们都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合99:1这个比例。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法。\n\n通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是O(logn)。每次求99%响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是O(1)。\n\n\n# 解答开篇\n\n学懂了上面的一些应用场景的处理思路，我们来解答一下开篇的那个问题。假设现在我们有一个包含10亿个搜索关键词的日志文件，如何快速获取到Top 10最热门的搜索关键词呢？\n\n处理这个问题，有很多高级的解决方法，比如使用MapReduce等。但是，如果我们将处理的场景限定为单机，可以使用的内存为1GB。那个这个问题该如何解决呢？\n\n因为用户搜索的关键词，有很多可能都是重复的，所以我们首先要统计每个搜索关键词出现的频率。我们可以通过散列表、平衡二叉查找树或者其他一些支持快速查找、插入的数据结构，来记录关键词及其出现的次数。\n\n假设我们选用散列表。我们就顺序扫描这10亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为1。以此类推，等遍历完这10亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。\n\n假设我们选用散列表。我们就顺序扫描这10亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为1。以此类推，等遍历完这10亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。\n\n然后，我们再根据前面讲的用堆求Top K的方法，建立一个大小为10的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。\n\n以此类推，当遍历完整个散列表中的搜索关键词之后，堆中的搜索关键词就是出现次数最多的Top 10搜索关键词了。\n\n上面的解决思路还是存在漏洞的。10亿的关键词还是很多的。我们假设10亿条搜索关键词中不重复的有1亿条，如果每个搜索关键词的平均长度是50个字节，那存储1亿个关键词起码需要5GB的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。而我们的机器只有1GB的可用内存空间，所以我们无法一次性将所有的搜索关键词加入到内存中。\n\n我们在哈希算法那一节讲过，相同数据经过哈希算法得到的哈希值是一样的。我们可以根据哈希算法的这个特定，将10亿条搜索关键词先通过哈希算法分片到10个文件中。\n\n具体可以这样做：我们创建10个空文件00，01，02，....，09。我们遍历这10亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同10取模，得到的结果就是这个搜索关键词应该被分到的文件编号。\n\n对这10亿个关键词分片之后，每个文件都只有1亿的关键词，去除重复的，可能就只有1000万个，每个关键词平均50个字节，所以总的大小就是500MB。1GB的内存完全可以放得下。\n\n我们针对每个包含1亿条搜索关键词的文件，利用散列表和堆，分别求出Top 10，然后把这个10个Top 10放在一块，然后取这100个关键词中，出现次数最多的10个关键词，这就是这10亿条数据中的Top10最频繁的搜索关键词了。\n\n\n# 内容小结\n\n我们今天主要讲了堆的几个重要的应用，它们分别是：优先级队列、求Top K问题和求中位数的问题。\n\n优先级队列是一种特殊的队列，优先级高的数据出队，而不再像普通的队列那样，先进先出。实际上，堆就可以看作优先级队列，知识称谓不一样罢了。求Top K问题又可以分为针对静态数据和针对动态数据，只需要利用一个堆，就可以做到非常高效率地查询Top K的数据。求中位数实际上还有很多变形，比如求99百分位数据、90百分位数据等，处理的思路都是一样的，即利用两个堆，一个大顶堆，一个小顶堆，随着数据的动态增加，动态调整两个堆中的数据，最后大顶堆的堆顶元素就是要求的数据。\n\n\n# D56(2020/12/12) 图的存储\n\n今天我们要学习的就是图这种数据结构。实际上，涉及图的算有很多，也非常复杂，比如图的搜索、最短路径、最小生成树、二分图等。我们今天聚焦在图存储这一方面，后面会分好几节来依次讲解图相关的算法。\n\n\n# 如何理解"图"\n\n我们前面讲过了树这种非线性表数据结构，今天我们要讲另一种非线性表数据结构，图。和数比起来，这是一种更加复杂的非线性表结构。\n\n我们知道，树中的元素我们称为节点，图中的元素我们就叫做顶点。从画的图中可以看出来，图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫做边。\n\n\n\n我们生活中就有很多符合图这种结构的例子。比如，开篇问题中讲到的社交网络，就是一个非常典型的图结构。\n\n我们就拿微信来举例子把。我们可以把每个用户看作一个顶点。如果两个用户之间互加好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。其中，每个用户有多少个好友，对应到图中，就叫做顶点的度，就是跟顶点相连接的边的条数。\n\n实际上，微博的社交关系跟微信还有点不一样，或者说更加复杂一点。微博允许单向关注，也就是说，用户A关注了用户B，但用户B可以不关注用户A。那我们如何用图来表示这种单向的社交关系呢？\n\n我们可以把刚刚讲的图结构稍微改造一下，引入边的"方向"的概念。\n\n如果用户A关注了用户B，我们就在图中画一条从A到B的带箭头的边，来表示边的方向。如果用户A和用户B互相关注了，那我们就画一条从A指向B的边，再画一条从B指向A的边。我们把这种边有方向的图叫做"有向图"。以此类推，我们把边没有方向的图就叫做"无向图"。\n\n\n\n我们刚刚讲过，无向图中有"度"这个概念，表示一个顶点有多少条边。在有向图中，我们把度分为入度和出度。\n\n顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。对应到微博的例子，入度就表示有多少粉丝，出度就表示关注了多少人。\n\n前面讲到了微信、微博、无向图、有向图，现在我们再来看另一种社交软件：QQ。\n\nQQ中的社交关系要更复杂一点。不知道有没有留意过QQ亲密度这样一个功能。QQ不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经常往来，亲密度就比较低。如果在图中记录这种好友关系的亲密度呢？\n\n这里就要用到另一种图，带权图。在带权图中，每条边都有一个权重，我们可以通过这个权重来表示QQ好友间的亲密度。\n\n\n\n\n# 邻接矩阵存储方法\n\n图最直观的一种存储方法就是，邻接矩阵。\n\n邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点i与顶点j之间有边，我们就将A[i][j]和A[j][i]标记为1；对于有向图来说，如果顶点i到顶点j之间，有一条箭头从顶点i指向顶点j的边，那我们就将A[i][j]标记为1。同理，如果有一条箭头从顶点j指向顶点i的边，我们就将A[j][i]标记为1。对于带权图，数组中就存储相应的权重。\n\n用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。为什么呢？\n\n对于无向图来说，如果A[i][j]等于1，那么A[j][i]也肯定等于1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或下面这样一半的空间就足够了，另外一半白白浪费掉了。\n\n还有，如果我们存储的是稀疏图，也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。比如微信有好多亿的用户，对应到图上就好几亿的顶点。但是每个用户的好友并不会很多，一般也就三五百个而已。如果我们用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。\n\n但这也并不是说，邻接矩阵的存储方法就完全没有优点。首先，邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。其次，用邻接矩阵存储图的另外一个好处是方便计算。这是因为，用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。比如求解最短路径问题时会提到一个Floyd算法，就是利用矩阵循环相乘若干次得到结果。\n\n\n# 邻接表存储方法\n\n针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，邻接表。\n\n下面画了一张邻接表的图，乍一看，邻接表有点像散列表。每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。另外我需要说明一下，图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点。\n\n\n\n还记得我们之前讲过的时间、空间复杂度互换的设计思想吗？邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储其阿里比较节省空间，但是使用起来就比较耗费时间。\n\n就像图中的例子，如果我们要确定，是否存在一条从顶点2到顶点4的边，那我们就要遍历顶点2对应的那条链表，看链表中是否存在顶点4。而且，我们前面也讲过，链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。\n\n在散列表的那几节中，我们讲到，在基于链表法解决冲突的散列表中，如果链表过长，为了提高查找效率，我们可以将链表换成其他更加高效的数据结构，比如平衡二叉查找树等。刚刚也提及，邻接表长得很像散列。所以，我们也可以将邻接表同散列表一样进行"改进升级"。\n\n我们可以将邻接表中的链表改成平衡二叉查找树。实际开发中，我们可以选择用红黑树。这样，我们就可以更加快速地查找两个顶点之间是否存在边了。当然，这里的二叉查找树可以换成其他动态数据结构，比如跳表、散列表等。除此之外，我们还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间是否存在边。\n\n\n# 解答开篇\n\n有了前面讲的理论知识，现在我们再来看下开篇的问题，如何存储微博、微信等社交网络中的好友关系？\n\n前面我们分析了，微博、微信是两种"图"，前者是有向图，后者是无向图。在这个问题上，两者的解决思路差不多，所以这里只拿微博来讲解。\n\n数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。针对微博用户关系，假设我们需要支持下面这样几个操作：\n\n * 判断用户A是否关注了用户B；\n * 判断用户A是否是用户B的粉丝；\n * 用户A关注用户B；\n * 用户A取消关注用户B；\n * 根据用户名称的首字母排序，分页获取用户的粉丝列表；\n * 根据用户名词的首字母排序，分页获取用户的关注列表。\n\n关于如何存储一个图，前面我们讲到两种主要的存储方法，邻接矩阵和邻接表。因为社交网络是一张稀疏图，使用邻接矩阵存储比较浪费存储空间。所以，这里我们采用邻接表来存储。\n\n不过，用一个邻接表来存储这种有向图是不够的。我们去查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就是用户的粉丝列表，是非常困难的。\n\n基于此，我们需要一个逆邻接表。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。对应到图上，邻接表中，每个顶点的链表中，存储的就是这个顶点指向的顶点，逆邻接表中，每个顶点的链表中，存储的是指向这个顶点的顶点。如果要查找某个用户关注了哪些用户，我们可以在邻接表中查找；如果要查找某个用户被哪些用户关注了，我们从逆邻接表中查找。\n\n\n\n基础的邻接表不适合快速判断两个用户之间是否是关注与被关注的关系，所以我们选择改进版本，将邻接表中的链表改为支持快速查找的动态数据结构。选择哪种动态数据结构呢？红黑树、跳表、有序动态数组还是散列表呢？\n\n因为我们需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或关注列表，用跳表这种结构再合适不过了。这是因为，跳表插入、删除、查找都非常高效，时间复杂度是O(logn)，空间复杂度上稍高，是O(n)。最重要的一点，跳表中存储的数据本来就是有序的了，分页获取粉丝列表或关注列表，就非常高效。\n\n如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，我们可以将整个社交关系存储在内存中，上面的解决思路是没有问题的。但是如果像微博那样有上亿用户，数据规模太大，我们就无法全部存储在内存中了。\n\n我们可以通过哈希算法等数据分片方式，将邻接表存储在不同的机器上。可以查看下面的图示，我们在机器1上存储顶点1，2，3的邻接表，在机器2上，存储顶点4，5的邻接表。逆邻接表的处理方式也一样。当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定义顶点所在的机器，然后再在相应的机器上查找。\n\n\n\n除此之外，我们还有另外一种解决思路，就是利用外部存储(比如硬盘)，因为外部存储的存储空间要比内存会宽裕很多。数据库是我们经常用来持久化存储关系数据的，下面就介绍一种数据库的存储方式。\n\n下面这张表来存储这样一个图。为了高效地支持前面定义的操作，我们可以在表上建立多个索引，比如第一列、第二列，给这两列都建立索引。\n\n\n\n\n# 内容小结\n\n今天我们学习了图这种非线性表数据结构，关于图，我们需要理解这样几个概念：无向图、有向图、带权图、顶点、边、度、入度、出度。除此之外，我们还学习了图的两个主要存储方式：邻接矩阵和邻接表。\n\n邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。\n\n\n# D57(2020/12/13) 深度和广度优先搜索\n\n上一节我们讲了图的表示方法，讲到如何用有向图、无向图来表示一个社交网络。在社交网络中，有一个六度分隔理论，具体是说，你与世界上的另一个间隔的关系不会超过六度，也就是说平均只需要六步就可以联系到任何两个互不相识的人。\n\n一个用户的一度连接用户很好理解，就是他的好友，二度连接用户就是他好友的好友，三度连接用户就是他好友的好友。在社交网络中，我们往往通过用户之间的连接关系，来实现推荐"可能认识的人"这么一个功能。今天的开篇问题就是，给你一个用户，如何找出这个用户的所有三度(其中包含一度、二度和三度)好友关系？\n\n这就要用到今天要讲的深度优先和广度优先搜索算法。\n\n\n# 什么是"搜索"算法？\n\n我们知道，算法是作用于具体数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于"图"这种数据结构的。这是因为，图这种数据结构的表达能力很强，大部分涉及受伤的场景都可以抽象成"图"。\n\n图上的搜索算法，最直接的理解就是，在图中找出从一个顶点触发，到另一顶点的路径。具体方法有很多，比如今天我们要讲的两种最简单、最"暴力"的深度优先、广度优先搜索，还有A*、IDA*等启发式搜索算法。\n\n我们上一节讲过，图有两种主要存储方法，邻接表和邻接矩阵。今天我们会用邻接表来存储图。\n\n这里先给出了图的代码实现。需要说明一下，深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。在今天的讲解中，针对的是无向图的讲解。\n\npublic class Graph { // 无向图\n  private int v; // 顶点的个数\n  private LinkedList<Integer> adj[]; // 邻接表\n\n  public Graph(int v) {\n    this.v = v;\n    adj = new LinkedList[v];\n    for (int i=0; i<v; ++i) {\n      adj[i] = new LinkedList<>();\n    }\n  }\n\n  public void addEdge(int s, int t) { // 无向图一条边存两次\n    adj[s].add(t);\n    adj[t].add(s);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 广度优先搜索(BFS)\n\n广度优先搜索(Breadth-First-Search)，我们平常都简称为BFS。直观地讲，它其实就是一种"地毯式"层层推进的搜索策略，即先查找离起点顶点最近的，然后是次近的，依次往外搜索。理解起来并不难，下面有一张示意图。\n\n\n\n尽管广度优先搜索的原理挺简单，但代码实现还是稍微有点复杂度。所以，我们重点讲一下它的代码实现。\n\n这里面，bfs()函数就是基于之前定义的，图的广度优先搜索的代码实现。其中s表示起点顶点，t表示终止顶点。我们搜索一条从s到t的路径。实际上，这样求得的路径就是从s到t的最短路径。\n\npublic void bfs(int s, int t) {\n  if (s == t) return;\n  boolean[] visited = new boolean[v];\n  visited[s]=true;\n  Queue<Integer> queue = new LinkedList<>();\n  queue.add(s);\n  int[] prev = new int[v];\n  for (int i = 0; i < v; ++i) {\n    prev[i] = -1;\n  }\n  while (queue.size() != 0) {\n    int w = queue.poll();\n   for (int i = 0; i < adj[w].size(); ++i) {\n      int q = adj[w].get(i);\n      if (!visited[q]) {\n        prev[q] = w;\n        if (q == t) {\n          print(prev, s, t);\n          return;\n        }\n        visited[q] = true;\n        queue.add(q);\n      }\n    }\n  }\n}\n\nprivate void print(int[] prev, int s, int t) { // 递归打印s->t的路径\n  if (prev[t] != -1 && t != s) {\n    print(prev, s, prev[t]);\n  }\n  System.out.print(t + " ");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n这段代码不是很好理解，里面有三个重要的辅助变量visited、queue、prev。只要理解三个变量，读懂这段代码估计就没什么问题了。\n\nvisited是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点q被访问，那相应的visited[q]会被设置为true。\n\nqueue是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第k层的顶点都访问完成之后，才能访问第k+1层的顶点。所以，我们用这个队列来实现记录的功能。\n\n> 入队，是指该顶点以及被访问了，但是该顶点的相连的顶点还没被访问。出队，是指该顶点以及被访问了，并且该顶点相连的顶点也被访问了。\n\nprev用来记录搜索路径。当我们从顶点s开始，广度优先搜索到顶点t后，prev数组中存储的就是搜索的路径。不过，这个路径是反向存储的。prev[w]存储的是，顶点w是从哪个前驱顶点遍历过来的。比如，我们通过顶点2的邻接表访问到顶点3，那prev[3]就等于2。为了正向打印出路径，我们需要递归地来打印，可以看下print()函数的实现方式。\n\n为了方便理解，这里画了一个广度优先搜索的分解图，可以结合着代码来看。\n\n\n\n掌握了广度优先搜索算法的原理，我们来看下，广度优先搜索的时间、空间复杂度是多少呢？\n\n最坏情况下，终止顶点t离起始顶点s很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边都会被访问一次，所以，广度优先搜索的时间复杂度是O(V+E)，其中，V表示顶点的个数，E表示边的个数。当然，对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，E肯定要大于等于V-1，所以，广度优先搜索的时间复杂度也可以简写为O(E)。\n\n广度优先搜索的空间消耗主要在几个辅助变量visited数组、queue队列、prev数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是O(V)。\n\n\n# 深度优先搜索(DFS)\n\n深度优先搜索(DFS)，最直观的例子就是"走迷宫"。\n\n假设我们站在迷宫的某个岔路口，然后想找到出口。我们随意选择一个岔路口来走，走着走着发现走不通的时候，我们就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。\n\n走迷宫的例子很容易能看懂，我们现在再来看下，如何在图中应用深度优先搜索，来找某个顶点到另一个顶点的路径。\n\n你可以看下面画的这幅图。搜索的起始顶点是s，终止顶点是t，我们希望在图中寻找一条从顶点s到顶点t的路径。如果映射到迷宫这个例子，s就是我们起始所在的位置，t就是出口。\n\n下面用深度递归算法，把整个搜索的路径标记出来了。张雷明实线箭头表示遍历，虚线箭头表示回退。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点s到顶点t的最短路径。\n\n\n\n实际上，深度优先搜索用的是一种比较著名的算分思想，回溯思想。这种思想解决问题的过程，非常适合用递归来实现。\n\n我们将上面的过程用递归来翻译出来，就是下面这个样子。我们发现，深度优先搜索代码实现也用到了prev、visited变量以及print()函数，它们跟广度优先搜索代码实现里的作用是一样的。不过，深度优先搜索代码实现里，有个比较特殊的变量found，它的作用是，当我们已经找到终止顶点t之后，我们就不再递归地继续查找了。\n\nboolean found = false; // 全局变量或者类成员变量\n\npublic void dfs(int s, int t) {\n  found = false;\n  boolean[] visited = new boolean[v];\n  int[] prev = new int[v];\n  for (int i = 0; i < v; ++i) {\n    prev[i] = -1;\n  }\n  recurDfs(s, t, visited, prev);\n  print(prev, s, t);\n}\n\nprivate void recurDfs(int w, int t, boolean[] visited, int[] prev) {\n  if (found == true) return;\n  visited[w] = true;\n  if (w == t) {\n    found = true;\n    return;\n  }\n  for (int i = 0; i < adj[w].size(); ++i) {\n    int q = adj[w].get(i);\n    if (!visited[q]) {\n      prev[q] = w;\n      recurDfs(q, t, visited, prev);\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n理解了深度优先搜索算法之后，我们来看，深度优先搜索的时间、空间复杂度是多少呢？\n\n从前面画的图可以看出，每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是O(E)，E表示边的个数。\n\n深度优先搜索算法的消耗内存主要是visited、prev数组和递归调用栈。visited、prev数组的大小跟顶点的个数V成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是O(V)。\n\n\n# 解答开篇\n\n了解了深度优先搜索和广度优先搜索的原理之后，开篇的问题就变得很简单了。我们来看下，如何找出社交网络中某个用户的三度好友关系？\n\n社交网络可以用图来表示。这个问题就非常适合用图的广度优先搜索算法来解决，因为广度优先搜索是层层往外推进的。首先，遍历与起始顶点最近的一层顶点，也就是用户的一度好友，然后再遍历与用户举例的边数为2的顶点，也就是二度好友关系，以及与用户距离的边数为3的顶点，也就是三度好友关系。\n\n\n# 内容小结\n\n广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，比如A*、IDA*等，要简单粗暴，没有什么优化，所以，也被叫做暴力搜索算法。所以，这两种搜索算法仅适用于状态空间不大，也就是说图不大的搜索。\n\n广度优先搜索，通俗来说，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先搜索的时间复杂度都是O(E)，空间复杂度是O(V)。\n\n\n# D58(2020/12/14)字符串匹配\n\n从今天开始，我们来学习字符串匹配算法。字符串匹配这样一个功能，大家都不会陌生。我们用的最多的就是编程语言提供的字符串查找函数，比如Java中的indexof()，Python中的find()函数等，它们底层就是依赖接下来要讲的字符串匹配算法。\n\n字符串匹配算法很多，这里会分为四节来讲解。今天会讲解两种比较简单那、好理解的，它们分别是：BF算法和RK算法。下一节，我们会学习比较难以理解、但更加高效的，它们是：BM算法和KMP算法。\n\n这两节讲的都是单模式串匹配的算分，也就是一个串跟一个串进行匹配。第三节、第四节，我们会讲两种多模式串匹配算法，也就是在一个串中同时查找多个串，它们分别是Trie树和AC自动机。\n\n今天讲的两个算法中，RK算法是BF算法的改进，它巧妙借助了我们前面讲过的哈希算法，让匹配的效率有了很大的提升。那RK算法是如何借助哈希算法来实现高效字符串匹配的呢？\n\n\n# BF算法\n\nBF算法中的BF是Brute Force的缩写，中文叫做暴力匹配算法，也叫朴素匹配算法。从名字可以看出，这种算法的字符串匹配方式很"暴力"，当然研究会比较简单、好懂，但相应的性能也不高。\n\n在开始讲解这个算法之前，我们先定义两个概念，方便后面讲解。它们分别是主串和模式串。\n\n比方说，我们在字符串A中查找字符串B，那字符串A就是主串，字符串B就是模式串。我们把主串的长度记住n，模式串的长度记住m。因为我们是在主串中查找模式串，所以n>m。\n\n作为最简单、最暴力的字符串匹配算法，BF算法的思想可以用一句话来概括，那就是，我们在主串中，检查起始位置分别是0、1、2...n-m 且长度为m的n-m+1个子串，看有么有跟模式串匹配的。\n\n\n\n从上面的算法思想和例子，我们可以看出，在极端情况下，比如主串是"aaa...aaaa"(省略号表示有很多重复的字符a)，模式串是"aaaaab"。我们每次都比对m个字符，要比对n-m+1次，所以，这种算法的最坏情况时间复杂度是O(n*m)。\n\n尽管理论上，BF算法的时间复杂度很高，是O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。\n\n第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把m个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。\n\n第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有bug也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常常所说的KISS(keep it simple and stupid)设计原则。\n\n所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。\n\n\n# RK算法\n\nRK算法理解起来也不是很难。个人觉得，它其实就是刚刚讲的BF算法的升级版。\n\n我们在讲解BF算法的时候讲过，如果模式串长度是m，主串的长度为n，那在主串中，就会有n-m+1个长度为m的子串，我们只需要暴力地对比这n-m+1个子串与模式串，就可以找出主串与模式串匹配的子串。\n\n但是，每次检查主串与子串是否匹配，需要依次比对每个字符，所以BF算法的时间复杂度就比较高，是O(n*m)。我们对朴素的字符串匹配算法稍加改造，引入哈希算法，时间复杂度立刻就会降低。\n\nRK算法的思路是这样的：我们通过哈希算法对主串中的n-m+1个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了(这里先不考虑哈希冲突的问题，后面我们会讲到)。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串与子串比较的效率就提高了。\n\n\n\n不过，通过哈希算法计算子串的哈希值的时候，我们需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。有没有方法可以提高哈希算法计算子串哈希值的效率呢？\n\n这就需要哈希算法设计的非常有巧妙了。我们假设要匹配的字符串的字符集中只包含K个字符，我们可以用一个K进制数来表示一个子串，这个K进制数转化成十进制数，作为子串的哈希值。表述起来有点抽象，举个如下的例子。\n\n比如要处理的字符串只包含a~z这26个小写字母，那我们就用二十六进制来表示一个字符串。我们把a~z这26个字符映射到0~25这26个数字，a就表示0，b就表示1，以此类推，z表示25。\n\n在十进制的表示法中，一个数字的值是通过下面的方式计算出来的。对应到二十六进制，一个包含a到z这26个字符的字符串，计算哈希的时候，我们只需要把进位从10改成26就可以。\n\n\n\n这个哈希算法应该可以看懂，现在，为了方便解释，在下面的讲解中，我假设字符串中只包含a~z这26个小写字符，我们用二十六进制来表示一个字符串，对应的哈希值就是二十六进制转化成十进制的结果。\n\n这种哈希算法有一个特点，在主串中，相邻两个子串的哈希值的计算公式有一定关系。这里有个例子，可以先看下规律。\n\n\n\n从这里例子中，我们很容易就能得出这样的规律：相邻两个子串s[i-1]和s[i] (i表示子串在主串中的起始位置，子串的长度都为m)，对应的哈希值计算公式有交集，也就是说，我们可以使用s[i-1]的哈希值很快的计算出s[i]的哈希值。如果用公式表示的话，就是下面这个样子：\n\n\n\n不过，这里有一个小细节需要注意，那就是26^(m-1)这部分的计算，我们可以通过查表的方式来提高效率。我们事先计算好26^0、26^1、26^2 .... 26^(m-1)，并且存储在一个长度为m的数组中，公式中的"次方"就对应数组的下标。当我们需要计算26的x次方的时候，就可以从数组的下标为x的位置取值，直接使用，省去了计算的时间。\n\n\n\n我们开头的时候提过，RK的算分的效率要比BF算分高，现在，我们来分析一下，RK算法的时间复杂度到底是多少呢？\n\n整个RK算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是O(n)。\n\n模式串哈希值与每个子串哈希值之间的比较的时间复杂度是O(1)，总共需要比较n-m+1个子串的哈希值，所以，这部分的时间复杂度也是O(n)。所以，RK算法整体的时间复杂度就是O(n)。\n\n这里还有一个问题就是，模式串很长，相应的主串中的子串也会很长，铜鼓上面的哈希算法计算得到的哈希值就可能很大，如果超过了计算机中整型数据可以表示的访问，那该如何解决。\n\n刚刚我们设计的哈希算法是没有散列冲突的，也就说，一个字符串与一个二十六进制数一一对应，不同的字符串的哈希值肯定不一样。因为我们是基于进制来表示一个字符串的，我们可以类比成十进制、十六进制来思考一下。实际上，我们为了能将哈希值落在整型数据范围内，可以牺牲一下，允许哈希冲突。这个时候的哈希算法该如何设计呢？\n\n哈希算法的设计方法有很多，这里举一个例子说明一下。假设字符串中只包含a~z这26个英文字母，那我们每个字母对应一个数字，比如a对应1，b对应2，以此类推，z对应26。我们可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值。这种哈希算法产生的哈希值的数据范围就相对要小很多了。\n\n不过，我们也应该发现，这种哈希算法的哈希冲突概率也是挺高的。当然，这里也只是举了一个最简单的设计方法，还有很多更加优化的方法，比如将每一个字母从小到达对应一个素数，而不是1，2，3.。。这样的自然数，这样冲突的概率就会降低一些。\n\n那现在新的问题来了。之前我们只需要比较一下模式串和子串的哈希值，如果两个值相等，那这个子串就一定可以匹配模式串。但是，当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身不匹配。\n\n实际上，解决方法很简单。当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。当然，如果子串的哈希值与模式串的哈希值不相等，那对应的子串和模式串肯定也是不匹配的，就不需要对比子串和模式串本身了。\n\n所以，哈希算法的冲突概率要相对控制得低一些，如果存在大量冲突，就会导致RK算法的时间复杂度退化，效率下降。极端情况下，如果存在大量的冲突，每次都要再对比子串和模式串本身，那时间复杂度就会退化到O(n*m)。但是也不要悲观，一般情况下，冲突不会很多，RK算法的效率还是比BF算法高的。\n\n\n# 解答开篇&内容小结\n\n今天我们讲了两种字符串匹配算法，BF算法和RK算法。\n\nBF算法是最简单、粗暴的字符串匹配算法，它的实现思路是，拿模式串与主串中是所有子串匹配，看是否有能匹配的子串。所以，时间复杂度也比较高，是O(n*m)，n、m表示主串和模式串的长度。不过，在实际的软件开发中，因为这种算法实现简单，对于处理小规模的字符串匹配很好用。\n\nRK算法是借助哈希算法对BF算法进行改造，即对每个子串分别求哈希值，然后拿子串的哈希值与模式串的哈希值比较，减少了比较的时间。所以，理想的情况下，RK算法的时间复杂度是O(n)，跟BF算法相比，效率提高了很多。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为O(n*m)。\n\n\n# D59(2020/12/16) 字符串匹配\n\n文本编辑器中的查找替换功能，比如，我们在word中把一个单词统一替换成另一个，用的就是这个功能。那么这个是怎么实现的呢？\n\n当然，我们可以用上一节的BF算法和RK算法，也可以实现这个功能，但是在某些极端情况下，BF算法性能会退化的比较严重，而RK算法需要用到哈希算法，而设计一个可以应对各种类型字符的哈希算法并不简单。\n\n对于工业级的软件开发来说，我们希望算法尽可能的高效，并且在极端情况下，性能也不要退化的太严重。那么，对于查找功能是重要功能的软件来说，比如一些文本编辑器，它们的查找功能都是用哪些算法来实现的呢？有没有比BF算法和RK算法更加高效的字符串匹配算法呢？\n\n今天，我们就来学习BM(Boyer-Moore)算法。它是一种非常高效的字符串匹配算法，有实验统计，它的性能是著名的KMP算法的3到4倍。BM算法的原理很复杂，比较难懂，学起来会比较烧脑。\n\n\n# BM算法的核心思想\n\n我们把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF算法和RK算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。\n\n在这个例子里，主串中的c，在模式串中是不存在的，所以，模式串向后滑动的时候，只要c与模式串有重合，肯定无法匹配。所以，我们可以一次性把模式串往后多滑动几位，把模式串移动到c的后面。\n\n由现象找规律，我们可以思考一下，当遇到不匹配的字符时，有什么固定的规律，可以加个模式串往后多滑动几位呢？这样一次性往后滑动好几位，那匹配的效率岂不是就提高了？\n\n我们今天要学习的BM算法，本质上其实就是在寻找这种规律。借助这种规律，在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后躲滑动几位。\n\n\n# BM算法原理分析\n\nBM算法包含两部分，分别是坏字符规则和好后缀规则。下面来依次来看，这两个规则分别都是怎么工作的。\n\n\n# 坏字符规则\n\n前面两节讲的算法，在匹配的过程中，我们都是按模式串的下标从小到大的顺序，依次与主串中的字符进行匹配的。这种匹配顺序比较符合我们的思维习惯，而BM算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的。如下面的图示来看。\n\n\n\n调整后的匹配顺序是如下：\n\n\n\n我们从模式串的末尾往前倒着匹配，当我们发现某个字符没法匹配的时候。我们把这个没有匹配的字符叫做坏字符(主串中的字符)。\n\n\n\n我们拿坏字符c在模式串中查找，发现模式串中并不存在这个字符，也就说，字符c与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位，将模式串滑动到c后面的位置，再从模式串的末尾字符开始比较。\n\n滑动如下了：\n\n\n\n这个时候，我们发现，模式串中最后一个字符d，还是无法跟主串中的a匹配，这个时候，还能将模式串往后滑动三位吗？答案是不行的。因为这个时候，怀字符a在模式串中是存在的，模式串中下标是0的位置也是字符a。这种情况下，我们可以将模式串往后滑动两位，让两个a上下对齐，然后再从模式串的末尾字符开始，重新匹配。\n\n\n\n第一次不匹配的时候，我们滑动了三位，第二次不匹配的时候，我们将模式串后移两位，那具体滑动多少位，到底有没有规律呢？\n\n当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作si。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作xi。如果不存在，我们把xi记作-1。那模式串往后移动的位数就等于si - xi。(注意，我们这里说的下标，都是字符在模式串的下标)。\n\n\n\n这里我要特别说明一点，如果坏字符在模式串里多次出现，那我们在计算xi的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略多。\n\n利用坏字符规则，BM算法在最好情况下的时间复杂度非常低，是O(n/m)。比如，主串是aaabaaabaaabaaab，模式串是aaaa。每次对比，模式串都可以直接后移四位，所以，匹配具有类似特点的模式串和主串的时候，BM算法非常高效。\n\n不过，单纯使用坏字符规则还是不够的。因为根据si-xi计算出来的移动位数，有可能是负数，比如主串是aaaaaaaaaa，模式串是baaa。不但不会向后滑动模式串，还有可能倒退。所以，BM算法还需要用到"好后缀规则"。\n\n\n# 好后缀规则\n\n好后缀规则实际上跟坏字符规则的思路很类似。看下面的图示。当模式串滑动到图中的位置的时候，模式串和主串有2个字符是匹配的，倒数第3个字符发生了不匹配的情况。\n\n\n\n这个时候该如何滑动模式串呢？当然，我们还可以利用坏字符规则来计算模式串的滑动位数，不过，我们也可以使用好后缀处理规则。两种规则到底如何选择，这边会稍后讲解。抛开这个问题，现在我们来看，好后缀规则是怎么工作的？\n\n我们把已经匹配的bc叫作好后缀，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。\n\n\n\n如果在模式串中找不到另一个等于{u}的子串，我们就直接将模式串，滑动到主串中{u}的后面，因为之前的任何一次往后滑动，都没有匹配主串中{u}的情况。\n\n\n\n不过，当模式串中不存在等于{u}的子串的时候，我们直接将模式串滑动到主串{u}的后面。这样做是否太过了？我们可以来看下面的这个例子。这里面bc是好后缀，尽管在模式串中没有另外一个相匹配的子串{u*}，但是如果我们将模式串移动到好后缀的后面，如图所示，那就会错过模式串和主串可以匹配的情况。\n\n\n\n如果好后缀在模式串中不存在可匹配的子串，那在我们一步一步往后滑动模式串的过程中，只要主串中的{u}与模式串有重合，那肯定就无法完全匹配。但是当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况。\n\n\n\n所以，针对这种情况，我们不仅要看好后缀在模式串中，是否有另一个匹配的子串，我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。\n\n所谓某个字符串s的后缀子串，就是最后一个字符跟s对齐的子串，比如abc的后缀子串就包括c,bc。所谓前缀子串，就是起始字符跟s对其的子串，比如abc的前缀子串有a,ab。我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的，假设是{v}，然后将模式串滑动到如图所示的位置。\n\n\n\n坏字符和好后缀的基本原理都讲完了，现在回答一下前面那个问题。当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？\n\n我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。\n\n\n# BM算法代码实现\n\n\n# BM算法的性能分析及优化\n\n我们先来分析BM算法的内存消耗。整个算法用到了额外的3个数组，其中bc数组的大小跟字符集大小有关，suffix数组和prefix数组的大小跟模式串长度m有关。\n\n如果我们处理字符集很大的字符串匹配问题，bc数组对内存的消耗就会比较多。因为好后缀和坏字符规则是独立的，如果我们运行的环境对内存要去苛刻，可以只使用好后缀规则，不使用坏字符规则，这样就可以避免bc数组过多的内存消耗。不过，单纯使用好后缀规则的BM算法效率就会下降一些了。\n\n实际上，我前面讲的BM算法是个初步版本。为了能更容易理解，有些复杂的优化这里没有将。基于目前讲的这个版本，在极端情况下，预处理计算suffix数组、 prefix 数组的性能会比较差。\n\n比如模式串是aaaaaaa这种包含很多重复的字符的模式串，预处理的时间复杂度就是O(m^2)。当然，大部分情况下，时间复杂度不会这么差。\n\n\n# 解答开篇&内容小结\n\n今天，我们讲了一种比较复杂的字符串匹配算法，BM算法。尽管复杂、难懂，但匹配的效率却很高，在实际的软件开发中，特别是一些文本编辑器中，应用比较多。\n\nBM算法的核心思想是，利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。BM算法构建的规则有两类，坏字符规则和好后缀规则。好后缀规则可以独立于坏字符规则使用。因为坏字符规则的实现比较耗内存，为了节省内存，我们可以只用好后缀规则来实现BM算法。\n\n\n# D60(2020/12/18) 字符串匹配\n\n上一节我们讲了BM算法，尽管它很复杂，也不好立即，但确实工程中非常常用的一种高效字符串匹配算法。有统计说，它是最高效、最常用的字符串匹配算法。不过，在所有的字符串匹配算法里，要说最知名的一种的话，那就非KMP算法莫属。很多时候，提到字符串匹配，我们首先想要的就是KMP算法。\n\n尽管在实际的开发中，我们几乎不大可能自己亲手实现一个KMP算法。但是，学习这个算法的思想，可以开拓眼界、锻炼下逻辑思维，也是极好的。\n\n实际上，KMP算法跟BM算法的本质是一样的。上一节，我们讲了好后缀和坏字符规则，今天，我们就来看下，如何借助于上一节的BM算分的讲解思路，更好地理解KMP算法？\n\n\n# KMP算法基本原理\n\nKMP算法的核心思想，跟上一节讲的BM算法非常接近。我们假设主串是a，模式串是b。在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候，我们希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况。\n\n上一节中我们讲解到了好后缀和坏字符。这里我们可以类比一下，在模式串和主串匹配的过程中，把不能匹配的那个字符仍然叫做坏字符，把已经匹配的那段字符串叫做好前缀。\n\n\n\n当遇到坏字符的时候，我们就要把模式串往后滑动，在滑动的过程中，只要模式串和好前缀有上下重合，前面几个字符的比较，就相当于拿好前缀的后缀子串，跟模式串的前缀子串在比较。可不可以不用一个字符一个字符地比较？\n\n\n\nKMP算法就是在试图寻找一种规律：在模式串和主串匹配的过程中，当遇到坏字符后，对于已经比对过的好前缀，能否找到一种规律，将模式串一次性滑动很多位？\n\n\n# D61(2020/12/18) Trie树\n\n搜索引擎的搜索关键词提示功能，为了方便快速输入，当我们在搜索引擎的搜索框中，输入要搜索的文字的某一部分的时候，搜索引擎就会自动弹出下拉框，里面是各种关键词提示。我们可以直接从下拉框中选择我们要搜索的东西，而不用把所有内容都输入进去，一定程度上节省了我们的搜索时间。\n\n像谷歌、百度这样的搜索引擎，它们的关键词提示功能非常全面和精准，肯定做了很多优化，但是万变不离其宗，底层最基本的原理就是今天要讲的这种数据结构：Trie树。\n\n\n# 什么是"Trie"树\n\nTrie树，也叫"字典树"。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。\n\n当然，这样一个问题可以有多种解决办法，比如散列表、红黑树，或者我们前面几节讲到的一些字符串匹配算法，但是，Trie树在这个问题的解决上，有它特有的优点。不仅如此，Trie树能解决的问题也不限于此。\n\n下面来看个简单的例子来说明一下。我们有6个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这6个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？\n\n这个时候，我们就可以先对这6个字符串做一下预处理，组织成Trie树的结构，之后每次查找，都是在Trie树中进行匹配查找。Trie树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。最后构造出来的就是下面这个图中的样子。\n\n其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串(注意：红色节点并不都是叶子节点)。\n\n为了让我们更容易理解Trie树是怎么构造出来的，这里画了一个Trie树构造的分解过程。构造过程的每一步，都相当于往Trie树中插入一个字符串。当所有字符串都插入完成之后，Trie树就构造好了。\n\n\n\n当我们在Trie树中查找一个字符串的时候，比如查找字符串"her"，那我们将要查找的字符串分隔成单个的字符h,e,r，然后从Trie树的根节点开始匹配。如图所示，绿色的路径就是在Trie树中匹配的路径。\n\n\n\n如果我们要查找的是字符串"he"呢？我们还用上面同样的方法，从根节点开始，沿着某条路径来匹配，如图所示，绿色的路径，是字符串"he"匹配的路径。但是，路径的最后一个节点"e"并不是红色的。也就是说，"he"是某个字符串的前缀子串，但并不能完全匹配任何字符串。\n\n\n\n\n# 如何实现一棵Trie树？\n\n现在来看下，如何用代码来实现一个Trie树。\n\n从刚刚Trie树的介绍来看，Trie树主要有两个操作，一个是将字符串集合构造成Trie树。这个过程分解开来的话，就是一个将字符串插入到Trie树的过程。另一个是在Trie树中查询一个字符串。\n\n了解了Trie树的两个主要操作之后，我们再来看下，如何存储一个Trie树？\n\n从前面的图中，我们可以看出，Trie树是一个多叉树。我们知道，二叉树中，一个节点的左右子节点是通过两个指针来存储的，如下所示Java代码。那对于多叉树来说，我们怎么存储一个节点的所有子节点的指针呢？\n\nclass BinaryTreeNode {\n  char data;\n  BinaryTreeNode left;\n  BinaryTreeNode right;  \n}\n\n\n1\n2\n3\n4\n5\n\n\n我们先介绍其中一个存储方式，也是经典的存储方式，大部分数据结构和算法书籍中都是这么讲解的。还记得我们前面讲到的散列表吗？借助散列表的思想，我们通过一个下标与字符一一映射的数组，来存储子节点的指针。这句话稍微有点抽象，不怎么好理解。\n\n假设我们的字符串只有a到z这26个小写字母，我们在数组中下标为0的位置，存储指向子节点a的指针，下标为1的位置存储指向子节点b的指针，以此类推，下标为25的位置，存储的是指向的子节点z的指针。如果某个字符的子节点不存在，我们就在对应的下标的位置存储null。\n\nclass TrieNode {\n  char data;\n  TrieNode children[26];\n}\n\n\n1\n2\n3\n4\n\n\n当我们在Trie树中查找字符串的时候，我们就可以通过字符的ASCII码减去"a"的ASCII码，迅速找到匹配的子节点的指针。比如，d的ASCII码减去a的ASCII码就是3，那子节点d的指针就存储在数组中下标为3的位置中。\n\n把上面的描述翻译成了代码如下。\n\npublic class Trie {\n  private TrieNode root = new TrieNode(\'/\'); // 存储无意义字符\n\n  // 往Trie树中插入一个字符串\n  public void insert(char[] text) {\n    TrieNode p = root;\n    for (int i = 0; i < text.length; ++i) {\n      int index = text[i] - \'a\';\n      if (p.children[index] == null) {\n        TrieNode newNode = new TrieNode(text[i]);\n        p.children[index] = newNode;\n      }\n      p = p.children[index];\n    }\n    p.isEndingChar = true;\n  }\n\n  // 在Trie树中查找一个字符串\n  public boolean find(char[] pattern) {\n    TrieNode p = root;\n    for (int i = 0; i < pattern.length; ++i) {\n      int index = pattern[i] - \'a\';\n      if (p.children[index] == null) {\n        return false; // 不存在pattern\n      }\n      p = p.children[index];\n    }\n    if (p.isEndingChar == false) return false; // 不能完全匹配，只是前缀\n    else return true; // 找到pattern\n  }\n\n  public class TrieNode {\n    public char data;\n    public TrieNode[] children = new TrieNode[26];\n    public boolean isEndingChar = false;\n    public TrieNode(char data) {\n      this.data = data;\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n在Trie树中，查找某个字符串的时间复杂度是多少？\n\n如果要在一组字符串中，频繁地查询某些字符串，用Trie树会非常高效。构建Trie树的过程，需要扫描所有的字符串，时间复杂度是O(n). n表示所有字符串的长度和。但是一旦构建成功之后，后续的查询操作会非常高效。\n\n每次查询的时候，如果要查询的字符串长度是k，那我们只需要比对大约k个节点，就能完成查询操作。跟原本那组字符串的长度和个数没有任何关系。所以说，构建好Trie树后，在其中查找字符串的时间复杂度是O(k)，k表示要查找的字符串的长度。\n\n\n# Trie树真的很耗内存吗？\n\n前面也讲了Trie树的实现，也分析了时间复杂度。现在可以知道，Trie树是一种非常独特的、高效的字符串匹配方法。但是，关于Trie树，有一种说法是："Trie树是非常耗内存的，用的是一种空间换时间的思路"。这是什么原因呢？\n\n刚刚我们在讲Trie树的实现的时候，讲到用数组来存储一个节点的子节点的指针。如果字符串中包含从a到z这26个字符，那每个节点都要存储一个长度为26的数组，并且每个数组元素要存储一个8字节指针(或者是4字节，这个大小跟CPU、操作系统、编译器等有关)。而且，即便一个节点只有很少的子节点，远小于26个，比如3，4个，我们也要维护一个长度为26的数组。\n\n前面讲过，Trie树的本质是避免重复存储一组字符串的相同前缀子串，但是现在每个字符(对应一个节点)的存储远远大于1个字节。按照我们上面举的例子，数组长度为26，每个元素是8字节，那么每个节点就会额外需要26*8=208个字节。而且这还是只包含26个字符的情况。\n\n如果字符串中不仅包含小写字母，还包含大写字母、数字、甚至是中文，那需要的存储空间就更多了。所以，也就是说，在某些情况下，Trie树不一定会节省存储空间。在重复的前缀并不多的情况下，Trie树不但不能节省内存，还有可能会浪费更多的内存。\n\n当然，我们不可否认，Trie树尽管有可能很浪费内存，但是确实非常高效。那为了解决这个内存问题，我们是否有其他方法呢？\n\n我们可以稍微牺牲一点查询的效率，将每个节点中的数组换成其他数据结构，来存储一个节点的子节点指针。用哪种数据结构呢？我们的选择其实有很多，比如有序数组、跳表、散列表、红黑树等。\n\n假设我们用有序数组，数组中的指针按照所指向的子节点中的字符的大小顺序排列。查询的时候，我们可以通过二分查找的方法，快速查找到某个字符应该匹配的子节点的指针。但是，在往Trie树中插入一个字符串的时候，我们为了维护数组中数据的有序性，就会稍微慢了点。\n\n替换成其他数据结构的思路是类似的。\n\n实际上，Trie树的变体有很多，都可以在一定程度上解决内存消耗的问题。比如，缩点优化，就是对只有一个子节点的节点，而且此节点不是一个串的结束节点，可以将此节点与子节点合并。这样可以节省空间，但却增加了编码难度。这里我就不展开了。\n\n\n\n\n# Trie树与散列表、红黑树的比较\n\n实际上，字符串的匹配问题，笼统上讲，其实就是数据的查找问题。对于支持动态数据高效操作的数据结构，例如散列表、红黑树、跳表等等。实际上，这些数据结构也可以实现在一组字符串中查找字符串的功能。我们选了你数据结构，散列表和红黑树，跟Trie树比较一下，看看它们各自的优缺点和应用场景。\n\n在刚刚讲的这个场景，在一组字符串中查找字符串，Trie树实际上表现得并不好。它对要处理的字符串有及其严格的要求。\n\n第一，字符串中包含的字符集不能太大。我们前面讲到，如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。\n\n第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。\n\n第三，如果要用Trie树解决问题，那我们就要自己从零开始实现一个Trie树，还要保证没有bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。\n\n第四，我们知道，通过指针串起来的数据库是不连续的，而Trie树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。\n\n综合这几点，针对在一组字符串中查找字符串的问题，我们在工程中，更倾向于用散列表或者红黑树。因为这两种数据结构，我们都不需要自己去实现，直接利用编译语言中提供的现成类库就行了。\n\n那Trie树是不是就没用了呢？实际上，Trie树是不适合精确匹配查找，这种问题更适合用散列表或红黑树来解决。Trie树比较适合的是查找前缀匹配的字符串。\n\n\n# 解答开篇\n\n如何利用Trie树，实现搜索关键词的提示功能？\n\n我们假设关键词库由用户的热门搜索关键词组成。我们将这个词库构建成一个Trie树。当用户输入其中某个单词的时候，把这个词作为一个前缀子串在Trie树中匹配。为了将解方便，我们假设词库只有hello、her、hi、how、so、see这6个关键词。当用户输入了字母h的时候，我们就把以h为前缀的hello、her、hi、how展示在搜索提示框内。当用户继续键入字母e的时候我们就把以he为前缀的hello、her展示在搜索提示框内。这就是搜索关键词提示的最基本的算法原理。\n\n\n\n不过，我将的都只是最基本的实现原理，实际上，搜索引擎的搜索关键词提示功能远非我们讲的这么简单。如果再稍微深入一点，我们就会想到，上面的解决问题遇到下面几个问题：\n\n * 我刚讲的思路是针对英文的搜索关键词提示，对于更加复杂的中文来说，词库中的数据又该如何构建成Trie树呢？\n * 如果词库中有很多关键词，在搜索提示的时候，用户输入关键词，作为前缀在Trie树中可以匹配的关键词也有很多，如何选择展示哪些内容呢？\n * 像Google这样的搜索引擎，用户单词拼写错误的情况下，google还是可以使用正确的拼写来做关键词提示，这个又是怎么做到的呢？\n\n实际上，Trie树的这个应用可以扩展到更加广泛的一个应用上，就是自动输入补全，比如输入法自动补全功能、IDE代码编辑器自动补全功能、浏览器网址输入的自动补全功能等等。\n\n\n# 内容小结\n\n今天我们讲了一种特殊的树，Trie树。Trie树是一种解决字符串快速匹配问题的数据结构。如果用来构建Trie树的这一组字符串中，前缀重复的情况不是很多，那Trie树这种数据结构总体上来讲是比较耗费内存的，是一种空间换时间的解决问题思路。\n\n尽管比较耗费内存，但是对内存不敏感或者内存消耗在接受范围内的情况下，在Trie树中做字符串匹配还是比较高效的，时间复杂度是O(k)，k表示要匹配的字符串的长度。\n\n但是，Trie树的优势并不在于，用它来做动态集合数据的查找，因为，这个工作完全可以用更加合适的散列表或红黑树来替代。Trie树最有优势的是查找前缀匹配的字符串，比如搜索引擎中的关键词提示功能这个场景，就比较适合用它来解决，也是Trie树比较经典的应用场景。\n\n\n# D62(2020/12/21) AC自动机\n\n今天要学习的内容是，如何用多模式串匹配实现敏感词过滤功能。\n\n很多支持用户发表的文本内容的网站，比如BBS，大都会有敏感词过滤功能，用来过滤掉用户输入的一些淫秽、反动、谩骂等内容。\n\n实际上，这些功能最基本的原理就是字符串匹配算法，也就是通过维护一个敏感词的字典，当用户输入一段文字内容之后，通过字符串匹配算法，来查找用户输入的这段文字，是否包含敏感词。如果有，就用星号替换掉。\n\n我们前面讲过的好几种字符串匹配算，它们都可以处理这个问题。但是，对于访问量巨大的网站来说，比如淘宝，用户每天的评论数有几亿、甚至几十亿。这个时候，我们对敏感词过滤系统的性能要求就要很高。毕竟，我们也不想，用户输入内容之后，要等几秒才能发送出去？我们也不想，为了这个功能耗费过多的机器。那如何才能实现一个高性能的敏感词过滤系统呢？这就是今天所说的多模式串匹配算法。\n\n\n# 基于单模式串和Trie树实现的敏感词过滤\n\n我们前面几节讲了好几种字符串匹配算法，有BF算法、RK算法、BM算法、KMP算法，还有Trie树。前面四种算法都是单模式匹配算法，只有Trie树是多模式串匹配算法。\n\n我们讲解过，单模式串匹配算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串。多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。\n\n尽管，单模式串匹配算法也能完成多模式串的匹配工作。例如开篇的思考题，我们可以针对每个敏感词，通过单模式串匹配算法(比如KMP算法)与用户输入的文字内容进行匹配。但是，这样做的话，每个匹配过程都需要扫描一遍用户输入的内容。整个过程下来就要扫描很多遍用户输入的内容。如果敏感词很多，比如几千个，并且用户输入的内容很长，加入有上千个字符，那我们就需要扫描几千遍这样的输入内容。很显然，这种处理思路比较低效。\n\n与单模式匹配算法相比，多模式匹配算法在这个问题的处理上就很高效了。它只需要扫描一遍主串，就能在主串中一次性查找多个模式串是否存在，从而大大提高匹配效率。我们知道，Trie树就是一种多模式匹配算法。那如何用Trie树来实现敏感词过滤功能呢？\n\n我们可以对敏感词字典进行预处理，构建成Trie树结构。这个预处理的操作只需要做一次，如果敏感字典动态更新了，比如删除、添加了一个敏感词，那我们只需要动态更新一下Trie树就可以了。\n\n当用户输入一个文本内容后，我们把用户输入的内容作为主串，从第一个字符(假设是字符C)开始，在Trie树中匹配。当匹配到Trie树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符C的下一个字符开始，重新在Trie树中匹配。\n\n基于Trie树的这种处理方法，有点类似单模式串匹配的BF算分。我们知道，单模式串匹配算法中，KMP算法对BF算法进行改进，引入了next数组，让匹配失败时，尽可能将模式串往后多滑动几位。借鉴单模式串的优化改进方法，能否对多模式串Trie树进行改进，进一步提高Trie树的效率呢？这就要用到AC自动机算法了。\n\n\n# 经典的多模式串匹配算法：AC自动机\n\n其实，Trie树跟AC自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟KMP算法之间的关系一样，只不过前者针对的是多模式串而已。所以，AC自动机实际上就是在Trie树之上，加了类似KMP的next数组，只不过此处的next数组是构建在树上罢了。\n\n\n# 解答开篇\n\n\n# 内容小结\n\n\n# D63(2020/12/22) 贪心算法\n\n接下来的几节，我们会讲解几种更加基本的算法。它们分别是贪心算法、分治算法、回溯算法、动态规划。更加确切地说，它们应该是算法思想，并不是具体的算法，常用来指导我们设计具体的算法和编码等。\n\n贪心、分治、回溯、动态规划这4个算法思想，原理解释起来都很简单，但是要真正掌握且灵活应用，并不是件容易的事情。所以，接下来的这4个算法思想的讲解，我依旧不会长篇大论地去讲理论，而是结合具体的问题，让我们自己感觉这些算法是怎么工作的，是如何解决问题的，让我们在问题中体会这些算法的本质。我觉得，这比单纯记忆原理和定义要更有价值。\n\n今天，我们先来学习一下贪心算法。贪心算法有很多经典的应用，比如霍夫曼编码、Prim和最小生成树算法、还有单源最短路径算法。今天讲解的是在霍夫曼编码中，是如何利用贪心算法来实现对数据压缩编码，有效节省数据存储空间的。\n\n\n# 如何理解"贪心算法"\n\n假设我们有一个可以容纳100kg物品的背包，可以装各种物品。我们有以下5种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？\n\n\n\n实际上，这个问题很简单，没错，我只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆，所以，我们可以往背包里装20kg黑豆、30kg绿豆、50kg红豆。\n\n第一步，当我们看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。\n\n类比到刚刚的例子，限制值就是重量不能超过100kg，期望值就是物品的总价值。这组数据就是5种豆子。我们从中选出一部分，满足重量不超过100kg，并且总价值最大。\n\n第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。\n\n类比到刚刚的例子，我们每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。\n\n第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来看，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。\n\n实际上，用贪心算法解决问题的思路，并不总能给出最优解。\n\n这里来举一个例子。在一个有权图中，我们从顶点S开始，找一条到顶点T的最短路径(路径中边的权值和最小)。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点T。按照这种思路，我们求出的最短路径是S->A->E->T，路径长度是1+4+4=9\n\n\n\n但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径S->B->D->T 才是最短路径，因为这条路径的长度是2+2+2 =6。为什么贪心算法在这个问题上不工作了呢？\n\n在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果我们第一步从顶点S走到顶点A，那接下来面对的顶点和边，跟第一步从顶点S走到顶点B，是完全不同的。所以，即便我们第一步选择最优的走法(边最短)，但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。\n\n\n# 贪心算法实战分析\n\n对于贪心算法，如果死抠理论的话，确实很难理解透彻。掌握贪心算法的关键是多练习。下面来分析几个具体的例子，帮助来深入理解贪心算法。\n\n\n# 分糖果\n\n我们有m个糖果和n个汉字。我们现在要把糖果分给这些孩子吃但是糖果少，孩子多(m<n)，所以糖果只能分配给一部分孩子。\n\n每个糖果的大小不等，这m个糖果的打下分别是s1,s2,s3, ....., sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这n个孩子对糖果大小的需求分别是g1,g2,g3, .... , gn。\n\n我的问题是，如何分配糖果，能尽可能满足最多数量的孩子？\n\n我们可以把这个问题抽象成，从n个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数(期望值)是最大的。这个问题的限制值就是糖果个数m。\n\n现在来看如何用贪心算法来解决。对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。\n\n我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。\n\n\n# 钱币找零\n\n这个问题在我们的日常生活中更加普遍。假设我们有1元，2元，5元，10元，20元，50元，100元这些面额的纸币，它们的张数分别是c1、c2、c5、c20、c50、c100。我们现在要用这些钱来支付K元，最少用多少张纸币呢？\n\n在生活中，我们肯定要先用面值最大的来支付，如果不够，就继续用更小一点的面值的，以此类推，最后剩下的用1元来补齐。\n\n在贡献相同期望值(纸币数目)的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。直觉告诉我们，这种处理方法就是最好的。实际上，要严谨地证明这种贪心算法的正确性，需要比较复杂的、有技巧的数学推导。\n\n\n# 区间覆盖\n\n假设我们有n个区间，区间的起始端点和结束端点分别是[l1,r1]，[l2,r2]，[l3,r3]，...., [ln,rn]。我们从这n个区间中选出一部分区间，这部分区间满足两两不想交(端点相交的情况不算相交)，最多能选出多少个区间呢？\n\n\n\n这个问题的处理思路稍微不是那么好懂，不过，我建议你最好能弄懂，因为这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。\n\n这个问题的解决思路是这样的：我们假设这n个区间中最左端点是lmin，最右端点是rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将[lmin, rmax]覆盖上。我们按照起始端点从小到大的顺序对这n个区间排序。\n\n我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。\n\n\n\n\n# 解答开篇\n\n现在我们来看下开篇的问题，如何用贪心算法实现霍夫曼编码？\n\n假设我有一个包含1000个字符的文件，每个字符占1个byte (1byte=8bits)，存储这1000个字符就一共需要8000bits，那有没有更加节省空间的存储方式呢？\n\n假设我们通过统计分析发现，这1000个字符中只包含6种不同字符，假设它们分别是a、b、c、d、e、f。而3个二进制位(bit)就可以表示8个不同的字符，所以，为了尽量减少存储空间，每个字符我们用3个二进制位来表示。那存储这1000个字符只需要3000bits就可以了，比原来的存储方式节省了很多空间。不过，还有没有更加节省空间的存储方式呢？\n\na(000)、b(001)、c(010)、d(011)、e(100)、f(101)\n\n\n1\n\n\n霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在20% ~ 90%之间。\n\n霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码视图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？ 根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长的一些编码。\n\n对于等长的编码来说，我们解压缩起来很简单。比如刚才那个例子中，我们用3个bit表示一个字符。在解压缩的时候，我们每次从文本中读取3位二进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取1位还是2位、3位等等来解压缩呢？这个问题就导致霍夫曼编码解压缩起来比较复杂。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。\n\n\n\n假设这6个字符出现的频率从高到低依次是a、b、c、d、e、f。我们把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀，在解压缩的时候，我们每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这1000个字符只需要2100bits就可以了。\n\n\n\n尽管霍夫曼编码的思想并不难理解，但是如何根据字符出现的频率的不同，给不同的字符进行不同长度的编码呢？这里的处理稍微有些技巧。\n\n我们把每个字符看作一个节点，并且附带着把频率放到优先级队列中。我们从队列中取出频率最小的两个节点A、B，然后新建一个节点C，把频率设置为两个节点的频率之和，并把这个新节点C作为A、B的父节点。最后再把C节点放入到优先级队列中。重复这个过程，直到队列中没有数据。\n\n\n\n现在，我们给每一条边加上画一个权值，指向左子节点的边我们统统标记为0，指向右子节点的边，我们统统标记为1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。\n\n\n\n\n# 内容小结\n\n今天我们学习了贪心算法。\n\n实际上，贪心算法适用的场景比较有限。这种算法思想更多的是指导设计基础算法。比如最小生成树算法、单源最短路径算法，这些算法都用到了贪心算法。从我个人的学习经验来讲，不要刻意去记忆贪心算法的原理，多练习才是最有效的学习方法。\n\n贪心算法的最难的一块是如何将要解决的问题抽象成贪心算法模型，只要这一步搞定之后，贪心算法的编码一般都很简单。贪心算法解决问题的正确性虽然很多时候都看起来是显而易见的，但是要严谨地证明算法能够得到最优解，并不是件容易的事。所以，很多时候，我们只需要多举几个例子，看一下贪心算法的解决方案是否真的能得到最优解就可以了。\n\n\n# D64(2020/12/23) 分治算法\n\nMapReduce是谷歌大数据处理的三驾马车之一，另外两个是GFS和Bigtable。它在倒排索引、PageRank计算、网页分析等搜索引擎相关的技术中都有大量的应用。\n\n尽管开发一个MapReduce看起来很高深，感觉跟我们遥不可及。实际上，万变不离其宗，它的本质就是我们今天要学的这种算法思想，分治算法。\n\n\n# 如何理解分治算法？\n\n为什么说MapRedue的本质就是分治算法呢？我们先来看，什么是分治算法？\n\n分治算法的核心思想其实就是四个字，分而治之，也就是将原问题划分成n个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。\n\n这个定义看起来有点难类似递归的定义。关于分治和递归的区别，我们在排序(下)的时候讲过，分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现呢。分治算法的递归实现中，每一层递归都会涉及这样三个操作：\n\n * 分解：将原问题分解成一系列子问题；\n * 解决：递归地求解各个子问题，若子问题足够小，则直接求解；\n * 合并：将子问题的结果合并成原问题。\n\n分治算法能解决的问题，一般需要满足下面这几个条件：\n\n * 原问题与分解成的小问题具有相同的模式；\n * 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；\n * 具有分解终止条件，也就是说，当问题足够小时，可以直接求解；\n * 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。\n\n\n# 分治算法应用举例分析\n\n理解分治算法的原理并不难，但是要想灵活应用并不容易。所以，接下来，我们会用分治算法解决我们在讲排序的时候涉及的一个问题，加深我们对分治算法的理解。\n\n还记得我们在排序算法里讲到的数据的有序度、逆序度的概念吗？当时我们讲到，我们用有序度来表示一组数据的有序程度，用逆序度表示一组数据的无序程度。\n\n假设我们有n个数据，我们期望数据从小到大排序，那完全有序的数据的有序度就是n(n-1)/2，逆序度等于0；相反，倒序排列的数据的有序度就是0，逆序度是n(n-1)/2。除了这两种极端情况外，我们通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。\n\n\n\n现在的问题是，如何编程求出一组数据的有序对个数或逆序对个数呢？因为有序对个数和逆序对个数的求解方式是类似的，所以我们只需要思考逆序对个数的求解方法。\n\n最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的k值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是O(n^2)。那有没有更加高效的处理方法呢？\n\n我们用分治算法来试试。我们套用分治的思想来求数组A的逆序对个数。我们可以将数组分成前后两半A1和A2，分别计算A1和A2的逆序对个数K1和K2，然后再计算A1与A2之间的逆序对个数K3。那数组A的逆序对个数就等于K1+K2+K3。\n\n我们前面讲过，使用分治算法其中一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了。那回到这个问题，如何快速计算出两个子问题A1与A2之间的逆序对个数呢？\n\n这里就要借助归并排序算法了。\n\n归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。实际上，在这个合并的过程中，我们就可以计算这两个小数组的逆序对个数了。每次合并操作，我们都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。\n\n\n\n具体的代码如下：\n\nprivate int num = 0; // 全局变量或者成员变量\n\npublic int count(int[] a, int n) {\n  num = 0;\n  mergeSortCounting(a, 0, n-1);\n  return num;\n}\n\nprivate void mergeSortCounting(int[] a, int p, int r) {\n  if (p >= r) return;\n  int q = (p+r)/2;\n  mergeSortCounting(a, p, q);\n  mergeSortCounting(a, q+1, r);\n  merge(a, p, q, r);\n}\n\nprivate void merge(int[] a, int p, int q, int r) {\n  int i = p, j = q+1, k = 0;\n  int[] tmp = new int[r-p+1];\n  while (i<=q && j<=r) {\n    if (a[i] <= a[j]) {\n      tmp[k++] = a[i++];\n    } else {\n      num += (q-i+1); // 统计p-q之间，比a[j]大的元素个数\n      tmp[k++] = a[j++];\n    }\n  }\n  while (i <= q) { // 处理剩下的\n    tmp[k++] = a[i++];\n  }\n  while (j <= r) { // 处理剩下的\n    tmp[k++] = a[j++];\n  }\n  for (i = 0; i <= r-p; ++i) { // 从tmp拷贝回a\n    a[p+i] = tmp[i];\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n但是，如果我告诉你可以借助归并排序算法来解决，那我们就应该要想到如何改造归并排序，来求解这个问题了。\n\n关于分治算法，这里还有两道比较经典的问题，我们可以自己练习一下。\n\n * 二维平面上有n个点，如何快速计算出两个距离最近的点对？\n * 有两个n*n的矩阵A，B，如何快速求解两个矩阵的乘积C=A*B ？\n\n\n# 分治思想在海量数据处理中的应用\n\n分治算法思想的应用是非常广泛的，并不仅局限于指导编程和算法设计。它还经常用在海量数据处理的场景中。我们前面讲的数据结构和算法，大部分都是基于内存存储和单机处理。但是，如果要处理的数据量非常大，没法一次性放到内存中，这个时候，这些数据结构和算法就无法工作了。\n\n比如，给10GB的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有10GB，而我们的机器的内存可能只有2、3GB这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。\n\n要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。我们可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据结合合并成大数据集合。实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或多机处理，加快处理的速度。\n\n比如刚刚举的那个例子，给10GB的订单排序，我们就可以先扫描一遍订单，根据订单的金额，将10GB的文件划分为几个金额区间。比如订单净额为1到100元的放到一个小文件，101到200之间的放到另一个文件，以此类推。这样每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终的10GB订单数据了。\n\n如果订单数据存储在类似GFS这样的分布式系统上，当10GB的订单被划分成多个小文件的时候，每个文件可以并行加载到多台机器上处理，最后再将结果合并在一起，这样并行处理的速度也加快了很多。不过，这里有一个点要注意，就是数据的存储与计算所在的机器是同一个或者在网络中靠的很近(比如一个局域网内，数据存取速度很快)，否则就会因为数据访问的速度，导致整个处理过程不但不会变快，反而有可能变慢。\n\n\n# 解答开篇\n\n这里我们来看下，为什么说MapReduce的本质就是分治思想呢？\n\n我们刚刚举的订单的例子，数据有10GB大小，可能给你的感受还不强烈。那如果我们要处理的数据是1T、10T、100T这样子的，那一台机器处理的效率肯定是非常低的。而对于谷歌搜索引擎来说，网页爬取、清洗、分析、分词、计算权重、倒排索引等等各个环节中，都会面对如此海量的数据(比如网页)。所以，利用集群并行处理显然是大势所趋。\n\n一台机器过于低效，那我们就把任务拆分到多台机器上来处理。如果拆分之后的小任务之间互不干扰，独立计算，最后再将结果合并。这不就是分治思想吗？\n\n实际上，MapReduce框架只是一个任务调度器，底层依赖GFS来存储数据，依赖Borg管理机器。它从GFS中拿数据，交给Borg中的机器执行，并且时刻监控机器执行的进度，一旦出现机器宕机、进度卡壳等，就重新从Borg中调度一台机器执行。\n\n尽管MapReduce的模型非常简单，但是在谷歌内部引用非常广泛。它除了可以用来处理这种数据与数据之间存在关系的任务，比如MapReduce的经典例子，就是统计文件中单词出现的频率。除此之外，它还可以用来处理数据与数据之间没有关系的任务，比如对网页分析、分词等，每个网页可以独立的分析、分词，而这两个网页之间并没有关系。网页几十亿、上百亿，如果单机处理，效率低下，我们就可以利用MapReduce提供的高可靠、高性能、高容错的并行计算框架，并行地处理这几十亿、上百亿的网页。\n\n\n# 内容小结\n\n今天我们讲解了一种应用非常广泛的算法思想，分治算法。\n\n分治算法用四个字概括就是"分而治之"，将原问题划分成n个规模较小而结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。这个思想非常简单，好理解。\n\n今天我们讲了两种分治算法的典型的应用场景，一个是用来指导编码，降低问题求解的时间复杂度，另一个是解决海量数据处理问题。比如MapReduce本质上就是利用了分治思想。\n\n\n# D65(2020/12/25) 回溯算法\n\n在之前我们学习图的过程中，提到了深度优先搜索算法利用的是回溯算法思想。这个算法思想非常简单，但是应用内非常广泛，它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。\n\n除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1背包、图的着色、旅行商问题、全排列等等。既然应用如此广泛，我们今天就来学习一下这个算法思想，看看它是如何知道我们解决问题的？\n\n\n# 如何理解"回溯算法"？\n\n在我们的一生中，会遇到很多重要的岔路口。在岔路口，每个选择都会影响我们今后的人生。有的人在每个岔路口都能做出最正确的选择，最后生活、事业都达到了一个很高的高度；而有的人一路选错，最后碌碌无为。如果人生可以量化，那如何才能在岔路口做出最正确的选择，让自己的人生"最优"呢？\n\n我们可以借助前面学过的贪心算法，在每次面对岔路口的时候，都做出看起来最优的选择，期望这一组选择可以使得我们的人生达到"最优"。但是，我们前面也讲过，贪心算法并不一定能得到最优解。那有没有什么办法能得到最优解呢？\n\n笼统地讲，回溯算法很多时候都应用在"搜索"这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。\n\n回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候(不符合期望的解)，就回退到上一个岔路口，另选一种走法继续走。\n\n理论的东西还是过于抽象，老规矩，这里还是举例说明一下。这里举一个经典的回溯的例子，八皇后的问题。\n\n我们有一个8*8的棋盘，希望往里放8个棋子(皇后)，每个棋子所在的行、列、对角线都不能有另一个棋子。可以看下面的图示，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。\n\n\n\n我们把这个问题划分成8个阶段，依次将8个棋子放到第一行、第二行、第三行....第八行。在放置的过程中，我们不停地检查当前放法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种放法，继续尝试。\n\n回溯算法非常适合用递归代码实现，所以，我把八皇后的算分翻译成了代码。我在代码里添加了详细的注释，可以对比看看。\n\nint[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列\npublic void cal8queens(int row) { // 调用方式：cal8queens(0);\n  if (row == 8) { // 8个棋子都放置好了，打印结果\n    printQueens(result);\n    return; // 8行棋子都放好了，已经没法再往下递归了，所以就return\n  }\n  for (int column = 0; column < 8; ++column) { // 每一行都有8中放法\n    if (isOk(row, column)) { // 有些放法不满足要求\n      result[row] = column; // 第row行的棋子放到了column列\n      cal8queens(row+1); // 考察下一行\n    }\n  }\n}\n\nprivate boolean isOk(int row, int column) {//判断row行column列放置是否合适\n  int leftup = column - 1, rightup = column + 1;\n  for (int i = row-1; i >= 0; --i) { // 逐行往上考察每一行\n    if (result[i] == column) return false; // 第i行的column列有棋子吗？\n    if (leftup >= 0) { // 考察左上对角线：第i行leftup列有棋子吗？\n      if (result[i] == leftup) return false;\n    }\n    if (rightup < 8) { // 考察右上对角线：第i行rightup列有棋子吗？\n      if (result[i] == rightup) return false;\n    }\n    --leftup; ++rightup;\n  }\n  return true;\n}\n\nprivate void printQueens(int[] result) { // 打印出一个二维矩阵\n  for (int row = 0; row < 8; ++row) {\n    for (int column = 0; column < 8; ++column) {\n      if (result[row] == column) System.out.print("Q ");\n      else System.out.print("* ");\n    }\n    System.out.println();\n  }\n  System.out.println();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 两个回溯算法的经典应用\n\n回溯算法的理论知识很容易弄懂。不过，对于新手来说，比较难的是用递归来实现。所以，我们再通过两个例子，来练习一下回溯算法的应用和实现。\n\n\n# 0-1背包\n\n0-1背包是非常经典的算法问题，很多场景都可以抽象成这个问题模型。这个问题的经典解法是动态规划，不过还有一种简单但没有那么高效的解法，那就是今天讲的回溯算法。\n\n0-1背包问题有很多变体，这里介绍一种比较基础的。我们有一个背包，背包总的承载重量是Wkg。现在我们有n个物品，每个物品的重量不等，并且不可分割。我们现在期望选择几件物品，装载到背包中。在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大？\n\n实际上，背包问题我们在贪心算法那一节，已经讲过一个了，不过那里讲的物品是可以分割的，我可以装某个物品的一部分到背包里面。今天讲的这个背包问题，物品是不可分割的，要么装要么不装，所以叫0-1背包问题。显然，这个问题已经无法通过贪心算法来解决了。我们现在来看看，用回溯算法如何来解决。\n\n对于每个物品来说，都有两种选择，装进背包或不装进背包。对于n个物品来说，总的装法就有2^n种，去掉总重量超过Wkg的，从剩下的装法中选择总重量最接近Wkg的。不过，我们如何才能不重复地穷举出这2^n种装法呢？\n\n这里就可以用回溯的方法。我们可以把物品依次排列，整个问题就分解为了n个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或不装进去，然后再递归地处理剩下的物品。\n\n这里还稍微用到了一点搜索减枝的技巧，就是当发现已经选择的物品的总量超过1Wkg之后，我们就停止继续探测剩下的物品。\n\npublic int maxW = Integer.MIN_VALUE; //存储背包中物品总重量的最大值\n// cw表示当前已经装进去的物品的重量和；i表示考察到哪个物品了；\n// w背包重量；items表示每个物品的重量；n表示物品个数\n// 假设背包可承受重量100，物品个数10，物品重量存储在数组a中，那可以这样调用函数：\n// f(0, 0, a, 10, 100)\npublic void f(int i, int cw, int[] items, int n, int w) {\n  if (cw == w || i == n) { // cw==w表示装满了;i==n表示已经考察完所有的物品\n    if (cw > maxW) maxW = cw;\n    return;\n  }\n  f(i+1, cw, items, n, w);\n  if (cw + items[i] <= w) {// 已经超过可以背包承受的重量的时候，就不要再装了\n    f(i+1,cw + items[i], items, n, w);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 正则表达式\n\n看懂了0-1背包问题，我们再来看另外一个例子，正则表达式匹配。\n\n对于一个开发工程师来说，正则表达式应该不会陌生。在平时的开发中，或多或少都应该用过。实际上，正则表达式里最重要的一种算法思想就是回溯。\n\n正则表达式中，最重要的就是通配符，通配符结合在一起，可以表达非常丰富的语义。为了方便讲解，我们假设正则表达式中只包含"*"和"?"这两种通配符，并且对这两个通配符的语义稍微做些改变，其中，"*"匹配任意多个(大于等于0个)任意字符，"?"匹配零个或者一个任意字符。基于以上背景假设，我们看下，如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？\n\n我们依次考察正则表达式的每个字符，当是非通配符的时候，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。\n\n如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口，比如"*"有多种匹配方案，可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。\n\n有了前面的基础，这个问题就好懂多了。可以详细看下下面的代码。\n\npublic class Pattern {\n  private boolean matched = false;\n  private char[] pattern; // 正则表达式\n  private int plen; // 正则表达式长度\n\n  public Pattern(char[] pattern, int plen) {\n    this.pattern = pattern;\n    this.plen = plen;\n  }\n\n  public boolean match(char[] text, int tlen) { // 文本串及长度\n    matched = false;\n    rmatch(0, 0, text, tlen);\n    return matched;\n  }\n\n  private void rmatch(int ti, int pj, char[] text, int tlen) {\n    if (matched) return; // 如果已经匹配了，就不要继续递归了\n    if (pj == plen) { // 正则表达式到结尾了\n      if (ti == tlen) matched = true; // 文本串也到结尾了\n      return;\n    }\n    if (pattern[pj] == \'*\') { // *匹配任意个字符\n      for (int k = 0; k <= tlen-ti; ++k) {\n        rmatch(ti+k, pj+1, text, tlen);\n      }\n    } else if (pattern[pj] == \'?\') { // ?匹配0个或者1个字符\n      rmatch(ti, pj+1, text, tlen);\n      rmatch(ti+1, pj+1, text, tlen);\n    } else if (ti < tlen && pattern[pj] == text[ti]) { // 纯字符匹配才行\n      rmatch(ti+1, pj+1, text, tlen);\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 内容小结\n\n回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。\n\n\n# D66(2020/12/26) 动态规划\n\n淘宝的"双十一"购物节有各种促销活动，比如"满200减50元"。假设购物车中有n个(n>100)想买的商品，她希望从里面选几个，在凑满满减条件的前提下，让选出来的商品价格和最大程度低接近满减条件(200元)，这样就可以极大限度地"薅羊毛"。\n\n要想高效地解决这个问题，就要用到我们今天讲的动态规划。\n\n\n# 动态规划学习路线\n\n动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。不过，它也是出了名的难学。它的主要学习难点跟递归类似，那就是，求解问题的过程不太符合人类常规的思维方式。对于新手来说，要想入门确实不容易。不过，等我们掌握了之后，就会发现，实际上并没有想象中那么难。\n\n为了让我们更容易理解动态规划，这里分了三节来讲解。这三节分别是，初识动态规划、动态规划理论、动态规划实战。\n\n第一节，我们会通过两个非常经典的动态规划问题模型，来展示我们为什么需要动态规划，以及动态规划解题方法是如何演化过来的。实际上，我们只要掌握了这两个例子的解决思路，对于其他很多动态规划问题，我们都可以套用类似的思路来解决。\n\n第二节，我们会总结动态规划适合解决的问题的特征，以及动态符合解题思路。除此之外，我们还会将贪心、分治、回溯、动态规划这四种算法思想放在一起，对比分析它们各自的特点以及适用的场景。\n\n第三节，我们会讲解如何应用第二节中讲的动态规划理论知识，实战解决三个非常经典的动态规划问题，加深对理论的理解。弄懂了这三节中的例子，对于动态规划这个知识点，我们就算是入门了。\n\n\n# 0-1背包问题\n\n我们在讲贪心算法、回溯算法的时候，多次讲到背包问题。现在，我们依旧拿这个问题来举例。\n\n关于这个问题，我们上一节讲了回溯的解决方法，也就是穷举搜索所有的可能的装法，然后找出满足条件的最大值。不过，回溯算法的复杂度比较高，是指数级别的。那有没有什么规律，可以有效降低时间复杂度呢？\n\n// 回溯算法实现。注意：我把输入的变量都定义成了成员变量。\nprivate int maxW = Integer.MIN_VALUE; // 结果放到maxW中\nprivate int[] weight = {2，2，4，6，3};  // 物品重量\nprivate int n = 5; // 物品个数\nprivate int w = 9; // 背包承受的最大重量\npublic void f(int i, int cw) { // 调用f(0, 0)\n  if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了\n    if (cw > maxW) maxW = cw;\n    return;\n  }\n  f(i+1, cw); // 选择不装第i个物品\n  if (cw + weight[i] <= w) {\n    f(i+1,cw + weight[i]); // 选择装第i个物品\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n规律是不是不好找？那我们就举了例子、画个图看看。我们假设背包的最大承载重量是9。我们有5个不同的物品，每个物品的重量分别是2，2，4，6，3。如果我们把这个例子的回溯求解过程，用递归树画出来，就是下面这个样子。\n\n\n\n递归树中的每个节点表示一种状态，我们用(i,cw)来表示。其中，i表示将要决策第几个物品是否装入背包，cw表示当前背包中物品的总重量。\n\n\n# 内容小结\n\n从今天讲解的例子中来看，应该能发现，大部分动态规则能解决的问题，都可以通过回溯算法来执行，只不过回溯算法解决起来效率比较地，时间复杂度是指数级的。动态规划算法，在执行效率方面，要高很多。尽管执行效率提高了，但是动态规划的空间复杂度也提高了，所以，很多时候，我们会说，动态规划是一种空间换时间的算法思想。\n\n\n# D67(2020/12/26) B+树索引\n\n数据库索引是如何实现的呢？底层使用的是什么数据结构和算法呢？\n\n\n# 算法解析\n\n\n# 解决问题的前提是定义清楚问题\n\n如何定义清楚问题呢？除了对问题进行详细的调研，还有一个办法，那就是，通过对一些模糊的需求进行假设，来限定要解决的问题的范围。\n\n如果我们对数据库的操作非常了解，针对我们现在这个问题，我们就能把索引的需求定义得非常清楚。但是，对于大部分软件工程师来说，我们可能只了解一小部分常用的SQL语句，所以，这里我们假设要解决的问题，只包含这样两个常用的需求：\n\n * 根据某个值查找数据，比如 select * from user where id =1234;\n * 根据区间值来查找某些数据，比如select * from user where id > 1234 and id < 2345;\n\n在执行效率方面，我们希望通过索引，查询数据的效率尽可能地高；在存储空间方面，我们希望索引不要消耗太多的内存空间。\n\n\n# 尝试用学过的数据结构解决这个问题\n\n问题的需求大致定义清楚了，我们现在回想一下，能否利用已经学习过的数据结构解决这个问题呢？支持快速查询、插入等操作的动态数据结构，我们已经学习过散列表、平衡二叉树、跳表。\n\n我们先来看散列表。散列表的查询性能很好，时间复杂度是O(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。\n\n我们再来看平衡二叉查找树。尽管平衡二叉查找树查询的性能也很高，时间复杂度是O(logn)。而且，对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但是这个仍然不足以支持按照区间快速查找数据。\n\n我们再来看跳表。跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是O(logn)。并且，跳表也支持按照区间快速地查找数据。我们只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止，这期间遍历得到的数据就是满足区间值的数据。\n\n\n\n这样看来，跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫做B+树。不过，它是通过二叉查找树演化过来的，而非跳表。为了还原发明B+树的整个思考过程，所以，接下来，我们还要从二叉查找树讲起，看它是如何一步一步被改造成B+树的。\n\n\n# 改造二叉查找树来解决这个问题\n\n为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。经过改造之后的二叉树，就像图中这样，看起来很像跳表。\n\n\n\n改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。\n\n\n\n但是，我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。\n\n比如，我们给一亿个数据构建二叉查找数索引，那索引中会包含大约1亿个节点，每个节点假设占用16个字节，那就需要大约1GB的内存空间。给一张表建立索引，我们需要1GB的内存空间。如果我们要给10张表建立索引，那就内存的需求是无法满足的。如何解决这个索引占用太多内存的问题呢？\n\n我们可以借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。\n\n这种将索引存储在硬盘中的方案，尽管减少了内存消耗，但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。\n\n二叉查找树，经过改造之后，支持区间查找的功能就实现了。不过，为了节省内存，如果把树存储在硬盘中，那么每个节点的读取(或者访问)，都对应一次磁盘IO操作。树的高度就等于每次查找数据时磁盘IO操作的次数。\n\n我们前面讲到，比起内存读写操作，磁盘IO操作非常耗时，所以我们优化的重点就是尽量减少磁盘IO操作，也就是，尽量降低树的高度。那如何降低树的高度呢？\n\n我们来看下，如果我们把索引构建成m叉树，高度是不是比二叉树要小呢？如图所示，给16个数据构建二叉树索引，树的高度是4，查找一个数据，就需要4个磁盘IO操作(如果根节点存储在内存中，其他节点存储在磁盘中)，如果对16个数据构建五叉树索引，那高度只有2，查找一个数据，对应只需要2次此案操作。如果m叉树中的m是100，那对一亿个数据构建索引，树的高度也只有3，最多只要3次磁盘IO就能获取到数据。磁盘IO变少了，查找数据的效率也就提高了。\n\n\n\n\n\n如果我们将m叉树实现B+树索引，用代码实现出来，就是下面这个样子(假设我们给int类型的数据库字段添加索引，所以代码中的keywords是int类型的):\n\n/**\n * 这是B+树非叶子节点的定义。\n *\n * 假设keywords=[3, 5, 8, 10]\n * 4个键值将数据分为5个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF)\n * 5个区间分别对应：children[0]...children[4]\n *\n * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：\n * PAGE_SIZE = (m-1)*4[keywordss大小]+m*8[children大小]\n */\npublic class BPlusTreeNode {\n  public static int m = 5; // 5叉树\n  public int[] keywords = new int[m-1]; // 键值，用来划分数据区间\n  public BPlusTreeNode[] children = new BPlusTreeNode[m];//保存子节点指针\n}\n\n/**\n * 这是B+树中叶子节点的定义。\n *\n * B+树中的叶子节点跟内部节点是不一样的,\n * 叶子节点存储的是值，而非区间。\n * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。\n *\n * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：\n * PAGE_SIZE = k*4[keyw..大小]+k*8[dataAd..大小]+8[prev大小]+8[next大小]\n */\npublic class BPlusTreeLeafNode {\n  public static int k = 3;\n  public int[] keywords = new int[k]; // 数据的键值\n  public long[] dataAddress = new long[k]; // 数据地址\n\n  public BPlusTreeLeafNode prev; // 这个结点在链表中的前驱结点\n  public BPlusTreeLeafNode next; // 这个结点在链表中的后继结点\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n对于相同个数的数据构建m叉树索引，m叉树中的m越大，那树的高度就越小，那m叉树中的m是不是越大越好呢？到底多大才最合适呢？\n\n不管是内存中的数据，还是磁盘中的数据，操作系统都是按页(一页大小通常是4KB，这个值可以通过getconfig PAGE_SIZE命令查看)来读取的，一次会读一页的数据。如果要读取的数据量超过一页的大小，就会触发多次IO操作。所以，我们在选择m大小的时候，要尽量让每个节点的大小等于一个页的大小。读取一个节点，只需要一次磁盘IO操作。\n\n\n\n尽管索引可以提高数据库的查询效率，但是，作为一名开发工程师，我们应该也知道，索引有利也有弊，它也会让写入数据的效率下降。这是为什么呢？\n\n数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。\n\n对于一个B+树来说，m值是根据页的大小事先计算好的，也就是说，每个节点最多只能有m个子节点。在往数据库中写入数据的过程中，这样就有可能使索引中某些节点的子节点个数超过m，这个节点的大小超过了一个页的大小，读取这样一个节点，就会导致多次磁盘IO操作。我们该如何解决这个问题呢？\n\n实际上，处理思路并不复杂。我们只需要将这个节点分裂成两个节点。但是，节点分裂之后，其上层父节点的子节点个数就有可能超过m个。不过这也没关系，我们可以用同样的方法，将父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点。这个分裂过程，我们可以结合着下面这个图一起看，会更容易理解(图中的B+树是一个三叉树。我们限定叶子节点中，数据的个数超过2个就分裂节点；非叶子节点中，子节点的个数超过3个就分裂节点)。\n\n\n\n正是因为要时刻保证B+树索引是一个m叉树，所以，索引的存在会导致数据库写入的速度降低。实际上，不光写入数据会变慢，删除数据也会变慢。这是为什么呢？\n\n我们在删除某个数据的时候，也要对应地更新索引节点。这个处理思路有点类似跳表中删除数据的处理思路。频繁的数据删除，就会导致某些节点中，子节点的个数变得非常少，长此以往，如果每个节点的子节点都比较少，势必会影响索引的效率。\n\n我们可以设置一个阀值。在B+树中，这个阀值等于m/2。如果某个节点的子节点个数小于m/2，我们就将它跟相邻的兄弟节点合并。不过，合并之后节点的子节点个数有可能会超过m。针对这种情况，我们可以借助插入数据时候的处理方法，再分裂节点。\n\n文字描述不是很直观，这里举了一个删除操作的例子，我们可以对比看着下(图中的B+树是一个五叉树。我们限定叶子节点中，数据的个数少于2个就合并节点；非叶子节点中，子节点的个数少于3个就合并节点。)\n\n\n\n数据库索引以及B+树的由来，到此已经讲完了。我们可以发现，B+树的结构和操作，跟跳表非常类似。理论上讲，对跳表稍加改造，也可以替代B+树，作为数据库的索引实现的。\n\n\n# 总结引申\n\n今天，我们讲解了数据库索引实现，依赖的底层数据结构，B+树。它通过存储在磁盘的多叉树结构，做到了时间、空间的平衡，既保证了执行效率，又节省了内存。\n\n前面的讲解中，为了一步一步详细地给你介绍B+树的由来，内容看起来比较零散。为了方便掌握和记忆，这里再总结一下B+树的特点：\n\n * 每个节点中子节点的个数不能超过m，也不能小于m/2;\n * 根节点的子节点个数可以不超过m/2，这是一个例外；\n * m叉树只存储索引，并不真正存储数据，这个有点类似跳表；\n * 通过链表将叶子节点串联在一起，这样可以方便按区间查找；\n * 一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。\n\n除了B+树，可能还听说过B树、B-树。实际上，B-树就是B树，英文翻译为B-Tree，这里的"-"并不是相对B+树中的"+"，而只是一个连接符。\n\n而B树实际上是低级版的B+树，或者说B+树是B树的改进版。B树跟B+树的不同点主要集中在这几个地方：\n\n * B+树中的节点不存储数据，只是索引，而B树中的节点存储数据；\n * B树中的叶子节点并不需要链表来串联。\n\n也就是说，B树只是一个每个节点的子节点个数不能小于m/2的m叉树。\n\n\n# D68(2020/12/27) 索引：如何在海量数据中快速查找某个数据\n\n在之前的学习中，我们讲解了MySQL数据库索引的实现原理。MySQL底层依赖的是B+树这种数据结构。那类似Redis这样的Key-Value数据库中索引，又是怎么实现的呢？底层依赖的又是什么数据结构呢？\n\n今天，我们就来讲一下索引这种常用的技术解决思路，底层往往会依赖哪些数据结构。同时，通过索引这个应用场景，也来回顾一下，之前我们学过的几种支持动态集合的数据结构。\n\n\n# 为什么需要索引？\n\n在实际的软件开发中，业务纷繁复杂，功能千变万化，但是，万变不离其宗。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为"对数据的存储和计算"。对应到数据结构和算法中，那"存储"需要的就是数据结构，"计算"需要的就是算法。\n\n对于存储的需求，功能上无外乎增删改查。这其实并不复杂。但是，一旦存储的数据很多，那性能就成了这些系统要关注的重点，特别是在一些跟存储相关的基础系统(比如MYSQL数据库、分布式文件系统等)、中间件(比如消息中间件RocketMQ等)中。\n\n"如何节省存储空间、如何提高数据增删改查的执行效率"，这样的问题就成了设计的重点。而这些系统的实现，都离不开一个东西，那就是索引。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。\n\n索引这个概念，非常好理解。我们可以类比书籍的目录来理解。如果没有目录，我们想要查找某个知识点的时候，就要一页一页翻。通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。\n\n\n# 索引的需求定义\n\n接下来，我们就分析一下，在设计索引的过程中，需要考虑到的一些因素，换句话说就是，我们该如何定义清楚需求呢？\n\n对于系统设计需求，我们一般可以从功能性需求和非功能性需求两方面来分析，这个问题也不例外。\n\n\n# 功能性需求\n\n对于功能性需求需要考虑的点，我把我们大致概括成下面的这几点。\n\n数据是格式化数据还是非格式化数据？要构建索引的原始数据，类型有很多。我把它分为两类，一类是结构化数据，比如，MySQL中的数据；另一类是非结构化数据，比如搜索引擎中网页。对于非结构化数据，我们一般需要做预处理，提取出查询关键词，对关键词构建索引。\n\n数据是静态数据还是动态数据？如果原始数据是一组静态数据，也就是说，不会有数据的增加、删除、更新操作，所以，我们在构建索引的时候，只需要考虑查询效率就可以了。这样，索引的构建就相对简单些。不过，不部分情况下，我们都是对动态数据构建索引，也就是说，我们不仅要考虑到索引的查询效率，在原始数据更新的同时，我们还需要动态地更新索引。支持动态数据集合的索引，设计起来相对也要更加复杂些。\n\n索引存储在内存还是硬盘？如果索引存储在内存中，那查询的速度肯定要比存储在磁盘中的高。但是，如果原始数据量很大的情况下，对应的索引可能也会很大。这个时候，因为内存有限，我们可能就不得不将索引存储在磁盘中了。实际上，还有第三种情况，那就是一部分存储在内存，一部分存储在磁盘，这样就可以兼顾内存消耗和查询效率。\n\n单值查找还是区间查找？所谓单值查找，也就是根据关键词等于某个值的数据。这种查询需求最常见。所谓区间查找，就是查找关键词处于某个区间值的所有数据。我们可以类比MySQL数据库的查询需求。实际上，不同的应用场景，查询的需求会多种多样。\n\n单关键词查找还是多关键词组合查找？比如，搜索引擎中构建的索引，既要支持一个关键词的查找，比如\'数据结构\'，也要支持组合关键词查找，比如"数据结构AND算法"。对于单关键词的查找，索引构建起来相对简单些。对于多关键词查询来说，要分多种情况。像MySQL这种结构化数据的查询需求，我们可以实现正对多个关键词的组合，建立索引；对于像搜索引擎这样的非结构化的查询需求，我们可以针对单个关键词构建索引，然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。\n\n实际上，不同的场景，不同的原始数据，对于索引的需求也会千差万别。我们这里只是列举了一些比较有共性的需求。\n\n\n# 非功能性需求\n\n讲完功能性需求，我们再来看，索引设计的非功能性需求。\n\n不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非常有限，一个中间件启动后就占用几个GB的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会查过原始数据。\n\n在考虑索引查询效率的同时，我们还要考虑索引的维护成本。索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，我们还要考虑到，索引的维护成本。因为在原始数据动态增删改查的同时，我们也需要动态地更新索引。而索引的更新势必会影响到增删改查操作的性能。\n\n\n# 构建索引常用的数据结构有哪些？\n\n我们刚刚从宏观的角度，总结了在索引设计的过程中，需要考虑的一些共性因素。现在，我们就来看，对于不同需求的索引结构，底层一般使用哪种数据结构。\n\n实际上，常用来构建索引的数据结构，就是我们之前讲过的几种支持动态数据集合的数据结构。比如，散列表、红黑树、跳表、B+树。除此之外，位图、布隆过滤器可以作为辅助索引，有序数组可以用来对静态数据构建索引。\n\n我们知道，散列表增删改查操作的性能非常好，时间复杂度是O(1)。一些键值数据库，比如Redis、Memcache，就是使用散列表来构建索引的。这类索引，一般都构建在内存中。\n\n红黑树作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是O(logn)，也非常适用来构建内存索引。EXT文件系统中，对磁盘块的索引，用的就是红黑树。\n\nB+树比起红黑树来说，更加适合构建存储在磁盘中的索引。B+树是一个多叉树，所以，对相同个数的数据构建索引，B+树的高度要低于红黑树。当借助索引查询数据的时候，读取B+树索引，需要的磁盘IO次数会更少。所以，大部分关系型数据库的所以，比如MySQL、Oracle，都是用B+树来实现的。\n\n跳表也支持快速添加、删除、查找数据。而且，我们通过公灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率。Redis的有序集合，就是用跳表来构建的。\n\n除了散列表、红黑树、B+树、跳表之外，位图和布隆过滤器这两个数据结构，也可以用索引中，辅助存储在磁盘中的索引，加速数据查找的效率。\n\n我们知道，布隆过滤器有一定的判错率。但是，我们可以规避它的短处，发挥它的长处。尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在。而且，布隆过滤器还有一个更大的特点，那就是内存占用非常少。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。\n\n实际上，有序数组也可以被作为索引。如果数据是静态的，也就是不会有插入、删除、更新操作，那我们可以把数据的关键词(查询用的)抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。\n\n\n# 总结引申\n\n今天这节算是一节总结课。我们从索引这个非常常用的技术方案，展示了散列表、红黑树、跳表、位图、布隆过滤器、有序数组这些数据结构的应用场景。从这一节内容中，我们应该可以看出，架构设计离不开数据结构和算法。要想成长为一个优秀的业务架构师、基础架构师，数据结构和算法的根基一定要打稳。因为，那些看似很惊艳的架构设计思路，实际上，都是来自最常用的数据结构和算法。\n\n\n# D69(2020/12/28) 并行算法\n\n时间复杂度是衡量算法执行效率的一种标准。但是，时间复杂度并不能跟性能划等号。在真实的软件开发中，即便在不降低时间复杂度的情况下，也可以通过一些优化手段，提升代码的执行效率。毕竟，对于实际的软件开发来说，即便是像10%、20%这样微小的性能提升，也是非常可观的。\n\n算法的目的就是为了提高代码执行的效率。当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢？我们今天就来讲解一种非常简单但又非常好用的优化方法，那就是并行计算。今天，我们就通过几个例子，来展示一下，如何借助并行计算的处理思想对算法进行改造？\n\n\n# 并行排序\n\n假设我们要给大小为8GB的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据。对于排序来说，最常用的就是时间复杂度为O(nlogn)的三种排序算法，归并排序、快速排序、堆排序。从理论上讲，这个排序问题已经很难再从算法层面优化了。而利用并行的处理思想，我们可以很轻松地将这个给8GB数据排序问题的执行效率提高很多倍。具体的实现思路有下面几种。\n\n\n# 归并排序中并行\n\n第一种是对归并排序并行化处理。\n\n我们可以将这8GB的数据划分成16个小的数据集合，每个集合包含500MB的数据。我们用16个线程，并行地对这16个500MB的数据集合进行排序。这16个小集合分别排序完成之后，我们再将这16个有序集合合并。\n\n\n# 快速排序中并行\n\n第二种是对快速排序并行化处理。\n\n我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成16个小区间。我们将8GB的数据划分到对应的区间中。针对这16个小区间的数据，我们启动了16个线程，并行地进行排序。等到16个线程都执行结束之后，得到的数据就是有序数据了。\n\n对比这两种处理思路，它们利用的都是分治的思想，对数据进行分片，然后并行处理。它们的区别在于，第一种处理思路是，先随意地对数据分片，排序之后再合并。第二种处理思路是，先对数据按照大小划分区间，然后在排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。\n\n\n# 并行查找\n\n我们知道，散列表是一种非常适合快速查找的数据结构。\n\n如果我们是给动态数据构建索引，在数据不断加入的时候，散列表的装载因子就越来越大。为了保证散列表性能不下降，我们就需要对散列表进行动态扩容。对如此大的散列表进行动态扩容，一方面比较耗时，另一方面比较消耗内存。比如，我们给一个2GB大小的散列表进行扩容，扩展到原来的1.5倍，也就是3GB大小。这个时候，实际存储在散列表中的数据只有不到2GB，所以内存的利用率只有60%，有1GB的内是空闲的。\n\n实际上，我们可以将数据随机分割成k份(比如16份)，每份中的数据只有原来的1/k，然后我们针对这k个小数据集合分别构建散列表。这样，散列表的维护成本就变低了。当某个小散列表的装载因子过大的时候，我们可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。\n\n还是刚才那个例子，假设现在有2GB的数据，我们放到16个散列表中，每个散列表中的数据大约是150MB。当某个散列表需要扩容的时候，我们只需要额外增加150*0.5=75MB的内存(假设还是扩容到原来的1.5倍)。无论从扩容的执行效率还是内存的利用率上，这种多个小散列表的处理方法，都要比大散列表高效。\n\n当我们要查找某个数据的时候，我们只需要通过16个线程，并行地在这16个散列表中查找数据。这样的查找性能，比起一个大散列表的做法，也并不会下降，反倒有可能提高。\n\n当往散列表中添加数据的时候，我们可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。\n\n\n# 并行字符串匹配\n\n我们前面学过，在文本中查找某个关键词这样一个功能，可以通过字符串匹配算法来实现。我们之前学过的字符串匹配算法有KMP、BM、RK、BF等。当在一个不是很长的文本中查找关键词的时候，这些字符串匹配算法中的任何一个，都可以表现得非常高效。但是，如果我们处理的是超级大的文本，那处理的瞬间可能就会变得很长，那有没有办法加快匹配速度呢？\n\n我们可以把大的文本，分割成k个小文本。假设k是16，我们就启动16个线程，并行地在这16个小文本中查找关键词，这样整个查找的性能就提高了16倍。16倍效率的提升，从理论的角度来说并不多。但是，对于真实的软件开发来说，这显然是一个非常可观的优化。\n\n不过，这里还有一个细节要处理，那就是原本包含在大文本中的关键词，被一分为二，分隔到两个小文本中，这就会导致尽管大文本中包含这个关键词，但在这16个小文本中查找不到它。实际上，这个问题也不难解决，我们只需要针对这种特殊情况，做一些特殊处理就可以了。\n\n我们假设关键词的长度是m。我们在每个小文本的结尾和开始各取m个字符串。前一个小文本的末尾m个字符和后一个小文本的开头m个字符，组成一个长度是2m的字符串。我们再拿关键词，在这个长度为2m的字符串中再重新查找一遍，就可以补上刚才的漏洞了。\n\n\n# 并行搜索\n\n前面我们学习过好几种搜索算法，它们分别是广度优先搜索、深度优先搜索、Dijkstra最短路径算法、A*启发式搜索算。对于广度优先搜索算法，我们也可以将其改造成并行算法。\n\n广度优先搜索是一种逐层搜索的搜索策略。基于当前这一层顶点，我们可以启动多个线程，并行地搜索下一层的顶点。在代码实现方面，原来广度优先搜索的代码实现，是通过一个队列来记录已经遍历到但还没有扩展的顶点。现在，经过改造之后的并行广度优先搜索算法，我们需要利用两个队列来完成扩展顶点的工作。\n\n假设这两个队列分别是队列A和队列B。多线程并行处理队列A中的顶点，并将扩展得到的顶点存储在队列B 中。等队列A中的顶点都扩展完成之后，队列A被清空，我们再并行地扩展队列B中的顶点，并将扩展出来的顶点存储在队列A。这样两个队列循环使用，就可以实现并行广度优先搜索算法。\n\n\n# 总结引申\n\n上一节，我们通过实际软件开发中的"索引"这一技术点，回顾了之前学过的一些支持动态数据集合的数据结构。今天，我们又通过"并行算法"这个话题，回顾了之前学过的一些算法。\n\n通过一些例子，比如并行排序、查找、搜索、字符串匹配，展示了并行处理的实现思路，也就是对数据进行分片，对没有依赖关系的任务，并行地执行。\n\n并行计算是一个工程上的实现思路，尽管跟算法关系不大，但是，在实际的软件开发中，它确实可以非常巧妙地提高程序的运行效率，是一种非常好用的性能优化手段。\n\n特别是，当要处理的数据规模达到一定程度之后，我们无法通过继续优化算法，来提高执行效率的时候，我们就需要在实现的思路上做文章，利用更多的硬件资源，来加快执行的效率。所以，在很多超大规模数据处理中，并行处理的思想，应用非常广泛，比如MapReduce实际上就是一种并行计算框架。\n\n\n# D70(2020/12/29) 算法实战：Redis\n\n今天我们就来学习一下，经典数据库Redis中的常用数据类型，底层都是用哪种数据结构实现的？\n\n\n# Redis数据库介绍\n\nRedis是一种键值(key-value)数据库。相对于关系型数据库Mysql，Redis也被叫做非关系型数据库。\n\n像MySQL这样的关系型数据库，表的结构比较复杂，会包含很多字段，可以通过SQL语句，来实现非常复杂的查询需求。而Redis中只包含"键"和"值"两部分，只能通过"键"来查询"值"。正是因为这样简单的存储结构，也让Redis的读写效率非常高。\n\n除此之外，Redis主要是作为内存数据库来使用，也就是说，数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中。\n\nRedis中，键的数据类型是字符串，但是为了丰富数据存储的方式，方便开发者使用，值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。\n\n"字符串string"这种数据类型非常简单，对应到数据结构里，就是字符串。\n\n下面我们重点看下，其他四种比较复杂的数据类型，看看它们底层都依赖哪些数据结构。\n\n\n# 列表(list)\n\n我们先来看看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是压缩列表ziplist，另一种是双向循环链表。\n\n当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面的两个条件：\n\n * 列表中保存的单个数据(有可能是字符串类型的)小于64字节；\n * 列表中数据少于512个。\n\n关于压缩列表，这里稍微解释一下。它并不是基础数据结构，而是Redis字节设计的一种数据结构。它有点类似数组，通过一片连续的内存空间，来存储数据。不过，它跟数组不同的一点是，它允许存储的数据大小不同。具体的存储结构也非常简单，可以看下面的图示。\n\n\n\n现在，我们来看看，压缩列表中的"压缩"两个字该如何理解？\n\n听到"压缩"两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存，是相较于数组的存储思路而言的。我们知道，数组要求每个元素的大小相同，如果我们要存储不同长度的字符串，那我们就需要用最大长度的字符串大小作为元素的大小(假设是20个字节)。那当我们存储小于20个字节长度的字符串的时候，便会浪费部分存储空间。\n\n\n\n压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。而且，因为数据存储在一片连续的内存空间，通过键来获取值为列表类型的数据，读取的效率也非常高。\n\n当列表中存储的数据量比较大的时候，也就是不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。\n\n在链表里，我们已经讲过双向循环链表这种数据结构了，这里我们着重看一下Redis中双向链表的编码实现方式。\n\nRedis的这种双向链表的实现方式，非常值得借鉴。它额外定义了一个list结构体，来组织链表的首、尾指针，还有长度等信息。\n\n// 以下是C语言代码，因为Redis是用C语言实现的。\ntypedef struct listnode {\n  struct listNode *prev;\n  struct listNode *next;\n  void *value;\n} listNode;\n\n\ntypedef struct list {\n  listNode *head;\n  listNode *tail;\n  unsigned long len;\n  // ....省略其他定义\n} list;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 字典(hash)\n\n字典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的压缩列表，另一种是散列表。\n\n同样，只有当存储的数据量比较小的情况下，Redis才使用压缩列表来实现字典类型。具体需要满足两个条件：\n\n * 字典中保存的键和值的大小都要小于64字节。\n * 字典中键值对的个数要小于512个。\n\n当不能同时满足上面两个条件的时候，Redis就使用散列表来实现字典类型。Redis使用MurmurHash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，Redis使用链表法来解决。除此之外，Redis还支持散列表的动态扩容、缩容。\n\n当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于1的时候，Redis会触发扩容，将散列表扩大为原来大小的2倍左右。\n\n当数据动态减少之后，为了节省内存，当装载因子小于0.1的时候，Redis就会触发缩容，缩小为字典中数据个数的大约2倍大小。\n\n我们前面讲过，扩缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。针对这个问题，Redis使用我们在散列表中讲的渐进式扩容缩容策略，将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。\n\n\n# 集合(set)\n\n集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。\n\n当要存储的数据，同时满足下面这样两个条件的时候，Redis就采用有序数组，来实现集合这种数据类型。\n\n * 存储的数据都是整数；\n * 存储的数据元素个数不超过512个\n\n当不能同时满足这两个条件的时候，Redis就使用散列表来存储集合中的数据。\n\n\n# 有序集合(sortedset)\n\n有序集合这种数据类型，我们在跳表中已经讲解过了。它用来存储一组数组，并且每个数据会附带一个得分。通过得分的大小，我们将数据组织成跳表这样的数据结构，以支持快速地按照得分值、得分区间获取数据。\n\n实际上，跟Redis的其他数据类型一样，有序集合也并不仅仅只有跳表这一种实现方式。当数据量比较小的时候，Redis会用压缩列表来实现有序集合。具体点说就是，使用压缩列表来实现有序集合的前提，有这样两个：\n\n * 所有数据的大小都要小于64字节。\n * 元素个数要小于128个。\n\n\n# 数据结构持久化\n\n尽管Redis经常会被用作内存数据库，但是，它也支持数据落盘，也就是将内存中的数据存储到硬盘中。这样，当机器断电的时候，存储在Redis中的数据也不会丢失。在机器重新启动之后，Redis只需要再将存储在硬盘中的数据，重新读取到内存，就可以继续工作了。\n\n刚刚我们讲到，Redis的数据格式由"键"和"值"两部分组成。而"值"又支持很多数据类型，比如字符串、列表、字典、集合、有序集合。像字典、集合等类型，底层用到了散列表，散列表中有指针的概念，而指针指向的是内存中的存储地址。那Redis 是如何将这一个跟具体内存地址有关的数据结构存储到磁盘中的呢？\n\n实际上，Redis遇到的这个问题并不特殊，很多场景都会遇到。我们把它叫做数据结构的持久化问题，或者对象的持久化问题。这里的"持久化"，我们可以笼统地理解为"存储到磁盘"。\n\n如何将数据结构持久化到磁盘？我们主要有两种解决思路。\n\n第一种是清除原有的存储结构，只将数据存储到磁盘中。当我们需要从磁盘还原数据到内存的时候，再重新将数据组织成原来的数据结构。实际上，Redis采用的就是这种持久化思路。\n\n不过，这种方式也有一定的弊端。那就是数据从磁盘还原到内存的过程，会耗用比较多的时间。比如，我们现在要将散列表中的数据存储到磁盘。当我们从磁盘中，取出数据重新构建散列表的时候，需要重新计算每个数据的哈希值。如果磁盘中存储的是几GB的数据，那重构数据结构的耗时就不可忽视了。\n\n第二种是保留原来的存储格式，将数据按照原有的格式存储在磁盘中。我们拿散列表这样的数据结构来举例。我们可以将散列表的大小、每个数据被散列到的槽的编号的信息，都保存在磁盘中。有了这些信息，我们从磁盘中将数据还原到内存中的时候，就可以避免重新计算哈希值。\n\n\n# 总结引申\n\n今天，我们学习了Redis中常用数据类型底层依赖的数据结构，总结婴喜爱大概有这五种：压缩列表(可以看作一种特殊的数组)、有序数组、链表、散列表、跳表。实际上，Redis就是这些常用数据结构的封装。\n\n\n# D71(2020/12/31) 高性能队列Disruptor\n\nDisruptor是一种内存消息队列。从功能上讲，它其实有点类似kafka。不过，和kafka不同的是，disruptor是线程之间用于消息传递的队列。它在Apache Storm、Camel、Log4j2等很多知名项目中都有广泛应用。\n\n之所以如此受青睐，主要还是因为它的性能表现非常优秀。它比Java中另外一个非常常用的内存消息队列ArrayBlockingQueue(ABS)的性能，要高一个数量级，可以算得上是最快的内存消息队列了。\n\n如此高性能的内存消息队列，在设计和实现上，必然后它独到的地方。\n\n\n# 基于循环队列的"生产者——消费者模型"\n\n内存消息队列，来源于"生产者——消费者模型"。在这个模型中，"生产者"生产数据，并且将数据放到一个中心存储容器中。之后，"消费者"从中心存储容器中，取出数据消费。\n\n这个模型还是非常好理解的，这里面存储数据的中心存储容器，是用什么样的数据结构来实现的呢？\n\n实际上，实现中心存储容器最常用的一种数据结构，就是我们所讲的队列。队列支持数据的先进先出。正是这个特性，使得数据被霞飞的顺序性可以得到保证，也就是说，早被生产的数据就会早被消费。\n\n我们在第9节中讲过，队列有两种实现思路。一种是基于链表实现的链式队列，另一种是基于数组实现的顺序队列。不同的需求背景下，我们会选择不同的实现方式。\n\n如果我们要实现一个无界队列，也就是说，队列的大小事先不确定，理论上可以支持无限大。这种情况下，我们适合选用链表来实现队列。因为链表支持快速地动态扩容。如果我们要实现一个有界队列，也就是说，队列的大小实现确定，当队列中数据满了之后，生产者就需要等待。直到消费者消费了数据，队列有空闲位置的时候，生产者才能将数据放入。\n\n实际上，相较于无界队列，有界队列的应用场景更加广泛。毕竟，我们的机器内存是有限的。而无界队列占用的内存数量是不可控的。对于实际的软件开发来说，这种不可控的因素，就会有潜在的风险。在某些极端情况下，无界队列就有可能因为内存持续增长，而导致OOM错误。\n\n在第9节中，我们还讲过了一种特殊的顺序队列，循环队列。我们讲过，非循环的顺序队列在添加、删除数据的工程中，会涉及数据的搬移操作，导致性能变差。而循环队列正好可以解决这个数据搬移的问题。所以，性能更加好。所以，大部分用到顺序队列的场景中，我们都选择用顺序队列中的循环队列。\n\n实际上，循环队列这种数据结构，就是我们今天要讲的内存消息队列的雏形。我们借助循环队列，实现了一个最简单的"生产者——消费者模型"。\n\n为了方便理解，对于生产者和消费者之间操作的同步，我并没有用到线程相关的操作。而是采用了"当队列满了之后，生产者就轮询等待；当队列空了之后，消费者就轮训等待"这样的措施。\n\npublic class Queue {\n  private Long[] data;\n  private int size = 0, head = 0, tail = 0;\n  public Queue(int size) {\n    this.data = new Long[size];\n    this.size = size;\n  }\n\n  public boolean add(Long element) {\n    if ((tail + 1) % size == head) return false;\n    data[tail] = element;\n    tail = (tail + 1) % size;\n    return true;\n  }\n\n  public Long poll() {\n    if (head == tail) return null;\n    long ret = data[head];\n    head = (head + 1) % size;\n    return ret;\n  }\n}\n\npublic class Producer {\n  private Queue queue;\n  public Producer(Queue queue) {\n    this.queue = queue;\n  }\n\n  public void produce(Long data) throws InterruptedException {\n    while (!queue.add(data)) {\n      Thread.sleep(100);\n    }\n  }\n}\n\npublic class Consumer {\n  private Queue queue;\n  public Consumer(Queue queue) {\n    this.queue = queue;\n  }\n\n  public void comsume() throws InterruptedException {\n    while (true) {\n      Long data = queue.poll();\n      if (data == null) {\n        Thread.sleep(100);\n      } else {\n        // TODO:...消费数据的业务逻辑...\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n\n\n\n# 基于加锁的并发"生产者-消费者模型"\n\n实际上，刚刚的"生产者——消费者模型"实现代码，是不完善的。\n\n如果我们只有一个生产者往队列中写数据，一个消费者从队列中读取数据，那上面的代码是没有问题的。但是，如果有多个生产者在并发地往队列中写入数据，或者多个消费者并发地从队列中消费数据，那上面的代码就不能正确工作了。\n\n在多个生产者或多个消费者并发操作队列的情况下，刚刚的代码主要会有下面的两个问题：\n\n * 多个生产者写入的数据可能会互相覆盖；\n * 多个消费者可能会读取重复的数据。\n\n因为第一个问题和第二个问题产生的原理是类似的。所以，我着重讲解第一个问题是如何产生的以及该如何解决。对于第二个问题，也可以类比对第一个问题的解决思路来想一想。\n\n两个线程同时往队列中添加数据，也就相当于两个线程同时执行类Queue中的add()函数。我们假设队列的大小size是10，当前的tail指向下标7，head指向下标3，也就是说，队列中还有空闲空间。这个时候，线程1调用add()函数，往队列中添加一个值为12的数据；线程2调用add()函数，往队列中添加一个值为15的数据。在极端的情况下，本来是往队列中添加了两个数据(12和15)，最终可能只有一个数据添加成功，另一个数据会被覆盖。这是为什么呢？\n\n\n\n为了方便查看队列Queue中的add()函数，我们把它从上面的代码中摘录了出来。\n\npublic boolean add(Long element) {\n  if ((tail + 1) % size == head) return false;\n  data[tail] = element;\n  tail = (tail + 1) % size;\n  return true;\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n从这段代码中，我们可以看出，第3行给data[tail]赋值，然后第4行才给tail的值加一。赋值和tail加一两个操作，并非原子操作。这就会导致这样的情况发生：当线程1和线程2同时执行add()函数的时候，线程1先执行完了 3行语句，将data[7] (tail等于7)的值设置为12。在线程1还未执行到第行语句之前，也就是还未将tail 加一之前，线程2执行了第3行语句，又将data[7]的值设置为15，也就是说，那线程2插入的数据覆盖了线程1插入的数据。原本应该插入两个数据(12和15)的，现在只插入了一个数据(15)。\n\n\n\n那如何解决这种线程并发往队列中添加数据时，导致的数据覆盖、运行不正确问题呢？\n\n最简单的处理方法就是给这段代码加锁，同一时间只允许一个线程执行add()函数。这就相当于将这段代码的执行，由并行改成了串行，也就不存在我们刚刚说的问题了。\n\n不过，加锁将并行改为串行，必然导致多个生产者同时生成数据的时候，执行效率的下降。当然，我们可以继续优化代码，用CAS操作等减少加锁的粒度。我这里直接看下disruptor的处理方法。\n\n\n# 基于无锁的并发"生产者 —— 消费者模型"\n\n尽管disruptor的源码读起来很复杂，但是基本其实非常简单。实际上，它是换了一种队列和"生产者- 消费者模型"的实现思路。\n\n之前的实现思路中，队列只支持两个操作，添加数据和读取并移除数据，分别对应代码中的add()函数和poll()函数，而disruptor采用了另一种实现思路。\n\n对于生产者来说，它往队列中添加数据之前，先申请可用空闲存储单元，并且是批量地申请连续的n个(n>=1)存储单元。当申请到这组连续的存储单元之后，后续往队列中添加元素，就可以不用加锁了，因为这组存储单元是这个线程独享的。不过，从刚刚的描述中，我们可以看出，申请存储单元的过程是需要加锁的。\n\n对于消费者来说，处理的过程跟生产者是类似的。它先去申请一批连续可读的存储单元(这个申请的过程也是需要加锁的)，当申请到这批存储单元之后，后续的读取操作就可以不用加锁了。\n\n不过，还有一个需要特别注意的地方，那就是，如果生产者A申请到了一组连续的存储单元，假设是下标为3到6的存储单元，生产者B紧跟着申请到了下标是7到9的存储单元，那在3到6没有完全写入数据之前，7到9的数据是无法读取的。这个也是disruptor实现思路的一个弊端。\n\n如下图所示：\n\n\n\n实际上，disruptor采用的是ringbuffer和availablebuffer这两个结构，来实现上面的功能。不过，因为我们主要聚焦在数据结构和算法上，所以对这两种结构做了简化，但是基本思想是一致的。\n\n\n# 总结引申\n\n今天，我们讲解了如何实现一个高性能的并发队列。这里的"并发"两个字，实际上就是多线程安全的意思。\n\n常见的内存队列往往采用循环队列来实现。这种实现方法，对于只有一个生产者和一个消费者的场景，已经足够了。但是，当存在多个生产者或多个消费者的时候，单纯的循环队列的实现方式，就无法正确工作了。\n\n这主要是因为，多个生产者在同时往队列中写入数据的时候，在某些情况下，会存在数据覆盖的问题。而多个消费者同时消费数据，在某些情况下，会存在消费重复数据的问题。\n\n针对这个问题，最简单、暴力的解决方法就是，对写入和读取过程加锁。这种处理方法，相当于将原来可以并行执行的操作，强制串行执行，相应地就会导致操作性能的下降。\n\n为了在保证逻辑正确的前提下，尽可能地提高队列在并发情况下的性能，disruptor采用了"两阶段写入"的方法。在写入数据之前，先加锁申请批量的空闲存储单元，之后往队列中写入数据的操作就不需要加锁了，写入的性能因此就提高了。Disruptor对消费过程的改造，跟对生产过程的改造是类似的。它先加锁申请批量的可读取的存储单元，之后从队列中读取数据的操作也就不需要加锁了，读取的性能因此也就提高了。\n\n这个优化思路非常简单。实际上，不管架构设计还是产品设计，往往越简单的设计思路，越能更好地解决问题。',normalizedContent:'# 《数据结构与算法之美》读书笔记\n\n从今天开始要开始学习王争老师的《数据结构与算法之美》，重要的坚持到底，深入的思考，为学习数据结构打下坚实的基础。\n\n第一遍先听录音，第二遍重点词汇和语句的摘抄，第三遍查看别人的重点标记，第四遍查看留言。\n\n\n# 前序\n\n今天主要学习的第一篇，入门篇，主要描述的学习数据结构与算法的重要意义。\n\n 1. 基础知识就像是一座大楼的地基，它决定了我们的技术高度。而要想快速做出点事情，前提前脚一定是基础能力过硬，“内功”要到位。\n 2. 为了由浅入深地学习，把专栏分成四个递进的模块。\n    * 第一部分是入门篇，通过这一模块，能掌握时间、空间复杂度的概念，大o表示法的由来，各种复杂度分析技巧，以及最好、最坏、平均、均摊复杂度分析方法。\n    * 第二部分是基础篇，是学习的重点。介绍了最基础、最常用的数据结构和算法。\n    * 第三部分是高级篇，主要会讲解一些不是那么常用的数据结构和算法。\n    * 第四部分是实战篇，主要围绕数据结构和算法在具体软件实践中的应用来讲的。\n 3. 人生道路上，我们会遇到很多的坎。跨过去，你就可以成长，跨不过去就是困难和停滞。而在后面很长的一段时间里，你都需要为这个困难买单。\n\n\n# d21(2020/10/07)\n\n今天是学习的是《数据结构与算法之美》的准备课，“如何抓住重点，系统高效地学习数据结构与算法”；\n\n\n# 如何抓住重点，系统高效地学习数据结构与算法\n\n\n# 什么是数据结构？什么是算法？\n\n从广义上来讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。\n\n从狭义上来讲，也就是我们专栏要讲的，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。这些经典的数据结构和算法，都是前人从很多实际操作场景中抽象出来的，可以高效的解决很多实际开发问题。\n\n# 数据结构与算法的关系\n\n数据结构与算法是相辅相成的。数据结构是为算法服务的，算法要作用在特定的数据结构之上。因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。\n\n数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构是没用的。\n\n例如，因为数组具有随机访问的特点，常用的二分查找算法需要用数组来存储数据。但如果我们选择链表这种数据结构，二分查找算法就无法工作了，因为链表并不支持随机访问。\n\n# 学习的重点在什么地方？\n\n梳理学习的重点，就是要了解应该先学什么，后学什么。\n\n要想学习数据结构与算法，首先要掌握一个数据结构与算法中最重要的概念 ---- 复杂度分析。\n\n数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。\n\n如果我们只是掌握了数据结构与算法的特点、用法，但是没有学会复杂度分析，那就相当于只知道操作口诀，而没有掌握心法。只有把心法了然于胸，才能做到无招胜有招。\n\n所以，结合我自己的学习心得，还有这些年的面试、开发经验，总结了20个最常用的、最基础的数据结构与算法，不管是应付面试还是工作需要，只要集中精力逐一攻克这20个知识点就足够了。\n\n这里面有10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、trie树；10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。\n\n在学习的过程中，要学习它的"来历" "自身的特点" "适合解决的问题"以及"实际的应用场景"。\n\n\n# 一些可以让你事半功倍的学习技巧\n\n 1. 边学边练，适度刷题\n\n 2. 多问、多思考、多互动\n    \n    可以多在留言区写下自己的疑问、思考和总结，也可以经常看看别人的留言，和他们进行互动。\n\n 3. 打怪升级学习法\n    \n    学习的过程中，我们碰到最大的问题就是，坚持不下来。\n    \n    在枯燥的学习过程中，也可以给自己设立一个切实可行的目标，就像打怪升级一样。\n    \n    比如，针对这个专栏，可以设立这样一个目标：每节课后的思考题都认真思考，并且回复到留言区。当你看到很多人给你点赞之后，你就会为了每次都能发一个漂亮的留言，而更加认真地学习。\n    \n    比如，每节课后都写一篇学习笔记或学习心得；或者你还可以每节课都找下讲的不对的、不合理的地方。\n\n 4. 知识需要沉淀，不要想试图一下子掌握所有\n    \n    学习知识的过程是反复迭代、不断沉淀的过程。\n    \n    如果碰到"拦路虎"，你可以尽情在留言区问我，也可以先沉淀一下，过几天再重新学一遍。所谓，书读百遍其义自见。\n\n\n# 复杂度分析(上)：如何分析、统计算法的执行效率和资源消耗？\n\n\n# 为什么需要复杂度分析？\n\n事后统计法：\n\n通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？\n\n上面的评估算法执行效率的方法是正确的。可以称为事后统计法，但是这种统计方法有非常大的局限性。\n\n 1. 测试结果非常依赖测试环境\n    \n    测试结果，依赖于服务器的环境的好坏。\n\n 2. 测试结果受数据规模的影响很大\n    \n    数据是否有序，数据规模的大小，都可能会对测试结果产生影响，可能无法真实地反映算法的性能。\n    \n    我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。\n\n\n# 大o复杂度表示法\n\n算法的执行效率，粗略地讲，就是算法代码执行的时间。\n\n下面的一行代码，就是求1,2,3 ... n的累加和。\n\nint cal(int n) {\n\tint sum = 0;\n\tint i = 1;\n\tfor(; i<=n; ++i) {\n\t\tsum  = sum + i;\n\t}\n\treturn sum;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n从cpu的角度来看，这段代码的每一行都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的cpu执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？\n\n第1行int cal(int n) {是代码程序的入口，如果要算是代码的执行时间的话，那就是1次(这里我们考虑的是单次调用的场景下的分析)。\n\n第2行int sum=0;和第3行int i=1;代码分别需要1个unit_time的执行时间，第4、5行都运行了n遍(分别i从1到n)，所以需要2n* unit_time的执行时间，所以这段代码总的执行时间就是(2n+2)*unit_time。可以看出来，所有代码的执行时间t(n)与每行代码的执行次数成正比。\n\n# 示例分析2\n\n按照上面分析的思路，来看下面的这段代码。\n\nint cal(int n) {\n\tint sum = 0;\n\tint i = 1;\n\tint j = 1;\n\tfor (; i<=n; ++i) {\n\t\tj = 1;\n\t\tfor (; j <=n; ++j) {\n\t\t\tsum  = sum + i*j;\n\t\t}\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n我们依旧假设每个语句的执行时间是unit_time。那么这段代码的总执行时间t(n)是多少呢？\n\n第2、3、4行代码，每行都需要1个unit_time的执行时间，第5、6行代码循环执行了n遍，需要2n* unit_time的执行时间，第7、8行代码循环执行了$n^2$ 遍，所以需要2$n^2$ * unit_time的执行时间。所以，整段代码总的执行时间t(n) = (2$n^2$ + 2n +3) * unit_time.\n\n\n# 大o表示法\n\n尽管我们不知道unit_time的具体值，但是通过这两段的代码执行时间的推导过程，我们可以得到一个非常重要的规律，那就是，所有代码的执行时间t(n)与每行代码的执行次数n成正比。\n\n我们可以把这个规律总结成一个公式。\n\nt(n) = o(f(n))\n\n其中，t(n)表示代码执行的时间；n表示数据规模的大小；f(n)表示每行代码执行的次数总和。公式中的o，表示代码的执行时间t(n)与f(n)表达式成正比。\n\n所以，第一个列子中的t(n) = o(2n+2)，第二个例子中的t(n) = o(2$n^2$ + 2n +3)。这就是大o时间复杂度表示法。大o时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫做渐进时间复杂度，简称为时间复杂度。\n\n当n很大时，可以把它想象成10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大o表示法刚讲的那两段代码的时间复杂度，就可以记为：t(n) = o(n); t(n) = o($n^2$) .\n\n分析代码的执行时间，假设每行的执行时间一样。--\x3e 每行代码的执行次数 --\x3e 用大o来表示执行次数和时间时间的正比关系 --\x3e 去除低阶、常量、系数，真正成为大o表示法 。\n\n\n# 时间复杂度分析\n\n前面介绍了大o时间复杂度的由来和表示方法。\n\n现在来看下，如何分析一段代码的时间复杂度？这里有三个比较实用的方法可以分享。\n\n 1. 只关注循环执行次数最多的一段代码\n    \n    刚才所说的，大o这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。\n    \n    所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环次数最多的那一段代码就可以了。这段核心代码执行次数的n的量级，就是整段要分析代码的时间复杂度。\n    \n    int cal(int n) {\n    \tint sum = 0;\n    \tint i = 1;\n    \tfor(; i<=n; ++i) {\n    \t\tsum  = sum + i;\n    \t}\n    \treturn sum;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    其中第2、3行代码都是常量级的执行时间，与n的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第4、5行代码，所以这块代码要重点分析。这两行代码被执行了n次，所以总的时间复杂度就是o(n)。\n\n 2. 加法法则：总复杂度等于量级最大的那段代码的复杂度\n    \n    int cal(int n) {\n    \tint sum_1 = 0;\n    \tint p = 1;\n    \tfor (; p<100; ++p) {\n    \t\tsum_1 = sum_1 + p;\n    \t}\n    \tint sum_2 =0;\n    \tint q = 1;\n    \tfor (; q<n; ++q) {\n    \t\tsum_2 = sum_2 + q;\n    \t}\n    \tint sum_3 = 0;\n    \tint i =1;\n    \tint j =1;\n    \tfor (; i<=n; ++i) {\n    \t\tj = 1;\n    \t\tfor (; j<=n; ++j) {\n    \t\t\tsum_3 = sum_3 + i*j;\n    \t\t}\n    \t}\n    \treturn sum_1 + sum_2 + sum_3;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    17\n    18\n    19\n    20\n    21\n    22\n    \n    \n    这个代码分为三个部分，分别是求sum_1、sum_2、sum_3。我们可以分别分析每一部分的时间复杂度，然后把它们放在一块，再去一个量级最大的作为整段代码的复杂度。\n    \n    第一段代码求sum_1，for循环中执行了100次，所以是一个常量的执行时间，跟n的规模无关。\n    \n    需要强调的是，即便这段代码循环了10000次、100000次，只要是一个已知的数，跟n无关，照样也是常数级的执行时间。当n无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率于数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。\n    \n    那第二段代码和第三段代码的时间复杂度是多少呢？答案是o(n)和o($n^2$)。\n    \n    综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就是o($n^2$) . 也就是说：总的时间复杂度就等于量级最大的那段代码的时间复杂度。\n    \n    如果t1(n) = o(f(n))，t2(n) = o(g(n))；那么t(n) = t1(n) + t2(n) = max(o(f(n)), o(g(n))) = o(max(f(n), g(n))).\n\n 3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积\n    \n    对应上面的加法法则，这里还有一个乘法法则。也就是如果t1(n)=o(f(n))，t2(n)=o(g(n)); 那么t(n)=t1(n)*t2(n) = o(f(n))*o(g(n)) = o(f(n)*g(n))\n    \n    也就是说，假设t1(n) = o(n)，t2(n) = o($n^2$), 则t1(n)*t2(n) = o($n^3$)。落实到具体的代码上，我们可以把乘法法则看成是嵌套循环。\n    \n    int cal(int n) {\n    \tint ret = 0;\n    \tint i = 1;\n    \tfor (; i<n; ++i) {\n    \t\tret = ret + f(i);\n    \t}\n    }\n    \n    int f(int n) {\n    \tint sum = 0;\n    \tint i = 1;\n    \tfor (; i<n; ++i) {\n    \t\tsum = sum +i;\n    \t}\n    \treturn sum;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    16\n    \n    \n    我们单独看cal()函数。假设f()只是一个普通的操作，那第4~6行的时间复杂度就是，t1(n) = o(n)。但f()函数本身不是一个简单的操作，它的时间复杂度是t2(n) = o(n)，所以整个cal()函数的时间复杂度就是，t(n) =t1(n) * t2(n) = o(n*n) =o($n^2$) .\n    \n    上面的三种复杂度分析技巧，不需要刻意记忆。实际上，复杂度分析这个东西关键在于"熟练"。只要多看案例，多分析，就能做到"无招胜有招"。\n\n\n# 几种常见时间复杂度案例分析\n\n下面的图示，展示的是常见的时间复杂度的各个量级，按照数量级递增。\n\n\n\n对于上面罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：o($2^n$) 和o(n!)\n\n我们把时间复杂度为非多项式量级的算法问题叫做np(非确定多项式)问题。\n\n当数据规模n越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。所以，关于np的时间复杂度就不展开讲了。下面主要讨论的是常见的多项式时间复杂度。\n\n 1. o(1)\n    \n    首先必须明确一个概念，o(1)只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。\n    \n    比如下面的这段代码中，即便有3行，它的时间复杂度也是o(1)，而不是o(3)。\n    \n    int i = 8;\n    int j = 6;\n    int sum = i + j;\n    \n    \n    1\n    2\n    3\n    \n    \n    需要了解的是，只要代码的执行时间不随n的增大而增长，这样代码的时间复杂度我们都记住o(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是o(1)。\n\n 2. o(logn)、o(nlogn)\n    \n    对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。\n    \n    i = 1;\n    while (i<=n) {\n    \ti = i*2;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    根据我们前面讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。\n    \n    从代码中可以看出，变量i的值从1开始取，每循环一次就乘以2。当大于n时，循环结束。这个就类似于高中所学的等比数列。实际上，变量i的取值就是一个等比数列。如果我们把它一个个列出来，就应该是下面这个样子:\n    \n    $2^0$ $2^1$ $2^2$ $2^3$ ... $2^k$ ... $2^x$ ，最终一直到i<=n才停止。\n    \n    所以，我们只要知道x值是多少，就知道这行代码执行的次数是多少了。通过求解$2^x$ = n，来求解x的值。根据对数可知，x = $\\log_2{n}$ ,所以，这段代码的时间复杂度就是o($\\log_2{n}$)\n    \n    那么现在，我们把代码稍微修改一下，再观察一下下面代码的时间复杂度是多少？\n    \n    i = 1;\n    while (i<=n) {\n    \ti = i*3;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    \n    \n    根据上面同样的算法，我们可以得出这段代码的时间复杂度为o($\\log_3{n}$).\n    \n    实际上，不管是以2为底、以3为底，还是以10为底，我们可以把所有对数阶的时间复杂度都记为o($\\log n$). 为什么呢？\n    \n    我们知道，对数之间是可以相互转换的，$\\log_3 n$ 就等于$\\log_3 2$ * $\\log_2 n$ ,所以o($\\log_3 n$) = o(c * $\\log_2 n$) ，其中c = $\\log_3 2$ 是一个常量。\n    \n    基于我们前面的一个理论：在采用大o标记复杂度的时候，可以忽略系数。所以，o($\\log_2 n$)就等于o($\\log_3 n$)。因此，在对数阶时间复杂度的表示方法里，我们忽略了对数的"底"，统一表示为o(logn).\n    \n    进而对于o(n$\\log n$)就很容易理解了，这个就类似于之前学习的乘法法则。如果一段代码的时间复杂度是o($\\log n$)，我们循环执行n遍，时间复杂度就是o(n$\\log n$)，这也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是o(n$\\log n$) 。\n\n 3. o(m+n)、o(m*n)\n    \n    下面的例子是不一样的时间复杂度，代码的复杂度是由两个数据的规模来决定的。\n    \n    int cal(int m, int n) {\n    \tint sum_1 = 0;\n    \tint i = 1;\n    \tfor (; i<m; ++i) {\n    \t\tsum_1 = sum_1 + i;\n    \t}\n    \tint sum_2 = 0;\n    \tint j = 1;\n    \tfor (; j<n; ++j) {\n    \t\tsum_2 = sum_2 + j;\n    \t}\n    \treturn sum_1 + sum_2;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    \n    \n    从代码中可以看出，m和n是表示两个数据规模。我们无法事先评估m和n谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是o(m+n).\n    \n    针对这种情况下，原来的加法法则就不正确了，我们需要将加法法则改为：t1(m) + t2(n) = o(f(m) + g(n))。但是乘法法则则继续有效：t1(m)* t2(n) = o(f(m)*f(n)).\n\n\n# 空间复杂度分析\n\n前面我们讲到，时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度的全称就是渐进式空间复杂度，表示算法的存储空间与数据规模之间的增长关系。\n\nvoid print(int n) {\n\tint i = 0;\n\tint[] a = new int[n];\n\tfor (i; i<n; ++i) {\n\t\ta[i] = i*i;\n\t}\n\tfor (i = n-1; i>=0; --i) {\n\t\tprint out a[i]\n\t}\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n空间时间复杂度，看的是代码申请的内存空间的情况，表示的是这个代码使用的内存空间与数据规模之间增长的关系；而时间复杂度看的是算法的执行时间，或者更进一步说是代码中某一行的执行次数与数据规模之间的增长关系。\n\n跟时间复杂度分析一样，我们可以看到，第2行代码中，我们申请了一个空间存储变量i，但是它是常量阶的，跟数据规模n没有关系，所以我们可以忽略。第3行申请了一个大小为n的int类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是o(n)。\n\n我们常用的空间复杂度就是o(1)、o(n)、o($n^2$)，像o($logn$)、o($nlogn$)这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单得多。\n\n\n# 内容小结\n\n复杂度分析也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算分，执行效率越低。\n\n常见的复杂度并不多，从低阶到高阶有：o(1)、o($\\log n$)、o(n)、o(n$\\log n$)、o($n^2$)。\n\n\n\n复杂度分析并不难，关键在于多练。\n\n\n# d22(2020/10/08)\n\n今天主要是要学习复杂度分析的下半部分，浅析最好、最坏、平均、均摊时间复杂度的内容。\n\n上一节，我们讲了复杂度的大o表示法和几个分析技巧，还举了一些常见复杂度分析的例子，比如o(1)、o($\\log n$)、o(n)、o(n$\\log n$)复杂度分析。\n\n今天我会继续给你讲四个复杂度分析方面的知识点，最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。\n\n\n# 最好、最坏情况时间复杂度\n\n先分析如下的代码的时间复杂度。\n\nint find(int[] array, int n, int x) {\n\tint i = 0;\n\tint pos = -1;\n\tfor (; i<n; ++i) {\n\t\tif (array[i] == x) {\n\t\t\tpos = i;\n\t\t}\n\t}\n\treturn pos;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n我们应该可以看出来，这段代码要实现的功能是，在一个无序的数组(arry)中，查找变量x出现的位置。如果没有找到，就返回-1。按照上节课讲的分析方法，这段代码的复杂度是o(n)，其中，n代表数组的长度。\n\n其实，我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到就可以提前结束循环了。所以说，我们可以如下优化一下这段查找代码。\n\nint find(int[] array, int n, int x) {\n\tint i = 0;\n\tint pos = -1;\n\tfor (; i<n; ++i) {\n\t\tif (array[i] == x) {\n\t\t\tpos = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn pos;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n这个时候，问题来了。我们优化后，这段代码的时间复杂度还是o(n)吗？很显然，我们上一节学习的分析方法，解决不了这个问题。\n\n因为，要查找的变量x可能出现在数组的任意位置。如果数组中第一个元素正好是要查找的变量x，那就不需要继续遍历剩下的n-1个数据了，那时间复杂度就是o(1)。\n\n但如果数组中不存在变量x，那我们就需要把整个数组都遍历一遍，时间复杂度就成了o(n)。所以，不同的情况下，这段代码的时间复杂度是不一样的。\n\n为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度。\n\n顾名思义，最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。就像我们刚刚了解的，在最理想的情况下，要查找的变量x正好是数组的第一个元素，这个时候对应的时间复杂度就是最好情况时间复杂度。\n\n同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。就像刚举的那个例子，如果数组中没有要查找的变量x，我们需要把整个数组都遍历一遍才行，所以这种最糟糕情况下对应的时间复杂度就是最坏情况时间复杂度。\n\n\n# 平均情况时间复杂度\n\n我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面我们简称为平均时间复杂度。\n\n平均时间复杂度又该怎么分析呢，继续上面的查找变量x的例子来说。\n\n要查找的变量x在数组中的位置，有n+1种情况：在数组的0~ n-1位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数(或者说执行代码的次数)累加起来，然后再除以n+1种情况，就可以得到需要遍历的元素个数的平均值，即：\n\n$\\frac{1+2+3+...+n+n}{n+1}$ = $\\frac{n(n+3)}{2(n+1)}$\n\n我们知道，在时间复杂度的大o标记法中，可以省略掉系数、低阶、常量，所以，咋们把刚刚这个公式简化之后，得到的平均时间复杂度就是o(n).\n\n这个结果虽然是正确的，但是计算过程稍微有点问题。我们刚讲的这个n+1种情况，出现的概率并不是一样的。\n\n我们知道，要查找的变量x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，为了方便理解，我们假设在数组中与不再数组中的概率都是1/2。另外，要查找的数据出现在0~n-1这n个位置的概率也是一样的，为1/n。所以，根据概率乘法法则，要查找的数据出现在0~ n-1中任意位置的概率就是1/(2n).\n\n我们前面在推导过程中存在的最大问题就是，没有将各种情况发生的概率考虑进去。如果我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了下面的这样：\n\n1$\\times$$\\frac{1}{2n}$ + 2$\\times$$\\frac{1}{2n}$ + 3$\\times$$\\frac{1}{2n}$ + ... + n$\\times$$\\frac{1}{2n}$ + n$\\times$$\\frac{1}{2}$ = $\\frac{3n+1}{4}$\n\n这个值就是概率论中的加权平均值，也叫做期望值，所以平均时间复杂度的全称应该叫做加权平均时间复杂度 或者 期望时间复杂度。\n\n引入概率之后，前面那段代码的加权平均值为(3n+1)/4。用大o表示法来表示，去掉系数和常量，这段代码的加权评平均时间复杂度仍然是o(n).\n\n我们可能会感觉到，平均时间复杂度貌似比较复杂，还要涉及到概率论的知识。实际上，在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。很多时间就像上一节课举的那些例子那样，使用一个复杂度就可以满足需求了。只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。\n\n\n# 均摊时间复杂度\n\n到此为止，我们已经掌握了算法复杂度分析的大部分内容了。下面需要描述的是一个更加高级的概念，均摊时间复杂度，以及它对应的分析方法，摊还分析(或者叫平摊分析)。\n\n均摊时间复杂度，听起来跟平均时间复杂度有点像。对于初学者来说，这两个概念确实非常容易弄混。大部分的情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限。\n\n// array 表示一个长度为n的数组\n// 代码中的array.length就等于n \nint[] array = new int[n];\nint count = 0;\n\nvoid insert(int val) {\n\tif (count == array.length) {\n\t\tint sum = 0;\n\t\tfor (int i =0; i < array.length; ++i) {\n\t\t\tsum = sum + array[i];\n\t\t}\n\t\tarray[0] = sum;\n\t\tcount = 1;\n\t}\n\tarray[count] = val;\n\t++count;\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n这个代码是外层有个循环在反复的调用的。\n\n这段代码实现了一个往数组中插入数据的功能。一开始count=0，先执行array[count] = val和++count，也就是如下的情况：\n\n这里假设array数组的长度为5。\ncount=0时，array[0] = val, count =1\ncount=1时，array[1] = val, count =2 \ncount=2时，array[2] = val, count =3\ncount=3时，array[3] = val, count =4\ncount=4时，array[4] = val, count =5\n\n这个时候count == array.length了，执行for循环5次(从i=0到i=4)，其中将前面几个array数组中的各个数组元素都相加起来，变成了sum。\n随后将sum赋值给了array[0]，原来的array[0]的值被替换掉了。\n将count 赋值为1，这个很重要。**实现了数组的清空**\n紧接着又开始了。。。\n\ncount=1时，array[1] = val, count =2 \ncount=2时，array[2] = val, count =3\ncount=3时，array[3] = val, count =4\ncount=4时，array[4] = val, count =5\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的count == array.length时，我们用for循环遍历数组求和，并清空数组(count = 1;)，将求和之后的sum值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。\n\n那这段代码的时间复杂度是多少呢？我们可以先用我们刚学的三种时间复杂度的分析方法来分析一下。\n\n最理想的情况下，数组中有空闲时间，我们只需要将数据插入到数组下标为count的位置就可以了，所以最好情况时间复杂度为o(1)。最坏情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为o(n)。\n\n那平均时间复杂度是多少呢？答案是o(1)。我们还是可以通过前面讲的概率论的方法来分析。\n\n假设数组的长度是n，根据数据插入的位置的不同，我们可以分为n种情况，每种情况的时间复杂度是o(1)。除此之外，还有一种"额外"的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是o(n)。而且，这n+1种情况发生的概率一样，都是1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是：\n\n1$\\times$$\\frac{1}{n+1}$ + 1$\\times$$\\frac{1}{n+1}$ +... + 1$\\times$$\\frac{1}{n+1}$ + n$\\times$$\\frac{1}{n+1}$ --\x3e o(1)\n\n到此为止，前面的最好、最坏、平均时间复杂度的计算，理解起来应该都没有问题。但是这个例子里的平均复杂度分析其实并不需要这么复杂，不需要引入概率论的知识。这是为什么呢？\n\n我们先来对比一下这个insert()的例子和前面那个find()的例子，我们就会发现这两者有很大差别。\n\n首先，find()函数在极端情况下，复杂度才为o(1)。但insert()在大部分情况下，时间复杂度都为o(1)。只有个别的情况下，复杂度才比较高，为o(n)。这是insert()第一个区别于find()的地方。\n\n第二个不同的地方。对于insert()函数来说，o(1)时间复杂度的插入和o(n)时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序的关系，一般都是一个o(n)插入之后，紧跟着n-1个o(1)的插入操作(是指已经满的情况下产生的规律)，循环往复。\n\n所以，针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。\n\n针对这种特殊的场景，我们引入了一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫做均摊时间复杂度。\n\n那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？\n\n我们还是继续看在数组中插入数据的这个例子。每一次o(n)的插入操作，都会跟着n-1次 o(1)的插入操作，所以把耗时多的那次操作摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是o(1)。(我的理解下，那一次的o(n)的插入操作的循环代码的次数为n，也就是n个unit_time，将n个单位的unit_time平均摊派到前面的n-1次中，大致每一次都分到一个unit_time，还是o(1)的时间复杂度。由于这个每一次的插入操作的概率都是相同的，所以这种均摊是可以成立的)\n\n均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。为了方便我们理解和记忆，我们需要简单总结一下它们的应用场景。\n\n对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，在能够引用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。\n\n尽管很多数据结构和算法书籍都花了很大力气来区分平均时间复杂度和均摊时间复杂度，但其实我个人认为，均摊时间复杂度就是一种特殊的平均时间复杂度，我们没必要花太多精力去区分它们。我们最应该掌握的是它的分析方法，摊还分析。\n\n\n# 内容小结\n\n今天我们学习了几个复杂度分析相关的概念，分别有：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。之所以引入这几个复杂度概念，是因为，同一段代码，在不同输入的情况下，复杂度量级有可能是不一样的。\n\n在引入这几个概念之后，我们可以更加全面地表示一段代码的执行效率。而且，这几个概念理解起来都不难。\n\n\n# d23(2020/10/09)\n\n今天主要是要学习的是数组方面的内容。\n\n在每一种编程语言中，基本都会有数组这种数据类型。不过，它不仅仅是一种编程语言中的数据类型，还是一种最基础的数据结构。尽管数组看起来非常基础、简单，但是我估计很多人都并没有理解这个基础数据结构的精髓。\n\n在大部分的编程语言中，数组都是从0开始编号的，但是是否下意识地想过，为什么数组要从0开始编号，而不是从1开始呢？ 从1开始不是更符合人类的思维习惯吗？\n\n\n# 如何实现随机访问?\n\n什么是数组呢？数组(array)是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。\n\n这个定义中，有几个关键词。\n\n\n# 第一是线性表\n\n顾名思义，线性表就是数据排成像一条线一样的结果。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。\n\n\n\n而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。\n\n\n\n\n# 第二个是连续的内存空间和相同类型的数据\n\n正是因为这两个限制，它才有了一个堪称"杀手锏"的特性："随机访问"。但有利也有弊，这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。\n\n\n# 随机访问实现原理\n\n说到数据的访问，数组是如何实现根据下标随机访问数组元素的？\n\n我们拿一个长度为10的int类型的数组int[] a = new int[10]来举例。在下面画的图示中，计算机给数组a[10]，分配了一块连续内存空间1000~1039，其中，内存块的首地址为base_address = 1000。1个字节理解为1个房间，一个int类型的元素会占据4个房间。\n\n\n\n我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素的时候，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：\n\na[i]_address = base_address + i * data_type_size\n\n\n1\n\n\n其中data_type_size 表示数组中每个元素的大小。根据这里的例子来看，数组中存储的是int类型的数据，所以data_type_size就是4个字节。\n\n根据这个公式，我们就能实现了数组元素的快速访问了。\n\n\n# 随机访问的注意\n\n这里需要纠正一个"错误"。在面试的时候，常常会问数组和链表的区别，很多人都回答说，"链表适合插入、删除，时间复杂度o(1)；数组适合查找，查找时间复杂度为o(1)".\n\n实际上，这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为o(1)。即使是排好序的数组，我们用二分查找，时间复杂度也是o(log n)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为o(1)。\n\n\n# 低效的"插入"和"删除"\n\n前面概念部分我们提到，数组为了保持内存数据的连续性，会导致插入、删除这两个操作比较低效。为什么会导致低效呢？又有哪些改进方法呢？\n\n\n# "插入"操作\n\n假设数组的长度是n，现在，如果我们需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，给新来的数据，我们需要将第k ~ n这部分的元素都顺序地往后挪一位。那插入操作的时间复杂度是多少呢？\n\n如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度是o(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是o(n)。因为我在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为(1+2+...+n)/n = o(n)。\n\n> 我的理解是，n个线性表的元素中，有n+1个插槽，每个位置的概率都是一样的，都是1/(n+1)。 而出现在不同的位置上，需要移动的元素的个数是不一样的，在末尾是0，倒数第二位是1，倒数第二位是2 。。。 直至开头是n。 根据加权平均数的求解，那就是1 * (1/(n+1)) + 2* (1/(n+1)) .... + n* (1/(n+1)) = {(1+n)n}/{2(n+1)} = n/2 ，也就是o(n)\n\n这里的场景中，数组初始化后默认都是每个位置都是有元素的，默认的没有填入的元素的值，就是0。\n\n\n# 降低"插入"时间复杂度的技巧\n\n如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移k之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数据插入到第k个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。\n\n为了更好地理解，我们举一个例子。假设数组a[10]中存储了如下5个元素：a,b,c,d,e\n\n我们现在需要将元素x插入到第3个位置。我们只需要将c放入到a[5]，将a[2]赋值为x即可。最后，数组中的元素如下：a，b, x,d,e,c\n\n\n\n利用这种处理技巧，在特定的场景下，在第k个位置插入一个元素的时间复杂度就会降为o(1)。这个处理思想会在快排中用到。\n\n\n# 删除操作\n\n跟插入数据类似，如果我们要删除第k个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。\n\n和插入类似，如果删除数组末尾的数据，则最好情况下时间复杂度为o(1)；如果删除开头的数据，则最坏情况时间复杂度为o(n)；平均情况时间复杂度也为o(n)，这个推导的原理和上面的插入是一样的。\n\n\n# 连续删除更高效\n\n实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？\n\n观看下面的例子。数组a[10]中存储了8个元素：a,b,c,d,e,f,g,h。现在，我们要依次删除a,b,c三个元素。\n\n\n\n为了避免d,e,f,g,h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据的时候，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。\n\n如果了解jvm，就会发现，这就是jvm标记清除垃圾回收算法的核心思想。很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。\n\n\n# 警惕数组的访问越界问题\n\n先看下面c语言代码的例子：\n\n# include <stdio.h>\nint main(int argc, char* argv[]) {\n\tint i =0;\n\tint arr[3] = {0};\n\tfor (; i<=3; i++) {\n\t\tarr[i] = 0;\n\t\tprintf("%s","hello world\\n");\n\t}\n\treturn 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n我们会发现代码的运行结果并非是打印三行"hello word"，而是会无限打印"hello world"，这是为什么呢？\n\n因为，数组大小为3，a[0]，a[1]，a[2]，而我们的代码因为书写错误，导致for循环的结束条件错写为了i<=3 而非 i<3，所以 当i=3的时候，数组a[3]访问越界。\n\n我们知道，在c语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。根据我们前面讲的数组寻址公式，a[3]也会被定位到某块不属于数组的内存地址上，而这个地址正好是存储变量i的内存地址，\n\n数组寻址公式是a[i]_address = base_address + i * data_type_size，\n\n> 函数体内的局部变量存在栈上，且是连续压栈。在linux进程的内存布局中，栈区在高地址空间，从高向低增长。变量i和arr在相邻地址，且i比arr的地址大，所以arr越界正好访问到i。当然，前提是i和arr元素同类型，否则那段代码仍是未决行为。\n> \n> 我觉得那个例子，栈是由高到低位增长的，所以，i和数组的数据从高位地址到低位地址依次是：i, a[2], a[1], a[0]。a[3]通过寻址公式，计算得到地址正好是i的存储地址，所以a[3]=0，就相当于i=0.\n\n数组越界在c语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问一段连续内存，只要数组通过偏移计算得到的内存地址是可用的，那么程序就可能不会报任何错误。\n\n这种情况下，一般都会出现莫名其妙的逻辑错误，就像我们刚刚举的那个例子，debug的难度非常的大。而且，很多计算机病毒也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统，所以写代码的时候一定要警惕数组越界。\n\n但并非所有的语言都像c一样，把数组越界检查的工作丢给程序员来做，像java本身就会做越界检查，比如下面这几行java代码，就会抛出java.lang.arrayindexoutofboundsexception 。\n\nint[] a = new int[3];\na[3] = 10;\n\n\n1\n2\n\n\n\n# 容器能否完全替代数组？\n\n针对数组类型，很多语言都提供了容器类，比如java中的arraylist、c++ stl中的vector。在项目开发中，什么时候适合用数组，什么时候适合用容器呢？\n\n用java语言来描述，arraylist和数组相比，到底有哪些优势呢？\n\n个人觉得，arraylist最大的优势就是可以将很多数组操作的细节封装起来。 比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容 。\n\n数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为10的数组，当第11个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。\n\n如果使用arraylist，我们就完全不需要关心底层的扩容逻辑，arraylist已经帮我们实现好了。每次存储空间不够的时候，它都会将空间自动扩容为1.5倍大小。\n\n不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建arraylist的时候事先指定数据大小。\n\n比如下面的情况下，我们要从数据库中取出10000条数据放入arraylist。我们看下面这几行代码，就会发现，相比之下，事先指定数据大小可以省略掉很多次内存申请和数据搬移操作。\n\narraylist<user> users =  new arraylist(10000);\nfor (int i = 0; i < 10000; ++i) {\n   users.add(xxx);\n}\n\n\n1\n2\n3\n4\n\n\n\n# 单独使用数组的场景\n\n作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有些时候，用数组会更合适一些，总结了如下几点自己的经验。\n\n 1. java arraylist无法存储基本类型，比如int、long，需要封装为integer、long类，而autoboxing、unboxing则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。\n 2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到arraylist提供的大部分方法，也可以直接使用数组。\n 3. 还有就是个人喜好，当要表示多维数组的时候，用数组往往会更加直观。比如object [][] array ; 而用容器的话则需要这样定义：arraylist <arraylist<object>> array.\n\n对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果我们是做一些非常底层的开发，比如开发网络架构，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。\n\n\n# 答疑开篇问题\n\n现在来解答一下开篇的问题：为什么大多数编程语言中，数组要从0开始编号，而不是从开始呢？\n\n从数组存储的内存模型上来看，"下标"最确切的定义应该是"偏移(offset)"。前面也提到，如果用a来表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址，a[k]就表示偏移k个type_size的位置，所以计算a[k]的内存地址只需要也能够这个公式：\n\na[k]_address = base_address + k * data_type_size\n\n但是，如果数组从1开始计数，那我们计算数组元素a[k]的内存地址就会变成：\n\na[k]_address = base_address + (k-1) * data_type_size\n\n对比两个公式，我们不难发现，从1开始编号，每次随机访问数组元素都多了一次减法运算，对于cpu来说，就是多了一次减法指令。\n\n数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从0开始编号，而不是从1开始。\n\n其实说，数组起始编号非0开始不是一定不可，最重要的可能是历史原因。\n\nc语言设计者用0开始计数数组下标，之后的java、javascript等高级语言都效仿了c语言，或者说，为了在一定程度上减少c语言程序员学习java的学习成本，因此急需沿用了从0开始计数的习惯。实际上，很多语言中数组也并不是从0开始计数的，甚至还有一些语言支持负数下标，比如python。\n\n\n# 内容小结\n\n我们今天学习了数组。它可以说是最基础、最简单的数据结构了。数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特定就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为o(n)。在平时的业务开发中，我们可以直接使用编程语言提供的容器类，但是，如果是特别底层的开发，直接使用数组可能会更合适。\n\n\n# 二维数组的寻址公式\n\n那么对于二维数组 x[]来说，求x[i][j]的时候（不会考虑i j越界的情况），要到i的时候，一定走完了i*a2的长度，在x[i][0]往后找j个长度就是x[i][j]，所以会从初始地址增加 （i*a2+j）个单位长度\n\n一维数组：（a1）x[i]_address = base_address + i * type_size\n\n二维数组：（a1a2）x[i][j]_address = base_address + (i * a2 + j ) type_size\n\n\n# d24(2020/10/10)\n\n今天需要学习的是链表linked list这个数据结构。学习链表又有什么作用呢？为了回答这个问题，先要讨论一个经典的链表应用场景，那就是lru缓存淘汰算法。\n\n\n# 链表(上)：如何实现lru缓存淘汰算法？\n\n缓存是一种提供数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的cpu缓存、数据库缓存、浏览器缓存等。\n\n缓存的大小有限，当缓存被用满的时候，哪些数据应该被清理出去，哪些数据应该被保留呢？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略fifo(first in, first out)、最少使用策略lfu(least frequently used)、最近最少使用策略lru(least recently used)。\n\n这里就需要关注的一个问题：如何用链表来实现lru缓存淘汰策略呢?\n\n\n# 链表与数组的区别\n\n相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍微难一些。数组和链表是两个非常基础、非常常用的数据结构，我们会常常放到一起比较。\n\n从底层的存储结构上来看：\n\n参考如下的图示，从图中我们可以看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请了一个100mb大小的数组，当内存中没有连续的、足够大的存储空间的时候，即便内存的剩余总可用空间大于100mb，仍然会申请失败。(java中的arraylist可用不连续)\n\n而链表恰恰相反，它并不需要一块连续的内存空间，它通过"指针"将一组零散的内存块串联起来使用，所以如果我们申请的是100mb大小的链表，根本不会有问题。\n\n\n\n\n# 单链表基本概念\n\n链表结构五花八门，重点需要学习的是三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。\n\n链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的"结点"。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如下面的图示，我们把这个记录下个结点地址的指针叫做后继指针next。\n\n\n\n从上面的单链表的图中，可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性把第一个结点叫做头结点，把最后一个结点叫做尾结点。其中头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是只想下一个结点，而是指向一个空地址null，表示这时候链表上最后一个结点。\n\n> 参考《大话数据结构》中的内容，对于头结点，头指针的区别和联系。\n> \n> 参考url：http://data.biancheng.net/view/103.html\n> \n> 头结点：是放在第一个元素结点之前的一个节点，其数据域一般无意义(也可以存放链表的长度)。头结点可有可无。\n> \n> 头指针：如果存在头节点，那么头指针则是指向头结点的指针。如果不存在头节点，那么头指针则是执行第一个结点元素的指针。头指针是链表必须存在的。\n\n\n\n\n# 单链表的查找/插入/删除\n\n与数组一样，链表也支持数据的查找、插入和删除操作。\n\n我们知道，在进行数组的插入、删除操作的时候，为了保证内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是o(n)。\n\n而在链表中插入或删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。\n\n从下面的图示中，可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针的改变，所以对应的时间复杂度是o(1).\n\n\n\n但是，有利就有弊。链表要想随机访问第k个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。\n\n我们把单链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第k位的人是谁的时候，我们就需要从第一个人开始，一个个第往下数。所以，链表随机访问的性能没有数组好，需要o(n)的时间复杂度。\n\n\n# 循环链表\n\n循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。\n\n\n\n而循环链表的尾结点指针是指向链表的头结点。从上面的循环链表的图示中，可以看出，它像一个环一样首尾相连，所以叫做"循环"链表。\n\n和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环形结构特点时，就特别适合采用循环链表。\n\n\n# 双向链表的概念\n\n在实际的软件开发中，也更加常用的链表结构：双向链表。\n\n单向链表只有一个方向，结点只有一个后继指针next指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。\n\n\n\n从上面的图示中可以看出，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。\n\n\n# 双向链表的优势\n\n那相比单链表，双向链表适合解决哪种问题呢？\n\n从结构上来看，双向链表可以支持o(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。\n\n可能我们会说，我们刚才讲的单链表的插入、删除操作的时间复杂度已经是o(1)了，双向链表还能再怎么高效呢？\n\n刚才我们的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。接下来，我们一起分析一下链表的两个操作。\n\n# 双向链表的删除操作\n\n在实际的软件开发中，从链表中删除一个数据无外乎这两种情况：\n\n * 删除结点中"值等于某个给定值"的结点；(如果是这种情况的删除，我们需要先定位到要删除的元素，然后再执行删除操作。定位到要删除的元素的平均复杂度是o(n)，执行删除操作的时间复杂度是o(1))\n * 删除给定指针指向的结点。(这就意味着不用我们再去遍历找到被删除的元素。直接删除指向这个元素的相应的指针。)\n\n对于第一种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我们前面讲的指针操作将其删除。\n\n尽管单纯的删除操作时间复杂度是o(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为o(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为o(n)。\n\n对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点q需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到p->next = q, 说明p是q的前驱结点。\n\n但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要o(n)的时间复杂度，而双向链表只需要在o(1)的时间复杂度内就搞定了。\n\n# 双向链表的插入操作\n\n同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。\n\n双向链表可以在o(1)时间复杂度搞定，而单向链表需要o(n)的时间复杂度。\n\n# 双向链表的查询\n\n除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。\n\n因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。\n\n\n# 双向链表总结\n\n从上面的例子中，可以看出双向链表确实是要比单链表要更加高效一些。这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。\n\n如果我们熟悉java语言，我们肯定就用过linkedhashmap这个容器。深入了解linkedhashmap的实现原理，就会发现其中就用到了双向链表这种数据结构。\n\n\n# 空间换时间or时间换空间\n\n这里用到了一个空间换时间的设计思想。\n\n当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或数据结构。相反，如果内存比较紧张，比如代码跑在手机或单片机上，这个时间，就要反过来用时间换空间的设计思路。\n\n还是开篇缓存的例子。缓存实际上就是利用了空间换时间的设计思想。如果我们把数据存储在硬盘上，会比较节省内存，但每次查找数据都要询问一次硬盘，会比较慢。但如果我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次数据查询的速度就大大提高了。\n\n所以总结一下，对于执行较慢的程序，可以通过消耗更多的内存(空间换时间)来进行优化；而消耗过多的内存程序，可以通过消耗更多的时间(时间换空间)来降低内存的消耗。\n\n\n# 双向循环链表\n\n我们可以将循环链表和双向链表结合在一起，就得到了双向循环链表。\n\n可以参见如下的图示，实际上每个链表中的结点，都可以理解为一个结构体，这个结构体中存储了相应的数据域，还有相应的结构体类型的指针，分别可以指向前驱和后驱的结构体类型的结点。\n\n下面的图像中，第一个节点的前驱指向末尾节点的结构体内存地址的指针，而末尾节点的后继指向第一个节点的结构体内存地址的指针。\n\n\n\n\n# 数组和链表性能比较\n\n通过学习，可以知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。\n\n\n\n不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构存储数据。\n\n 1. 数组简单易用，在实现上使用的是连续的内存空间，可以借助cpu的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对cpu缓存不友好，没有办法有效预读。\n\n 2. 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致"内存不足(out of memory)"。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，这是它与数组最大的区别。\n    \n    上一节的学习中，我们知道了java中arraylist容器，也可以支持动态扩容。根据上一节的内容，我们知道，当我们往支持动态扩容的数组中插入一个数据的时候，如果数组中没有空闲空间时，就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的。\n\n 3. 除此之外，如果我们的代码对内存的使用非常苛刻，那就使用数组更合适。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是java语言，就有可能导致频繁的gc(garbage collection，垃圾回收)\n\n\n# 解答开篇的问题\n\n如何基于链表实现lru缓存淘汰算法？\n\n我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。\n\n 1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。\n 2. 如果此数据没有在缓存链表中，又可以分为两种情况：\n    * 如果此时缓存未满，则将此结点直接插入到链表的头部；\n    * 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。\n\n这样我们就用链表实现了一个lru缓存。\n\n现在我们再来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为o(n)。\n\n实际上，我们可以继续优化这个实现思路，如果引入散列表(hash table) 记录每个数据的位置，将缓存访问的时间复杂度降到o(1)。\n\n\n# 内容小结\n\n今天我们学习了一种跟数组"相反"的数据解雇，链表。它跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数据稍微复杂，从普通的单链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。\n\n和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。\n\n\n# 问题思考\n\n 1. c语言中指针占用几个字节\n    \n    指针即为地址，指针几个字节跟语言无关，而是跟系统的寻址能力有关。现在一般是32位系统，所以是4个字节，64为的系统，就是8个字节。可以通过如下的代码来查看运行环境中指针占多大字节。\n    \n    #include <stdio.h>   \n    int main(void)  \n    {  \n        int a=1;  \n        char b=\'a\';  \n        float c=1.0;  \n        void *p;  \n        p=&a;  \n        printf("a的地址为：0x%x，其字节数为：%d\\n",p,sizeof(p));  \n        p=&b;  \n        printf("b的地址为：0x%x，其字节数为：%d\\n",p,sizeof(p));  \n        p=&c;  \n        printf("c的地址为：0x%x，其字节数为：%d\\n",p,sizeof(p));  \n        return 0;  \n    } \n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n    \n    参考如下的网上资源：\n    \n    > https://blog.csdn.net/iosshan/article/details/88944637\n    > \n    > https://blog.csdn.net/koches/article/details/7627381#\n\n 2. 单链表中尾结点的后继指针问题\n    \n    在单链表中，尾结点的后继指针next指向的不再是下一个结点。而是指向一个空地址null。这样做的好处在于：防止尾结点的后继指针next成为一个野指针，导致遍历链表根本停不下来，或者出现一些本不属于该链表的垃圾数据。\n\n 3. 参考《大话数据结构》中的内容，对于头结点，头指针的区别和联系。\n    \n    参考url：http://data.biancheng.net/view/103.html\n    \n    头结点：是放在第一个元素结点之前的一个节点，其数据域一般无意义(也可以存放链表的长度)。头结点可有可无。\n    \n    头指针：如果存在头节点，那么头指针则是指向头结点的指针。如果不存在头节点，那么头指针则是执行第一个结点元素的指针。头指针是链表必须存在的。\n    \n    \n\n 4. 单链表实现lru，数组实现lru\n    \n    单链表lru：我们约定，越靠近链表尾部的结点是越早之前访问的(当然也可以是链尾是越后访问的)。当有一个新的数据被访问的时候，我们从链表头开始顺序遍历链表。\n    \n    * 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部；\n    * 如果此数据没有在缓存链表中，可以分为两种情况：如果此时缓存未满，则将此结点直接插入到链表的头部；\n    * 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。\n    \n    数组lru：同样我们约定，越靠近数组尾部的元素是越迟访问的。当有一个新的数据被访问的时候，我们就从数组的第一个元素开始顺序遍历数组。\n    \n    * 如果此数据之前已经被缓存在数组中了，我们遍历得到这个数据对应的元素的位置，并将其从原来的位置删除，然后再插入到数组的尾部。\n    * 如果此数据没有在缓存的数组中，可以分为两种情况：如果此时缓存未满，则将此元素直接插入到数组的末尾；\n    * 如果此时缓存已满，则将数组的第一个元素删除，将新的元素插入到数组的尾部。\n\n 5. 判断一个字符串是否是回文字符串，这个字符串是通过单链表来存储的。\n    \n    使用快慢两个指针找到链表中点，\n    慢指针每次前进一步，快指针每次前进两步。\n    在慢指针前进的过程中，同时修改其next指针，\n    使得链表前半部分反序。\n    最后比较中点两侧的链表是否相等。\n    时间复杂度：o(n)\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    \n\n\n# d25(2020/10/11)\n\n今天主要是要学习的是链表的下半部分，主要学习和描述的内容是如何轻松地写出正确的链表代码。\n\n要想写好链表代码并不是容易的事，尤其是那些复杂的链表操作，比如链表反转、有序链表合并等，写的时候非常容易出错。\n\n为什么链表代码这么难写？究竟怎样才能轻松地写出正确的链表代码呢？\n\n只要愿意投入时间，我觉得大多数人都是可以学会的。比如说，如果你真的能花上一个周末或者一整天的时间，就去写链表反转这一代码。\n\n当然，自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要一些方法和技巧。我根据自己的学习经历和工作经验，总结了几个写链表代码技巧。如果你能熟练掌握这几个技巧，加上自己的主动和坚持，轻松拿下链表代码完全没有问题。\n\n\n# 技巧一：理解指针或引用的含义\n\n事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以，要想写对链表代码，首先就要理解好指针。\n\n我们知道，有些语言有"指针"的概念，比如c语言；有些语言没有指针，取而代之的是"引用"，比如java、python。不管是"指针"还是"引用"，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。\n\n接下来，我会拿c语言中的"指针"来讲解，如果你用的是java或者其他没有指针的语言也没有关系，你把它理解成"引用"就可以了。\n\n实际上，对应指针的理解，我们只需要记住下面这句话就可以了：\n\n将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。\n\n在编写链表代码的时候，我们经常会有这样的代码：p->next = q。这行代码是说，p结点中的next指针存储了q结点的内存地址。\n\n还有一个更复杂的，也是我们写链表代码经常会用到的：p->next = p->next->next . 这行代码表示，p结点的next指针存储了p结点的下下一个结点的内存地址。\n\n\n# 技巧二：警惕指针丢失和内存泄漏\n\n很多时候，在写链表代码的时候，很容易不知道指针指向哪里了。\n\n指针往往都是怎么弄丢的呢？下面看一个单链表插入的操作例子：\n\n\n\n如上图所示，我们希望在结点a和相邻的结点b之间插入结点x，假设当前指针p指向结点a。如果我们将代码实现变成下面的样子，就会发生指针丢失和内存泄漏.\n\np->next = x;   //将p的next指针指向x结点\nx->next = p->next; //将x的结点的next指针指向b\n\n\n1\n2\n\n\n在这里是错误的。p->next指针在完成第一步操作之后，已经不再指向结点b了，而是指向结点x。第2行代码相当于将x赋值给x->next，自己指向自己。因此，整个链表也就断成了两半，从结点b往后的所有结点都无法访问到了。(这里的意思是，由于指向性的丢失，结点b往后的所有结点都无法访问到了，也就是说无法去释放它们了。没有了指针，程序也不知道从哪个位置开始取释放内存空间)\n\n对于有些语言来说，比如c语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄漏。所以，我们插入结点时，一定要注意操作的顺序，要先将结点x的next指针指向结点b，再把结点a的next指针指向结点x，这样才不会丢失指针，导致内存泄漏。所以，对于刚刚的插入代码，我们只需要把第1行和第2行代码的顺序颠倒一下就可以了。\n\n同理，删除链表结点的时候，也一定要记得手动释放内存空间。否则，也会出现内存泄漏的问题。当然，对于像java这种虚拟机自动管理内存的编程语言来说，就不需要考虑这么多了。\n\n\n# 技巧三：利用哨兵简化实现难度\n\n首先，我们先来回顾一下单链表的插入和删除操作。如果我们在结点p后面插入一个新的结点，只需要下面两行代码就可以搞定。\n\nnew_node->next = p->next; \n//原先p的后继指针里面的地址位置赋值给了新节点的后继指针里面\np->next = new_node;  \n//将新节点的地址值，赋给p的next后继指针\n\n\n1\n2\n3\n4\n\n\n但是，当我们要向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。我们需要进行下面这样的特殊处理，其中head表示链表的头结点。所以，从这段代码，我们可以发现，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不一样的。\n\nif (head == null) {\n    head = new_node;\n}\n\n\n1\n2\n3\n\n\n我们再来看单链表结点删除操作。如果要删除结点p的后继结点，我们只需要一行代码就可以了。\n\np->next = p->next->next;\n\n\n1\n\n\n但是，如果我们要删除链表中的最后一个结点，前面的删除代码就不能工作了。跟插入操作类似，我们也需要对于这种情况特殊处理。代码如下：\n\nif (head->next == null) {\n    head = null;\n}\n\n\n1\n2\n3\n\n\n从前面的一步一步分析来看，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理。 这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。可以参考如下的方法来解决问题。\n\n可以利用哨兵，哨兵，解决的是国家之间的边界问题。同理，这里说的哨兵也是解决"边界问题"的，不直接参与业务逻辑。\n\n上面用head = null来表示链表中没有结点了。其中head表示头结点指针，指向链表中的第一个结点。\n\n如果我们引入哨兵结点，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。\n\n下面的图示是一个带头链表，我们可以发现，哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。\n\n\n\n实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。下面是用c写的例子。\n\n//在数组a中，查找key，返回key所在的位置\n// 其中，n表示数组a的长度\nint find(char* a, int n, char key) {\n\t//边界条件处理，如果a为空，或者n<=0，说明数组中没有数据，就不用while循环比较了\n\tif (a == null || n <=0) {\n\t\treturn -1;\n\t}\n\tint i = 0;\n\t// 这里有两个比较操作：i<n和a[i] == key.\n\twhile (i<n) {\n\t\tif (a[i] == key) {\n\t\t\treturn i;\n\t\t}\n\t\t++i;\n\t} \n\treturn -1;\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n代码二：\n\n// 在数组a中，查找key，返回key所在的位置\n// 其中，n表示数组a的长度\nint find(char* a, int n, char key) {\n\tif (a == null || n <=0) {\n\t\treturn -1;\n\t}\n\tif (a[n-1] == key) {\n\t\treturn n-1;\n\t}\n\tchar tmp = a[n-1];\n\ta[n-1] = key;\n\tint i = 0;\n\twhile (a[i] != key) {\n\t\t++i;\n\t}\n\ta[n-1] = tmp;\n\tif (i == n-1) {\n\t\treturn -1;\n\t} else {\n\t\treturn i;\n\t}\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n对比两段代码，在字符串a很长的时候，比如几万、几十万，哪段代码运行的更快点呢？答案是代码二，因为两段代码中执行次数最多就是while循环那一部分。第二段代码中，我们通过一个哨兵a[n-1] = key，成功省掉了一个比较语句i<n(i<n在while循环中存在的意义是，帮助控制循环的次数，不要无限循环下去。而这里设置了a[n-1]=key，就保证了while循环会在最后一个数组元素后退出了)。不要小看这一条语句，当累积执行万次、几十万次时，累积的时间就很明显了。\n\n当然，这只是为了举例说明哨兵的作用，我们写代码的时候千万不要写第二段那样的代码，因为可读性太差了。大部分情况下，我们并不需要如此追求极致性能。\n\n\n# 技巧四：重点留意边界条件处理\n\n软件开发中，代码在一些边界或异常的情况下，最容易产生bug。链表代码也不例外。要想实现没有bug的链表代码，一定要在编写的过程中以及编写完成之后，检查边界条件是否考虑全面，以及代码在边界条件下是否能正确运行。\n\n我经常用来检查链表代码是否正确的边界条件有如下几个：\n\n * 如果链表为空时，代码是否能正常工作？\n * 如果链表只包含一个结点的时候，代码是否能正常工作？\n * 如果链表只包含两个结点的时候，代码是否能正常工作？\n * 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？\n\n当我们写完链表代码之后，除了看下我们写的代码在正常的情况下能否工作，还要看下在上面我列出的几个边界条件下，代码仍然能否正确工作。如果这些边界条件下都没有问题，那基本上可以认为没有问题了。\n\n\n# 技巧五：举例画图，辅助思考\n\n对于稍微复杂的链表操作，我们可以使用举例法和画图法。\n\n我们可以找一个具体的例子，把它画在纸上，释放一些脑容量，留更多的给逻辑思考，这样就会感觉到思路清晰很多。比如往单链表中插入一个数据这样一个操作，我一般都是把各种情况都举一个例子，画出插入前和插入后的链表变化。\n\n\n# 技巧六：多写多练，没有捷径\n\n精选了5个常见的链表操作。我们需要把这几个操作都能写熟练，不熟就多写几遍。\n\n * 单链表反转\n * 链表中环的检测\n * 两个有序的链表合并\n * 删除链表倒数第n个结点\n * 求链表的中间结点\n\n\n# 内容小结\n\n这里主要罗列了写链表代码的六个技巧。分别是理解指针或引用的含义、警惕指针丢失和内存泄漏、利用哨兵简化实现难度、重点留意边界条件处理，以及举例画图、辅助思考，还有多写多练。\n\n写链表代码是最考验逻辑思维能力的。\n\n\n# 问题思考\n\n 1. c语言中，内存泄漏的问题\n    \n    对于c语言而言，删除链表的结点时，也一定要手动释放结点所占用的内存空间，否则也会容易产生内存泄漏的问题。\n\n 2. 如何获取结构体的首地址\n    \n    参考如下的代码：\n    \n    #include<stdio.h>\n    typedef struct\n    {\n     int a;\n     int b;\n    }t;\n    int  main()\n    {\n    \tt t;\n    \tvoid *p1;\n    \tvoid *p2;\n    \tp1 = &t;\n    \tp2 = &t.a;\n    \tprintf("p1 = %p, p2 = %p\\n",p1,p2);\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    12\n    13\n    14\n    15\n    \n\n\n# d26(2020/10/12)\n\n今天主要是要学习的是栈相关方面的内容。\n\n这里我先引出一个浏览器中前进和后退的例子。\n\n当我们依次访问完一串页面a-b-c之后，点击浏览器的后退按钮，既可以看到之前浏览过的页面b和a。当我们后退到页面a，点击前进按钮，就可以重新查看页面b和c。但是，如果我们后退到了页面b之后，点击了新的页面d了，那就无法再通过前进、后退功能查看页面c了。\n\n\n# 如何理解"栈"？\n\n关于"栈"，有个非常贴切的例子，就是一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个个放；取的时候，我们也是从上往下一个个的依次取，不能从中间任意抽出。后进者先出，新进者后出，这就是典型的"栈"结构 。\n\n从栈的操作特性来看，栈是一种"操作受限"的线性表，只允许在一端插入和删除数据。\n\n那对于这种栈的结构，存在的意义是什么呢？相比于数组和链表，栈带给我们的好像只有限制，并没有任何优势。我们可以直接使用数组或链表来替代栈，我们为什么还要用这个"操作受限"的栈呢？\n\n事实上，从功能上来说，数组或链表确实可以替代栈，但我们需要知道，特定的数据结构是对特定场景的抽象，而且，数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就跟容易出错。\n\n当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选"栈"这种数据结构。\n\n\n# 如何实现一个"栈"？\n\n从刚才栈的定义里面，我们可以看出，栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。\n\n实际上，栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫做顺序栈，用链表来实现的栈，我们叫做链式栈。\n\n下面的代码是用java写的用数组实现的顺序栈\n\n\n\n\n1\n\n\n了解了定义和基本操作，那它的操作的时间、空间复杂度是多少呢？\n\n不管是顺序栈还是链式栈，我们存储数据只需要一个大小为n 的数组够了。在入栈和出栈的过程中，只需要一两个临时变量存储空间，所以空间复杂度是o(1).\n\n注意，这里存储数据需要一个大小为n的数组，并不是说空间复杂度就是o(n)。因为，这n个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。\n\n不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是o(1)。\n\n\n# 支持动态扩容的顺序栈\n\n刚才那个基于数组实现的栈，是一个固定大小的栈，也就是说，在初始化栈时需要事先指定栈的大小。当栈满之后，就无法再往栈里添加数据了。尽管链式栈的大小不受限，但要存储next指针，内存消耗相对较多。那我们如何基于数组实现一个可以支持动态扩容的栈呢？\n\n我们在数组那一节描述过，当数组空间不够的时候，我们就重新申请一块更大的内存，将原来数组中数据统统拷贝过去。这样就实现了一个支持动态扩容的数组。\n\n所以，如果要实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中。如下图所示：\n\n\n\n实际上，支持动态扩容的顺序栈，在平时的开发中并不常见。我们可以根据这个场景，复习一下复杂度的分析。\n\n对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是o(1)。但是，对于入栈操作来说，情况就不一样了。当栈中有空闲空间的时候，入栈的操作的时间复杂度是o(1)。但是当空间不够的时候，就需要重新申请内存和数据搬移，所以时间复杂度就变成了o(n)。\n\n也就是说，对于入栈操作来说，最好情况时间复杂度是o(1)，最坏情况的时间复杂度是o(n)。那平均情况下的时间复杂度又是多少呢？这个入栈操作的平均情况下的时间复杂度可用用摊还分析法来分析。\n\n为了分析的方便，我们需要事先做一些假设和定义：\n\n * 栈空间不够的时候，我们重新申请一个是原来大小两倍的数组；\n * 为了简化分析，假设只有入栈操作没有出栈操作；\n * 定义不涉及内存搬移的入栈操作为simple-push操作，时间复杂度为o(1)。\n\n如果当前栈大小为k，并且已满，当再有新的数据要入栈的时候，就需要重新申请2倍大小的内存，并且做k个数据的搬移操作，然后再入栈。但是，接下来的k-1次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这k-1次入栈操作都只需要一个simple-push操作就可以完成。如下图所示：\n\n\n\n我们可以看出，这k次入栈操作，总共涉及了k个数据的搬移，以及k次simple-push操作。将k个数据搬移均摊到k次入栈操作，那每个入栈操作只需要一个数据搬移和一个simple-push操作。以此类推，入栈操作的均摊时间复杂度是o(1)。\n\n从这个例子的分析中，可以看出，均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分的情况下，入栈操作的时间复杂度o都是o(1)，只有在个别的时刻才会退化为o(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况系的耗时就接近o(1)。\n\n\n# 栈在函数调用时的应用\n\n栈作为一个比较基础的数据结构，经典的一个应用场景就是函数调用栈。\n\n操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成"栈"这种结构，用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。\n\n可以查看下如下的代码：\n\n# include <stdio.h>\nint add(int x, int y) {\n\tint sum = 0;\n\tsum = x + y;\n\treturn sum;\n}\n\nint main() {\n\tint a = 1;\n\tint ret = 0;\n\tint res = 0;\n\tret = add( 3, 5);\n\tres = a + ret;\n\tprintf("%d", res);\n\treturn 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n从代码中我们可以看出，main()函数调用了add()函数，获取计算结果，并且与临时变量a相加，最后打印res的值。下面的图示中展示的，对应的函数栈里出栈、入栈的操作。\n\n\n\n\n# 栈在表达式求值中的应用\n\n这里展示的是栈的另一个常见的应用场景，编译器如何利用栈来实现表达式求值。\n\n为了方便解释，我将算术表达式简化为只包含加减乘除四则运算，比如：34+13*9+44 - 12/3。\n\n对于计算机来说，编辑器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。\n\n如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或相同，从运算符栈中取栈顶运算符，从操作数的栈顶取2个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。\n\n下面是将3+ 5*8 -6 这个表达式的计算过程画成了一张图\n\n\n\n\n# 栈在括号匹配中的应用\n\n除了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。\n\n我们同样简化一下背景。我们假设表达式中只包含三种括号，圆括号()、方括号[]和花括号{}，并且它们可以任意嵌套。比如，{[{}]} 或者是[{()}([])]等都是合法格式，而{[}()] 或[({)] 为不合法的格式。如果现在有一个包含三种括号的表达式字符串，如何检查它是否合法呢？\n\n这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号的时候，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如"("跟")"匹配，"["跟"]"匹配，"{"跟"}"匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。\n\n当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。\n\n>  1. 扫描到左括号时，将其压入栈。\n>  2. 当扫描到右括号时，从栈顶取出数据。\n>  3. 如果能够匹配，则继续扫描；反之，就是非法。\n>  4. 最后假如栈中没有数据，表示格式非法。\n\n\n# 解答开篇\n\n我们再来看看开篇的思考题，如何实现浏览器的前进、后退功能？其实，用两个栈就可以非常完美地解决这个问题。\n\n我们使用两个栈，x和y，我们把首次浏览的页面依次压入栈x，当点击后退按钮的时候，再依次从栈x中出栈，并将出栈的数据依次放入栈y。当我们点击前进按钮的时候，我们依次从栈y中取出数据，放入栈x中。当栈x中没有数据的时候，那就说明没有页面可以继续后退浏览量。当栈y中没有数据，那就说明没有页面可以点击前进按钮浏览了。\n\n> x栈是用来实现后退功能的，但是每后退依次，从x栈中取出一个数据，需要放入到y栈中。\n> \n> 当需要实现前进功能的时候，就依次从y栈中取出一个数据。同时将这个数据压如栈x中。\n> \n> 每浏览一个页面就将其数据压入栈x，同时清理y栈。\n> \n> 需要注意的是，如果是访问新的页面的时候，就需要清空y栈，不管是刚开始登录页面，还是回退到某个页面后点击的新页面，这两个时候都不能使用y栈的数据了，也就是说，不能进行前进的操作了。\n\n具体我们可以参照下面的图示来解决问题：\n\n比如我们顺序查看了a,b,c三个页面，我们就依次把a,b,c压入栈，这个时候，两个栈的数据就是下面的样子。\n\n\n\n当我们通过浏览器的后退按钮，从页面c后退到页面a之后，我们就依次把c和b从栈x中弹出，并且依次放入到栈y。这个时候，两个栈的数据就是下面的样子:\n\n\n\n这个时候我们又想看页面b，于是我们又点击前面按钮回到b 页面，我们就把b再从栈y中出栈，放入栈x中。此时两个栈的数据是下面的样子了：\n\n\n\n这个时候，我们通过页面b又跳转到新的压面d上了，页面c就无法再通过前进、后退按钮重复查看了，所以需要清空栈y。此时两个栈的数据如下样子了：\n\n\n\n\n# 内容小结\n\n栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都是o(1)。\n\n\n# 问题思考\n\n 1. 为什么函数调用要用"栈"来保存临时变量？用其他数据结构不行吗？\n    \n    其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。\n    \n    从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一次新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。\n\n 2. 内存管理上的"栈"和数据结构上的"栈"到底是不是一回事。\n    \n    不是。\n\n\n# d27(2020/10/13)\n\n今天主要是要学习的是队列方面的内容。队列在线程池等有限资源池中的应用。\n\n我们知道，cpu资源是有限的，任务的处理速度与线程个数并不是线程正相关的。(不是说，线程个数越多，任务处理的速度越快，当线程很多很多事，频繁的cpu上下文切换，也会导致性能处理下降) 相反，过多的线程反而会导致cpu频繁切换，处理性能下降。\n\n所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。\n\n当我们向固定大小的线程池中请求一个线程的时候，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求呢？是拒绝请求还是排队请求？各种处理策略又是怎么实现的？\n\n实际上，这些问题并不复杂，其底层的数据结构就是我们今天要学习的内容，队列(queue)。\n\n\n# 如何理解"队列"？\n\n队列这个概念非常好理解。可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的"队列"\n\n我们知道，栈只支持两个基本操作：入栈push()和出栈pop()。队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队enqueue()，放一个数据到队列尾部；出队dequeue()，从队列头部取一个元素。\n\n\n\n所以，队列跟栈一样，也是一种操作受限的线性表数据结构\n\n队列的概念很好理解，基本操作也是很容易掌握的。作为一种非常基础的数据结构，队列的应用也是非常广泛，特别是一些具有额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。 比如高性能队列disruptor、linux环形缓存，都用到了循环并发队列；java concurrent并发包利用arrayblockingqueue来实现公平锁等。\n\n\n# 顺序队列和链式队列\n\n如何实现一个队列呢？\n\n跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的栈叫做顺序栈，用链表实现的栈叫做链式栈。同样，用数组实现的队列叫做顺序队列，用链表实现的队列叫做链式队列。\n\n下面的是用java代码试实现的顺序队列的内容：\n\n\n\n\n1\n\n\n相对于栈来说，我们只需要一个栈顶指针就可以了。但是队列需要两个指针：一个是head指针，指向队头；一个是tail指针，指向队尾。\n\n从下面的图示来看。当a、b、c、d依次入队之后，队列中的head指针指向下标为0的位置，tail指针指向下标为4的位置。\n\n\n\n当我们调用两次出队操作之后，队列中head指针指向下标为2的位置，tail指针仍然指向下标为4的位置。\n\n我们会发现你，随着不停地进入入队、出队操作，head和tail都会持续往后移动。当tail移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。这个问题该如何解决？(这是由于只有队头会删除元素，队尾只会增加元素的特性导致的)\n\n我们还记得，在数组那一节，也遇到类似的问题，就是数组的删除操作会导致数组中的数据不连续。我们当时只能使用数据搬移来解决这个问题。但是，每次进行出队操作都相当于删除数组下标为0的数据，要搬移整个队列中的数据，这样出队操作的时间复杂度就会从原来的o(1)变成了o(n)。\n\n实际上，我们在出队列的时候可以不用搬移数据。如果没有空闲空间了，我们只需要在入队的时候，再集中触发一次数据的搬移操作。借助这个思想，出队函数dequeue()保持不变，我们稍加改造一下入队函数enqueue()的实现，就可以轻松解决刚才的问题了。\n\n\n\n\n1\n\n\n从代码中我们看到，当队列的tail指针移动到数组的最右边后，如果有新的数据入队，我们可以将head到tail之间的数据，整体搬移到数组中0到tail-head的位置。\n\n\n\n这种实现的思路中，出队操作的时间复杂度仍然是o(1)，但入队操作的时间复杂度还是o(1)吗？ 出队的情况不考虑，不管是什么情况下的，它的时间复杂度都是o(1)。入队的操作中，最好情况下，这个队列先是空的，有n个位置，可以入队n-1个元素，这个时间复杂度是o(1)。最后当tail == n的时候，就需要移动一次了，最差情况下，需要移动n-1个元素的操作，也是就是o(n)的复杂度了，大致上好像可以理解为摊还分析法来看是o(1)。\n\n\n# 链表队列的实现\n\n我们再来看下链表队列的实现方法。\n\n基于链表的实现，我们同样需要两个指针：head指针和tail指针。它们分别指向链表的第一个结点和最后一个结点。如下图所示，入队时，tail->next = new_node，tail = tail->next；出队的时候，head = head->next。\n\n\n\n\n# 循环队列\n\n我们刚才用数组来实现队列的时候，在tail == n时，会有数据搬移操作，这样入队操作性能就会受到影响。为了避免数据搬移，这里引出了循环队列的解决思路。\n\n循环队列，顾名思义，它长得像一个环。原本数组是有头有尾的，是一条直线。我们可以把首尾相连，扮成一个环。\n\n\n\n我们可以看到，图中这个队列的大小为8，当前head=4，tail=7。当有一个新的元素a入队的时候，我们放入下标为7的位置。但这个时候，我们并不把tail更新为8，而是将其在环中后移一位，到下标为0的位置。当再有一个元素b入队的时候，我们将b放入下标为0的位置，然后tail加1更新为1。所以，在a,b依次入队之后，循环队列中的元素就变成了下面的样子。\n\n\n\n我们通过这样的方法，就成功避免了数据搬移的操作。看起来不难理解，但是循环队列的代码实现难度要比前面的非循环队列难多了。要想写出没有bug的循环队列的实现代码，个人觉得，最关键的是，确定好队空和队满的判定条件。\n\n在用数组实现的非循环队列中，队满的判断条件是tail == n，队空的判断条件是head == tail。那针对循环队列，如何判断队空和队满呢？\n\n队列为空的判断条件仍然是head == tail。但队列满的判断条件就稍微有些复杂了。可以看如下图示：\n\n\n\n就像图中画的队满的情况，tail=3，head=4，n=8的时候，可以总结一下规律如下：(3+1)%8 = 4。多判断几种情况下，我们就会发现，当队满的时候，(tail+1)%n = head.\n\n我们可以发现，当队列满的时候，图中的tail指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。\n\n可以查看如下的实现代码：\n\n\n\n\n1\n\n\n\n# 阻塞队列和并发队列\n\n队列这种数据结构很基础，平时的业务开发不太可能从零实现一个队列，设置都不会直接用到。而一些具有特殊特性的队列应用却比较广泛，比如阻塞队列和并发队列。\n\n阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。 简单来说，会在队空和队满的时候，阻塞操作。\n\n\n\n我们可以看到，上述的定义就是一个"生产者-消费者模型" 。的确，我们可以使用阻塞队列，轻松实现一个"生产者-消费者模型"！\n\n这种基于阻塞队列实现的"生产者-消费者模型"，可以有效地协调生产和消费的速度。当"生产者"生产数据的速度过快，"消费者"来不及消费的时候，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到"消费者"消费了数据，"生产者"才会被唤醒继续"生产"。\n\n而且不仅如此，基于阻塞队列，我们还可以通过协调"生产者"和"消费者"的个数，来提高数据的处理效率。基于上面的例子，我们可以多配置几个"消费者"，来应对一个"生产者"。\n\n\n\n前面讲到的阻塞队列，在多线程的情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？\n\n线程安全的队列，我们叫做并发队列。最简单直接的实现方式是直接在enqueue()、dequeue()方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存活取操作。实际上，基于数组的循环队列，利用cas原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。\n\n\n# 解答开篇\n\n这里回到了开篇的问题。线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？\n\n我们一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。那如何存储排队的请求呢？\n\n我们希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求。我们前面说过，队列有基于链表和基于数组这两种实现方式。这两种实现方式对于排队请求又有什么区别呢?\n\n基于链表的实现方式，可以实现一个支持无限排队的无界队列(unbounded queue)，但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。\n\n而基于数组实现的有界队列(bounded queue)，队列的大小有限，所以线程池中排队的请求超过队列大小的时候，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。\n\n除了前面所说的队列应用在线程池请求排队的场景之外，队列可以应用在任何有限资源池中，队列还可以应用在任何有限资源池中，用于排队请求，比如数据库连接池等。实际上，对于大部分资源有限的场景，当没有空闲资源的时候，基本上都可以通过"队列"这种数据结构来实现请求排队。\n\n\n# 内容小结\n\n今天我们学习了一种和栈很相似的数据结构，队列。\n\n队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。特别是长得像一个环的循环队列。在数组实现队列的时候，会有数据搬移操作，要想聚集数据搬移的问题，我们就需要像环一样的循环队列。\n\n循环队列是重点，关键是要确定好队空和队满的判定条件，具体的代码需要能写出来。\n\n除此之外，我们还讲了几种高级的队列结构，阻塞队列、并发队列，底层都还是队列这种数据结构，只不过在之上附加了很多其他功能。阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全。\n\n\n# d28(2020/10/14)\n\n今天主要学习的是递归方面的内容。提出的问题是，如何用三行代码找到"最终推荐人"。\n\n\n# 如何理解"递归"？\n\n从学习数据结构和算法的经历来看，有个最难理解的知识点，一个是动态规划，另一个就是递归。\n\n递归是一种应用非常广泛的算法(或者编程技巧)。之后我们要讲的很多数据结构和算法的编码实现都要用到递归，比如dfs深度优先搜索、前中后序二叉树遍历等。所以，搞懂递归非常重要，否则，后面复杂一些的数据结构和算法学起来就会比较吃力。\n\n这就是一个非常标准的递归求解问题的分解过程，去的过程叫"递"，回来的过程叫"归"。基本上，所有的递归问题都可以用递归公式来表示。上面的那个例子中，我们用递归公式来表示出来就这样：\n\nf(n)=f(n-1)+1 其中，f(1)=1\n\nf(n)表示我们想知道自己在哪一排，f(n-1)表示前面一排所在的排数，f(1)=1表示第一排的人知道自己在第一排。有了这个递推公式，我们可以将它改为递归代码。\n\nint f(int n){\n   if (n==1) return 1;\n   return f(n-1) + 1;\n}\n\n\n1\n2\n3\n4\n\n\n\n# 递归需要满足的三个条件\n\n什么样的问题可以用递归来解决呢？总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决。\n\n 1. 一个问题的解可以分解为几个子问题的解\n    \n    何为子问题？子问题就是数据规模更小的问题。比如，前面讲的电影院的例子，需要知道，"自己在哪一排"的问题，可以分解为"前一排的人在哪一排"这样一个子问题。\n\n 2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样\n    \n    比如电影院的那个例子，在求解"自己在哪一排"的思路，和前面一排人求解"自己在哪一排"的思路，是一模一样的。\n\n 3. 存在递归终止条件。\n    \n    把问题分解为子问题，把子问题再分解为子字问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。\n    \n    还是电影院的例子，第一排的人不需要再继续询问任何人，就知道自己在哪一排，也就是 f(1) =1, 这就是递归的终止条件。\n\n\n# 如何编写递归代码？\n\n写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很简单了。\n\n假如这里有n个台阶，每次你可以跨1个台阶或2个台阶，请问走这n个台阶有多少种走法？如果有7个台阶，你可以2，2，2，2，1这样子上去，也可以1，2，1，1，2这样子上去，总之走法有很多，如何用编程求得总共有多少种走法呢？\n\n我们仔细想下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了1个台阶，另一类是第一步走了2个台阶。所以n个台阶的走法就可简化为，先走了1个台阶后，n-1个台阶的走法，然后再加上，先走了2阶后，n-2个台阶的走法。用公式表示就是：\n\nf(n) = f(n-1) + f(n-2)\n\n有了递推公式，递归代码基本上就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法。所以f(1)=1。这个递归终止条件足够吗？我们可以用n=2, n=3这样比较小的数试验一下。\n\nn=2时，f(2)=f(1)+f(0)。如果递归终止条件只有一个f(1)=1，那f(2)就无法求解了。所以除了f(1)=1这一个递归终止条件外，还要有f(0)=1，表示走0个台阶有一种走法，不过这样子看起来就不符合正常的逻辑思维了。\n\n所以，我们可以把f(2)=2作为一种终止条件，表示走2个台阶，有两种走法，一步走完或分两步来走。\n\n所以，递归终止条件就是f(1)=1, f(2)=2。这个时候，你可以再拿n=3，n=4来验证一下，这个终止条件是否足够并且正确。\n\n我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：\n\nf(1) = 1;\nf(2) =2;\nf(n) = f(n-1) + f(n-2)\n\n\n1\n2\n3\n\n\n有了这个公式，我们转化成递归代码就简单多了。最终的递归代码是这样的：\n\nint f(int n) {\n    if (n==1) return 1;\n    if (n==2) return 2;\n    return f(n-1) + f(n-2);\n}\n\n\n1\n2\n3\n4\n5\n\n\n总结一下，写递归代码的关键是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。\n\n刚才讲的电影院的例子，我们的递归调用只有一个分支，也就说"一个问题只需要分解为一个子问题"，我们很容易能够想清楚"递"和"归"的每一个步骤，所以写起来、理解起来都不难。\n\n但是，当我们面对的是一个问题要分解为多个子问题的情况，递归代码没那么好理解了。\n\n在第二个例子中，人脑几乎没有办法把整个"递"和"归"的过程一步步都想清楚。\n\n计算机擅长做重复的事情，所以递归正合它的胃口。而我们人脑更喜欢平铺直叙的思维方式。当我们看到递归的时候，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。\n\n对于递归代码，这种试图想清楚整个递归过程的做法，实际上是进入了一个思维误区。很多时候，我们理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。那正确的思维方式应该是怎样的呢？\n\n如果一个问题a可以分解为若干个子问题b、c、d，你可以假设子问题b、c、d已经解决，在此基础上思考如何解决问题a。而且，你只需要思考问题a与子问题b、c、d两层之间的关系即可，不需要一层层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。\n\n因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。\n\n\n# 递归代码要警惕堆栈溢出\n\n在实际的软件开发中，编写递归代码时，我们会遇到很多问题，比如堆栈溢出。而堆栈溢出会造成系统性崩溃，后果非常严重。为什么递归代码容易造成堆栈溢出呢？我们又该如何预防堆栈溢出呢？\n\n我们在"栈"那一节讲过，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会堆栈溢出的风险。\n\n如何避免堆栈溢出呢？\n\n我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度之后，我们就不继续往下再递归了，直接返回报错。下面是根据例子，写的伪代码，为了代码简洁，有些边界条件没有考虑，比如x <=0.\n\n// 全局变量，表示递归的深度\nint depth = 0;\n\nint f(int n) {\n\t++depth;\n\tif (depth > 1000) throw exception;\n\t\n\tif (n==1) return 1;\n\treturn f(n-1) + 1;\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n但是这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最大深度比较小，比如10，50，就可以用这种方法，否则这种方法并不是很实用。\n\n\n# 递归代码要警惕重复计算\n\n除此之外，使用递归时还会出现重复计算的问题。从刚才看到的第二个递归代码的例子，如果我们把整个递归过程分解一下的话，那就是这样的：\n\n从图中，可以直观看出，想要计算 f(5)，需要先计算f(4)和f(3)，而计算f(4)的时候还需要再次计算 f(3)，因此，f(3)就被计算了很多次，这就是重复计算问题。\n\n为了避免重复计算，我们可以通过一个数据结构(比如散列表)来保存已经求解过的f(k)。当递归调用到f(k)时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。\n\n/**\n * recursion\n */\npublic class recursion {\n\n    public int f(int n) {\n        if (n==1) return 1;\n        if (n==1) return 2;\n    }\n\n    // hassolvedlist可以理解为一个map，key是n，value是f(n)\n    if (hassolvedlist.containskey(n)) {\n        return hassolvedlist.get(n);\n    }\n\n    int ret = f(n-1) + f(n-2);\n    hassolvedlist.put(n, ret);\n    return ret;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n除了堆栈溢出、重复计算这两个常见的问题。递归代码还有很多别的问题。\n\n在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大的时候，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度的时候，需要额外考虑这部分的开销，比如我们前面讲的电影院递归代码，空间复杂度并不是o(1)，而是o(n)。\n\n\n# 怎么将递归代码改写为非递归代码？\n\n我们刚才说到，递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；而弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多等问题。所以，在开发过程中，我们要根据实际情况来选择是否需要用递归的方式来实现。\n\n我们可以将电影院的例子，将f(x) = f(x-1)+1这个递推公式，进行改写。\n\nint f(int n) {\n\tint ret = 1;\n\tfor (int i=2; i<=n; ++i) {\n\t\tret = ret + 1;\n\t}\n\treturn ret;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n同样的，第二个例子也可以改为非递归的实现方式。\n\nint f(int n) {\n\tif (n==1) return 1;\n\tif (n==2) return 2;\n\t\n\tint ret = 0;\n\tint pre = 2;\n\tint prepre = 1;\n\tfor (int i=3; i<=n; ++i) {\n\t\tret = pre + prepre;\n\t\tprepre = pre;\n\t\tpre = ret;\n\t}\n\treturn ret;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n笼统地讲，我们可以将递归代码改写为 迭代循环的非递归写法。因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。\n\n\n# 解答开篇\n\n解决下开篇的问题：如何找到"最终推荐人"？\n\nlong findrootrefrrerid(long actorid) {\n\tlong referrerid = select referrer_id from [table] where actor_id = actorid;\n\tif (referrerid == null) return actorid;\n\treturn findrootreferrerid(referrerid);\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# d29(2020/10/15)\n\n今天主要是学习的是排序中的第一个章节的内容，提出的问题是，"为什么插入排序比冒泡排序更受欢迎？"\n\n我们学习的第一个算法中，可能就是排序。大部分的编程语言中，也都提供了排序函数。在平常的项目中，我们也经常会用到排序，排序非常重要，我们会多花一点时间来学习。\n\n排序算法太多了，很多连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。这里只是研究众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。\n\n在这里我们按照时间复杂度把它们分成了三类，分三次来讲解。\n\n\n\n带着问题去学习，是最有效的学习方法。所以按照惯例，我们还是先提出一个思考题：插入排序和冒泡排序的时间复杂度相同，都是o($n^2$)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？\n\n\n# 如何分析一个"排序算法"？\n\n学习排序算法，我们除了学习它的算法原理、代码实现之外，更重要的是要学会如何评价、分析一个排序算法。那分析一个排序算法，要从哪几个方面入手呢？\n\n\n# 排序算法的执行效率\n\n对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：\n\n 1. 最好情况、最坏情况、平均情况时间复杂度\n    \n    我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。\n    \n    除此之外，我们还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。\n    \n    为什么要区分这三种时间复杂度呢？第一，有些排序算法要进行区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。\n\n 2. 时间复杂度的系数、常数、低阶\n    \n    我们知道，时间复杂度反映的是数据规模n 很大的时候的一整个增长趋势，所以它表示的时候会忽略系数、常数、低阶。\n    \n    但是实际的软件开发中，我们排序的可能是10个、100个、1000个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。\n\n 3. 比较次数和交换(或移动)次数\n    \n    这一节和下一节讲的基于比较的排序算法。基于比较的排序算法的执行过程，会涉及到两种操作，一种是元素比较大小，另一种是元素交换或移动。\n    \n    所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换(或移动)次数也考虑进去。\n\n\n# 排序算法的内存消耗\n\n我们前面也讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序(sort in place)。 原地排序算法，就是特指空间复杂度是o(1)的排序算法。今天所描述的三种排序算法，都是原地排序算法。\n\n\n# 排序算法的稳定性\n\n仅仅用执行效率和内存消耗来衡量算法的好坏是不够的的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。\n\n通过一个例子来解释。比如我们有一组数据2，9，3，4，8，3，按照大小排序之后就是2，3，3，4，8，9.\n\n这组数据里有两个3。经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫做稳定的排序算法；如果前后顺序发生了变化，那对应的排序算法就叫做不稳定的排序算法。\n\n两个3，谁在前，谁在后，有什么关系呢？稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？\n\n很多数据结构和算法课程，在讲排序的时候，都是用整数来举例，但是在真正的软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。\n\n比如说，我们现在要给电商交易系统中的"订单"排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有10万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，如何来做？\n\n最先想到的办法是：我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间来排序。这种排序思路理解起来不难，但是实现起来会很复杂。\n\n如果我们借助于稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意这里是按照下单时间，而非金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？\n\n稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的顶你单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同的金额的订单仍然保持下单时间从早到晚有序。\n\n\n\n\n# 冒泡排序\n\n从冒泡排序开始，学习今天的三种排序算法。\n\n冒泡排序只会操作相邻的两个数据。每次冒泡排序都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序工作。\n\n从下面的例子来看下整个冒泡排序的整个过程。我们要对一组数据45，6，3，2，1 从小到大进行排序。第一次冒泡操作的详细过程如下：\n\n\n\n可以看出，经过一次冒泡排序之后，6这个元素已经存储在正确的位置上。要想完成所有数据的排序，我们只需要进行6次这样的冒泡操作就行了。\n\n\n\n实际上，刚才的冒泡过程还是可以优化的。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。参照下面的例子来看，这里面有6个元素排序，只需要4次冒泡操作就可以了。\n\n\n\n冒泡排序算法的原理比较容易理解，代码如下：\n\n\n\n\n1\n\n\n结合代码，我们可以思考三个问题。\n\n\n# 第一，冒泡排序是原地排序算法吗？\n\n冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为o(1)，是一个原地排序算法。\n\n\n# 第二，冒泡排序是稳定的排序算法吗？\n\n在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。\n\n\n# 第三，冒泡排序的时间复杂度是多少？\n\n最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是o(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行n次冒泡操作，所以最坏情况时间复杂度是o($n^2$) 。\n\n最好情况时间复杂度(在定义第1趟排序中，对n个元素，进行n-1次的比较。发现都不满足于前面数组元素要大于紧跟着的相邻的数组元素，那么始终都无法将flag调整为true，进行外层的for循环的第二趟的比较，就直接跳出了整个两层for循环)，结果只有内层for循环的n-1的比较判断操作，直接复杂度是o(n)。\n\n最坏情况时间复杂度，就是整个数据都是正好从大到小的，里面的for循环，和外面的for循环，总共共执行时间复杂度是o($n^2$) 。\n\n\n\n最好、最坏情况下的时间复杂度很容易分析，那平均情况下的时间复杂度是多少呢？我们前面讲过，平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。\n\n对于包含n个数据的数组，这n个数据就有n!种排列方式。不同的排列方式，冒泡排序执行的时间肯定时不同的。比如我们刚才举的例子中，其中一个要进行6次冒泡，而另一个只需要4次。如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。\n\n这里还有一个思路，通过"有序度"和"逆序度"这两个概念来进行分析。\n\n有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式就是这样：\n\n如果i < j，有序元素对：a[i] < = a[j]，就是一个有序元素对\n\n\n1\n\n\n\n\n同理，对于一个倒序排序的数组，如6,5,4,3,2,1， 其有序度是0；对于一个完全有序的数组，比如1，2，3，4，5，6，有序度就是n*(n-1) /2 ，也就是15.我们把这种完全有序的数组的有序度叫做满有序度。\n\n> 第一种理解：假设一个完全有序的数组长度是n，则第1项与其后面的各项有n-1个组合，第2项与其后面各项有n-2个组合，第3项与其后面各项有n-3个组合，第n-1项与其后面各项有1个组合，第n项与其后面各项有0个组合。所以这个公式为：(n-1)+(n-2)+(n-3)+(n-4)+....+(n-1)+0 = (首项+末项)*项数 / 2 ，所以就等于 (n-1 + 0)*n / 2\n> \n> 第二种理解：从n个元素中任意抽取2个元素，其形成的就是一个有序对，我们用cn 2来做，怎么理解呢？ c这种排列，本身对于取的数的先后顺序是不做考虑的(取的两个数，先取哪个，后取哪个，这是一个方法)，而这里恰巧数据就是有序的，有序的意思，就是不用考虑两个数的先后顺序了。符合c排列的概念。cn 2 = n!/(2! (n-2)!) = (n-1)n / 2\n\n逆序度的定义正好跟有序度相反(默认情况下，我们称数据从小到大为有序)，我们应该可以想到了。\n\n逆序元素对：a[i] > a[j]， 如果i < j。\n\n\n1\n\n\n关于这三个概念，我们还可以得到一个公式：逆序度 = 满有序度 - 有序度 。 我们排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序读，就说明排序完成了。\n\n我还是拿前面举的那个冒泡排序的例子来说明。要排序的数组的初始状态是4，5，6，3，2，1。其中，有序元素对有(4,5) (4,6) (5,6)，所以有序度是3。n=6，所以排序完成之后终究的满有序度为 n*(n-1) /2 = 15.\n\n\n\n冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加1。不管算法怎么改进，交换次数总是确定的，即为逆序度 ， 也就是 n*(n-1)/2 - 初始有序度。此例子中就是15-3 =12，要进行12次交换操作。\n\n对于包含n个数据的数组进行冒泡排序，平均交换次数是多少呢？ 最坏情况下，初始状态的有序度是 0，所以要进行n*(n-1) /2次交换。最好情况下，初始状态的有序度是n*(n-1)/2，就不需要进行交换。我们可以取个中间值n*(n-1)/4， 来表示初始有序度既不是很高也不是很低的平均情况。\n\n换句话说，平均情况下，需要n*(n-1)/4次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是o($n^2$) , 所以平均情况下的时间复杂度就是o($n^2$) .\n\n这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。等我们讲到快排的时候，还会再次用这种"不严格"的方法来分析平均时间复杂度。\n\n\n# 插入排序\n\n我们来看一个问题。一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。\n\n\n\n这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。\n\n\n# 插入排序的实现\n\n插入排序具体是如何借助于上面的思想来实现排序的呢？\n\n首先，我们将数组中的数据分为两个区间，已排序区间 和未排序区间 。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。\n\n如图所示，要排序的数据是4，5，6，1，3，2，其中左侧为已排序区间，右侧是未排序区间。\n\n\n\n插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。 当我们需要将一个数据a插入到已排序区间时，需要拿a与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素a插入。\n\n对于不同的查找插入点方法(从头到尾、从尾到头)，元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度。\n\n为什么说移动次数就等于逆序度呢？拿刚才的例子画了一个图表，来观察一下。满有序度是n*(n-1) /2 =15，初始序列的有序度是5，所以逆序度是10。插入排序中，数据移动的个数总和也等于10 = 3+3+4\n\n\n\n下面可以看下插入排序的代码：\n\n// 插入排序，a表示数组，n表示数组大小\npublic void insertionsort(int[] a, int n) {\n  if (n <= 1) return;\n  //外层的for循环，每次从无序的区间中取出一个元素\n  //除去首个数组元素的值，一共要取n-1次\n  for (int i = 1; i < n; ++i) {\n    // 将取出的一个无序区间的一个数组元素的值，赋值给value\n    int value = a[i];\n    // 定义变量j，将i-1的值赋给j\n    int j = i - 1;\n    // 查找插入的位置\n    // 依次从有序的区间的元素，从尾巴到头的方式去遍历\n    // 每次取尾巴的一个元素，直至取到有序区间元素的头部元素\n    for (; j >= 0; --j) {\n      // 如果取出的有序序列中的元素大于，外层for循环取出的要插入的那个无序区间的那个元素\n      if (a[j] > value) {\n        // 就将a[j]的值赋值给a[j+1]\n        // 由于有序的区间的元素都是从小到大排列的\n        // 不存在有序区间中，一些大于value，一些小于value的数\n        // 如果有序区间首个末尾的值要小于value\n        // 那么取的这个外层的无序的要插入的这个元素，就插入在原来的位置上a[j+1] = value\n        // 如果有序区间首个末尾的值要大于value\n        // 那内部的for循环，每次去和有序区间元素比较的时候，\n        // 都去将a[j]赋予到了a[j+1]的位置\n        // 目的是空出a[j]的位置\n        a[j+1] = a[j];  // 数据移动\n      } else {\n        break;\n      }\n    }\n    // 上述循环结束后，就已经找到了需要在a[j]后面的位置\n    // 是插入数据所要在的位置\n    // 注意上述for循环结束后，j是--j，不满足上面的if条件，跳出了循环\n    // 所以相应的是是a[j+1]才是新增数据的位置\n    a[j+1] = value; // 插入数据\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n现在来看三个问题：\n\n第一，插入排序是原地排序算法吗？\n\n从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是o(1)，也就是说，这是一个原地排序算法。\n\n第二，插入排序是稳定的排序算法吗？\n\n在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现的元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。\n\n第三，插入排序的时间复杂度是多少？\n\n如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为o(n)。注意，这里是从尾到头遍历已经有序的数据。\n\n如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度是o($n^2$).\n\n记得在数组中插入一个数据的平均时间复杂度是多少？是o(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行n次插入操作，所以平均时间复杂度为o($n^2$)\n\n\n# 选择排序\n\n选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\n\n\n\n同理需要了解三个问题。\n\n首先，选择排序空间复杂度是o(1)，是一种原地排序算法。选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都是o($n^2$)\n\n那选择排序是稳定的排序算法吗？\n\n答案是否定的，选择排序是一种不稳定的排序算法。从我前面画的那张图中，可以看出来，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。\n\n比如5，8，5，2，9这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素2，与第一个5交换位置，那第一个5和中间的5的顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。\n\n选择排序的代码示例：\n\n// package array;\n/*\n * 选择排序：  3,1,5,2,4,9,6,8,7\n * 稳点性差\n */\npublic class selectsort {\n\tpublic static void selectsort(int [] x) {\n\t\tif(x.length!=0) {\n\t\t\tint temp = 0;\n\t\t\tfor(int i = 0;i<x.length;i++) {\n\t\t\t\tint min = i;\n\t\t\t\tfor(int j = i;j<x.length;j++) {\n\t\t\t\t\tif(x[j] <= x[min]) {\n\t\t\t\t\t\tmin = j;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttemp = x[min];\n\t\t\t\tx[min] = x[i];\n\t\t\t\tx[i] = temp;\n\t\t\t}\n\t\t}\n\t}\n\tpublic static void main(string[] args) {\n\t\tint [] arr = {3,1,5,2,4,9,6,8,7};\n\t\tsystem.out.print("原始数组是：");\n\t\tfor(int a :arr) {\n\t\t\tsystem.out.print(a+",");\n\t\t}\n\t\tselectsort(arr);\n\t\tsystem.out.println();\n\t\tsystem.out.print("排序之后的数组是：");\n\t\tfor(int i = 0;i<arr.length;i++) {\n\t\t\tsystem.out.print(arr[i]+",");\n\t\t}\n\t}\n \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n最外面一层的for循环，表示的要进行多少次的排序，在一个n个元素的数组中，需要进行n次排序。内层的for循环，是用于在无序的各个数组元素中，通过比较选出一个最小的，然后放在有序的数列的末尾。\n\n不管是最好最坏情况下，最外层的for循环n次是少不了。里面的for循环，各个元素的比较也是少不了了，最好最坏的差异只是在于，找到内层for循环的找到最小的元素时，貌似通过第三方变量交换的步骤，也少不了。\n\n时间复杂度分析：\n\n> https://blog.csdn.net/yuzhihui_no1/article/details/44339673\n> \n> 选择排序的时间复杂度不像前面几种排序方法那样，前面几种排序方法的时间复杂度不是一眼就能看出来的，而是要通过推导计算才能得到的。一般会涉及到递归和完全二叉树，所以推导也不是那么容易。但是选择排序就不一样了，你可以很直观的看出选择排序的时间复杂度：就是两个循环消耗的时间；\n> \n> 比较时间：t = （n-1)）+ （n -2）+（n - 3）.... + 1; ===>> t = [n*(n-1) ] / 2；\n> \n> 交换时间：最好的情况全部元素已经有序，则 交换次数为0；最差的情况，全部元素逆序，就要交换 n-1 次；\n> \n> 所以最优的时间复杂度 和最差的时间复杂度 和平均时间复杂度 都为 ：o(n^2)\n\n\n# 解答开篇\n\n冒泡排序和插入排序的时间复杂度都是o($n^2$)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？\n\n我们前面分析冒泡排序和插入排序的时候讲到，冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。\n\n但是，从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要3个赋值操作，而插入排序只需要1个。\n\n冒泡排序中数据的交换操作：\nif (a[j] > a[j+1]) {\n    // 交换\n    int tmp = a[j];\n    a[j] = a[j+1];\n    a[j+1] = tmp;\n    flag = true;\n}\n\n插入排序中数据的移动操作：\nif (a[j] > value) {\n    // 数据移动\n    a[j+1] = a[j];\n} else {\n    break;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n我们把执行一个赋值语句的时间粗略地计为单位时间(unit_time)，然后分别用冒泡排序和插入排序对同一个逆序度是k的数组进行排序。用冒泡排序，需要k次交换操作，每次需要3个赋值语句，所以交换操作总耗时就是3*k单位时间。而插入排序中数据移动操作只需要k个单位时间。\n\n这个只是非常理论的分析，为了实验，针对上面的冒泡排序和插入排序的java代码，我们写了一个性能对比测试程序，随机生成10000个数组，每个数组中包含200个数据，然后在我们的机器上分别用冒泡和插入排序算法来排序，冒泡排序算法大约有700ms才能执行完成，而插入排序只需要100ms左右就能搞定。\n\n所以，虽然冒泡排序和插入排序在时间复杂度是是一样的，都是o($n^2$)，但是如果我们希望把性能优化做到极致，那肯定首选是插入排序。插入排序的算法思路也有很大的优化空间，我们只是讲了最基础的一种。\n\n\n# 内容小结\n\n分析、评价一个排序算法，需要从执行效率、内存消耗和稳定性三个方面来看。因此，这里分析了三种时间复杂度都是o($n^2$) 的排序算法，冒泡排序、插入排序、选择排序。\n\n\n\n在这三种时间复杂度为o($n^2$) 的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面，学习的目的也只是为了开拓思维，实际开发中应用并不多，但是插入排序还是挺有用的。后面讲排序优化的时候，会讲到，有些编程语言中的排序函数的实现原理会用到插入排序算法。\n\n今天学习的三种排序算法，实现代码都非常简单，对于小规模数据的排序，用起来非常高效。但是在大规模数据排序的时候，这个时间复杂度还是稍微有点高，所以我们更倾向于用下一节所用的时间复杂度为o(nlong n)的排序算法。\n\n\n# d30(2020/10/17)\n\n今天主要是对本周学习的基础知识内容，进行一次总结。\n\n整理的内容涵盖了学习方法、复杂度分析方面的内容，数组，链表，栈，队列，递归以及排序的一部分的内容。\n\n\n# 学习方法\n\n在数据结构和算法这部分的知识来看，确实比较难，比较抽象，比较考验人的耐心。对于这门难啃的知识来说，我们首先思想要重视，不要认为看一遍就可以都会了，不要以为粗粗看看就行，不要以为这些都是些过时的、没用的知识，不要以为有侥幸的心理，要学会深究，学会反复思考。\n\n\n# 数据结构和算法的概念\n\n从广义上来看，数据结构就是指一组数据的存储结构，算法就是操作数据的一组方法。\n\n从狭义上来看，是指某些著名的数据结构和算法。比如队列、栈、堆、二分查找、动态规划等。\n\n\n# 数据结构和算法的关系是什么呢？\n\n 1. 数据结构是为算法来服务的，数据结构是静态的，它只是组织数据的一种方式。\n 2. 算法要作用在特定的数据结构之上的，孤立的数据结构是没有用的。\n 3. 例如，因为数据具有随机访问的特点，常用的二分查找算法炫耀用数组来存储数据，但是如果我们选择链表这种数据结构，二分查找算法就无法工作了，因为链表并不只是随机访问。\n\n\n# 学习重点是什么\n\n 1. 先重点学习，算法的复杂度分析的内容，包括各种数据结构和相应算法的最好情况时间复杂度，最坏情况时间复杂度，平均情况时间复杂度。并且分别对应的那种情况的数据排列的情况。\n\n 2. 随后中重点是要学习的是常用的一些数据结构，以及对应的基于这些数据结构上的算法。\n\n 3. 共计20个常用的数据结构和算法。\n    \n    * 10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、trie树。\n    * 10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算分、分治算法、回溯算法、动态规划、字符串匹配算法。\n\n\n# 学习思维\n\n 1. 首先要学习这个数据结构和算法的"来历"，怎么会有出现这种数据结构或相应的算法的，之前的不香吗，之前的是出现了什么问题，要引入新的数据结构和算法呢？\n\n 2. 学习它的"自身特点"，每种特定的数据结构和算法，都有其各自的特点。这些特点，会体现在解决问题的新的思维上。包含了具体的算法实现上，结合我目前的实际情况，我重点关注的c++的实现，以及java的实现，顺带着考虑python的实现。考研的笔试中，会有相应的c++的编程题，而在复试的机试中会有相应的上机题。\n    \n    特别特别要注意，各种数据结构和算法的代码实现上，不要求一开始要掌握全部，不着急，慢慢来，常用的就那么几个，手指头都算的过来。把心思和精力放在，是否真的理解了，是否自己能不看材料，自己写出来。\n\n 3. 学习它的“适合解决的问题”，以及它的“实际应用场景”。新的数据结构和算法肯定要结合具体的场景来存在的，深入理解这些场景，就是为了从这些场景中，提取算法的共性，了解其算法的本质，进而可以推广应用到其他场景中去。\n\n\n# 学习技巧\n\n 1. 边学边练，适度刷题\n 2. 学习的过程，是一个延迟享受的过程，克服自己焦躁的心态。多问、多思考、多互动，没有一个问题是愚蠢的问题。可以躲在留言区写下自己的疑问、思考和总结，也可以经常看看别人的留言，和他们进行互动。\n 3. 打怪升级法。通过设立目标，留言学习，每节课后都写一篇学习笔记或学习心得。\n 4. 知识需要沉淀，不要试图一下子掌握所有。学习知识的过程是反复迭代、不断沉淀的过程。如果碰到"拦路虎"，我们可以尽情在留言区提问，也可以先沉淀一下，过几天再重新学习一边。所谓，书读百遍其义自见。\n\n\n# 复杂度分析\n\n\n# 为什么需要复杂度分析？\n\n 1. 事后分析法的局限性\n    \n    通过统计、监控，就能得到算法执行的时间和占用的内存大小。\n    \n    但是，"事后"测试结果依赖测试环境，不同的测试环境中的测试结果是不一样的。"事后"测试的结果受到数据规模影响很大，数据量少的情况下，很难反应出程序代码的真正效率。\n\n 2. 这个时候，我们需要一个不用具体的测试数据，来粗略估计算法的执行效率的方法。\n\n\n# 引入大o表示法\n\n 1. 从算法的执行效率，粗略来看，就是算法代码执行时间。\n\n 2. 我们假设每行代码都执行类似的操作，读取数据->运算->写数据的操作。\n\n 3. 假设每行代码执行时间都是一样的unit_time。 尽管每行代码对应的cpu执行的个数、执行的时间都不一样。但是，我们这里只是粗略的估计。\n\n 4. 示例代码一分析：\n    \n    int cal(int n) {\n    \tint sum = 0;\n    \tint i = 1;\n    \tfor(; i<=n; ++i) {\n    \t\tsum  = sum + i;\n    \t}\n    \treturn sum;\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    \n    \n    在这段代码中总的执行时间是，(2n+2)* unit_time。\n\n 5. 示例代码二分析：\n    \n    int cal(int n) {\n    \tint sum = 0;\n    \tint i = 1;\n    \tint j = 1;\n    \tfor (; i<=n; ++i) {\n    \t\tj = 1;\n    \t\tfor (; j <=n; ++j) {\n    \t\t\tsum  = sum + i*j;\n    \t\t}\n    \t}\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    11\n    \n    \n    在这段代码中，总的执行时间是，t(n) = (2$n^2$ + 2n +3) * unit_time.\n    \n    第2、3、4行代码，每行都需要1个unit_time的执行时间，\n    \n    第5、6行代码循环执行了n遍，需要2n 个unit_time的执行时间，\n    \n    第7、8行代码循环执行了$n^2$遍，所以需要2$n^2$个unit_time的执行时间。\n    \n    所以，整段代码总的执行时间是：t(n) = (2$n^2$ + 2n +3) * unit_time.\n\n\n# 分析大o表示法\n\n 1. t(n) = o(f(n))\n    \n    t(n)表示的是代码执行的时间，n表示数据规模的大小；f(n)表示的是每行代码执行的次数总和。公式中的o，表示的是代码的执行时间t(n)与f(n)表达式成正比。\n\n 2. 用大o来表示\n    \n    在第一个例子中的t(n) = o(2n+2), 简写为t(n) = o(n).\n    \n    在第二个例子中的t(n) = o(2$n^2$ + 2n + 3)，简写为t(n) = o($n^2$)\n\n 3. 大o时间复杂度实际上并不具体表示代码的真正的执行时间，而是表示的是代码执行时间随着数据规模增长的变化趋势，所以也叫做渐进式时间复杂度，简称为时间复杂度。\n\n 4. 大o的演进\n    \n    分析代码的执行时间，假设每行代码中的执行时间都是一样的。--\x3e 进而考虑的是每行代码的执行次数 --\x3e 用大o来表示执行次数和执行时间的正比关系 --\x3e 去除低阶、常量、系数，真正成为大o表示法。\n\n\n# 时间复杂度分析\n\n 1. 只关注循环次数最多的一段代码。大o复杂度表示法，只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。\n\n 2. 加法法则\n    \n    所谓的加法法则，是指总的时间复杂度等于量级最大的那段代码的复杂度。\n    \n    例如，一个程序中有三段代码，综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就是o($n^2$) .\n\n 3. 乘法法则\n    \n    所谓的乘法法则，是指嵌套代码的复杂度，等于嵌套内外代码复杂度的乘积。\n    \n    可以把乘法法则看成是嵌套循环。\n\n\n# 常见时间复杂度分析\n\n在我们常见的时间复杂度分析的常见量级中，大致可以分为 非多项式量级和多项式量级。\n\n# 非多项式量级\n\n所谓的非多项式量级中，只有两个：o($2^n$) 和o(n!) 。\n\n非多项式的问题，也就是所谓的np(非确定多项式)问题。\n\n在上述的两种非多项式量级中，都是非常低效的。\n\n当数据规模n越来越大的时候，非多项式量级算法的执行时间会急剧增加，求解问题的执行施加会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。\n\n# 多项式量级\n\n在所谓的多项式量级的分类中，我们大致分为三大类：常量级o(1)，对数级o(logn)和o(nlog n)，还有就是复杂度是由两个数据的规模来决定的，o(m+n)和o(m*n)。\n\n# o(1)\n\n表达的是常量级的时间复杂度，这里的1并不是说只执行一行代码，而是指只要代码执行时间不随n的增大而增大，这样代码的时间复杂度都可以记住o(1).\n\n一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万的代码，其时间复杂度也是o(1)。\n\n# o(log n)\n\n在下面的示例代码中，就是一个o(log n)的代码复杂度。\n\ni = 1;\nwhile (i<=n) {\n\ti = i*2;\n}\n\n\n1\n2\n3\n4\n\n\n变量i的值从1开始取，每循环一次就乘以2，大于n时，循环结束。\n\n变量i的取值就是一个等比数列：\n\n$2^1$ , $2^2$ , $2^3$ , $2^4$ , ... $2^k$ ... $2^x$\n\n我们这里先假设当2的x次方的值等于 n的时候，恰好循环结束。通过求解x的值，根据对数的知识可知， x = $log_2 n$ ，所以代码复杂度就是o($log_2 n$)\n\n我们知道对数之间是可以相互转换的，例如$log_3 n$ 就等于$log_3 2$ * $log_2 n$ ，所以我们可以得知o($log_3 n$) = o(c* $log_2 n$) ，其中 c= $log_3 2$就是一个常量。\n\n所以对我们来说，我们可以忽略系数，忽略对数的"底"，所以说，我们在对数阶的时间复杂度的表示方法中，我们忽略了对数的"底"，统一表示为o($log n$)\n\n# o(nlogn)\n\n通过乘法法则，n与log n嵌套循环。\n\n# o(m+n)/o(m*n)\n\n在这里，复杂度是由两个数据的规模来决定的。我们无法事先评估m和n两个数的谁的量级大，不能简单利用加法法则，来忽略掉其中一个。\n\n例如下面的代码：\n\nint cal(int m, int n) {\n\tint sum_1 = 0;\n\tint i = 1;\n\tfor (; i<m; ++i) {\n\t\tsum_1 = sum_1 + i;\n\t}\n\tint sum_2 = 0;\n\tint j = 1;\n\tfor (; j<n; ++j) {\n\t\tsum_2 = sum_2 + j;\n\t}\n\treturn sum_1 + sum_2;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n在上面的例子中，我们无法判断是m的量级大还是n的量级大，所以，这里只能是o(m+n)。\n\n\n# 空间复杂度分析\n\n 1. 表示算法的存储空间与数据规模之间的增长关系。\n\n 2. 一般是指算法额外申请的内存空间的情况。\n\n 3. 与时间复杂度分析的区别来看，时间复杂度看的是算法的执行时间，更进一步来说是，代码中某一行的执行次数与数据规模之间的增长关系。而空间复杂度看的是算法的内存空间，是这个代码使用的内存空间与数据规模之间增长的关系。\n\n 4. 空间复杂度分析示例：\n    \n    void print(int n) {\n    \tint i = 0;\n    \tint[] a = new int[n];\n    \tfor (i; i<n; ++i) {\n    \t\ta[i] = i*i;\n    \t}\n    \tfor (i = n-1; i>=0; --i) {\n    \t\tprint out a[i]\n    \t}\n    }\n    \n    \n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n    \n    在第3行中申请了一个大小为n的int类型数组，整段代码的空间复杂度就是o(n).\n\n\n# 低阶到高阶的排序\n\n从低阶到高阶的排序如下：\n\no(1) < o($log n$) < o(n) < o(n$log n$) < o($n^2$)\n\n\n# d31(2020/10/19)\n\n今天继续对之前学习的数据结构和算法的知识点进行整理。\n\n主要涵盖的是复杂度分析、数组、链表、栈、队列、递归以及排序的一部分的内容。\n\n\n# 复杂度分析\n\n\n# 最好/最坏时间复杂度\n\n# 引入最好/最坏时间复杂度\n\n例如下面的代码中，主要的功能是从一个数组中，找到数据元素值为x的元素，然后返回对应的数组下标。\n\nint find(int[] array, int n, int x) {\n\tint i = 0;\n\tint pos = -1;\n\tfor (; i<n; ++i) {\n\t\tif (array[i] == x) {\n\t\t\tpos = i;\n\t\t}\n\t}\n\treturn pos;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n由于代码中没有break强制的退出，也就是说即使已经找到某个值的数组元素，也要执行完整个for循环。时间复杂度不分情况，都是o(n).\n\n# 进一步分析\n\n我们对上面的代码进行优化，在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，因为有可能中途找到了几可以提前结束循环了。\n\nint find(int[] array, int n, int x) {\n\tint i = 0;\n\tint pos = -1;\n\tfor (; i<n; ++i) {\n\t\tif (array[i] == x) {\n\t\t\tpos = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn pos;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n从上面的代码可以看出，由于查找变量x 可能出现在数组的任意位置。\n\n最好情况下，如果数组中第一个元素正好是要查找的变量x，那就不需要继续遍历剩下的n-1个数据了，那此时的时间复杂度就是o(1)。\n\n但是如果数组中不存在变量x，那么这个时候我们就需要把整个数组都遍历一遍，这个时候的时间复杂度就成了o(n)了。\n\n\n# 平均时间复杂度\n\n如果我们想要考虑的是这个代码在平均情况下的时间复杂度的情况呢？\n\n还是上面从数组中find数组元素的代码，查找的变量x在数组中的位置，有n+1种的情况，也就是在数组的0~ n-1的位置中，和不在数组中。\n\n一般思维：我们统计出所有情况下，代码的执行的次数，然后将这些次数进行累加起来求和。然后再除以n+1种情况，以求解到这个代码遍历元素个数的平均数了。$\\frac{1+2+3+...+n+n}{n+1}$ = $\\frac{n(n+3)}{2(n+1)}$ ，所以这种情况下，我们可以判断出这个平均时间复杂度就是o(n).\n\n仔细思考：n+1种情况，出现的概念并不是一样的。我们可以分析出每种情况的出现的概率，然后每种情况遍历比较的次数 * 该种情况出现的概率，将所有的这些情况都累加起来，得到的就是一个加权的平均数了。\n\n例如，我们要查找的变量x，要么在数组里，要么就不在数组里面。这两种情况对应的概率统计起来很麻烦，为了方便理解，我们假设在数组中与不在数组中的概率都是1/2。另外，要查找的数据出现在0~n-1这n个位置的概率也是一样的，为1/n。所以，根据概率乘法法则，要查找的数据出现在0~n-1种任意位置的概率就是1/(2n).\n\n1$\\times$$\\frac{1}{2n}$ + 2$\\times$$\\frac{1}{2n}$ + 3$\\times$$\\frac{1}{2n}$ + ... + n$\\times$$\\frac{1}{2n}$ + n$\\times$$\\frac{1}{2}$ = $\\frac{3n+1}{4}$\n\n我们得到的加权平均值就是(3n+1)/4 ，由此可以得到平均时间复杂度仍然是o(n).\n\n\n# 均摊时间复杂度\n\n# 总体概述\n\n相比与平均时间复杂度，均摊时间复杂度的使用场景，更为特殊。有哪些场景可以用的上均摊时间复杂度呢？\n\n * 对一个数据结构进行一组连续操作中，大部分的情况下的时间复杂度都很低，只有在个别情况下的时间复杂度比较高。\n * 这些连续的操作之间，存在前后连贯的时序关系，可以将这一组操作放在一块儿分析。\n * 看的是，能否将这些较高的时间复杂度那次操作的耗时，平摊到其他的那些时间复杂度比较低的操作上。\n * 在能够引用均摊时间复杂度分析的场合中，一般均摊时间复杂度就等于最好情况下的时间复杂度。\n\n# 摊还分析法\n\n简而言之，就是利用摊还分析法，来得到均摊时间复杂度。\n\n例如，每一次o(n)的插入操作，都会跟着n-1次的o(1)的插入操作。耗时多的那次操作摊到接下来的n-1次耗时少的操作上。通过均摊下来，这一组连续的操作的均摊时间复杂度就是o(1)了。\n\n我的理解，那一次的o(n)的插入操作的循环代码的次数为n，也就是n个unit_time。将n个单位的unit_time平均摊派到前面的n-1次中，大致每一次都能分到一个unit_time，这样的话，还是o(1)的时间复杂度。由于这个每一次的插入操作的概率都是相同的，所以这种均摊是可以成立的。\n\n# 代码示例\n\n代码中，是一个连续的往数组中插入的操作，数组未满时，赋予相应的数组元素对应的值，并且将count++。当数组满的时候，将现有的数组中的各个元素求和赋予给第一个数组元素的值，同时将count置为1，以便于后面继续从1开始插入元素。\n\n// array 表示一个长度为n的数组\n// 代码中的array.length就等于n \nint[] array = new int[n];\nint count = 0;\n\nvoid insert(int val) {\n\tif (count == array.length) {\n\t\tint sum = 0;\n\t\tfor (int i =0; i < array.length; ++i) {\n\t\t\tsum = sum + array[i];\n\t\t}\n\t\tarray[0] = sum;\n\t\tcount = 1;\n\t}\n\tarray[count] = val;\n\t++count;\n} \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n这个代码是有个循环在反复的调用的。理想的情况下，数组中有空闲的元素位置(count未到数组的大小值)，我们只需要将数据插入到数组下标为count的位置就可以了，所以最好情况时间复杂度是o(1)。\n\n最坏情况下，数组中没有空闲位置了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度是o(n)。\n\n平均情况下，假设数组的长度是n，根据数据插入的位置的不同，我们可以分为n种情况，每种情况的时间复杂度是o(1)。除此之外，还有一种"额外"的情况，就是在数组中没有空闲空间的的时候插入一个数据时，这个时候的时间复杂度是o(n)。而且，这个n+1种情况发生的概率是一样的，都是1/(n+1)。所以，根据加权平均的计算方法，我们求到的平均时间复杂度就是：1* $\\frac{1}{n+1}$ + 1* $\\frac{1}{n+1}$ +... + 1 * $\\frac{1}{n+1}$ + n * $\\frac{1}{n+1}$ --\x3e o(1)\n\n\n# insert()与find()代码比较\n\n# 第一个区别\n\n * find()函数在极端的情况下，复杂度才是o(1)\n * insert()在大部分的情况下，时间复杂度都是o(1)\n * insert()只有在个别的情况下，复杂度才比较高，为o(n)\n\n# 第二个区别\n\n对于insert()函数来说，o(1)的时间复杂度的插入和o(n)的时间复杂度的插入，出现的概率是非常有规律的，而且有一定的前后时序的关系，一般的都是一个o(n)插入之后，紧跟着n-1个o(1)的插入操作(是指已经满的情况下产生的规律)，循环往复。\n\n\n# 数组\n\n\n# 基本概念\n\n 1. 数组array是一种线性表的数据结构\n 2. 数组是用一组连续的内存空间，来存储一组具有相同类型的数据。\n 3. 重点理解:\n    * 第一是线性表。线性表就是数据排成像一条线一样的结果；每个线性表上的数据最多只有前和后两个方向；除了数组之外，链表、队列、栈等也是线性表的结构；非线性表有二叉树、堆、图，这些数据之前并不是简单的前后关系。\n    * 第二，连续内存空间和相同数据类型。这种数据结构的特点优势，就是可以"随机访问"；劣势在于，删除/插入数据，都需要数据移动。\n\n\n# 随机访问实现原理\n\n 1. 计算机会给每个内存单元分配一个地址。\n 2. 计算机通过地址来访问内存中的数据。\n 3. 随机访问数组中某个元素的时候，由于连续内存空间，可以通过寻址公式来查找。a[i]_address = base_address + i * data_type_size\n 4. 例如，int数组元素是需要有4个字节。内存块的首地址是base_address = 1000，那么a[3]地址是: 1000 + 3*4\n\n\n# 数组的时间复杂度描述\n\n 1. 数组适合查找，但是其查找的时间复杂度并非是o(1)。\n 2. 数组支持随机访问，我们可以根据下标来访问数组元素，其时间复杂度是o(1).\n 3. 即使数组元素有序时，二分查找法的时间复杂度也是o(log n)\n\n\n# 低效的"插入"\n\n# "插入"操作\n\n假设线性表的长度是n，现在，如果我们需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，给新插入的数据，我们需要将第k~n这部分的元素都顺序地往后挪一位。\n\n 1. 如果在线性表末尾插入元素，就不需要移动数据了，这个时候的时间复杂度为o(1)，也是最好情况的时间复杂度。\n 2. 如果我们从数组的头部来插入元素，那所有元素都需要依次往后移动一位，所以最坏的时间复杂度是o(n)\n 3. 在每个位置上插入元素的概率都是一样的，平均时间复杂度是o(n)。在一个数组大小为n中，有n个插槽，每个位置的概率都是一样的，都是1/n。而出现在不同的位置上，需要移动的元素的个数是不一样的，在末尾的话是0，倒数第二个是1，倒数第二位是2 .... 直至开头是n。根据加权平均数的求解，那就是1* 1/n + 2* 1/n ... + n* (1/n) = (n+1)/2 ，也就是o(n)。\n\n# 降低"插入"时间复杂度\n\n如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。\n\n如果要将某个数据插入到第k个位置，为了避免大规模的数据搬移。我们就需要直接将第k位的数据搬移到数组元素的最后。然后把新的元素直接放入第k个位置，这个时候的时间复杂度就降为o(1)。\n\n\n# 低效的"删除"\n\n# "删除"操作\n\n跟插入数据类似，如果我们要删除第k个位置的数据，为了保证内存的连续性，也需要搬移数据。不然的话，中间就会出现空洞，内存就不连续了。\n\n如果删除线性表末尾的数据，则最好情况下时间复杂度是o(1)；如果删除线性表开头的数据，那所有元素都需要依次往前移动一位，最坏情况时间复杂度是o(n)；每个位置删除元素的概率也是一样的，参照上面分析的插入的平均时间复杂度来看，是为o(n)。\n\n# 连续删除更高效\n\n 1. 原先的情况下，如果删除多个元素，之前的数组，就需要将数据移动多次了。\n 2. 在某些特殊的场景下，我们并不一定非得追求数组中数据的连续性。\n 3. 这个时候的话，我们每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。\n 4. 当数据中，没有更多空间来存储数据的时候，就触发执行一次真正的删除操作。\n 5. 这样的好处在于，可以大大减少了删除操作导致的数据搬移。\n\n\n# 数组越界问题\n\n在下面的代码中，由于在for循环的结束条件中错写了i < =3，而非 i<3，结果就导致了数据越界。该段代码会无限打印 "hello world"\n\n# include <stdio.h>\nint main(int argc, char* argv[]) {\n\tint i =0;\n\tint arr[3] = {0};\n\tfor (; i<=3; i++) {\n\t\tarr[i] = 0;\n\t\tprintf("%s","hello world\\n");\n\t}\n\treturn 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n原因在于，函数体内的局部变量存在栈上，且是连续压栈。\n\n在linux进程的内存布局中，栈去在高地址空间，从高向低增长。在上面的main 函数中，首先向函数的栈区压入i = 0，紧接着是一个int attr[3]，也就是一个陆续压入了attr[2]、attr[1]、 attr[0]。我们知道数组的内存空间是从低到高的，而栈区的内存地址是从高到低的。\n\n当我们通过attr[3]寻址公式，计算得到地址正好是i的存储地址，所以当对attr[3] = 0 ,也就是对i赋值为0了，结果导致了无限循环打印。\n\n\n# 高级语言的容器类与数组\n\n# 容器类的优势\n\n 1. arraylist最大的优势就是可以将很多数组操作的细节封装起来了。比如前面提到的数组插入、删除数据时需要搬移其他数据等。\n 2. arraylist支持动态扩容，使用arraylist，我们就完全不需要关心底层的扩容逻辑了，arraylist已经帮我们实现好了，每次存储空间不够的时候，它都会将空间自动扩容为1.5倍大小。\n 3. 为了避免频繁内存的申请和数据搬移，arraylist最好事先指定数据大小。\n 4. 在业务的开发中，直接使用容器类，可以省时省力，只会损耗一点点性能。\n\n# 单独使用数组的场景\n\n 1. arraylist无法存储基本类型，比如int、long，需要封装为interger、long类，而autoboxing、unboxing则有一定的性能损耗。\n 2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到arraylist提供的大部分方法。\n 3. 在多维数组的使用上，更加清晰的使用。\n 4. 在基于底层架构开发的时候，可以将性能优化到极致。\n\n\n# 为何数组从0开始\n\n"下标"最确切的定义应该是"偏移(offset)"，a[0]就是偏移量为0的位置。这个时候，计算a[k]的内存地址只需要使用下面的这个公式：a[k]_address = base_address + k*data_type_size\n\n如果数组从1开始，多一层cpu减去指令。这个时候，计算数组元素a[k]的内存地址就会变成，a[k]_address = base_address + (k-1)* data_type_size.\n\n\n# 二维数组寻址公式\n\n一维数组的寻址公式为：\n\n(a1)*[i]_address = base_address + i* type_size\n\n二维数组的寻址公式为：\n\n(a1*a2)* [i][j]_address = base_address + (i*a2 +j) * type_size\n\na2是列的长度，一定要走完i * a2的长度，随后在a[i][o] 的位置开始，往后找j 个长度就是 a[i] [j] 了。\n\n\n# 链表\n\n这个章节，主要整理的是链表的知识点的内容。\n\n\n# 引入链表\n\n由于数组需要一块连续的内存空间，如果这个时候没有连续的、足够大的内存空间的话，就会申请失败。注意：java的arraylist不存在这个问题。\n\n这个时候，就引出了链表，申请一个链表，不需要连续的空间，链表通过"指针"将一组零散的内存块串联起来。\n\n\n# 单链表\n\n在描述单链表的信息时，从基本概念入手，随后讲解单链表的相关操作：单链表的查询、单边表的插入/删除。\n\n# 基本概念\n\n 1. "结点"，在链表中，一个内存块就被称为一个"结点"。\n 2. "后继指针next"：记录下一个结点地址的指针。\n 3. "头结点"：通常我们认为是，放在第一个元素结点之前的一个节点，其数据域一般无意义(也可以存放链表的长度)，头结点可有可无。\n 4. "头指针"：在链表中，头指针是必须要存在的。若存在头结点，头指针是指向头结点的；但是若不存在头结点，头指针指向第一个结点元素。\n 5. 第一个元素结点，顾名思义，就是第一个存放元素值的节点。\n 6. "尾结点"：是指最后一个结点。\n 7. 尾结点的next指针指向一个空地址null，或者可以说，当一个结点的next指针指向null空地址的时候，这个就表示这个结点是链表上的最后一个结点了。\n\n# 单链表的查询操作\n\n当需要随机访问链表的第k个元素的时候，这个场景下面，就和数组有很大的不同了。\n\n需要根据指针一个结点一个结点的一次遍历，单链表的随机访问没有数组好，需要o(n)的时间复杂度。\n\n# 单链表的插入/删除操作\n\n 1. 数组的插入/删除操作，需要保证内存连续性，为了保证数组内存的连续性，在做插入/删除操作的时候，要配套做大量的数据搬移工作，其时间复杂度是o(n)。\n 2. 链表的插入/删除操作，不需要考虑内存的连续性，也不需要进行数据搬移。只需要改变相邻结点的指针，理论上其时间复杂度是o(1)\n\n\n# 循环链表\n\n循环链表是特殊的单链表，与单链表唯一的区别在于尾结点。\n\n单链表的尾结点的指针指向的是null，循环链表的尾结点指针指向链表的头结点(或者说是第一个元素结点)，这种把链尾和链头连接了起来，适合处理环形的数据结构。\n\n\n# 双向链表\n\n# 基本概念\n\n 1. 双向链表与单链表相比，多了一个前驱指针prev。\n 2. 前驱指针prev，会指向前面的结点。\n 3. 后继指针next，指向后面的结点。\n 4. 缺点：双向链表需要额外存储两个指针。\n 5. 优点：支持向前向后(双向)遍历。\n\n# 删除操作\n\n在双向链表中，对某个结点的删除，有两种场景，第1种场景是：删除结点中"值等于某个给定值"的结点；第2种场景是：删除给定指针指向的结点。\n\n# 删除"值等于某个给定值"的结点\n\n 1. 我们要先遍历定位要删除的元素，这时候的遍历的程序代码的时间复杂度是o(n)。\n 2. 执行删除查询到的结点，这个删除的操作的时间复杂度是o(1)。\n 3. 在这种场景下，双链表的删除操作和单链表的删除操作都是一样的，都需要先一个个遍历，时间复杂度也是相同的。\n\n# 删除给定指针指向的结点\n\n在这个场景中，就意味着不用我们再去遍历，找到需要被删除的元素了，而是直接去删除指向这个元素的相应的指针就可以了。\n\n 1. 首先，我们已经找到了要删除的结点。\n 2. 但是，删除结点，需要知道该结点的前驱指针，这个结点的前驱指针指向的是前面的那个结点，我们需要改写前面结点的后继指针，将这个指针指向其他的结点。\n 3. 单链表不支持直接获取其前面结点的指针。单链表需要从头开始遍历链表，这个时候的时间复杂度是o(n)。\n 4. 但是双向链表，可以通过"删除结点"的前驱指针来找到前面的结点了。\n 5. 这个时候的双向链表的删除操作的时间复杂度是o(1)\n\n# 插入操作\n\n对于双向链表而言，可以选择插入到某个结点后面，或者是插入到某个结点前面。\n\n# 插入到某个结点后面\n\n在这个时候，单链表和双链表的时间复杂度就都是一样的了。因为这种场景下，单链表已经知道了"要插入的位置的前面结点"的信息了。\n\n# 插入到某个结点前面\n\n这个时候，如果是单链表的话，操作的步骤和删除操作类似，需要先遍历找到前面结点的信息。这个时候单链表的时间复杂度是o(n)，双链表的操作和上面的删除操作是一样的，时间复杂度是o(1)。\n\n# 查询\n\n如果链表是无序的话，单链表和双链表的查询的时间复杂度都是一样的。\n\n如果链表是有序的，双向链表按值来查询要比单链表要高，双向链表可以根据查询到的值，和目标值进行比较，决定是往前查找，还是往后查找。\n\n# 应用场景\n\n在实际的开发过程中，大多数使用的双向链表。虽然会多存储一些内存空间，这也是用空间换时间的设计体现。\n\n\n# 数组与链表的比较\n\n# 从内存是否连续上\n\n数组简单易用，需要连续的内存空间，可以借助cpu的缓存机制，预读数据，访问效率高。\n\n链表在内存中是非连续的，对cpu缓存不友好，无法预读数据。\n\n# 动态扩容上\n\n数组如果过小，需要申请更大的内存，这个时候需要原来的数组都拷贝过去，非常耗时。\n\n链表本身没有大小限制，天然的就支持动态扩容。\n\n与数组相比，链表更适合插入/删除操作频繁的场景。\n\n\n# 书写链表代码\n\n# 理解指针或引用的含义\n\n在c语言中是有指针的概念的，而在java、python中是没有指针的概念的，取而代之的是引用的概念。\n\n我们可以将某个变量的地址赋值给指针。这样的话，这个指针中就纯粹了这个变量的内存地址，指向了这个变量。通过指针我们就能够找到这个变量。\n\n# 警惕指针丢失和内存泄漏\n\n由于在链表的删除中，可能会由于指针指向的问题，结果就导致了有些链表分成了两个部分。或者说是某个链表结点的指向性丢失了，也就是说无法去释放这些空间的内存了。\n\n程序本身自己是无法知道这部分内存空间在哪里了，更加无法去释放了。\n\n# 利用哨兵来简化实现\n\n我们在单链表中的插入和删除操作中，如果我们在结点p后面插入一个新的结点，只需要下面两行代码就可以了。\n\nnew_node->next = p->next;\np->next = new_node;\n\n\n1\n2\n\n\n但是，当我们要向一个空链表中插入第一个结点时，上面的代码就不能用了。我们需要进行下面的特殊处理，其中 head表示链表的头结点。所以，从这段代码来看，我们可以发现，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不一样的。\n\nif (head == null) {\n    head = new_node;\n}\n\n\n1\n2\n3\n\n\n我们再来看下单链表中的结点删除操作。如果要删除结点p的后继结点，我们只需要一行代码就可以搞定。\n\np->next = p->next->next;\n\n但是，如果我们要删除链表中的最后一个结点，前面的删除代码就不work了。跟插入类似，我们也需要对这种情况特殊处理。写成代码如下：\n\nif (head->next == null) {\n    head = null;\n}\n\n\n1\n2\n3\n\n\n从前面的操作可以看出，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况来进行特殊处理。\n\n为了考虑代码的简洁，可以利用哨兵。引入哨兵结点，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫做带头链表。相反，没有哨兵结点的链表就叫做不带头链表。\n\n哨兵结点不存储数据，因为哨兵结点一直存在，原来的插入第一个结点的场景 --\x3e 就是在哨兵结点后插入结点；删除最后一个结点的场景 --\x3e 这个不是最后一个结点，删除后，还有个哨兵结点。\n\n# 留意边界条件\n\n考虑的边界条件有，链表为空的时候；链表只包含一个结点的时候；链表包含两个结点的时候；处理头结点和尾结点的时候。\n\n\n# 栈\n\n\n# 什么是栈\n\n后进者先出，先进者后出，这就是典型的"栈"结构。\n\n栈是一种"操作受限"的线性表，只允许在一端插入和删除数据。\n\n\n# 栈存在的意义\n\n从功能上来看，数组或链表都可以替代栈。\n\n了解到特定的数据结构是对特定场景的抽象，数组或链表暴露了太多的操作接口，在操作撒花姑娘的确灵活自由，但是在使用上就比较不可控了，自然就更容易出错。\n\n栈的数据结构，是应用于当某个数据集合只涉及在一端插入和删除数据的场景，并且满足于后进先出、先进后出的特性的时候，我们就应该首选"栈"这种数据结构。\n\n\n# 如何实现一个"栈"\n\n用数组来实现的栈，是顺序栈；用链表来实现的栈，是链式栈。栈一般来说有两个操作，入栈(在栈顶插入一个数据)，出栈(从栈顶删除一个数据)。\n\n在栈的使用中，我们需要强化一下空间复杂度的概念，我们所说的空间复杂度，是指除了原本的数据额外的存储空间外，算法运行还需要额外的存储空间。\n\n不管顺序栈还是链式栈，不管是入栈还是出栈，时间复杂度都是o(1)。\n\n\n# 栈在函数调用中的应用\n\n场景描述是，函数调用栈。\n\n 1. os会给每个线程分配一块独立的内存空间。\n 2. 这块内存被组织成"栈"这种数据结构。用来存储函数调用时的临时变量。\n 3. 每进入一个函数，就会将临时变量作为一个栈帧入栈。\n 4. 当调用函数执行完成后，返回之后，会将这个函数对应的栈帧出栈。\n\n\n# 栈在括号匹配中的应用\n\n场景的描述是，我们可以借助栈来检查表达式的括号是否匹配。\n\n 1. 我们可以把([{ 这些都叫做左括号，}])这些都叫做右括号。\n 2. 我们用栈来保存未匹配的左括号\n 3. 从左到右一次扫描字符串\n 4. 当扫描到左括号的时候，将其压入栈\n 5. 当扫描到右括号的时候，从栈顶取出一个左括号，如果这是时候能匹配的上，则继续扫描剩下的字符串；如果不能匹配，或栈中没有数据的话，则说明为非法格式。\n\n\n# 栈在页面前进后退中的应用\n\n我们可以用栈，来实现网页的前进和后退。\n\n 1. 我们可以借助于两个栈来实现。\n 2. 首次浏览页面的时候，依次压入x栈。\n 3. 后退的时候，依次从x栈中弹栈，放入y栈。\n 4. 前进按钮，依次从y栈中取出数据，放入x栈。\n 5. 当访问新页面的时候，压入x栈，同时清空y栈。\n 6. 当x栈没有数据的时候，表明无法进行后退了。\n 7. 当y栈没有数据的时候，表明无法前进了。\n\n\n# d32(2020/10/21)\n\n今天主要是要完成队列、递归，以及排序的一部分的内容。\n\n\n# 队列\n\n\n# 引出队列\n\n我们在线程池，多并发的开发场景中，这个时候如果线程满了。我们该如何去处理新的线程请求呢？拒绝请求还是排队请求呢？\n\n\n# 理解"队列"\n\n先进先出，就是典型的"队列"。与栈有些类似，都有两个基本的操作，入队和出队的操作。我们是从队尾入队，从队头出队。与栈类似，队列是一种操作受限的线性表。\n\n\n# 顺序队列/链式队列\n\n从队列的定义来看，我们把用数组来实现的队列叫做顺序队列；把用链表实现的队列叫做链式队列。\n\n# 顺序队列的两个指针\n\nhead指针，指向队头元素；tail指针，指向队尾元素的下一个位置，需要注意的是tail指针指向的不是队尾元素，而是队尾的下一个，这么做是为了区分队列为空和队列中只有一个元素的两种场景。\n\n判断队列为空的条件：head == tail\n\n判断队列只有一个元素：tail - head = 1\n\n# 顺序队列入队\n\n当tail == n的时候，也就是说队列末尾没有空间了，但是，不存在当tail == n的时候的数组a[tail]。\n\n只有当tail == n并且head == 0的时候，表示队列都占满了。这个时候将触发数据搬移工作，当数据搬迁完以后，再更新head和tail .\n\nfor (int i = head; i< tail; ++i) {\n    item[i-head] = items[i];\n}\n\n\n1\n2\n3\n\n\n更新tail和head指针\n\ntail = tail - head;\nhead = 0;\n\n\n1\n2\n\n\n如果当队列中队尾还有空间，直接一个插入操作就可以了，这个时候对于顺序队列的插入而言，是最好情况下的时间复杂度，此时为o(1)。\n\n当队列中没有空间的时候，for循环来搬移元素，会执行n-1次 ??\n\n当频繁调用顺序队列入队列的时候，从队列为空开始吗，入一个队列，出一个队列的方式的话，这种情况下的程序的代码复杂度是从o(1)到o(n)，利用摊还分析法来看的话是o(1)。\n\n# 链表队列\n\n在链表队列中，head指针指向的是链表的第一个结点，tail指针指向的是最后一个结点，在入队的时候，可以做如下的操作：\n\n在入队的时候，主要有下面的两个代码：\n\ntail->next = new_node\ntail = tail->next\n\n\n1\n2\n\n\n我们将新结点的内存地址，赋值给了原来的tail->next，原来的链表的tail->next指向的是null。这样的话，新结点就可以连接上了原来的链表的后的一个元素了。最后，再修改tail的指针。\n\n出队列的时候，head = head->next\n\n# 循环队列\n\n引入循环队列：\n\n为什么要引入循环队列呢？在原有的顺序队列中，当tail == n的时候，会有数据搬移的操作，循环队列就是为了避免搬移操作。\n\n在循环队列中，我们是将顺序队列首尾相连。\n\n当达到head == tail 的场景中，这是判断队列为空的条件。\n\n当达到(tail+1)%n = head的时候，这个时候是判断队列满的条件，在这个时候tail指向的位置实际上没有存储数据的，这会浪费一个数组的元素的存储空间。\n\n入队列的时候，我们需要移动tail指针，iterms[tail] = item; tail = (tail+1)%n;\n\n出队列的时候，我们需要移动head指针，string ret = iterm[head]; head = (head+1)%n;\n\n不管是head还是tail，加上1后，然后对n取余的目的，在于逆时针移动一格。\n\n# 阻塞队列\n\n我们在队列的基础上增加了阻塞的操作。当队列为空的时候，从队头取数据会被阻塞直到队列中有了数据才能返回；当队列满的时候，插入数据的操作会被阻塞，直到队列中有空闲位置后再插入数据，再返回；基于阻塞队列可以实现"生产者 - 消费者模型"，通过协调"生产者"和"消费者"的个数，可以提高数据的处理效率。\n\n# 并发队列\n\n当几个线程同时操作队列的时候，会引发线程安全的问题。\n\n当多个线程访问某个方法的时候，不管你通过怎样的调用方式或者说这些线程如何交替的执行，我们在主程序中不需要去做任何的同步，这个类的结果行为都是我们设想的正确行为，那么我们就可以说这个类是线程安全的。\n\n但是如果多个线程操作共享变量的时候，就会出现错误了。\n\n线程安全的队列，也叫做并发队列。最简答的直接实现方式是，在enqueue()、dequeue()方法上加锁，实际上基于数组的循环队列(避免搬移数据)的cas原子操作，可以实现高效的并发队列。\n\n\n# 递归\n\n理解递归，去的过程叫"递"，回来的过程叫"归"。\n\n所有的递归问题都可以用递推公式来表示，例如f(n) = f(n-1) +1，其中f(1)=1.\n\n\n# 递归的三个条件\n\n当打算使用递归的时候，必须满足下面三个条件的内容：\n\n 1. 一个问题的解可以分解为几个子问题的解。\n 2. 这个问题与分解之后的子问题，除了数据规模的不同，求解的思路是完全一样的。\n 3. 存在递归终止的条件。\n\n\n# 如何写"递归"代码\n\n 1. 关键在于找到如何将大问题分解为小问题的规律\n 2. 基于分解后的小问题，来写出相应的递推公式\n 3. 再写出推敲递归终止的条件。\n 4. 最后将递推公式和终止条件都翻译成代码。\n\n\n# 递归代码的注意点\n\n 1. 抽象成一个递推公式。\n 2. 不用去想一层层的调用关系。\n 3. 不要试图用人脑去分解递归的每个步骤。\n 4. 警惕堆栈的溢出，可以通过限定深度大小的方式，适用于最大深度比较小的情况下。\n 5. 警惕重复的计算，可以利用散列表来存放已经求解的值。\n 6. 警惕空间复杂度。\n\n\n# 排序\n\n\n# 如何分析一个"排序算法"\n\n# 排序算法的执行效率\n\n排序算法的执行效率来看，可以分为最好情况/最坏情况/平均情况时间的复杂度，我们需要了解到最好情况下要排序的原始数据是什么样的，最坏情况下要排序的原始数据是什么样的。\n\n当我们对同一阶时间复杂度的排序算法进行性能比较的时候，要把系数、常数、低阶也要考虑进去。\n\n在基于比较的排序算法中，这里会涉及到两种操作：一种是元素比较大小，一种是元素交换或移动。\n\n# 排序算法的内存消耗\n\n在分析一个排序算法的时候，要考虑到算法的空间复杂度的情况，当我们描述一个排序算法是一个原地的排序算法的时候，特指这个程序的空间复杂度是o(1)的排序算法。\n\n# 排序算法的稳定性\n\n一般来说，如果待排序的序列中存在值相等的元素，经过排序之后，相等的元素之间原有的先后顺序不变。如果是这种情况的话，我们就说这个排序算法是稳定的。\n\n同理而言，如果同值的两个数，排序后，前后顺序发生了变化，这个时候，我们就称这个排序算法是不稳定的排序算法。\n\n\n# 有序度\n\n有序度是指，在数组中具有有序关系的元素对的个数。\n\n如果i<j，有序元素对：a[i] <= a[j]，这种情况，就叫做一个有序对。\n\n逆序度：\n\n数组中具有逆序关系的元素对的个数。逆序度 = 满有序度 - 有序度，相对而言就是，如果i < j，逆序元素对是 a[i] > a[j]\n\n满有序度：\n\n一般来说，完全有序的数组的有序度叫做满有序度。\n\n满有序度的大小可以用： n*(n-1) / 2来表示大小。\n\n\n# 冒泡排序\n\n# 排序思想\n\n外层for循环，是排序的趟数。而内层的for循环，是对相邻的两个元素的比较饿交换。\n\n我们在对内层的for循环中，会对相邻的两个元素进行比较，如果满足条件，则需要去进行交换，每交换一次，这个数组的序列，有序度就会相应的加上1，相应的逆序度就减1了。\n\n# 冒泡排序代码\n\n具体的代码如下：\n\n/**\n * bubblesort\n */\npublic class bubblesort {\n\n    // 冒泡排序，a表示数组，n表示数组大小\n    public void bubblesort(int[] a, int n) {\n        if (n < =1) return;\n\n        for (int i=0; i <n; ++i) {\n            // 提前退出冒泡循环的标志位\n            boolean flag = false;\n            for (int j =0; j< n-i-1; ++j) {\n                if (a[j] > a[j+1]) {\n                    //交换\n                    int tmp = a[j];\n                    a[j] = a[j+1];\n                    a[j+1] = tmp;\n                    // 表示有数据交换\n                    flag = true;\n                }\n            }\n            // 没有数据交换，提前退出\n            if (!flag) break;     \n        }   \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n优化的冒泡：\n\n * 冒泡排序的时候，也是将数组的元素，分为了两部分：待排序，和已排序的区间。\n * 在外层的for循环内，设置了一个boolean类型的变量flag，初始的默认值为false。\n * 在进行某一趟的，待排序的各个数组元素，进行比较的时候，发现没有移动位置，那么可以判断出这个时候待排序的区间的各个数组元素，实际上都是已经有序了。\n * 可以这样设置，如果内层的for循环中，如果发生了交换，我们就把flag的值设置为true，如果没有发生交换，那么就什么也没做，保留flag的值为false。\n * 随后，我们紧接着是一个if对flag的条件判断，如果flag的值为flase，那么我们就跳出外层的for循环，就此终止所有的排序操作。\n\n\n# 插入排序\n\n从打扑克牌的时候，不断的拿牌，不断的插入排序的思路来理解。\n\n从动态插入新数据的方式，引入了插入排序的方式。这种插入排序的方法同样使用于 静态的数据排序。\n\n# 排序思想\n\n我们把排序的数组，分为两个区间，已排序区间和未排序区间。\n\n初始的已排序的区间只有一个元素，就是数组的第一个元素。\n\n核心思想：取出未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序的区间中元素为空，这样的话，算法结束。\n\n# 插入排序的操作\n\n插入排序的操作中一共包含两种操作，一种是元素的比较，一种是元素的移动。\n\n元素的比较，是指拿需要排序的数a与已排序区间的元素依次比较比较，找到合适和插入位置。\n\n元素的移动，找到对应的插入点之后，还需要将插入带你之后的元素顺序往后移动一位，这样的话才能腾出位置给元素a来插入。\n\n不同的查找插入方法，对于元素的比较次数还是有所区别的。对于不同的查找插入点方法(从头到尾、从尾到头)，元素的比较次数是有所区别的。\n\n在给定的初始序列中，移动操作的次数总是固定的，就等于逆序度。\n\n# 分析插入排序\n\n插入排序算法的运行并不需要额外的存储空间，空间复杂度是o(1)，所以说是原地的排序算法。\n\n对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保证原有的前后顺序不变，所以说插入排序是稳定的排序算法。\n\n从时间复杂度这个角度来看，当排序的数据已经是有序了，也是不需要搬移任何数据的。当从尾到头的方式去查找插入位置后，算法的主要消耗在那未排序的区间的每个元素去和每个元素进行比较。依次比较的次数，也是一个n的等差数列，得到的算法的时间复杂度是o(n)\n\n当数组的各个元素都是倒序的情况的时候，外层的 for循环去寻找位置的o(n)是跑不掉的，内层的for循环需要对找到的位置往后的元素依次往后移动，所以总体来说，其时间复杂度是o($n^2$).\n\n这个平均时间复杂度，用加权求平均数也是可以的，一个数组为n的，有n+1个位置，每个位置出现的概率都是一样的，可以理解为1/(n+1)，然后分析下，每个槽位大致的查找的位置的时间复杂度和移动数组元素的时间复杂度，可以得到该算法的平均时间复杂度是o($n^2$).\n\n\n# 选择排序\n\n# 排序思想\n\n在选择排序中，也是分为已排序区间和未排序区间的。\n\n我们每次会从未排序区间中找到最小的元素，然后将其放到已排序区间的末尾。\n\n# 代码实现\n\n下面的代码是一个选择排序的代码示例：\n\n// package array;\n/*\n * 选择排序：  3,1,5,2,4,9,6,8,7\n * 稳点性差\n */\npublic class selectsort {\n\tpublic static void selectsort(int [] x) {\n\t\tif(x.length!=0) {\n\t\t\tint temp = 0;\n\t\t\tfor(int i = 0;i<x.length;i++) {\n\t\t\t\tint min = i;\n\t\t\t\tfor(int j = i;j<x.length;j++) {\n\t\t\t\t\tif(x[j] <= x[min]) {\n\t\t\t\t\t\tmin = j;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttemp = x[min];\n\t\t\t\tx[min] = x[i];\n\t\t\t\tx[i] = temp;\n\t\t\t}\n\t\t}\n\t}\n\tpublic static void main(string[] args) {\n\t\tint [] arr = {3,1,5,2,4,9,6,8,7};\n\t\tsystem.out.print("原始数组是：");\n\t\tfor(int a :arr) {\n\t\t\tsystem.out.print(a+",");\n\t\t}\n\t\tselectsort(arr);\n\t\tsystem.out.println();\n\t\tsystem.out.print("排序之后的数组是：");\n\t\tfor(int i = 0;i<arr.length;i++) {\n\t\t\tsystem.out.print(arr[i]+",");\n\t\t}\n\t}\n \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n最外面的一层for循环，表示的是要进行多少次的排序，在一个n个元素的数组中，需要进行n次排序。内层的for循环中，是用于在无序的各个数组元素中，通过比较选出一个最小的，然后放在有序的数组的末尾。\n\n# 分析选择排序\n\n在选择排序中，不需要额外的存储空间，其空间复杂度是o(1)，所以说选择排序是一个原地排序算法。\n\n选择排序的思想是从未排序的区间元素中，选择一个最小值，然后将与有序区间末尾的无序区间的第一个元素，交换位置。交换位置的过程，会导致相同元素的前后顺序颠倒。\n\n举例来说，比如5，8，5，2，9这样一组数据，使用选择排序算法来排序的话，第一次找到的最小元素为2，与第一个5交换位置，那第一个5和中间的5的顺序就变了，所以就不稳定了。\n\n最好情况的时间复杂度为o($n^2$)，最外层是对n个数组元素去寻找合适的位置，需要n 趟，如果这个时候数组的各个元素是有序的，里面的for循环，还是需要依次为某个位置，去寻找合适的最小值元素，里面的比较次数n，也是少不了的。在这种情况下，数据是否有序，已经不重要了，比较的次数占了大头。\n\n最坏情况的时间复杂度是o($n^2$)，不管是最好最坏的情况下，最外层的for循环n次是少不了的。里面的for循环，各个元素的比较也是少不了的，最好最坏的差异只是在于，找到内层for循环的时候，利用第三方的变量的交换的步骤，比较的复杂度和交换的时间复杂度是一样的都是o($n^2$)\n\n平均情况下的时间复杂度是o($n^2$)\n\n\n# 插入排序比冒泡排序更好\n\n对于相同的数组元素，究竟是使用插入排序好，还是使用冒泡排序好呢？\n\n这两种排序算法中，不管是什么情况的数组元素，元素交换移动的次数是一个固定值。为什么呢？这是由于各个元素的交换移动的次数，是由数据的逆序度决定的，相同的数组元素，其逆序度是一样的。\n\n但是，在冒泡排序中，需要k次交换操作，是利用一个第三方的中间变量，来实现两个相邻元素的交换的，每次这种交换的操作都需要3个赋值语句，所以交换操作总耗时可以理解为3*k个单位时间。\n\n而在插入排序中，由于直接是用了a[j+1] = a[j]的方式直接赋值了，替换了。只需要k个单位时间。\n\n所以在最好、最坏、平均时间复杂度相同的冒泡排序和插入排序的两种排序算法的比较中，较优的是插入排序的算法。\n\n\n# d33(2020/10/22)\n\n今天开始主要是要整理已经学习的一些数据结构和算法的一些常见代码。\n\n参考github中的思路，参考《大话数据结构》中的思路，从c/c++开始总结起来，其次是java。\n\n在github中虽然有些代码阅读起来比较麻烦，没有统一的风格和格式，甚至还无法运行，有错误。\n\n但是总体来说，还是提供了一些代码的书写的思路和方向。\n\n\n# 抽象数据结构类型\n\n\n# 数据类型\n\n数据类型：是指一组性质相同的值的集合及定义在此集合上的一些操作的总称。\n\n在c语言中，按照取值的不同，数据类型可以分为两类：\n\n * 原子类型：是不可以再分解的基本类型，包括整型、实型、字符型等。\n * 结构类型：由若干个类型组合而成，是可以再分解的。例如，整型数组是由若干整型数据组成的。\n\n抽象是指抽取出事物具有的普遍性的本质。\n\n\n# 抽象数据类型\n\n抽象数据类型(abstract data type, adt)：是指一个数学模型及定义在该模型上的一组操作。抽象数据类型的定义仅取决于它的一组逻辑特性，而与在计算机内部如何表示和实现无关。\n\n抽象数据类型体现了程序设计中问题分解、抽象和信息隐藏的特性。\n\n抽象数据类型的标准格式：\n\nadt  抽象数据类型名\ndata \n     数据元素之间逻辑关系的定义\noperation\n     操作1\n          初始条件\n          操作结果描述\n     操作2\n          ...\n     操作n\n          ...\nendadt\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 数组(线性表)\n\n\n# 线性表的抽象数据类型\n\n我感觉理解每种数据结构，及其对应的算法，就应该从抽象数据类型开始。我们大多人的学习认识都是平铺直叙的方式，适合我们的是循序渐进的认知方式，从这里开始，是认识理解代码的第一步。\n\nadt 线性表(list)\ndata  \n     线性表的数据对象集合为{a1 ... an},每个元素的类型均为datatype。\n     其中，除了第一个元素a1外，每个元素有且只有一个直接前驱元素，\n     除了最后一个元素an外，每一个元素有且只有一个直接后继元素。\n     数据元素之间的关系是一对一的关系。\noperation \n     initlist (*l): 初始化操作，建立一个空的线性表l。\n     listempty (l): 若线性表为空，返回true，否则返回false.\n     clearlist (*l): 将线性表清空。\n     getelem (l,i,*e): 将线性表l中的第i个位置元素值返回给e。\n     locateelem (l,e): 在线性表l中查找与给定值e相等的元素，\n                       如果查找成功，返回该元素在表中序号表示成功；\n                       否则，返回0表示失败。\n     listinsert (*l,i,e): 在线性表l中的第i个位置插入新元素e。\n     listdelete (*l,i,*e): 删除线性表l中第i个位置元素，并用e返回其值。\n     listlength (l): 返回线性表l的元素个数。\nendadt\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n# 线性表的顺序存储结构\n\n参照下面的代码来实现线性表的顺序存储结构。\n\n# define maxsize 20  /*存储空间初始分配量*/\ntypedef int elemtype;  /*elemtype类型根据实际情况而定，这里假设为int*/\n\ntypedef struct\n{\n    elemtype data[maxsiez];  /*数组存储数据元素，最大值为maxsize*/\n    int length;          /*线性表当前长度*/\n} sqlist;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n注意这里几个概念：\n\n * 存储空间的起始位置：数组data，它的存储位置就是存储空间的存储位置。\n * 线性表的最大存储容量：数组长度maxsize.\n * 线性表的当前长度：length.\n * 数组的长度是存放线性表的存储空间的长度。\n * 线性表的长度是线性表中数据元素的长度。\n * 任意时候，线性表的长度应该小于等于数组的长度。(保证数组的空间可用，不去考虑数组空间不够的时候，数组扩容的问题)\n\n\n# 获得元素操作\n\n也就是线性表adt中的getelem，在这个方法中，getelem (l,i,*e): 将线性表l中的第i个位置元素值返回给e。l是线性表的名称，i是数组的第几个位置的元素。\n\n问题1：*e是什么呢？ 准确的说，应该是elemtype *e是什么呢？\n\n * elemtype使我们自己定义的数组类型，这里我们可以大致用int来替换理解下。\n * 应该来说在我们定义的一个getelem的方法中，statu getelem (sqlist l, int i, elemtype *e)中，我们把elemtype *e要理解为elemtype类型的指针，这个指针的变量是e。\n * 单独出现的*e是指，取地址变量e中，所存放的值。\n * 为什么要在这个方法中，有这个指针入参呢？直接写个int e不香吗？这里涉及到一个问题，我们大多数使用数组的元素的各个方法中，可以会涉及到修改数据里面的元素的值的情况，如果不是用对应的指针变量作为方法的入参的话，那么就肯定会出现无法修改数组元素值的情况。\n * 重点：在c/c++ 中，如果在某个方法的入参中，出现了指针变量，注意了，有可能会改变这个变量值的情况。另外一种修改全局变量值的方法，就是引用，在方法的入参中，出现了符号 &.\n\n代码如下：\n\n# define ok 1\n# define error 0\n# define true 1\n# define false 0\ntypedef int status;\n/*这里把status也是一个int类型的数据类型*/\n/*status是函数的类型，其值是函数结果状态代码，如ok等*/\n/*初始条件：顺序线性表l已存在，1<=i<=listlength(l)*/\n/*操作结果：用*e返回l中第i个数据元素的值*/\nstatus getelem (sqlist l, int i,elemtype *e)\n{\n    if (l.length==0 || i<1 || i>l.length)\n        return error;\n    *e = l.dat[i-1];\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n这里的返回类型status是一个整型，返回ok代表1，error代表0。\n\n\n# 插入操作\n\n这个操作方法的主要的目的，是往线性表l的第i个位置插入新的元素e。\n\nlistinsert (*l,i,e)\n\n插入算法的思路是：\n\n * 如果插入位置不合理，就抛出异常；\n * 如果线性表长度大于等于数组长度，则抛出异常或动态增加容量；\n * 从最后一个元素开始向前遍历到第i个位置，分别将它们都向后移动一个位置；\n * 将要插入元素填入位置i处；\n * 表长加1.\n\n实现代码如下：\n\n# define maxsize 20  /*存储空间初始分配量*/\ntypedef int elemtype;  /*elemtype类型根据实际情况而定，这里假设为int*/\n\n/*定义了一个结构体类型sqlist*/\ntypedef struct\n{\n    elemtype data[maxsiez];  /*数组存储数据元素，最大值为maxsize*/\n    int length;          /*线性表当前长度*/\n} sqlist;\n\n# define ok 1\n# define error 0\n# define true 1\n# define false 0\ntypedef int status;\n/*这里把status也是一个int类型的数据类型*/\n/*status是函数的类型，其值是函数结果状态代码，如ok等*/\n/*初始条件：顺序线性表l已存在，1<=i<=listlength(l)*/\n/*操作结果：用*e返回l中第i个数据元素的值*/\n\n/*初始条件：顺序线性表l已存在，1<= i <= listlength(l)*/\n/*操作结果：在l中第i个位置上插入新的数据元素e，l的长度加1*/\n\nstatus listinsert (sqlist *l, int i, elemtype e)\n{\n    int k;\n    /*顺序线性表已经满*/\n    if (l->length == maxsize)\n      return error;\n    /*当i不在范围内时*/\n    if (i<1 || i>l->length+1)\n      return error;\n    /*若插入数据位置不在表尾*/\n    if (i<= l->length) {\n        /*将要插入位置后数据元素向后移动一位*/\n        /*length-1是目前线性表中，在数组中的最大的下标*/\n        /*线性表的第i个位置，也就是数组下标是i-1的位置*/\n        for (k= l->length -1; k>=i-1; k--) {\n            l->data[k+1] = l->data[k];\n        }\n    }\n    /*将新元素插入*/\n    l->data[i-1]=e;\n    l->length++;\n    return ok;\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 删除操作\n\n这个删除方法中，也就是从一个线性表l中，删除第i个数据元素。\n\n删除算法的思路：\n\n * 如果删除位置不合理，抛出异常；\n * 取出删除元素；\n * 从删除元素位置开始遍历到最后一个元素位置，分别将它们都向前移动一个位置。\n * 表长度减去1\n\n参考的代码如下：\n\n# define maxsize 20  /*存储空间初始分配量*/\n# define ok 1\n# define error 0\n# define true 1\n# define false 0\ntypedef int status;\n/*这里把status也是一个int类型的数据类型*/\n/*status是函数的类型，其值是函数结果状态代码，如ok等*/\n/*初始条件：顺序线性表l已存在，1<=i<=listlength(l)*/\n/*操作结果：用*e返回l中第i个数据元素的值*/\n\ntypedef int elemtype;  /*elemtype类型根据实际情况而定，这里假设为int*/\n\ntypedef struct\n{\n    elemtype data[maxsize];  /*数组存储数据元素，最大值为maxsize*/\n    int length;          /*线性表当前长度*/\n} sqlist;\n\n/*初始条件：顺序线性表l已存在，1<= i <= listlength(l) */\n/*操作结果：删除l的第i个数据元素，并用e返回其值，l的长度减1*/\nstatus listdelete (sqlist *l, int i, elemtype *e) {\n    int k;\n    /*线性表为空*/\n    if (l->length == 0) {\n        return error;\n    }\n    /*删除位置不正确*/\n    if (i<1 || i>l->length) {\n        return error;\n    }\n    *e = l->data[i-1];\n    /*如果删除不是最后的位置*/\n    if (i< l->length) {\n        /*将删除位置后继元素前移*/\n        for (k = i; k< l->length; k++) {\n            l->data[k-1] = l->data[k];\n        }\n    }\n    l->length--;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\n\n# d34(2020/10/23)\n\n今天主要是要复习的是链表的各种算法代码。\n\n\n# typedef与struct\n\nstruct是用来定义结构体的，typedef是用来用于自定义类型的。\n\n参考网上的文档。\n\n> https://blog.csdn.net/weixin_41262453/article/details/88120561\n\n\n# 定义结构体/无变量/无类型\n\n这里定义的是最初始的结构体，没有基于定义结构体的时候，定义了该结构体的变量，也没有给这个结构体定义一个自定义的类型，这个结构体中也没有结构体指针。\n\nstruct node {\n    // 一些基本的数据结构或自定义的数据类型\n};\n\n\n1\n2\n3\n\n\n这个时候如果要定义这种类型的结构体的变量时，要这么写struct node n;\n\n\n# 定义结构体(包含结构体变量)\n\n这个时候定义的结构体中，顺带定义了结构体类型的变量，而且这种变量和普通变量一样。这个时候，结构体理解为是一种数据类型。包含结构体类型的变量，和指向这种结构体类型内存地址指针的变量。\n\nstruct studentinfo {\n\tint id;\n\tchar gender; //\'f\' or \'m\'\n\tchar name[20];\n\tchar major[20];\n} alice, bob, stu[1000], *p;\n\n\n1\n2\n3\n4\n5\n6\n\n\n在这个结构体定义的例子中，studentinfo是结构体的名字，alice和bob是代表着两个结构体变量，stu[1000]代表的是这种结构体的数组，而*p代表的是这种结构体类型的指针变量p，这个变量p存储着一个这种结构体数据类型的内存地址。\n\n\n# 定义结构体(typedef定义了这种结构体的别名)\n\n在这个案例中，会使用到了typedef这个关键字了，定义了这种结构体数据类型的一个别名。\n\ntypedef struct node \n{\n    int no;\n    char name[10];\n} stu, student;\n\n\n1\n2\n3\n4\n5\n\n\n后面的stu和student都是这种结构体类型的别名。我们在声明这种结构体类型的变量的时候，可以这么写了。\n\nstu a1; 或者是student a2;\n\n\n# typedef定义符合类型(指针或数组)\n\n这个理解起来，有些绕口，但是还是会有这样的代码出现。\n\ntypedef还可以用来掩饰复合类型，如指针和数组。\n\n定义一个typedef，每当要用到相同类型和大小的数组时，可以这样书写：\n\ntypedef char line[81];\n\n此时line类型即代表了具有81个元素的字符数组(也就是char[81]的意思)，使用方法，例如line text.\n\n再比如下面，定义了下面，利用typedef定义结构体别名，单独来定义了这个结构体类型的指针变量。\n\n/*线性表的单链表存储结构*/\ntypedef struct node\n{\n    elemtype data;\n    struct node *next;\n} node;\ntypedef struct node *linklist;  /*定义linklist*/\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n其中typedef struct node *linklist中的struct可以省略。\n\nlinklist就node类型的指针变量。\n\n\n# 定义结构体(内含同类型结构体指针变量)\n\n这个结构体node里面又嵌套了一个node的数据类型的指针变量，这个指针变量中存放的内存地址，是下个结点的内存地址。\n\nstruct node \n{\n    int age;\n    struct node *next;\n};\n\n\n1\n2\n3\n4\n5\n\n\n通过测试，不管是在c还是c++中，结构体里面的那个struct也是可以去掉的。\n\n\n# 定义结构体(typdef与内含结构体指针的结合)\n\n先看下面的例子是错误的写法。\n\ntypedef struct \n{\n\tint age;\n\tstudent1 *next; \n}student1,*studentptr;\n\n\n1\n2\n3\n4\n5\n\n\n这是由于student1是在结构体的末尾定义的，在结构体的内部是无法识别这个被typedef定义为别名student1，这个类型的。\n\n应该改为如下的写法：\n\ntypedef struct student1\n{\n\tint age;\n\tstudent1 *next; \n}student1,*studentptr;\n\n\n1\n2\n3\n4\n5\n\n\n里面的*next前面的student1实际上是一开始typedef struct后面的名字，这样里面的结构体指针定义的时候，才能正确识别。\n\n\n# 结构体指针初始化\n\n\n# 单链表\n\n下面是单链表的存储结构：\n\n/*线性表的单链表存储结构*/\ntypedef struct node\n{\n    elemtype data;\n    struct node *next;\n} node;\ntypedef struct node *linklist;  /*定义linklist*/\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n从这个单链表的存储结构体来看，这个结点由存放数据元素的数据域和存放后继结点地址的指针域组成。\n\n定义了一个node类型的指针变量，用来存储node类型的内存地址，该变量的名称叫做linklist.\n\n假设p是指向该单链表的第i个元素的指针，结点$a_i$的数据域，可以用p->data来表示，也就是p->data = $a_i$ , 同样p->next指向的是$a_i$的下一个元素，也就是指向第i+1个元素，即指向$a_{i+1}$的指针，那么p->next->data = $a_{i+1}$ .\n\n\n# 单链表的读取1(大话系列)\n\n和数组有些类似，同样要实现的是读取线性表的第i个元素。该操作的方法，同样定义为status getelem (linklist l, int i, elemtype *e)\n\n获得链表第i个数据的算法的思路：\n\n 1. 声明一个指针p指向链表第一个结点，初始化j从1开始；\n 2. 当j<i时，就遍历链表，让p的指针向后移动，不断指向下一个结点，j累加1；\n 3. 若到链表末尾p为空，则说明第i个元素不存在；\n 4. 否则查找成功，返回结点p的数据。\n\n代码如下：\n\n# define ok 1\n# define error 0\n# define true 1\n# define false 0\n/*这里把status也是一个int类型的数据类型*/\n/*status是函数的类型，其值是函数结果状态代码，如ok等*/\ntypedef int status;\n\n/*elemtype类型根据实际情况而定，这里假设为int*/\ntypedef int elemtype;  \n\n/*线性表的单链表存储结构*/\ntypedef struct node\n{\n    elemtype data;\n    struct node *next;\n} node;\ntypedef struct node *linklist;  /*定义linklist*/\n\n\n/*初始条件：顺序线性表l已存在，1<= i <= listlength(l)*/\n/*操作结果：用e返回l中第i个数据元素的值*/\nstatus getelem (linklist l, int i, elemtype *e) \n{\n    int j;\n    /*声明了指针p*/\n    linklist p;\n    /*让p指向链表l的第一个结点*/\n    /*也可以改写为p=head->next;这里的l可以理解为头指针*/\n    p = l->next;\n    /*j为计数器*/\n    j = 1;\n    /*p不为空或计数器j还没有等于i的时候，循环继续*/\n    while (p && j<i) {\n        /*让p指向下一个结点*/\n        p = p->next;\n        ++j;\n    }\n    /*第i个元素不存在*/\n    /*定位位置不合理：空表或i小于0或i大于表长*/\n    if (!p || j>i) {\n        return error;\n    }\n    /*取第i个元素的数据*/\n    *e = p->data;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n# 单链表(按位序查找)\n\n上面的代码的实际情况，根据提供需要查找的线性表(链表)的第几个位置，来进行返回。\n\n//按位置来查找链表元素\nint getelem(int i) \n{\n    // 获取第i个数据元素的值\n    node *p;\n    p = head->next;\n    int j = 1;\n    while (p && j<i) {\n        p=p->next;\n        j++;\n    }\n    // 定位位置不合理：空表或i小于0或i大于表长\n    if (!p || j>i) {\n        cout << "位置异常";\n        return -1;\n    } else {\n        return p->data;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# 单链表查询(按值查找)\n\n按照来查找的操作是，在链表中来查找是否有结点值等于给定值key的结点，若有，则返回首次找到的值为key的结点的存储位置；否则返回null。查找过从开始结点出发，顺着链表逐个将结点的值和给定值key做比较。\n\n//按值来查找，匹配值的链表的位置\nint locateelem(int e)\n{\n    int j=1;\n    node *p;\n    p = head->next;\n    while (p && p->data!=e) {\n        p = p->next;\n        j++;\n    }\n    if (p == null) {\n        return 0; //0表示不存在，而非0，则表示存在，返回位置\n    } else {\n        return j;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 单链表的插入\n\n已知一个单链表，在这个单链表的第i个结点位置上，插入一个新的结点，这个新结点中有新插入的结点元素的值。\n\n单链表的第i个数据插入结点的算法思路：\n\n 1. 声明一结点p指向链表的第一个结点，初始化j从1开始；\n 2. 当j<i的时候，就遍历链表，让p的指针向后移动，不断指向下一个结点，j累加1；\n 3. 若到链表末尾p为空，则说明第i个元素不存在；\n 4. 否则查找成功，在系统中生成一个空结点s；\n 5. 将数据元素e赋值给s->data;\n 6. 单链表的插入标准语句s->next = p->next； p->next=s;\n 7. 返回成功。\n\n具体的代码如下，重点在于，p结点是要插入位置的前面一个结点，先将原有的p->next值 ，赋值给结点s->next；然后将新增的s结点的内存地址赋值给p->next.\n\n/*初始条件：顺序线性表l已存在，1<= i <= listlength(l)*/\n/*操作结果：在l中第i个位置之前插入新的数据元素e，l的长度加1*/\nstatus listinsert(linklist *l, int i, elemtype e) \n{\n    int j;\n    linklist p,s;\n    p = *l;\n    j = 1;\n    //寻找第i个结点\n    while (p && j < i) {\n        p = p->next;\n        ++j;\n    }\n    // 第i 个元素不存在\n    if (!p || j>1) {\n        return error;\n    }\n    // 生成新结点, malloc是c语言的标准函数\n    s = (linklist) malloc(sizeof(node));\n    s->data = e;\n    // 将p的后继结点赋值给s的后继\n    s->next = p->next;\n    // 将s赋值给p的后继\n    p->next = s;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n简化后的c++的单链表插入的代码如下：\n\nvoid listinsert(int i, int e)\n{\n    int j = 0;\n    node *p;\n    p = head;\n    // 定位到插入点之前\n    while (p && j < i-1) {\n        p = p->next;\n        j++;\n    }\n    // 插入位置不合理，i<0或者i>表长\n    if (!p || j > i-1) {\n        cout << "位置异常，结点插入失败！";\n        return;\n    } else {\n        node *s;\n        s = new node;\n        s->data = e;\n        s->next = p->next;\n        p->next = s;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n单链表的插入数据元素，无需像顺序表那样移动其后续数据元素，算法的时间主要耗费在查找正常的插入位置，最好o(1)，最坏o(n)，平均o(n).\n\n\n# 单链表的创建\n\n对于每个链表来说，它所占用空间的大小和位置是不需要预先分配划定的，可以根据系统的情况和实际的需求即时生成。\n\n创建单链表的过程就是一个动态生成链表的过程。即从"空表"的初始状态起，依次建立各元素结点，并逐个插入链表。\n\nvoid createlisthead(linklist *l, int n) 的单链表的整表创建的算分思路：\n\n 1. 声明一结点p和计数器变量i;\n 2. 初始化一空链表l;\n 3. 让l的头结点的指针指向null，即建立一个带头结点的单链表；\n 4. 循环：\n    * 生成一新结点赋值给p；\n    * 随机生成一数组赋值给p的数据域p->data;\n    * 将p插入到头结点与前一新结点之间。\n\n# 头插法\n\n顾名思义，就是始终让新结点在第一的位置。我们可以把这种算法简称为头插法。\n\nc语言的代码示例如下：\n\n/*随机产生n个元素的值，建立带表头结点的单链线性表l(头插法)*/\nvoid createlisthead(linklist *l, int n) \n{   \n    //这里的入参中的  linklist *l  是否可以改为linklist l\n    // 结点p是要插入的新结点\n    // 结点l的指针指向的地址，可以理解为头指针\n    linklist p;\n    int i;\n    /*初始化随机数种子*/\n    srand(time(0));\n    *l = (linklist) malloc(sizeof(node));\n    /*先建立一个带头结点的单链表*/\n    (*l)->next = null;\n    for (i=0; i<n; i++) {\n        /*生成新结点*/\n        p = (linklist) malloc(sizeof(node));\n        /*随机生成100以内的数字*/\n        p->data = rand()%100+1;\n        p->next = (*l)->next;\n        /*插入到表头*/\n        (*l)->next = p;\n    }\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n重点代码是，一开始创建了一个带有头结点的单链表，这个结点中没有存放任何的数据，只是这个头结点的后继指针指向null，指向这个头结点的指针(l)理解为head指针。\n\n关键代码：p->next = (*l)->next;和(*l)->next = p;\n\n用c++的代码来描述：\n\nvoid createlist1(int n)\n{\n    //头插法创建线性表\n    // p指向头结点的头指针，s是新增的结点\n    node *p, *s;\n    p = head;\n    cout << "请依次输入" <<n<< "个数据元素值：" <<endl;\n    for (int i=1; i<=n; i++) {\n        // 新建结点\n        s = new node;\n        cin >> s->data;\n        // 新结点插入表头\n        s->next = p->next;\n        // 将指针变量s的值赋值给p->next\n        p->next = s;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\nc++的代码相对来说比较清晰，创建了node 类型的 结点p和结点s这两个指针变量，其中变量p存放的理解为是头结点上的头指针，而变量s存放的理解为是要新增结点上面的指针变量的内存地址。\n\n# 尾插法\n\n头插法虽然算法简单，如果按$a_1$，$a_2$, ... , $a_n$的顺序插入结点元素，头插法插入后展示的链表的各个元素的情况，正好和输入的顺序相反。即首先被插入的结点是线性表的最后一个数据元素$a_n$，最后被插入的结点是线性表的第一个数据元素$a_1$。\n\n为了实现创建链表过程中结点的输入顺序与结点实际的逻辑次序相同，这里可以采用尾插入法。尾插入法每次讲新生成的结点插入到当前链表的表尾上。这里需要增加一个尾指针，始终指向当前链表的尾结点。\n\n如下是c语言的代码：\n\n/*随机产生n个元素的值，建立带表头结点的单链线性表l(尾插法)*/\nvoid createlisttail(linklist *l, int n) \n{\n    //p是新增的结点，r是尾部指针\n    linklist p,r;\n    int i;\n    /*初始化随机数种子*/\n    srand (time(0));\n    /*为整个线性表*/\n    *l = (linklist) malloc(sizeof(node));\n    /*r为指向尾部的结点*/\n    r = *l;\n    for (i = 0; i < n; i++) {\n        /*生成新结点*/\n        p = (node *) malloc(sizeof(node));\n        /*随机生成100以内的数字*/\n        p->data = rand()%100+1;\n        /*将表尾终端结点的指针指向新结点*/\n        r->next = p;\n        /*将当前的新结点定义为表尾终端结点*/\n        r = p;\n    }\n    /*表示当前链表结束*/\n    r->next = null;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n重点在于两段代码r->next = p; r = p;, 在这里l与r的关系，l是指整个单链表，而r是指向的是尾结点的变量，r会随着循环不断地变化结点，而l则是随着循环增长为一个多结点的链表。\n\n如下是c++的代码示例：\n\nvoid createlist2(int n)\n{\n    //尾插法创建线性表\n    // p结点是尾巴指针变量\n    // s结点是新插入的结点的指针变量\n    node *p,*s;\n    p = head;\n    cout << "请依次输入："<< n <<"个数据元素值:" <<endl;\n    for (int i = 1; i <= n; i++) {\n        // 新建结点\n        s = new node;\n        cin >> s->data;\n        // 新结点插入表尾\n        p->next = s;\n        p = s;\n    } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 单链表的单个元素删除\n\n这里所指的是将这个单链表中的第i个结点删去，也就是改变$a_{i-1}$ 、$a_i$与$a_{i+1}$之间的链接关系。\n\n因为在单链表中结点$a_i$的存储地址是在其前驱结点$a_{i-1}$的指针域next中。\n\n 1. 所以必须首先找到$a_{i-1}$的存储位置p。\n 2. 然后令p->next指向$a_i$的后继结点，即将$a_i$从链表上摘下。\n 3. 最后释放结点$a_i $的空间。\n\n单链表删除第i个数据结点的步骤如下：(p为ai-1 结点，q为ai结点)\n\n 1. 声明一结点p指向链表第一个结点，初始化j从1开始\n 2. 当j<i的时候，就遍历链表，让p的指针向后移动，不断指向下一个结点，j累加1；\n 3. 若到链表末尾p为空，则说明第i个元素不存在；\n 4. 否则查找成功，将欲删除的结点p->next赋值给q；\n 5. 单链表的删除标准语句p->next = q->next\n 6. 将q结点中的数据赋值给e，作为返回；\n 7. 释放q结点；\n 8. 返回成功。\n\n如下是c语言的代码：\n\n/*初始条件：顺序线性表l已存在，1<=i<=listlength(l)*/\n/*操作结果：删除l的第i个数据元素，并用e返回其值，l的长度减1*/\nstatus listdelete (linklist *l, int i, elemtype *e) \n{\n    int j;\n    linklist p, q;\n    p = *l;\n    j = 1;\n    /*遍历寻找第i个元素*/\n    while (p->next && j < i) {\n        p = p->next;\n        ++j;\n    }\n    if (!(p->next) || j>i) {\n        /*第i个元素不存在*/\n        return error;\n    }\n    q = p->next;\n    /*将q的后继赋值给p的后继*/\n    p->next = q->next;\n    /*将q结点中的数据给e*/\n    *e = q->data;\n    /*让系统回收此结点，释放内存*/\n    free(q);\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n相对c++的代码如下所示：\n\n// i 是第i个位置，删除数据元素e\nnode* listdelete(int i, int e)\n{\n    int j = 0;\n    node *p;\n    p =  head;\n    // 定位到删除点之前\n    while (p && j < i-1)\n    {\n        p = p->next;\n        j++;\n    }\n    if (!p || j > i-1)\n    [\n        cout<<"位置异常，结点插入失败!";\n        return ;\n    ]\n    // 插入位置不合理，i<0或i>表长\n    else {\n        node *s;\n        s = p;\n        p->next = p->next->next;\n        return s;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 单链表整体删除\n\n当我们不打算使用这个单链表的时候，我们就需要将它销毁，其实也就是在内存中将它释放掉。\n\n单链表整表删除的算法思路如下：\n\n 1. 声明一个结点p和q\n 2. 将第一个结点赋值给p；\n 3. 循环：\n    * 将下一个结点赋值给q；\n    * 释放p；\n    * 将q赋值给p\n\nc语言中实现的代码如下：\n\n/*初始条件：顺序线性表l已经存在，操作结果：将l重置为空表*/\nstatus clearlist(linklist *l)\n{\n    linklist p,q;\n    /*p指向第一个结点*/\n    p = (*l)->next;\n    /*没到表尾*/\n    while (p)\n    {\n        q = p->next;\n        free(p);\n        p=q;\n    }\n    /*头结点指针域为空*/\n    (*l)->next = null;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n这里需要注意的是，q变量的存在还是很有意义的。在循环体内，不能直接写free (p) ; p = p->next，如果直接free(p)的话，那么就删除了p的整个结点，包含 p的数据域和指针域，就找不到后面的一个链表元素了。这里用q=p->next的目的，就是在于在删除前，先保留住p结点后面的结点的指针地址。\n\n\n# d35(2020/10/26)\n\n今天需要总结的是静态链表的相关知识点。\n\n\n# 静态链表的概念\n\n静态链表是指用一维数组表示的单链表。在静态链表中，用数据元素在数组中的下标作为单链表。\n\n静态链表的特点如下：\n\n 1. 静态的含义是指静态链表采用一维数组表示，表的容量是一定的，因此称为静态。\n 2. 静态链表中结点的指针域next存放的是其后继结点在数组中的位置(即数组下标)\n\n\n# 静态链表的定义\n\n下面的代码就是实现了一个静态链表的结构，实际上是一个结构体数组。\n\n/*线性表的静态链表存储结构*/\n/*假设链表的最大长度是1000*/\n# define maxsize 1000\ntypedef struct \n{\n    elemtype data;\n    /*游标curosr，为0的时候表示无指向*/\n    /*cur相当于单链表中的next指针*/\n    /*存放该元素的后继在数组中的下标*/\n    int cur;\n} component, staticlinklist[maxsize];\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n一般来说对于数组的第一个和最后一个元素作为特殊元素处理，不存数据。\n\n例如数组a[0]作为备用链表表头，备用链表的作用是回收数组中未使用或之前使用过(目前未使用)的存储空间，留待后期使用。也就是说备用链表的表头位于数组下标为0 (a[0])的位置。\n\n数据链表的表头位于数组下标为1 (a[1])的位置。\n\n下面是c++中的代码，定义的静态列表的结构：\n\n\n# 初始化静态列表\n\n下面是用c写的静态列表的初始化，在初始化的时候，我们只是在这里定义了 每个结构体数组中的cur的值为下一个元素在这个结构体数组中存储的下标，相当于单链表中的next 指针。\n\n/*将一维数组space中各分量链成一备用链表*/\n/*space[0].cur为头指针，"0"表示空指针*/\nstatus initlist(staticlinklist space)\n{\n    int i;\n    for (i=0; i<maxsize-1; i++) {\n        space[i].cur = i+1;\n    }\n    /*目前静态链表为空，最后一个元素的cur为0ss*/\n    space[maxsize-1].cur = 0;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n# 静态链表的插入操作\n\n定义了一个分配节点malloc的函数。\n\n/*若备用空间链表非空，则返回分配的结点下标，否则返回0*/\nint malloc_sll(staticlinklist space)\n{\n    /*当前数组第一个元素的cur存的值*/\n    /*就是要返回的第一个备用空闲的下标*/\n    int i = space[0].cur;\n    \n    if (space[0].cur) {\n        /*由于要拿出一个分量来使用了，所以我们*/\n        /*就得把它的下一个分量用来做备用*/\n        space[0].cur = space[i].cur;\n    }\n    return i;\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n下面是在一个静态的链表中，插入一个元素。这个时候是不需要移动数据的。例如这个时候需要在第i的位置上插入一个元素e，那就先让第i-1位置上的元素的cur下标指向先插入的元素e，然后将新的元素e的下标指向原来第i 位置所在的下标就可以了。\n\n/*在l中第i个元素之前插入新的数据元素e*/\nstatus listinsert(staticlinklist l, int i, elemtype e)\n{\n    int j, k, l;\n    /*注意k首先是最后一个元素的下标*/\n    k = max_size -1;\n\n    if (i < 1 || i > listlength(l) + 1) {\n        return error;\n    }\n    /*获得空闲分量的下标*/\n    j = malloc_sll(l);\n    \n    if (j) {\n        /*将数据赋值给此分量的data*/\n        l(j).data = e;\n        // 找到第i个元素之前的位置\n        for (l = 1; l <= i -1; l++) {\n            k = l[k].cur;\n        }\n        // 把第i个元素之前的cur赋值给新元素的cur\n        l[j].cur = l[k].cur;\n        // 把新元素的下标赋值给第i个元素之前元素的cur\n        l[k].cur = j;\n        return ok;\n    }\n    return error;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n下面是c++的实现代码：\n\n// 插入数据元素\nint slinklist::slinklistmalloc()\n{\n    // 若链表非空，则返回分配的结点下标，否则返回0\n    int i;\n    i = space[0].next;\n    if (space[0].next) {\n        space[0].next = space[i].next;\n    }\n    return i;\n}\n\nvoid slinklist::slinklistinsert(int i, int e)\n{\n    // 在静态链表中第i个位置插入数据元素\n    int j =1, m;\n    while (j<i-1) {\n        j = space[j].next;\n    }\n    m = slinklistmalloc();\n    space[m].data = e;\n    space[m].next = space[j].next;\n    space[j].next = m;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# 静态链表的删除操作\n\nc语言中实现的释放结点的函数free()。\n\n/*删除在l中第i个数据元素e*/\nstatus listdelete (staticlinklist l, int i)\n{\n    int j, k;\n    if (i < 1 || i > listlength(l)) {\n        return error;\n    }\n    k = max_size -1;\n    for (j = 1; j <= i -1; j++) {\n        k = l[k].cur;\n    }\n    j = l[k].cur;\n    l[k].cur = l[j].cur;\n    free_ssl(l, j);\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n最终实现的删除代码如下：\n\n/*将下标为k的空想结点回收到备用链表*/\nvoid free_ssl(staticlinklist space, int k) {\n    // 把第一个元素cur值赋给要删除的分量cur\n    space[k].cur = space[0].cur;\n    // 把要删除的分量下标赋值给第一个元素的cur\n    space[0].cur = k;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n下面是c++的代码实现：\n\n//删除数据元素\nint slinklist::slinklistdelete(int i)\n{\n    //  删除表中第i个数据元素，并用e返回其值\n    int j = 1, k=maxsize-1;\n    int e;\n    if (i<1 || i>length) {\n        cout<<"位置不合法";\n        return -1;\n    }\n    for (j=1; j<i; j++) {\n        k = space[k].next;\n    }\n    j = space[k].next;\n    space[k].next = space[j].next;\n    e = space[j].data;\n    free(j);\n    return e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n# d36(2020/10/27)\n\n今天主要是学习和复习两块的知识点，循环链表和双向链表，以及两者的结合，双向循环链表。\n\n增加学习栈的内容。\n\n\n# 循环链表\n\n将单链表中终端结点的指针端由空指针改为指向头结点(或理解为第一个元素结点)，就使得整个单链表形成一个环，这种头尾相接的单链表称为单循环链表，简称为循环链表。\n\n循环链表解决了一个很麻烦的问题，就是如何从当中一个结点出发，访问到链表的全部结点。\n\n在循环链表的遍历操作中，单循环链表的终止条件不再像非循环链表那样判断某个指针是否为空，而是判断该指针是否等于某一指定指针(如头指针或尾指针)。\n\n循环链表的类定义与单链表一样，只是使用时间尾结点的指针域由空改为指向头结点。\n\n\n# 双向链表+双向循环链表\n\n双向链表是在单链表的每个结点中，再设置一个指向其前驱结点的指针域。所以在双向链表中的结点都有两个指针域，一个指向直接后继，另一个指向直接前驱。\n\n我们更多的是将双向链表和循环链表组合而成，双向循环链表。\n\n\n# 双向链表的结构\n\n下面使用c语言的结构体来定义的双向链表的存储结构。\n\n// 线性表的双向链表存储结构\ntype struct dulnode\n{\n    elemtype data;\n    // 直接前驱指针\n    struct dulnode *prior;\n    // 直接后继指针\n    struct dulnode *next;\n} dulnode, *dulinklist;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 双向循环链表带头结点的空链表\n\n如下图示是，双向链表的循环带头结点的空链表。\n\n\n\n\n# 插入一个结点\n\ns是存储元素e的，要插入的结点。我们需要在p结点后面插入s结点。\n\n整体的思路是：\n\n * 先搞定s的前驱和后继；\n * 再搞定"后结点"的前驱；\n * 最后解决"前结点"的后继\n\n\n\n插入的代码示例如下：\n\n//如图中的1，把p赋值给s的前驱\ns->prior = p;\n//如图中的2，把p->next赋值给s的后继\ns->next=p->next;\n//如图中的3，把s赋值给p->next的前驱\np->next->prior = s;\n//如图中的4，把s赋值给p的后继\np->next = s;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 删除一个结点\n\n在循环双向链表中，删除一个结点的话，需要下面两个步骤：\n\n这两个语句的先后顺序是可以颠倒的，图示中，表明的是如何删除一个结点p.\n\n从结点p这个角度出发，可以顺序修改结点p的后继，结点p的前驱。\n\n\n\n// 把p->next赋值给p->prior的后继，如图示中的1\np->prior->next = p->next;\n// 把p->prior赋值给p->next的前驱，如图示中的2\np->next->prior = p->prior;\n// 释放结点\nfree(p);\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 栈\n\n栈是限定仅在表尾进行插入和删除操作的线性表。\n\n允许插入和删除的一端称为栈顶(top)，如果顺序栈，那就是表尾；另外一端称为栈底。\n\n栈的插入操作，叫做进栈，也称为压栈、入栈。push\n\n栈的删除操作，叫做出栈，也有的叫做弹栈。pop\n\n用一个栈顶指针top来指示栈顶，栈底指针base来指示栈底。\n\n\n# 栈的抽象数据类型\n\n我们把栈的插入操作叫做push，把栈的删除操作叫做pop。\n\nadt 栈(stack)\ndata \n    同线性表。元素具有相同的类型，相邻的元素具有前驱和后继关系。\noperation\n    initstack(*s): 初始化操作，建立一个空栈s。\n    destorystack(*s): 若栈存在，则销毁它。\n    clearstack(*s): 将栈清空\n    stackempty(s): 若栈为空，返回true，否则返回false.\n    gettop(s,*e): 若栈存在且非空，用e返回s的栈顶元素.\n    push(*s, *e): 若栈s存在，插入新元素e到栈s中并成为栈顶元素\n    pop(*s, *e): 删除栈s中栈顶元素，并用e返回其值\n    stacklength(s): 返回栈s的元素个数。\nendadt\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 顺序栈的概念\n\n顺序栈，即用一组地址连续的存储单元一次存放自栈底到栈顶的数据元素。top指针来指示栈顶元素在顺序栈中的位置。\n\n顺序栈的栈顶是数组的末尾(线性表的末尾)，栈底是数组的开头(线性表的开头)。\n\n如果用top=0来表示空栈的话，由于数组的下标一般约定从0开始，如此设定会有些不便。\n\n用如下的方法对栈进行初始化，其中stacksize表示栈当前用于存储数据元素的数组长度。\n\n顺序栈中，栈底指针base始终指向栈底的位置，所以若base=null，则表明栈结构不存在。\n\n\n\n(1) 栈空的时候，栈顶指针top=base.\n\n(2) 入栈的时候，栈顶指针top=top+1.\n\n(3) 出栈的时候，栈顶指针top=top-1.\n\n(4) 栈满的时候，栈顶指针top=stacksize-1.\n\n\n# 顺序栈的类定义\n\n栈的抽象数据类型的类定义在顺序栈存储结构下用c++实现。\n\nclass sqstack\n{\n    private:\n    // 栈底指针\n      int *base;\n    // 栈顶\n      int top;\n    // 栈容量\n      int stacksize;\n    public:\n    // 构建一个长度为m的栈\n      sqstack(int m);\n    // 销毁栈\n      ~sqstack() {delete [] base; top=-1; stacksize=0};\n    // 入栈\n      void push(int e);\n    // 出栈\n      int pop();\n    // 获取栈顶元素\n      int gettop();\n    // 测栈空\n      int stackempty();\n    // 显示栈中元素\n      void stacktranerse();\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 顺序栈入栈算法\n\n顺序栈入栈算法的操作步骤如下。\n\n第一步：如果栈满，则追加存储空间；否则直接执行下一步。\n\n第二步：将新元素插入栈顶位置。\n\n第三步：栈顶指针增加1.\n\n下面是c++的算法实现：\n\nvoid push(int e)\n{\n    if (top == stacksize -1) {\n        cout<<"栈满，无法入栈";\n        return;\n    }\n    top++;\n    base[top] = e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 顺序栈出栈算法\n\n顺序栈的出栈算法的操作步骤如下：\n\n第一步：如果栈空，则返回错误信息，操作结束；否则执行下一步\n\n第二步：取出栈顶元素赋值给e\n\n第三步：栈顶指针减去1，并返回e\n\n下面是通过代码给出具体的出栈算法:\n\nint pop() \n{\n    int e;\n    if (top==-1) {\n        cout<<"栈空，不能出栈";\n        return -1;\n    }\n    //base是个int类型的指针，这种写法对吗？\n    e = base[top--];\n    return e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 链式栈的概念\n\n如果栈采用的是链式存储，那么我们就把这种栈叫做链栈。通常链栈采用单链表表示，链栈的插入和删除操作只能在表头进行(链表头)，这一点倒是和数组结构的线性表有些区别的，线性栈是在数组尾巴(表尾)进行的。\n\n为什么这么做呢？\n\n * 数组从尾巴插入删除方便，算法的时间复杂度是o(1)，如果从数组头部插入删除，那么每次操作的时间复杂度就是o(n).\n * 链式表有头插法和尾插法两种，如果考虑的头插法，那么就是从表头来插入和删除的。\n\n1). 入栈的时候，将新创建的结点s加入到链表表头，并将栈顶指针top指向s.\n\n2). 出栈时，栈顶指针top指向链表第一个结点的下一个结点。\n\nq：链式栈中，为什么不考虑链的尾插法，而是采用头插法呢？\n\n\n# 链栈的类定义\n\n链栈的结点结构与单链表的结点结构相同，因此链栈类定义的c++描述如下：\n\nstruct node {\n    int data;\n    node *next;\n};\nclass linkstack {\n    private:\n    // 栈顶指针即链栈的头指针\n        node *top;\n    public:\n    // 构造函数，置空链栈\n        linkstack() {top=null};\n    // 析构函数，释放链栈中各结点的存储空间\n        ~linkstack();\n    // 将元素e入栈\n        void push(int e);\n    // 将栈顶元素出栈\n        int pop();\n    // 取栈顶元素(并不删除)\n        int gettop() {if (top!=null) retur top->data;}\n    // 判断链栈是否为空栈\n        bool empty() {top==null? return 1; return 0;}\n};\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 链栈入栈算法\n\n链栈入栈即在链表表头插入新结点，且栈顶指针指向该结点。\n\n第一步：创建一个新结点s，将s的值设为入栈元素值。\n\n第二步：将新结点s插入表头。\n\n第三步：栈顶指针指向s。\n\n用c++实现的代码如下：\n\nvoid linkstack::push(int e)\n{\n    s = new node;\n    if (!s) {\n        cout<< "内存分配失败";\n        return ;\n    }\n    //申请一个数据域为e的结点s\n    s->data = e;\n    // 将结点s插在栈顶\n    s->next = top; \n    top = s;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 链栈出栈算法\n\n链栈出栈即删除链表的首元素结点，具体操作步骤如下。\n\n第一步：如果栈空，则返回错误信息，操作结束；否则继续执行下一步。\n\n第二步：取出栈顶元素赋值给e。\n\n第三步：栈顶指针后移一位，并删除原栈顶结点，返回e。\n\n下面是链栈出栈的c++代码：\n\nint linkstack::pop()\n{\n    if (top==null) {\n        cout<< "溢出";\n        return -1;\n    }\n    //暂存栈顶元素\n    x = top->data;\n    // 将栈顶指针指向后移\n    p = top;\n    top = top->next;\n    delete p;\n    return x;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# d37(2020/10/28)\n\n主要进行基于队列的代码的整理，包括顺序队列，循环队列，链式队列。\n\n\n# 队列的概念\n\n队列是另一种限定存取位置的线性表。它只允许在表的一端插入，在另一端删除，其中允许插入的一端称为队尾(rear)，允许删除的一端称为队头(front)。从队尾插入元素的操作称为入队；从队头删除元素的操作称为出队。\n\n\n# 队列的抽象数据类型\n\n队列的操作与栈类似，不同的是队列的删除操作是在表的头部(队头)进行的。下面给出队列的抽象数据类型定义：\n\nadt queue {\ndata \n    同线性表。元素具有相同的类型，相邻元素具有前驱和后继关系。\noperation\n    initqueue(*q): 初始化操作，建立一个空队列q。\n    destroyqueue(*q): 若队列q存在，则销毁它。\n    clearqueue(*q): 将队列q清空\n    queueempty(q): 若队列q为空，返回true，否则返回false.\n    gethead(q, *e): 若队列q存在且非空，用e返回队列q的队头元素\n    enqueue(*q, e): 若队列q存在，插入新元素e到队列q中并成为队尾元素。\n    dequeue(*q, *e): 删除队列q中队头元素，并用e返回其值\n    queuelength(q): 返回队列q的元素个数\n}\nendadt\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 顺序队列\n\n在研究队列的时候，实际上有两个大方向的问题，需要研究和讨论。队列是否采用数组还是链表，队列是否需要循环。\n\n下面会从几个简单的实例来实践一下，如果用数组的方式来实现，需要对该数组的队列方式进行改造，并且明确的指出非循环的队列，肯定会存在问题，所以说到队列，一定会去使用循环队列。\n\n\n# 顺序队列的存储不足\n\n假设这里有一个队列有n个元素是用数组实现的顺序队列，数组下标为0的一端是队头。\n\n入队列的操作，其实就是在队尾追加一个元素，这个时候不需要移动任何元素，因此时间复杂度是o(1)。\n\n但是出队列的时候，是从队头(数组下标为0的位置)删除元素数据的，这样的话，由于数组顺序存储的连续性的要求，后续的数组元素都要依次往前移动，这样的话算法的时间复杂度就是o(n)。\n\n# 解决队头删除元素o(n)的问题\n\n为什么出队列的时候一定要全部移动呢？如果不去限制队列的元素必须存储在数组的前n个单元这一条件，出队的性能就会大大增加。 --\x3e 队头不需要一定在下标为0的位置。\n\n那么就需要一个队头指针和一个队尾指针，入队列一个，队尾指针往后移动一个。出队列一个，队头指针往后移动一个。front指针指向队头元素，rear指针指向队尾元素的下一个位置，当front=rear的以后，此时队列不是还是剩下一个元素，而是空队列。\n\n\n\n提出新问题？"假溢出"\n\n由于front指针随着出队列，不断的往后移动。rear直至随着入队列也不断的往后移动，很有可能会出现rear指针移动到了数组之外，而这个时候数组是有位置的。这种现象就叫做"假溢出"。\n\n\n\n\n# 循环队列定义\n\n为了解决上面的假溢出的问题，就出现了循环队列。解决的方法就是，把一个单一的队列组成一个首尾相连的队列。我们把队列的这种头尾相接的顺序存储结构称为循环队列。\n\n# 循环队列判断队空和队满\n\n之前我们定义的队空的时候是，front == rear，但是如果是循环队列的时候，如果这个时候队满了，也会出现front == rear了，这就会存在问题。\n\n解决办法1：增加设置一个标志位置变量flag，当front == rear，且flag = 0的时候为队列空，当front == rear，且flag = 1的时候为队列满。\n\n解决办法2：当队列空的时候，条件就是front == rear，当队列满的时候，我们修改其条件，保留一个元素空间。也就是说，队列满的时候，数组中还有一个空闲单元了。\n\n\n\n# 保留一个元素空间的方法来判断队满\n\n如果我们采用了上面的第二种方法的话，由于rear可能比front大，也可能比front小，所以尽管它们只相差一个位置时就是满的情况，但也可能是相差整整一圈。\n\n所以若队列的最大尺寸是queuesize，那么队列满的条件是(rear+1)% queuesize == front(取模%的目的就是为了整合rear与front大小为一个问题)。\n\n\n\n# 保留一个元素空间的方法来计算队列长度\n\n当采用上面第二中方法，通过留下一个空闲的位置，来判断队列满的情况的话。\n\n有时候，我们front指针会小于rear，也就是rear还没转圈的时候，此时队列的长度为rear - front；有时候，当rear转圈后，就会发现rear < front了，这个时候的队列的长度分为两段了，一段是queuesize - front，另一端是0 + rear，加在一起，队列长度为rear - front + queuessize。\n\n因此通用的计算队列长度公式为：(rear - front + queuesize)% queuesize\n\n\n# 循环队列的顺序存储结构\n\n下面是用c定义的循环队列中的顺序存储结构的代码。\n\n// qelemtype类型根据实际情况而定，这里设置为int\ntypedef int qelemtype;\n// 循环队列的顺序存储结构\ntypedef struct \n{\n    qelemtype data[maxsize];\n    // 头指针\n    int front;\n    // 尾指针，若队列不空，指向队尾元素的下一个位置\n    int rear;\n} sqqueue;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 初始化一个空队列\n\n这里用c的代码来实现了初始化一个空队列的目的。\n\n// 初始化一个空队列q\nstatus initqueue(sqqueue *q)\n{\n    q->front = 0;\n    q->rear = 0;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 返回队列的当前长度\n\n下面用c的代码实现的是，返回当前队列的长度。\n\n// 返回q的元素个数，也就是队列的当前长度\nint queuelength (sqqueue q)\n{\n    return (q.rear - q.front + maxsize )%maxsize;\n}\n\n\n1\n2\n3\n4\n5\n\n\n\n# 循环队列的入队列\n\n下面是用c++代码实现的，循环队列中入队列的代码。\n\n// 循环队列的入队列操作代码如下：\nstatus enqueue(sqqueue *q, qelemtype e) \n{\n    // 队列满的判断\n    if ((q->rear + 1)%maxsize == q->front) {\n        return error;\n    }\n    // 将元素e赋值给队尾\n    q->data[q->rear] = e;\n    // rear直至向后移动一个位置\n    // 若到了最后则转到数组头部\n    q->rear = (q->rear + 1)% maxsize;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n下面使用c++代码实现的循环队列入队列的情况。\n\n// 如下使用循环队列中的入队代码\nvoid enqueue(int e)\n{\n    if ((rear + 1)% queuesize == front) {\n        cout<<"上溢，无法入队";\n        return;\n    }\n    base[rear] = e;\n    rear = (rear+1)% queuesize;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# 循环队列的出队列\n\n下面是用c代码实现的，循环队列中出队列的代码。\n\n// 循环队列的出队列的操作代码如下\n// 若队列不空，则删除q中队头元素，用e返回其值\nstatus dequeue(sqqueue *q, qelemtype *e)\n{\n    // 队列空的判断\n    if (q->front == q->rear) {\n        return error;\n    }\n    // 将队头元素赋值给e\n    *e = q->data[q->front];\n    // front指针向后移动一个位置\n    // 若到最后则转到数组头部\n    q->front = (q->front + 1)% maxsize;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n下面是用c++代码实现的，在循环队列中出队列的代码情况：\n\n// 如下使用的是循环队列中的出队代码\nint dequeue() \n{\n    int e;\n    if (front == rear) {\n        cout<<"下溢，不能出队";\n        return -1;\n    }\n    e = base[front];\n    return e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 链式队列\n\n队列的链式存储结构，其实就是线性表的单链表，只不过它只能尾进头出而已，我们把它简称为链队列。\n\n为了操作上的方便，我们将队头指针指向链队列的头结点，而队尾指针指向终端结点。(这里同样引入了头结点，头结点不存储任何元素，只是为了保证为当这个链队列为空的时候，和非空的队列的操作是一致的)。\n\n这个是和上面的顺序队列不一样了，在顺序队列中，头指针指向的是数组下标为0的位置，链尾指针指向的是数组中队尾元素的下一个位置。\n\n\n\n空队列的时候，front和rear都指向头结点。\n\n\n\n\n# 链队列的结构\n\n下面的c语言代码展示的是，链队列的结构。\n\n// qelemtype类型根据实际情况而定，这里可以假设为int\ntypedef int qelemtype;\n\n// 结点结构\ntypedef struct qnode\n{\n    qelemtype data;\n    struct qnode *next;\n} qnode, *queueptr;\n\n// 队列的链表结构\ntypedef struct\n{\n    // 队头、队尾指针\n    queueptr front,rear;\n} linkqueue;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 链队列的入队操作\n\n在链队列中，入队的时候，实际上就是在链表的尾部插入结点。\n\n链队列的入队操作中，相比与上面的顺序队列的入队操作来说，不需要去判断这个队列是否满了，也就是不需要去判断数组是否溢出了。\n\n下面是用c语言代码实现的操作：\n\n// 链队列入队的操作\n// 插入元素e为q的新的队列元素\nstatus enqueue(linkqueue *q, qelemtype e)\n{\n    queueptr s = (queueptr) malloc(sizeof(qnode));\n    // 存储分配失败\n    if (!s) {\n        exit (overflow);\n    }\n    s->data = e;\n    s->next = null;\n    // 把拥有元素e新结点s赋值给原队尾结点的后继\n    q->rear->next = s;\n    // 把当前的s 设置为队尾结点，rear指向s\n    q->rear = s;\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n下面是用c++语法实现的链队列入队的代码：\n\n// 链队列入队算法\nvoid linkqueue::enqueue(int e)\n{\n    node *s;\n    s = new node;\n    s->data = e;\n    // 下面的代码可以直接改写为s->next = null\n    s->next = rear->next;\n    //将结点s的地址赋值给了原来的队尾指针\n    //也就是说原来的最后一个元素不是最后一个了，\n    //这个结点的next不是null，而是s结点了\n    rear->next = s;\n    //最后将 s结点的地址赋值给rear指针了\n    //也就是说 rear指针指向的是s结点了\n    rear = s;\n    //如果原来的队列是个空队列\n    //那么就将front指针指向的是新加入的结点s\n    if (front->next == null) {\n        front->next = s;\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n# 链队列的出队操作\n\n出队操作时，也就是头结点的后继结点出队，将头结点的后继改为它后面的结点(后继的后继)，若链表除头结点外只剩下一个元素的时候，则需要将rear指向头结点。\n\n\n\n下面是用c语言实现的链队列出队的代码.\n\n// 链队列出队的操作\n// 若队列不空，删除q的队头元素，用e返回其值，并返回ok，否则返回error\nstatus dequeue(linkqueue *q, qelemtype *e)\n{\n    queueptr p;\n    if (q->front == q->rear) {\n        return error;\n    }\n    // 将要删除的队头结点暂存给p\n    p = q->front->next;\n    // 将预删除的队头结点的值赋值给e\n    *e = p->data;\n    // 将原队头结点后继p->next赋值给头结点后继\n    q->front->next = p->next;\n    // 若队头是队尾，则删除后将rear指向头结点\n    if (q->rear == p) {\n        q->rear = q->front;\n    }\n    free(p);\n    return ok;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n下面是c++实现的链队列出队的算法。\n\n// 链队列出队算法\n// front是头结点\nint linkqueue::dequeue()\n{\n    int e;\n    node *p;\n    // 队空，则下溢\n    if (rear == front) {\n        cout<< "下溢";\n        return -1;\n    }\n    // 先将第一个结点的内存地址保存到指针p中\n    p = front->next;\n    // 将第一个结点的data值，保存到变量e中\n    e = p->data;\n    // 将第一个结点的后继赋值给  头结点的后继\n    front->next = p->next;\n    // 如果要删除的这个结点是唯一的结点元素的话\n    if (p->next == null) {\n    // 则将rear的末尾结点的指针指向了头结点\n        rear = front;\n    }\n    delete p;\n    return e;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# d38(2020/10/29)\n\n今天继续要学习下排序的知识点。\n\n上一节我们学习的冒泡排序、插入排序、选择排序这三种排序算法，它们的时间复杂度都是o($n^2$)，比较高，适合小规模数据的排序。\n\n今天，要学习的两种时间复杂度为o(nlogn)的排序算法，归并排序和快速排序。这两种排序算法适合大规模的数据排序，比上一节讲的那三种排序算法要更常用。\n\n归并排序和快速排序都用到了分治思想，非常巧妙。我们可以借鉴这种思想，来解决非排序的问题，比如：如何在o(n)的时间复杂度内查找一个无序数组中的第k大元素？\n\n\n\n\n# 归并排序的原理\n\n\n# 归并排序(merge sort)\n\n我们如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。\n\n归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。\n\n分治思想就是我们之前所讲的递归思想很像。分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。\n\n\n# 递归代码来实现归并排序\n\n之前我们所学习到的书写递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，先得写出归并排序的递推公式。\n\n递推公式：\nmerge_sort(p...r) = merge(merge_sort(p..q), merge_sort(q+1 .... r))\n终止条件：\np >= r 不用再继续分解\n\n\n1\n2\n3\n4\n\n\n理解上面的递推公式。\n\nmerge_sort(p...r)表示，给下标从p到r之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p...q)和merge_sort(q+1 ... r)，其中下标q等于p和r的中间位置，也就是(p+r)/2。当下标从p到q和从q+1到r这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从p到r之间的数据就也排好序了。\n\n有了递推公式，我们就可以转化为代码了，这样就实现了归并排序的算法。\n\n// 归并排序算法，a是数组，n表示数组大小\nmerge_sort(a, n) {\n    merge_sort_c(a, 0, n-1)\n}\n// 递归调用函数\nmerge_sort_c(a,p,r) {\n    // 递归终止条件\n    if p >= r then return\n    // 取p到r之间的中间位置q\n    q = (p+r)/2\n    // 分治递归\n    merge_sort_c(a, p, q)\n    merge_sort_c(a, q+1, r)\n    // 将a[p...q]和a[q+1...r]合并为a[p...r]\n    merge(a[p...r],a[p...q],a[q+1...r])\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n这里，我们可以发现了，merge(a[p...r],a[p...q],a[q+1...r])这个函数的作用就是，将已经有序的a[p...q]和a[q+1 ... r]合并成一个有序的数组，并且放入a[p...r]。那个具体的合并的操作该如何实现呢？\n\n如图所示，我们申请一个临时数组tmp，大小与a[p...r]相同。我们用啊领个游标i和j，分别指向a[p...q]和a[q+1 ... r]的第一个元素。比较这两个元素a[i]和a[j]，如果a[i]<=a[j]，我们就把a[i]放入到临时数组tmp，并且i后移一位，否则将a[j]放入到数组tmp，j后移一位。\n\n继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组tmp中的数据拷贝到原数组a[p...r]中。\n\n\n\n我们把merge()函数写成伪代码，就是下面的样子：\n\nmerge(a[p...r], a[p...q], a[q+1 ... r]) {\n    // 初始化变量i,j,k\n    var i:=p, j:=q+1, k:=0\n    // 申请一个大小跟a[p...r]一样的临时数组\n    var tmp:= new arrray[0 ... r-p]\n    while i<=q and j<=r do {\n        if a[i] <= a[j] {\n        // i++等于i:=i+1\n            tmp[k++] = a[i++]\n        } else {\n            tmp[k++] = a[j++]\n        }\n    }\n}\n// 判断哪个子数组中有剩余的数据\nvar start:= i, end:=q\nif j<=r then start :=j, end:=r\n// 将剩余的数据拷贝到临时数组tmp\nwhile start<= end do  {\n    tmp[k++] = a[start++]\n}\n// 将tmp中的数组拷贝会a[p...r]\nfor i:=0 to r-p do {\n    a[p+i] = tmp[i]\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n\n# 归并排序的性能分析\n\n下面来分析一下归并排序的三个问题。\n\n\n# 第一，归并排序是稳定的排序算法吗？\n\n结合前面画的那张图和归并排序的伪代码，我们应该能发现，归并排序稳不稳定关键要看merge()函数，也就是两个有序子数组合并成一个有序数组的那部分代码。\n\n在合并的过程中，如果a[p...q]和a[q+1...r]之间有值相同的元素，那我们可以像伪代码中的那样，先把a[p...q]中的元素放入tmp数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。\n\n\n# 第二，归并排序的时间复杂度是多少？\n\n归并排序涉及递归，时间复杂度的分析稍微有点复杂。我们可以借此来分析一下递归代码的时间复杂度。\n\n在递归那一节，我们了解到，递归适用的场景是，一个问题a可以分解为多个子问题b、c，那求解问题a就可以分解为求解问题b、c。问题b、c解决之后，我们再把b、c 的结果合并成a的结果。\n\n如果我们定义求解问题a的时间t(a)，求解问题b、c的时间分别是t(b)和t(c)，那我们就可以得到这样的递推关系式：\n\nt(a) = t(b) + t(c) +k\n\n其中k等于将两个子问题b、c的结果合并成问题a的结果所消耗的时间。\n\n从刚才的分析，可以看出：不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。\n\n套用这个公式，我们来分析一下归并排序的时间复杂度。\n\n我们假设对n个元素进行归并排序需要的时间是t(n)，那分解成两个子数组排序的时间都是t(n/2)。我们知道，merge()函数合并两个有序子数组的时间复杂度是o(n)。所以，套用前面的公式，归并排序的时间复杂度的计算公式就是：\n\nt(1) = c; n=1时，只需要常量级的执行时间，所以表示为c\nt(n) = 2* t(n/2) + n; n>1\n\n\n1\n2\n\n\n通过这个公式，如何来求解t(n)呢？我们可以再进一步分解计算过程。\n\nt(n) = 2*t(n/2) + n\n     = 2*(2*t(n/4) + n/2) + n = 4*t(n/4) + 2*n\n     = 4*(2*t(n/8) + n/4) + 2*n = 8*t(n/8) + 3*n\n     = 8*(2*t(n/16) + n/8) + 3*n = 16*t(n/16) + 4*n\n     ......\n     = 2^k * t(n/2^k) + k * n\n     ......\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n通过这样一步一步分解推导，我们可以得到t(n)= 2^k * t(n/2^k) + k * n。当t(n/2^k) = t(1)时，也就是n/2^k = 1，我们得到k=$log_2{n}$ . 我们将k值代入上面的公式，得到t(n)=cn + n$log_2{n}$。如果我们用大o标记法来表示的话，t(n)就等于o(nlogn)。\n\n从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度非常稳定，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是o(nlogn).\n\n\n# 第三，归并排序的空间复杂度是多少？\n\n归并排序的时间复杂度任何情况下都是o(nlogn)，看起来非常优秀。(和下面的快速排序相比，最坏情况天，时间复杂度也是o($n^2$))。但是，归并排序并没有像快排那样，应用广泛，这是为什么呢？\n\n这是因为它有一个致命的"弱点"，那就是归并排序不是原地排序算法。\n\n这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组的时候，需要借助额外的存储空间。但是，归并排序的空间复杂度到底是多少呢？\n\n如果我们继续按照分析递归时间复杂度的方法，通过递推公式来求解，那整个归并过程需要的空间复杂度就是o(nlogn)。不过，类似分析时间复杂度那样来分析空间复杂度，这个思路是不对的。\n\n实际上，递归代码的空间复杂度并不能像时间复杂度那样累加。刚刚我们忘记了最重要的一点，那就是，尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间也就释放掉了。在任意时刻，cpu只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过n个数据的大小，所以空间复杂度是o(n).\n\n\n# 快速排序的原理\n\nquicksork，习惯性地叫做"快排"。快排利用的也是分治思想。\n\n快排的思想是这样：如果要排序数组中下标从p到r之间的一组数据，我们选择p从r之间的任意一个数据作为pivot (分区点)。\n\n我们遍历p到r之间的数据，将小于pivot的放到左边，将大于pivot的放到右边，将pivot放到中间。经过这一步骤之后，数组p到r之间的数据就被分成了三个部分，前面p到q-1之间都是小于pivot的，中间是pivot，后面的q+1到r之间是大于pivot的。\n\n\n\n根据分治、递归的处理思想，我们可以用递归排序下标从p到q-1之间的数据和下标从q+1到r之间的数据，直到区间缩小为1，就说明所有的数据都有序了。\n\n如果我们用递推公式来将上面的过程写出来的话，就是这样：\n\n递推公式：\nquick_sort(p...r) = quick_sort(p...q-1) + quick_sort(q+1 ... r)\n终止条件：\np >= r\n\n\n1\n2\n3\n4\n\n\n我们将递推公式转化成递归代码。跟归并排序一样，下面用伪代码来实现下。\n\n// 快速排序，a是数组，n表示数组的大小\nquick_sort(a, n) {\n    quick_sort_c(a, 0, n-1)\n}\n// 快速排序递归函数，p，r为下标\nquick_sort_c(a, p, r) {\n    if p >= r then return\n    // 获取分区点\n    q = partition(a, p, r)\n    quick_sort_c(a, p, q-1)\n    quick_sort_c(a, q+1, r)\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n归并排序中有一个merge()合并函数，我们这里有一个partition()分区函数。partition()分区函数实际上我们前面已经了解了，就是随机选择一个元素作为pivot(一般情况下，可以选择p到r区间的最后一个元素)，然后对a[p...r]分区，函数返回pivot的下标。\n\n如果我们不考虑空间消耗的话，partition()分区函数可以写得非常简单。我们申请两个临时数组x和y，遍历a[p...r]，将小于pivot的元素都拷贝到临时数组x，将大于pivot的元素都拷贝到临时数组y，最后再将数组x和数组y中数据顺序拷贝到a[p...r].\n\n\n\n但是，如果按照这种思路实现的话，partition()函数就需要很多额外的内存空间，所以快排就不是原地排序算法了。如果我们希望快排是原地排序算法，那它的空间复杂度就得是o(1)，那partition()分区函数就不能占用太多额外的内存空间，我们就需要在a[p...r]的原地完成分区操作。\n\n原地分区函数的实现思路非常巧妙，如下面的伪代码.\n\npartition(a, p, r) {\n    privot:= a[r]\n    i := p\n    for j := p to r-1 do {\n        swap a[i] with a[j]\n        i := i+1\n    }\n}\nswap a[i] with a[r]\nreturn i\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n这里的处理有点类似选择排序。我们通过游标i把a[p...r-1]分成两部分。a[p...i-1]的元素都是小于pivot的，我们暂且叫它"已处理区间"，a[i...r-1]是"未处理区间"。我们每次都从未处理的区间a[i...r-1]中取一个元素a[j]，与pivot对比，如果小于pivot，则将其加入到已处理区间的尾部，也就是a[i]的位置。\n\n在数组的插入操作中，在数组某个位置插入元素，需要搬移数据，非常耗时。当时我们也提及到一种处理技巧，就是交换，在o(1)的时间复杂度内完成插入操作。这里我们也借助这个思想，只需要将a[i]和a[j]交换，就可以在o(1)的时间复杂度内将a[j]放到下标为i的位置。\n\n如下图所示的，就是一个快速排序中的图示。\n\n\n\n因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个6的相对先后顺序就会改变。所以，快速排序并不是一个稳定的排序算法。\n\n到此，快速排序的原理也了解了。现在，我们提出一个问题：快排和归并用的都是分治思想，递推公式和递归代码也非常相似，它们的区别在哪里？\n\n\n\n可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。\n\n归并排序虽然是稳定的、时间复杂度为o(nlogn)的排序算法，但是它是非原地排序算法。前面提到过，归并排序之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。\n\n\n# 快速排序的性能分析\n\n接下来我们分析一下快速排序的性能。在讲解快速排序的实现原理的时候，已经分析了稳定性和空间复杂度。快排是一种原地、不稳定的排序算法。\n\n快排也是用递归来实现的。对于递归代码的时间复杂度，前面提及的总结的公式，这里还是适用的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是o(nlogn).\n\nt(1) = c;  n=1时，只需要常量级的执行时间，所以表示c\nt(n) = 2*t(n/2) + n; n>1\n\n\n1\n2\n\n\n但是，公式成立的前提是每次分区操作，我们选择的pivot都很合适，正好能将大区间对等地一分为二。\n\n我们举一个比较极端的例子。如果数组中的数据原来就已经是有序的了，比如1，3，5，6，8。如果我们每次选择最后一个元素作为pivot，那每次分区得到的两个区间都是不均等的。我们需要进行大约n次分区操作，才能完成快排的整个过程。每次分区我们平均要扫描大约n/2 个元素，这种情况下，快排的时间复杂度就从o(nlogn) 退化成了o($n^2$) .\n\n刚才看到的是两个极端情况下的时间复杂度，一个是分区极其均衡，一个是分区极其不均衡。它们分别对应快排的最好情况时间复杂度和最坏情况时间复杂度。那快排的平均情况时间复杂度是多少呢？\n\n我们假设每次分区操作都将区间分成大小为9:1的两个小区间。我们继续套用递归时间复杂度的递推公式，就会变成这样：\n\nt(1) = c; n=1时，只需要常量级的执行时间，所以表示为c。\nt(n) = t(n/10) + t(9*n/10) +n; n>1\n\n\n1\n2\n\n\n这个公式的递推求解的过程非常复杂，虽然可以求解，但是我们不推荐用这种方法。实际上，递归的时间复杂度的求解方法除了递推公式之外，还有递归树。这里先记住结论：t(n)在大部分的情况下的时间复杂度都可以做到o(nlogn)，只有在极端的情况下，才会退化到o($n^2$)。\n\n\n# 解答开篇\n\n快排的核心思想就是分治和分区，我们可以利用分区的思想，来解答开篇的问题：o(n)时间复杂度内求无序数组中的第k大元素。比如，4,2,5,12,3这样一组数据，第3大元素就是4。\n\n我们选择数组区间a[0...n-1]的最后一个元素a[n-1]作为pivot，对数组a[0...n-1]原地分区，这样数组就分成了三部分，a[0...p-1]、a[p]、a[p+1 ... n-1].\n\n如果p+1 =k，那么a[p]就是要求解的元素；如果k>p+1，说明第k大元素出现在a[p+1...n-1]区间，我们再按照上面的思路递归第在a[p+1...n-1]这个区间内查找。同理，如果k<p+1，那我们就在a[0...p-1]区间查找。\n\n\n\n我们再来看，为什么上述解决思路的时间复杂度是o(n)？\n\n第一次分区查找，我们需要对大小为n的数组执行分区操作，需要遍历n个元素。第二次分区查找，我们只需要对大小为n/2的数组执行分区操作，需要遍历n/2个元素。依次类推，分区遍历元素的个数分别为n/2、n/4、n/8、n/16 ....直到区间缩小为1.\n\n如果我们把每次分区遍历的元素个数加起来，就是：n+n/2 +n/4 +n/8 +...+1. 这是一个等比数列求和，最后的和是2n-1、所以，上述解决思路的时间复杂度是o(n).\n\n\n# 内容小结\n\n归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归俩实现，过程非常相似。理解归并排序的重点是理解递推公式和merge()合并函数。同理，理解快排的重点也是理解递推公式，还有partition()分区函数。\n\n归并排序算法是一种在任何情况下时间复杂度都比较稳定的排序算法，这也使它存在致命的缺点，即归并排序不是原地排序算法，空间复杂度比较高，是o(n)。正因为此，它也没有快排应用广泛。\n\n快速排序算法虽然最坏情况下的时间复杂度是o($n^2$)，但是平均情况下时间复杂度都是o(nlogn)。不仅如此，快速排序算法时间复杂度退化到o($n^2$)的概率非常小，我们可以通过合理地选择pivot来避免这种情况。\n\n\n# d39(2020/11/02)\n\n今天主要是再次梳理一下归并排序和快速排序的内容，熟悉书写的代码和排序的原理。\n\n\n# 归并排序\n\n归并排序(merge sort)是一类借助"归并"进行排序的方法。\n\n归并的含义是将两个或两个以上的有序序列归并为一个有序序列的过程。归并排序按所合并的表的个数可分为二路归并排序和多路归并排序。\n\n二路归并排序(2-way merge sort)的基本思想：将待排序的n个元素看成是n个有序的子序列，每个子序列的长度是1，然后两两归并，得到[$\\frac{n}{2}$] 向下取整，个长度为2或1(最后一个有序序列的长度可能是1)的有序子序列；再两两归并，得到[$\\frac{n}{4}$] 向下取整，个长度为4或小于4(最后一个有序序列的长度可能小于4)的有序子序列；再两两归并，.... 直至得到一个长度为n的有序序列。\n\n\n# 二路归并排序的操作步骤如下\n\nstep1: 将待排序的列划分为两个长度相当的子序列。\n\nstep2: 若子序列长度大于1，则对子序列执行一次归并排序。\n\nstep3: 执行下列步骤对子序列两两合并成有序序列。\n\n 1. 创建一个辅助数组temp[]. 假设两个子列的长度分别为u、v，两个子列的下标为0~u，u+1 ~ v+u+1。设置两个子表的起始下标和辅助数组的起始下标：i=0; j=u+1; k=0\n 2. 若i>u或j>v+u+1，说明其中一个子表已经合并完毕，执行第4步。\n 3. 选取r[i]和r[j]中关键字较小的存入辅助数组temp[]; 若r[i].key < r[j].key，则temp[k]=r[i]; i++;k++; 否则temp[k]=r[j]; j++; k++。返回第2步。\n 4. 将尚未处理完的子表元素依次存入temp[]，结束合并。\n\n\n# 快速排序\n\n\n# 快速排序的基本思想\n\n快速排序是通过对关键字的比较和交换，以待排序列中的某个数据为支点，将待排序列分成两部分，其中左半部分数据小于等于支点，右半部分数据大于等于支点。然后，对左右两部分分别进行快速排序的递归处理，直到整个序列按关键字有序为止。\n\n\n# 快速排序与冒泡排序\n\n在冒泡排序中，元素的比较和移动是在相邻位置进行的，元素的每次交换只能前移或后移一个位置，因而总的比较次数和移动次数较多。\n\n而在快速排序中，元素的比较和移动是从两端向中间进行的，关键字较大的记录一次就能从前面移动到后面，关键字较小的记录一次就能从后面移动到前面，记录移动的距离较远，从而减少了总的比较次数和移动次数。\n\n因此，可以将快速排序视为对冒泡排序的一次改进。\n\n\n# 快速排序的流程图\n\n下面是一个快速排序的完整的流程图，其中low是序列中的第一个结点，high指向的是序列中的最后一个结点，且令r[low]为支点。\n\n\n\n\n# 快速排序的基本步骤\n\nstep1: 如果待排子序列中元素的个数等于1，则排序结束；否则以r[low]为支点，按如下方式进行一次划分；\n\n 1. 设置两个搜索指针：low是向后搜索指针，初始指向序列第一个结点；high是向前搜索指针，初始指向最后一个结点；取第一个记录为支点，low位暂时取值为支点privotkey = r[low].key\n 2. 若low=high，一次划分结束\n 3. 若low<high 且r[high].key >= privotkey, 则从high 所指定的位置向前搜索：high =high -1，重新执行1.3；否则若有low<high并且有r[high].key < privotkey，交换r[high].key和r[low].key,然后令low=low +1，执行下面第4步；若有low>=high，则指向上面第2步；\n 4. 若low<high且r[high].key<= privotkey。则从low所指的位置开始往后搜索：low = low +1，然后再重新执行第4步；否则若有low < high并且有r[low].key > pivotkey，则交换r[high].key和r[low].key，然后令high = high -1，执行上面第3步；若有low >= high，则执行第2步。\n\nstep2: 对支点左半子序列重复step1\n\nstep3: 对支点右半子序列重复step1.\n\n\n# 快速排序代码\n\n分为分区函数和主函数两块的代码。\n\n# include <stdio.h>\n\n// 对序列进行一次划分\nint partition (sqlist &l, int low, int high)\n{\n    int pivotkey;\n    // 关键字\n    privotkey = l.r[low];\n\n    // 从表的两端交替向中间扫描\n    while (low < high) \n    {\n        while (low<high && l.r[high] >=privotkey) {\n            --high;\n        }\n        // 交换位置\n        l.r[low] = l.r[high];\n        while (low<high && l.r[low] <=pivotkey) {\n            ++low;\n        }\n        // 交换位置\n        l.r[high] = l.r[low];\n    }\n    // 返回中间的位置\n    return low;\n}\n\n// 主程序，按分区对子程序进行调用\nvoid quicksort1 (sqlist &l, int low, int high)\n{\n    int mid;\n    if (low < high) {\n        mid = partition(l, low, high);\n        //对低子表进行排序\n        quicksort1 (l, low, mid-1);\n        //对高子表进行排序\n        quicksort1 (l, mid+1, high);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# d40(2020/11/03)\n\n今天主要是要学习的桶排序、计数排序，还有基数排序中的内容。\n\n上两节中，我们学习了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。今天，我们会重点学习三种时间复杂度是o(n)的排序算法：桶排序、计数排序、基数排序。因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫做线性排序(linear sort)。之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作。\n\n这几种排序算法理解起来都不难，时间、空间复杂度分析起来也和简单，但是对要排序的数据要求很苛刻，所以学习的重点是掌握这些排序算法的适用场景。\n\n\n# 桶排序(bucket sort)\n\n首先，我们来看桶排序。桶排序，顾名思义，会用到"桶"，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。\n\n\n\n桶排序的时间复杂度为什么是o(n)呢？\n\n如果要排序的数据有n个，我们把它们均匀地划分到m个桶内，每个桶里就有k= n/m 个元素。每个桶内部使用快速排序，时间复杂度为o(k* logk)。m个桶排序的时间复杂度就是o(m*k* logk)，因为k=n/m，所以整个桶排序的时间复杂度就是o(n* log(n/m))。当桶的个数m接近数据个数n的时候，log(n/m)就是一个非常小的常量，这个时候桶排序的时间复杂度接近o(n).\n\n\n# 桶排序看起来很优秀，是不是可以替代之前讲的排序算法？\n\n答案是否定的。上面的例子是为了轻松理解桶排序的核心思想，做了很多的假设。实际上，桶排序对要排序数据的要求是非常苛刻的。\n\n首先，要排序的数据需要很容易就能划分成m个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。\n\n其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不均匀，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为o(nlogn)的排序算法了。\n\n桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。\n\n比如说我们有10gb的订单数据，我们希望按订单金额(假设金额都是正整数)进行排序，但是我们的内存有限，只有几百mb，没办法一次性把10gb的数据都加载到内存中。这个时候该怎么办呢？\n\n现在可以来思考一下，如何借助桶排序的处理思想来解决这个问题。\n\n我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是1元，最大是10万元。我们将所有订单根据金额划分到100个桶里，第一个桶我们存储金额在1元到1000元之内的订单，第二桶存储金额在1001到2000元之间的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名(00, 01, 02... 99)。\n\n理想的情况下，如果订单金额在1到10万元之间均匀分布，那订单会被均匀划分到100个文件中，每个小文件中存储大约100mb的订单数据，我们就可以将这100个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。\n\n不过，我们也可能会发现，订单按照金额在1元到10万元之间并不一定是均匀分布的，所以10gb订单数据是无法均匀地被划分到100个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存。这又该如何？\n\n针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在1元到1000元之间的比较多，我们就将这个区间继续划分为10个小区间，1元到100元，101元到200元，201元到300元 ... 901元到1000元。如果划分之后，101元到200元之间的订单还是太多，无法一次性读入内存，那就继续再划分，直到所有的文件都能读入内存为止。\n\n\n# 计数排序(counting sort)\n\n个人认为，计数排序其实是桶排序的一种特殊情况。当要排序的n个数据，所处的范围并不大的时候，比如最大值是k，我们就可以把数据划分成k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。\n\n如果所在的省有50万考生，如何通过成绩快速排序得出名次呢？\n\n考生的满分是900分，最小是0分买这个数据的范围很小，所以我们可以分成901个桶，对应分数从0分到900分。\n\n根据考生的成绩，我们将这50w考生划分到这901个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序。我们只需要一次扫描每个桶，将桶内的考试依次输出到一个数组中，就实现了50万考试的排序。因为只涉及扫描遍历操作，所以时间复杂度是o(n).\n\n计数排序的算法思想，跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个排序算法叫"计数"排序呢？\n\n我们还是用考生的例子，对数据规模做了简化。假设只有8个考生，分数在0到5分之间。这8个考生的成绩我们放在一个数组a[8]中，它们分别是：2，5，3，0，2，3，0，3。\n\n考生的成绩从0到5分，我们使用大小为6的数组c[6]表示桶，其中下标对应分数。不过，c[6]内存储的并不是考生，而是对应的考生个数。以刚才的例子，我们只需要遍历一遍考试分数，就可以得到c[6]的值。\n\n\n\n从图中可以看出，分数为3分的考生有3个，小于3分的考试有4个，所以，成绩为3分的考试在排序之后的有序数组r[8]中，会保存下标4，5，6的位置。\n\n\n\n那我们如何快速计算出，每个分数的考试在有序数组中对应的存储位置呢？这个处理方法非常巧妙。\n\n思路是这样的：我们对c[6]数组顺序求和，c[6]存储的数据就变成了下面的样子。c[k]里存储小于等于分数k的考生个数。\n\n\n\n我们从后到前依次扫描数组a。比如，当扫描到3的时候，我们可以从数组c中取出下标为3的值7，也就是说，到目前为止，包括自己在内，分数小于等于3的考生有7个，也就是说3是数组r中的第7个元素(也就是数组r中下标为6的位置)。当3放入到数组r中后，小于等于3的元素就只剩下6个了，所以相应的c[3]要减1，变成6。\n\n> 理解：从后到前的方向来扫描数组a，扫描到一个元素值的时候，到c这个数组下面，根据a中扫描的那个值的作为c的下标，得到c中对应的下标的元素的值，我们从c中得到值x，减去1，放入到r中(x-1)的数组下标下面。完成一次后，将c中对应的下标中的值x也减去1。\n\n以此类推，当我们扫描到第2个分数为3的考生的时候，就会把它放入数组r中的第6个元素的位置(也就是下标为5的位置)。当我们扫描完整个数组a后，数组r内的数据就按照分数从小到大有序排列的了。\n\n计数排序只能用在数据范围不大的场景中，如果数据范围k比要排序的数据n大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变大小的情况下，转化为非负整数。\n\n比如，还是考生的例子。如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以10，转化成整数，然后再放到9010个桶内。再比如，如果要排序的数据中有负数，数据的范围是[-1000,1000]，那我们就需要先对每个数据都加1000，转化成非负整数。\n\n\n# 基数排序(radix sort)\n\n再看一个排序问题。假设我们有10万个手机号码，希望将这10万个手机号码从小到大排序。\n\n利用之前讲的快排，时间复杂度可用做到o(nlogn)，是否还有更加高效的排序算法？桶排序、计数排序能排上用场吗？\n\n由于手机号码有11位，范围太大了，显然不适合用桶排序或者计数排序。这里，我们可以用到新的排序算法，基数排序。\n\n刚刚这个问题里有这样的规律：假设要比较两个手机号码a，b的大小，如果在前面几位中，a手机号码已经比b手机号码大了，那后面的几位就不用看了。\n\n借助稳定排序算法，这里有一个巧妙的实现思路。我们这里先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过11次排序之后，手机号码就都有序了。\n\n\n\n这里按照每位来排序的排序算法要是稳定的。因为如果是非稳定的排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，这样的话，低位的排序就完全没有意义了。\n\n根据每一位来排序，我们可以用刚讲过的桶排序或计数排序，它们的时间复杂度可以做到o(n)。如果要排序的数据有k位，那我们就需要k次桶排序或计数排序，总的时间复杂度是o(k*n)。当k不大的时候，比如手机号码排序的例子，k最大就是11，所以基数排序的时间复杂度就近视于o(n).\n\n实际上，有时候要排序的数据并不都是等长的，例如英文单词，最短的只有1个字母，最长的有45个字母。对于这种不等长的数据，基数排序如何去做呢？\n\n我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补"0"。因为根据ascii值，所有字母都大于"0"，所以补"0"不会影响到原有的大小顺序。这样就可以继续使用基数排序了。\n\n总结来说，基数排序对要排序的数据是有要求的，需要可以分割出独立的"位"来比较，而且位之间有递进的关系，如果a数据的高位比b数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到o(n)了。\n\n\n# 解答开篇\n\n再来看看开篇的思考题：如何根据年龄给100w用户排序？\n\n实际上，根据年龄给100w用户排序，就类似按照成绩给50w考生排序。我们假设年龄的范围最小1岁，最大不超过120岁。我们可以遍历这100万用户，根据年龄将其划分到这120个桶里，然后依次顺序遍历这120个桶内的元素。这样就得到了按照年龄排序的100万用户的数据。\n\n\n# 内容小结\n\n今天，学习了3种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到o(n).\n\n桶排序和计数排序的排序思想非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序。\n\n基数排序要求数据可以划分为高低位，位之间有递进的关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一位的排序工作。\n\n\n# d41(2020/11/04)\n\n今天主要是要学习的是，对各个排序算法的一种大概的总结。\n\n几乎所有的编程语言都会提供排序函数，比如c语言中qsort()，c++ stl中的sort()、stable_sort()，还有java语言中的collections.sort()。在平时的开发中，我们也都是直接使用这些现成的函数来实现业务逻辑中的排序功能。\n\n\n# 如何选择合适的排序函数？\n\n如果要实现一个通用的、高效率的排序函数，我们应该选择哪种排序算法呢？\n\n\n\n\n# 冒泡排序\n\n# 冒泡排序的思想\n\n对于冒泡排序来说，会有两个for循环，外层的for循环，控制的是排序的趟数，内层的for循环，是在某一趟内，对相邻的两个元素进行比较和交换。\n\n# 冒泡排序的优化\n\n这里会涉及到一个对冒泡排序的优化，在外层for循环排序那，设置一个flag标签，外层flag的初始值是false。在实际的冒泡排序中，会存在这么一种场景，当内层的for循环在执行的过程中，是对前后相邻的两个数进行比较，但是如果这个时候发现相邻的两个数没有交换(已经有序)了，那么后面的数也是有序的了。所以来说，只有当内存for循环执行了交换，才会去修改flag的值为true，否则如果flag依旧还是为false的话，那么就直接跳出\n\n# 冒泡排序的代码\n\n代码示例如下：\n\nint main()\n{\n    int n;\n    int arry[10];\n    for (int i = 0; i < n - 1; i++)\n    {\n        bool flag = false;\n        for (int j = 0; j < n - i - 1; j++)\n        {\n            if (arry[j] > arry[j + 1])\n            {\n                int tmp = arry[j];\n                arry[j] = arry[j + 1];\n                arry[j + 1] = tmp;\n                flag = true;\n            }\n        }\n\n        if (!flag)\n        {\n            break;\n        }\n    }\n\n    for (int i = 0; i < n; i++)\n    {\n        printf("%d\\n", arry[i]);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n书写冒泡排序有两个重要点：\n\n 1. 要注意两个for循环的i的范围和j的范围，i的范围从[0, n-1]，j的范围是从[0,n-i-1]。\n 2. 注意flag的位置，初始设置flag=false的位于最外层for循环开始的位置，而flag = true则设置在内层循环的交换完毕后，设置的值，具体的位置应该是在if (a[j]>a[j+1]) 里面的方法体内，具体位置，在交换位置的代码的最后。确保只有发生了交换，才会去设置这个flag 为true\n\n# 冒泡排序算法分析\n\n 1. 冒泡排序是原地的排序算法。在整个的排序的过程中，只是涉及到相邻的数据的交换的操作，只需要引入一个临时的数组元素变量，不需要申请其他的内存空间了。所以冒泡排序的空间复杂度是o(1)，是一个原地排序算法。\n\n 2. 冒泡排序是稳定的排序算法。在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换。这样的话，相同大小的数据在排序前后不会改变顺序。\n\n 3. 冒泡排序的时间复杂度。\n    \n    * 最好情况时间复杂度o(n)。当要排序的数据已经是有序的了，我们只需要最外层的一个for循环就可以了(表示的要比较的趟数)，里面的for循环(初始的去判断一次，由于不满足条件，没有去交换元素，直接将flag设置为了true)，从而直接跳出了外层的for循环。整体的最好情况时间复杂度是o(n).\n    \n    * 最坏情况时间复杂度o($n^2$)。最外层的for循环需要n趟，里层的for循环也要有n次，所以最坏情况下的时间复杂度是o($n^2$)。\n\n\n# 插入排序\n\n# 插入排序思想\n\n插入排序整体思想是，将数组中的数据分为两个区间，已排序区间和未排序区间。\n\n初始的已排序的区间，我们设置为只有1个元素，这个元素就是数组中的第一个元素。\n\n核心思想就是，取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。\n\n# 有序度/逆序度/满有序度\n\n 1. 有序度是指，数组中具有有序关系的元素对的个数。\n    \n    2,4,3,1,5,6这组数据的有序度是11，因为其有序元素对为11个。分别是：\n    (2,4) (2,3) (2,5) (2,6) \n    (4,5) (4,6) (3,5) (3,6)\n    (1,5) (1,6) (5,6)\n    \n    \n    1\n    2\n    3\n    4\n    \n\n 2. 满有序度是指，数组中完全有序的数组的有序度，也就是完全有序的有序关系的元素对的个数。\n    \n    对于一个完全有序的数组，比如1,2,3,4,5,6，\n    有序度就是n*(n-1)/2，也就是15\n    \n    \n    1\n    2\n    \n\n 3. 逆有序度是指，数组中无序关系的元素对的个数。同时，我们还可以了解到逆序度=满有序度-有序度.\n    \n    对于冒泡排序或者是插入排序，这种包含比较和交换两种操作的排序来说。不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是n*(n-1)/2 -初始有序度。\n\n# 插入排序代码\n\n代码示例如下：\n\n#include <stdio.h>\nvoid insertsort(int *a, int length)\n{\n    int i, j, tmp;\n    // 外层的循环，表示的是需要有n-1个数组元素需要进行插入排序\n    // 默认认定a[0]这个元素是有序的\n    for (i = 1; i < length; i++)\n    {\n        // 将要插入的数a[i]赋值给tmp\n        // 为什么要引入tmp这个中间变量呢？\n        // 后续里面从a[0]到a[i]中的元素，都有可能出现移动，而导致原来a[i]的值出现改变\n        // 或者可以理解为先将a[i]这个值拿出来\n        tmp = a[i];\n        // 从i-1开始，逐个向前递减，循环来比较\n        for (j = i - 1; j >= 0; j--)\n        {\n            // a[j]代表的是有序的数组序列中的一个元素\n            // 如果要插入的元素，比a[j]要小\n            if (tmp < a[j])\n            {\n                // 将a[j]的值赋值给a[j+1]\n                // 这样就实现了a[j]往后移动一个位置的目的\n                // 这里用a[j+1]而不是a[i]\n                // a[j+1]是个动态变化的过程\n                a[j + 1] = a[j];\n            }\n            else\n            {\n                // 如果要插入的元素，大于等于a[j]\n                // 直接跳出内层的循环\n                break;\n            }\n        }\n        // 将要插入的数tmp放入a[j+1]中\n        a[j + 1] = tmp;\n    }\n}\n\nint main()\n{\n    int array[10] = {9, 4, 2, 10, 5, 1, 3, 5, 7, 9};\n    \n    insertsort(array, sizeof(array) / sizeof(array[0]));\n    int i;\n    for (i = 0; i < sizeof(array) / sizeof(array[0]); i++)\n    {\n        printf("%d ", array[i]);\n    }\n    printf("\\n");\n    return 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\n在书写插入排序代码的时候，还是有一定的难度的。\n\n总体来说，插入排序的逻辑还是比较和交换。思想是将数组分为两个部分，有序子序列，和无序的子序列。我们先默认有序子序列只有1个，是数组的第一个元素a[0]，然后依次从无序子序列中取出一个数组元素，加入到有序子序列中，通过将拿出来的那个无序的元素，通过从后往前的一个个比较，找到合适的位置，然后再将其插入，相依的原有的有序子序列往后移动。\n\n代码分为两层for循环，外层for循环是依次从无序的子序列中取出一个元素，外层for循环i从 1开始，小于n，i++，共需要取n-1次。内层的for循环，表示的是要比较的数组元素，j从i-1开始，j>=0，j--。由于内层的for循环都是有序的子序列，所以最多需要比较i-1次，一旦发现有序序列中的某个元素是比 外层拿出来比较的元素要来的小，那就跳出内层for循环。\n\n# 书写代码的注意点\n\n 1. 外层for循环，i从1开始，i<n，i++\n 2. 内层for循环，j从i-1开始，j>=0， j--\n 3. 在外层for循环内，引入一个中间变量tmp，用来临时存储a[i]，这是由于取出a[i]放入有序序列进行比较的过程中，会有各个有序序列的移动，这样的话，要插入的元素a[i]的值就会有变化了，所以先取出来，放入一个中间变量中，待找到合适的位置的时候，再插入进去。\n 4. 内层循环中，判断条件是 if (tmp < a[j])，那么a[j+1] = a[j]。也就是a[j]元素的值，放入到a[j+1]的上面了。\n 5. 内层循环中，如果条件 if (tmp >= a[j])的话，直接跳出内层for循环。\n 6. 注意最后的赋值出现在内层for循环中，a[j+1] = tmp。这个要分两种情况来理解，一种是完整执行完毕内层for循环，跳出了循环，这个时候j=-1了，所以需要j+1。第二种情况，是中途当发现tmp>=a[j]的话的时候，直接跳出循环了，将tmp放入a[j+1]的位置。\n\n# 插入排序代码分析\n\n 1. 插入排序是原地排序算法。在整个代码的实现过程中，可以看出，插入排序算法中，除了引入了一个tmp的临时变量，没有开辟其他的内存空间。所以空间复杂度是o(1)，这是一个原地的排序算法。\n 2. 插入排序是稳定的排序算法。在将取出的无序的子序列的一个元素tmp的时候，会依次与a[j]开始比较，只有当tmp <a[j]的时候，才会去交换外置，等于的时候，不变化位置，这样的话，就可以保证原有的前后顺序不变，所以说插入排序是稳定的排序算法。\n 3. 插入排序的时间复杂度。如果数组的元素都是有序的情况下，只需要一个外层的每个取无序数组元素的一个操作，内层for循环由于不满足tmp < a[j]，直接就跳出了循环，所以最好情况下的时间复杂度是o(n)。如果数组的元素都是无序的，那么除了外层的取各个元素的for 循环操作，内层for循环还需要挨个的比较，交换位置。所以最坏情况时间复杂度是o($n^2$)。平均时间复杂度是o($n^2$)，数组中插入一个数据的评卷时间复杂度是o(n)，循环n次，就是o($n^2$)。\n\n\n# 选择排序\n\n# 选择排序思想\n\n选择排序算法的实现思路也有点类似插入排序，也分为已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。\n\n# 选择排序代码\n\n# include <stdio.h>\n\nvoid select_sort(int *a, int len)\n{\n    int i;\n    int j;\n    // 定义最小值\n    int min_value;\n    // 定义最小值的位置\n    int min_pos;\n\n    // n个元素，需要进行n-1趟的排序\n    // 看作是n个位置上逐个从剩余的元素中找最小值\n    // 第一个位置，从n个元素中找\n    // 第二个位置，从n-1个元素中找\n    // 第n-1个位置，剩下的2个元素中来找\n    // 第n个位置，不要找了，就是最后一个元素\n    for (i=0; i< len -1; i++)\n    {\n        // 先定义一个初始的最小值和最小值的位置\n        min_value = a[i];\n        min_pos = i;\n\n        // 内层循环从a[i+1]开始，直到a[len-1]结束的无序数组中来找寻最大值\n        for (j=i+1; j< len; j++)\n        {\n            // 如果无序子序列中的某个元素值，要小于初始定义的最小值\n            if (a[j] < min_value)\n            {\n                //  将a[j]赋值给min_value\n                min_value = a[j];\n                // 将j赋值给min_pos\n                min_pos = j;\n            }\n        }\n        // 将内层的for循环中找到的最小值，和初始的最小值进行比较\n        // 如果内层的无序的for循环中的最小值要比初始定义的最小值，还要小\n        if (min_value < a[i]) \n        {\n            // 将原先定义的最小值放入，后面无序子序列中找到的最小值的位置上\n            // 也就是实现了a[i]和后面找到的a[min_pos]的两个值的交换\n            a[min_pos] =a[i];\n            // 将后面找到的最小值赋值给a[i]\n            a[i] = min_value;\n        }\n    }\n}\n\nint main()\n{\n    int a[9] = {12, 44, 0, 12, 45, 666, 5, 4, 3};\n    select_sort(a, 9);\n    for (int i=0; i<9; i++)\n    {\n        printf("%d ", a[i]);\n    }\n    printf("\\n");\n    return 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n\n\n# 选择排序代码分析\n\n 1. 选择排序是原地排序代码。只是引入了一个int 最小值，和一个最小值所在的位置，空间复杂度是o(1)。所以选择排序是一种原地排序算法。\n 2. 选择排序是不稳定的排序算法。比如5,7,5,2,9这样一组数据，使用选择排序算法来排序的话，第一次找到最小元素2，与第一个5进行了交换位置，那么第一个5和中间的5的顺序就变了，所以就不稳定了。\n 3. 选择排序的时间复杂度。最好情况，最坏情况，和平均情况的时间复杂度都是o($n^2$)。这是由于外层逐个位置的for循环不会少，内层的从无序子序列中找寻最小值的for循环也不会少，只是后面的if代码会有些变化，但是总体的时间复杂度还是o($n^2$) .\n\n\n# 归并排序\n\n# 归并排序思想\n\n基本思想就是：将两个或两个以上的有序子序列"归并"为一个有序序列。如果是两个有序子序列的话，就是2-路归并排序。\n\n即将两个位置相邻的有序子序列r[l...m]和r[m+1...n]归并为一个有序序列r[l...n].\n\n# 归并排序代码\n\n#include <iostream>\nusing namespace std;\n\n// l是指arr数组中最开始的位置，数组下标\n// r是指arr数组中最右边的位置，数组下标\n// m是(l+r)/2后得到的中间位置的数组下标\n// arr是一个数组，这个数组有个特点，前半部分有序，后半部分也有序\nvoid merge(int *arr, int l, int m, int r){\n\tint left_size = m-l;\n\tint right_size = r-m+1;\n\tint *l_arr = new int[left_size];\n\tint *r_arr = new int[right_size];\n\n\t// 1. fill in the left sub array\n\tfor(int i=l; i<m; i++)\n\t\tl_arr[i-l] = arr[i];\n\t// 2. fill in the right sub array\n\tfor(int i=m; i<=r; i++)\n\t\tr_arr[i-m] = arr[i];\n\t\n\t// i表示的是left_size数组中一开始的下标\n\t// j表示的是right_size数组中一开始的下标\n\t// 定义k一开始指向原数组最开始的位置,也就是l，由于是反复的嵌套调用，所以需要是l\n\tint i = 0, j = 0, k = l;\n\t// i和j都不碰到各自s数组的最大的限制\n\twhile(i < left_size && j < right_size)\n\t    //判断两个数组中，下标为i和下标为j 的这两个谁大\n\t\tif(l_arr[i]<r_arr[j])\n\t\t    // 可以改写为arr[k]=left[i]; i++; k++;\n\t\t\tarr[k++] = l_arr[i++];\n\t\telse\n\t\t   // 可以改写为arr[k]=right[j]; j++; k++;\n\t\t\tarr[k++] = r_arr[j++];\n\t// 上面那个while结束后，如果i还小于left_size的话\n\t// 就将left数组中的剩余的元素都挨个，赋值到arr数组中\n\twhile(i < left_size)\n\t    // 下面代码可以改写为arr[k]=left[i];i++;k++;\n\t\tarr[k++] = l_arr[i++];\n\t// 上面那个while结束后，如果j还小于right_size的话\n\t// 就将right数组中的剩余的元素都挨个，赋值到arr数组中\n\twhile(j < right_size)\n\t    // 下面代码可以改写为arr[k]=right[j];j++;k++;\n\t\tarr[k++] = r_arr[j++];\n}\n\nvoid merge_sort(int *arr, int l, int r){\n\t// 左边的下标和右边下标相等的话，\n\t// 就认为是已经能拍好顺序了\n\t// 递归的终止条件\n\tif(l == r)\n\t\treturn;\n\telse\n\t{\n\t\tint m = (l+r)/2;\n\t\tmerge_sort(arr, l, m);\n\t\tmerge_sort(arr, m+1, r);\n\t\tmerge(arr, l, m+1, r);\n\t}\n}\n\nvoid print_sort(int *arr, int num)\n{\n\tfor(int i=0; i < num; i++)\n\t\tcout << arr[i] << "   ";\n\tcout << endl;\n}\n\nint main()\n{\n\tint arr[11] ={6, 3};\n\tmerge_sort(arr, 0, 1);\n\tprint_sort(arr, 2);\n\treturn 0;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n\n\n# 归并排序代码分析\n\n 1. 归并排序不是原地的排序算法，需要额外申请一个和原数组大小一样的数组来临时存放数据。空间复杂度是o(n).\n 2. 归并排序是稳定的排序算法。在比较两个有序子序列的时候，我们可以将两个相等元素，不交换位置。\n 3. 归并排序的最好，最坏的时间复杂度都是o(nlogn).\n\n\n# 快速排序\n\n# 排序思想\n\n快排是以待排序列中的某个数据为支点，将待排序的序列分为两个部分，其中左半部分的数据要小于等于支点，而右半部分的数据要大于等于支点。\n\n分别三个下标，privotkey是我们选择的支点，可以选择是第一个或最后一个元素。low是我们选择的从数组最开始往后搜索的指针，而high是从数组中最末尾向前搜索的指针。\n\n如果privotkey是第一个元素，那么先从high 与 privotkey比较，如果high > privotkey则继续high向前移动，直到小于privotkey的时候，与low交换位置。交换完后，low开始向后比较，直到low所在的元素比pri大的时候，与high 交换位置。最后当high与low重合的时候，该趟停止。\n\n然后将在pirvotkey的两边，分成两个子序列，循序继续上面的操作。直到所有都有序。\n\n# 快速排序的代码\n\n# include <stdio.h>\n\n// 对序列进行一次划分\nint partition (sqlist &l, int low, int high)\n{\n    int pivotkey;\n    // 关键字\n    privotkey = l.r[low];\n\n    // 从表的两端交替向中间扫描\n    while (low < high) \n    {\n        while (low<high && l.r[high] >=privotkey) {\n            --high;\n        }\n        // 交换位置\n        l.r[low] = l.r[high];\n        while (low<high && l.r[low] <=pivotkey) {\n            ++low;\n        }\n        // 交换位置\n        l.r[high] = l.r[low];\n    }\n    // 返回中间的位置\n    return low;\n}\n\n// 主程序，按分区对子程序进行调用\nvoid quicksort1 (sqlist &l, int low, int high)\n{\n    int mid;\n    if (low < high) {\n        mid = partition(l, low, high);\n        //对低子表进行排序\n        quicksort1 (l, low, mid-1);\n        //对高子表进行排序\n        quicksort1 (l, mid+1, high);\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n# 分析快速排序的代码\n\n 1. 快速排序是不稳定的排序算法。无法保证两个相等的元素，排序前后的位置是一致的。\n 2. 快速排序是原地的排序算法，不需要一看如额外的内存空间。\n 3. 快排的时间复杂度，快排的大部分时候的时间复杂度是o(nlogn)，但是当分区均衡的时候，是o(nlogn)。而当分区及其不均衡的时候，是指已经是一个有序的数组了，这种情况下分区是及其不均衡的，这个时候的时间复杂度会退化到o($n^2$) .\n\n\n# 桶排序\n\n# 桶排序的排序思想\n\n首先对于使用桶排序，还是对数组序列有一定的要求的。这些要排序的数据需要很容易的划分成m个桶。\n\n桶与桶之间有着天然的大小顺序，这个每个桶内的数据都排好序后，桶与桶之间的额数据不需要再进行排序了。另外桶的范围不能太大了。\n\n 1. 首先将要排序的数据分到几个有序的桶里。\n 2. 每个桶里的数据再单独进行排序。\n 3. 桶内排完序后，再把每个桶内的数据按照顺序依次取出。\n 4. 最后组成的序列就是有序的了。\n\n# 分析桶排序代码\n\n 1. 桶排序是稳定的排序算法，原数组扫描的时候，如果从头开始扫描依次放入各个桶内，然后从桶内读取数据，也依次读出，可以保证有序性。\n 2. 桶排序是非原地的排序算法。需要引入额外的内存空间，桶。\n 3. 桶排序的时间复杂度。\n    * 最好情况下，n个数据，均匀地分到m个桶内，每个桶内就是k=n/m个数据。每个桶内使用快排，时间复杂度就是o(k*logk)。m个桶排序的时间复杂度就是o(m*k*logk)，因为k=n/m，所以整个桶的排序就是o(n*log(n/m)).当桶的个数m接近数据个数n的时候，log(n/m)就是一个非常小的常量。这个时候的桶排序时间复杂度接近o(n).\n    * 最坏情况下，同上面的具体情况，如果极度不均匀，就只有一个桶的时候，一个桶的排序就是一个快排了o(nlogn).\n\n\n# 计数排序\n\n# 计数排序的算法思想\n\n普通思想：\n\nn个数据，所处的范围并不大的时候。比如最大值为k，我们就设定每个值是1个桶，就把数据划分成了k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。最后依次扫描各个桶，形成排好序的完整数组。\n\n稳定的算法思想：\n\n存储每个值的桶内的原始值，为数组前后的相加。这样每个桶位置上的数值，就是小于等于该桶下标的值的数量。我们先从尾到头遍历一下原来要排序的数组a，找到a中的一个元素值x，利用这个值，到c桶数组中，找到c桶中对应的这个x值的下标的值是y。将a中的这个元素值，写入到完整排好序的数组r[y-1]的位置。继续从为到头遍历数组a，这样继续从尾巴到头遍历数组a，下来，数组中相等元素的位置就是稳定的了。\n\n# 计数排序的特殊要求\n\n计数排序只能是针对大量重复数据，在一个小范围内的排序，如果数据范围k比要排序的数据n大得多，就不适合用计数排序了。\n\n计数排序其实是桶排序的一种特殊情况。\n\n计数排序中的桶大小粒度，与桶排序中的粒度是不一样的。桶排序中，考虑的是每个桶内要均匀，而在计数排序中的桶，考虑的是涵盖一个小的范围，每个值就是一个桶。\n\n计数排序只能给非负整数排序，如果要排序的数据是其他类型，要将其在不改变相对大小的情况下，转化为非负整数。\n\n# 计数排序的算法分析\n\n 1. 计数排序可以做到稳定的排序算法。\n 2. 计算排序不是原地排序算法，同样，在排序的过程中，需要额外申请一个桶c的数组k，还有一个最后拍好序的数组n.\n 3. 计数排序的时间复杂度。由于计数排序的要求是特殊场景下，才能实现的，只要满足这种场景的情况下使用的计数排序，都是o(n).\n\n\n# 基数排序\n\n# 基数排序的特殊要求\n\n 1. 要排序的数据，可以分割出独立的"位"来比较，位数不够的，需要补足位数。补足的位数，不要影响原有的大小顺序。数据可以划分"高低"位。\n 2. 位与位之间有递进的关系，如果a数据的高位比b数据大，那么剩下的低位就不用比较了。\n 3. 每一位的数据范围不能太大，要可以使用线性排序算法。由于要对每一位上的数据范围，进行桶排序或者是计数排序。\n\n# 基数排序的排序思想\n\n 1. 所有数据先按最后一位来排序，可以使用桶排序或计数排序。\n 2. 接下来是倒数第二位排序。\n 3. 直到最后对第一位排序。\n\n# 基数排序算法分析\n\n 1. 基数排序是稳定的排序算法，在对每一位进行桶或计数排序的时候，可以保证整个数据的前后顺序的一致性。\n 2. 基数排序不是原地的排序算法。由于每一位的排序中，用到了桶排序或计数排序，而桶排序需要一个c数组和一个新牌号序的数组r，计数排序也是同样需要的。\n 3. 基数排序算法的时间复杂度。道理同上面的计数排序，基数排序还是非常特殊的排序，用在特殊的场景下，如果这个时候要排序的数据有k位，需要使用k次桶排序或计数排序，总的时间复杂度就是o(k*n)。当k不大的时候，可以将基数排序时间复杂度近似于o(n)。\n\n\n# 整体分析\n\n线性排序算法(桶排序、计数排序、基数排序)的时间复杂度比较低，使用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。\n\n如果对小规模数据进行排序，可以选择时间复杂度是o($n^2$)的算法；如果udine大规模数据进行排序，时间复杂度是o(nlogn)的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是o(nlogn)的排序算法来实现排序函数。\n\n时间复杂度是o(nlogn)的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面还会有堆排序。堆排序和快速排序都有比较多的应用，比如java语言采用堆排序实现排序函数，c语言使用快速排序实现排序函数。\n\n实际过程中，使用归并排序的情况其实并不多。我们知道，快排在最坏情况下的时间复杂度是o($n^2$) ，而归并排序可以做到平均情况、最坏情况下的时间复杂度都是o(nlogn)，从这点上看起来很诱人，那为何归并排序使用不多呢？\n\n这是由于归并排序并不是原地排序算法，空间复杂度是o(n)。\n\n前面提及了，快速排序比较适合来实现排序函数，但是，我们也知道，快速排序子最坏情况下的时间复杂度是o($n^2$)，如何来解决这个问题呢？\n\n\n# 如何优化快速排序？\n\n快排算法，为什么在最坏情况下的时间复杂度是o($n^2$)呢？如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算就会变得非常糟糕，时间复杂度就会退化为o($n^2$)。实际上，这种o($n^2$) 时间复杂度出现的这主要原因还是因为我们分区点选得不够合理。\n\n那如何来选择好的分区点呢？\n\n最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。\n\n如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点，肯定会出现之前的情况。在某些情况下，排序的最坏情况时间复杂度是o($n^2$)。\n\n\n# 三数取中法\n\n我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好。但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”。\n\n\n# 随机法\n\n随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选得很差的情况，所以平均情况下，这样选的分区点是比较好的。时间复杂度退化为最糟糕的 o(n2) 的情况，出现的可能性不大。\n\n我们知道，快速排序是用递归来实现的。我们在递归那一节讲过，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。\n\n\n# 举例分析排序函数\n\nglibc 中的 qsort() 函数举例说明一下。虽说 qsort() 从名字上看，很像是基于快速排序算法实现的，实际上它并不仅仅用了快排这一种算法。\n\n如果你去看源码，你就会发现，qsort() 会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是 o(n)，所以对于小数据量的排序，比如 1kb、2kb 等，归并排序额外需要 1kb、2kb 的内存空间，这个问题不大。现在计算机的内存都挺大的，我们很多时候追求的是速度。还记得我们前面讲过的用空间换时间的技巧吗？这就是一个典型的应用。\n\n但如果数据量太大，就跟我们前面提到的，排序 100mb 的数据，这个时候我们再用归并排序就不合适了。所以，要排序的数据量比较大的时候，qsort() 会改为用快速排序算法来排序。\n\nqsort() 选择分区点的方法就是“三数取中法”。还有我们前面提到的递归太深会导致堆栈溢出的问题，qsort() 是通过自己实现一个堆上的栈，手动模拟递归来解决的。\n\n实际上，qsort() 并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于 4 时，qsort() 就退化为插入排序，不再继续用递归来做快速排序，因为我们前面也讲过，在小规模数据面前，o(n2) 时间复杂度的算法并不一定比 o(nlogn) 的算法执行时间长。\n\n我们在讲复杂度分析的时候讲过，算法的性能可以通过时间复杂度来分析，但是，这种复杂度分析是比较偏理论的，如果我们深究的话，实际上时间复杂度并不等于代码实际的运行时间。\n\n时间复杂度代表的是一个增长趋势，如果画成增长曲线图，你会发现 o(n2) 比 o(nlogn) 要陡峭，也就是说增长趋势要更猛一些。但是，我们前面讲过，在大 o 复杂度表示法中，我们会省略低阶、系数和常数，也就是说，o(nlogn) 在没有省略低阶、系数、常数之前可能是 o(knlogn + c)，而且 k 和 c 有可能还是一个比较大的数。\n\n假设 k=1000，c=200，当我们对小规模数据（比如 n=100）排序时，n2的值实际上比 knlogn+c 还要小。\n\n所以，对于小规模数据的排序，o(n2) 的排序算法并不一定比 o(nlogn) 排序算法执行的时间长。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法。\n\n\n# d42(2020/11/10)\n\n今天要学习的是二分查找的第一部分。\n\n今天要学习的是一种针对有序数据集合的查找算法：二分查找算法，也叫折半查找算法。二分查找的思想非常简单，但是看似简单的东西往往越掌握好，想要灵活应用就更加困难。\n\n\n# 无处不在的二分思想\n\n假设有10个订单，订单金额分别是：8，11，19，23，27，33，45，55，67，98。我们如何利用二分查找的思想，查找到是否存在金额等于19元的订单。如果存在，则返回订单数据，如果不存在则返回null.\n\n利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围。为了更加直观，画了一张查找过程的图。其中，low和high表示待查找区间的下标，mid表示待查找区间的中间元素下标。\n\n\n\n二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，知道找到要查找的元素，或者区间被缩小为0。\n\n\n# o(logn)惊人的查找速度\n\n我们假设数据大小是n，每次查找后数据都会缩小为原来的一半，也就是会除以2。最坏情况下，直到查找区间被缩小为空，才停止。\n\n\n\n可以看出来，这是一个等比数列。其中n/ $2^k$ = 1时，k的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了k次区间缩小操作，时间复杂度就是o(k)。通过n/ ($2^k$) = 1, 我们可以求得k= log2 n ,所以时间复杂度就是o(logn)。\n\n二分查找是我们目前为止遇到的第一个时间复杂度是o(logn)的算法。后面章节还会讲到堆、二叉树的操作等等，它们的时间复杂度也是o(logn)。\n\no(logn)这种对数时间复杂度，是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级o(1)的算法还要高效。\n\n因为logn是一个非常"恐怖"的数量级，即便n非常非常大，对应的logn也很小。比如n等于2的32次方，这个数大约是42亿。也就是说，如果我们在42亿个数据中用二分查找一个数据，最多需要比较32次。\n\n我们前面讲过，用大o标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，o(1)有可能表示的是一个非常大的常量值，比如o(1000)、o(10000)。所以，常量级时间复杂度的算法有时候可能还没有o(logn)的算法执行效率高。\n\n\n# 二分查找的递归与非递归实现\n\n简单的二分查找，并不难写，如下所示。\n\n最简单的情况就是有序数组中不存在重复元素，我们在其中用二分查找值等于给定值的数据。如下面的java代码：\n\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n\n  while (low <= high) {\n    int mid = (low + high) / 2;\n    if (a[mid] == value) {\n      return mid;\n    } else if (a[mid] < value) {\n      low = mid + 1;\n    } else {\n      high = mid - 1;\n    }\n  }\n\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n在这个代码中，low、high、mid都是指数组下标，其中low和high表示当前查找的区间范围，初始low=0，high=n-1。mid表示[low, high]的中间位置。我们通过对比a[mid]与value的大小，来更新接下来要查找的区间范围，直到找到或者区间缩小为0，就退出。这里有三个容易出错的地方.\n\n 1. 循环退出条件\n    \n    注意是low <= high，而不是low < high.\n\n 2. mid的取值\n    \n    实际上，mid=(low+high)/2这种写法是由问题的。因为如果low和high比较大的话，两者之和就有可能会溢出。改进的方法是将mid的计算方式写成low+(high-low)/2。更进一步，如果要将性能优化到极致的话，我们可以将这里的除以2操作转化成位运算low+((high-low)>>1)。因为相比除法运算来说，计算机处理位运算要快得多。\n\n 3. low和high的更新\n    \n    low = mid +1, high=mid -1。注意这里的+1和-1，如果直接写成low=mid 或者high=mid，就可能会发生死循环。比如，当high=3, low=3时，如果a[3]不等于value，就会导致一直循环不退出。\n\n实际上，二分查找除了用循环来实现，还可以用递归来实现，过程也非常简单。\n\n如下面的java代码所示：\n\n// 二分查找的递归实现\npublic int bsearch(int[] a, int n, int val) {\n  return bsearchinternally(a, 0, n - 1, val);\n}\n\nprivate int bsearchinternally(int[] a, int low, int high, int value) {\n  if (low > high) return -1;\n\n  int mid =  low + ((high - low) >> 1);\n  if (a[mid] == value) {\n    return mid;\n  } else if (a[mid] < value) {\n    return bsearchinternally(a, mid+1, high, value);\n  } else {\n    return bsearchinternally(a, low, mid-1, value);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 二分查找应用场景的局限性\n\n二分查找的时间复杂度是o(logn)，查找数据的效率非常高。不过，并不是什么情况下都可以用二分查找，它的应用场景是有很大局限性的。那什么情况下适合用二分查找，什么情况下不适合用呢？\n\n首先，二分查找依赖的是顺序表结构，简单来说就是数组。\n\n如果我们对链表是否能使用二分查找呢？答案是不可以的，主要原因是二分查找算法需要按照下标随机访问元素。数组按照下标随机访问数据的时间复杂度是o(1)，而链表随机访问的时间复杂度是o(n)。所以，如果数据使用链表来存储的话，二分查找的时间复杂度就会变得很高。\n\n二分查找只能用在数据是通过顺序表来存储的数据结构上。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。\n\n其次，二分查找针对的是有序数据。\n\n二分查找对这一点的要求比较苛刻，数据必须是有序的。如果数据没有序，我们需要先排序。前面提及，排序的时间复杂度最低是o(nlogn)。所以，如果我们针对的是一组静态的数据，没有频繁地插入、删除，我们可以进行一次排序，多次二分查找。这样排序的成本可被均摊，二分查找的边际成本就会比较低。\n\n但是，如果我们的数据集合有频繁的插入和删除操作，要想用二分查找，要么每次插入、删除操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，无论哪种方法，维护有序的成本都是很高的。\n\n所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用。那针对动态数据集合，会考虑使用二叉树。\n\n再次，数据量太小的不适合二分查找。\n\n如果要处理的数据量很小，完全没有必要使用二分查找，顺序遍历就足够了。比如我们在一个大小为10的数组中查找一个元素，不管用二分查找还是顺序遍历，查找速度都差不多。只有数据量比较大的时候，二分查找的优势才会比较明显。\n\n不过，这里也有一个例外。如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找。比如，数组中存储的都是长度超过300的字符串，如此长的两个字符串之间比对大小，就会非常耗时。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。\n\n最后，数据量太大也不适合二分查找。\n\n二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有1gb大小的数据，如果希望用数组来存储，那就需要连续的，连续的，连续的1gb的内存空间。\n\n\n# 解答开篇\n\n虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决。但是，我们后面会了解到，不管是散列表还是二叉树，都会需要比较多的额外的内存空间。\n\n如果用散列表或者二叉树来存储这1000万的数据，用100mb的内存肯定是存不下的。二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式。\n\n\n# d43(2020/11/11)\n\n今天要学习二分查找的第二部分。如何快速定位ip对应的额省份地址？\n\n我们把要查询202.102.133.13这个ip地址的归属地的问题，转换为查找这个ip地址是否落在了[202.102.133.0,202.102.133.255]这个地址范围内，我们利用这个地址范围来给出这个ip地址的归属地了。\n\n我们的问题是，在一个庞大的地址库中逐一比对ip地址所在的区间，是非常耗时的。假设我们有12万条这样的ip区间与归属地的对应关系，如何快速定位出一个ip地址的归属地呢？\n\n上一节所学的二分查找的代码实现并不难写，那是因为上一节讲的只是二分查找中最简单的一种情况，在不存在重复元素的有序数组中，查找值等于给定值的元素。最简单的二分查找写起来确实不难，但是，二分查找的变形问题就没那么好写了。\n\n下面要描述的是4种二分查找的变形问题，分别是查找第一个值等于给定值的元素；查找最后一个值等于给定值的元素；查找第一个大于等于给定值的元素；查找最后一个小于等于给定值的元素。\n\n\n# 变体一：查找第一个值等于给定值的元素\n\n上一节的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。如果我们将这个问题稍微修改一下，有序数据集合中存在重复的数据，我们希望找到第一个值等于给定值的数据，这样之前的二分查找代码还能继续工作吗？\n\n比如下面这样一个有序数组，其中，a[5]，a[6]，a[7]的值都等于8，是重复的数据。我们希望查找第一个等于8的数据，也就是下标是5的数据。\n\n\n\n如果我们用上一节课讲的二分查找的代码实现，首先拿8与区间的中间值a[4]比较，8比6大，于是在下标5到9之间继续查找。下标5和9的中间位置是下标7，a[7]正好等于8，所以代码就返回了。\n\n尽管a[7]也等于8，但它并不是我们想要找的第一个等于8的元素，因为第一个值等于8的元素是数组下标为5的元素。我们上一节讲的二分查找代码就无法处理这种情况了。所以，针对这个变形问题，我们可以改造上一节的代码。\n\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n  while (low <= high) {\n    int mid =  low + ((high - low) >> 1);\n    if (a[mid] > value) {\n      high = mid - 1;\n    } else if (a[mid] < value) {\n      low = mid + 1;\n    } else {\n      if ((mid == 0) || (a[mid - 1] != value)) return mid;\n      else high = mid - 1;\n    }\n  }\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\na[mid]跟要查找的value的大小关系有三种情况：大于、小于、等于。对于a[mid]>value的情况，我们需要更新high=mid -1; 对于a[mid] < value的情况，我们需要更新low = mid+1。这两点很好理解。那当a[mid] = value的时候应该如何处理呢？\n\n如果我们查找的是任意一个值等于给定值的元素，当a[mid]等于要查找的值的时候，a[mid]就是我们要找的元素。但是，这里我们要求解的是第一个值等于给定值的元素，当a[mid]等于要查找的值时，我们就需要确认一下这个a[mid]是不是第一个值等于给定值的元素。\n\n我们来看第11行代码。\n\nif ((mid == 0) || (a[mid - 1] != value)) return mid;      \nelse high = mid - 1;\n\n\n1\n2\n\n\n如果mid等于0，那这个元素已经是数组的第一个元素，那它肯定是我们要找的；如果mid不等于0，但a[mid]的前一个元素a[mid-1]不等于value，那也说明a[mid]就是我们要找的第一个值等于给定值的元素。\n\n如果经过检查之后发现a[mid]前面的一个元素a[mid-1]也等于value，那说明此时的a[mid]肯定不是我们要查找的第一个值等于给定值的元素。那我们就更新high=mid -1，因为要找额元素肯定出现在[low, mid-1]之间。\n\n\n# 变体二：查找最后一个值等于给定值的元素\n\n如果这里要查找的是最后一个值等于给定值的元素，该如何写代码。\n\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n  while (low <= high) {\n    int mid =  low + ((high - low) >> 1);\n    if (a[mid] > value) {\n      high = mid - 1;\n    } else if (a[mid] < value) {\n      low = mid + 1;\n    } else {\n      if ((mid == n - 1) || (a[mid + 1] != value)) return mid;\n      else low = mid + 1;\n    }\n  }\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n上面的代码是根据第一种问题的改写，得来的。\n\n重点还是在第11行代码，如果a[mid]这个元素已经是数组中的最后一个元素了，那它肯定是我们要找的；如果a[mid]的后一个元素a[mid+1]不等于value，那也说明a[mid]就是我们要找的最后一个值等于给定值的元素。\n\n否则的话(这个时候已经找到了mid是给定的值，但是不是最后一个，后一个元素还是等于给定值)，那么就将low = mid+1，也就是说将low下标往后移动一格，继续去执行while循环。直到满足下面的条件，才返回。\n\nif ((mid == n - 1) || (a[mid + 1] != value)) return mid;      \nelse low = mid + 1;\n\n\n1\n2\n\n\n如果我们经过检查之后，发现a[mid]后面的一个元素a[mid+1]也等于value，那说明当前的这个a[mid]并不是最后一个值等于给定值的元素。我们就更新low=mid+1，因为要找的元素肯定出现在[mid+1,high]之间。\n\n\n# 变体三：查找第一个大于等于给定值的元素\n\n现在我们再来看另外一个变形问题。在有序数组中，查找第一个大于等于给定值的元素。比如，数组中存储的这样一个序列：3，4，6，7，10。如果查找第一个大于等于5的元素，那就是6。\n\npublic int bsearch(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n  while (low <= high) {\n    int mid =  low + ((high - low) >> 1);\n    if (a[mid] >= value) {\n      if ((mid == 0) || (a[mid - 1] < value)) return mid;\n      else high = mid - 1;\n    } else {\n      low = mid + 1;\n    }\n  }\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n如果a[mid]小于要查找的值value，那要查找的值肯定在[mid+1, high]之间，所以，我们更新low=mid+1。\n\n对于a[mid]大于等于给定值的value的情况，我们要先看下这个a[mid]是不是我们要找的第一个值大于等于给定值的元素。如果a[mid]前面已经没有元素，或者前面一个元素小于要查找的值value，那a[mid]就是我们要找的元素。这段逻辑对应的代码是第7行。\n\nif (a[mid] >= value) {      \n    if ((mid == 0) || (a[mid - 1] < value)) return mid;      \n    else high = mid - 1;    \n} else {      \n    low = mid + 1;    \n    }  \n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n如果a[mid-1]也大于等于要查找的值value，那说明要查找的元素在[low, mid-1]之间，所以，我们将high更新为mid-1。\n\n\n# 变体四：查找最后一个小于等于给定值的元素\n\n现在，我们来看最后一种二分查找的变形问题，查找最后一个小于等于给定值的元素。比如，数组中存储了这样一组数据：3，5，6，8，9，10。最后一个小于等于7的元素就是6。\n\npublic int bsearch7(int[] a, int n, int value) {\n  int low = 0;\n  int high = n - 1;\n  while (low <= high) {\n    int mid =  low + ((high - low) >> 1);\n    if (a[mid] > value) {\n      high = mid - 1;\n    } else {\n      if ((mid == n - 1) || (a[mid + 1] > value)) return mid;\n      else low = mid + 1;\n    }\n  }\n  return -1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 解答开篇\n\n如何快速定位出一个ip地址的归属地？\n\n如果ip区间与归属地的对应关系不经常更新，我们可以先预处理这12万条数据，让其按照起始ip从小到大排序。我们知道，ip地址可以转化为32位的整型数。所以，我们可以将起始地址，按照对应的整型值的大小关系，从小到大进行排序。\n\n然后，这个问题就可以转化为第四种变形问题 "在有序数组中，查找最后一个小于等于某个给定值的元素"了。\n\n当我们要查询某个ip归属地时，我们可以通过二分查找，找到最后一个起始ip小于等于这个ip的ip区间，然后，检查这个ip是否在这个ip区间内，如果在，我们就取出对应的归属地显示；如果不在，就返回未查找到。\n\n\n# 内容小结\n\n凡是用二分查找能解决的，绝大部分我们更倾向于用散列表或二叉查找树。即便是二分查找在内存使用上更节省，但是毕竟内存如此紧缺的情况并不多。\n\n实际上，上一节我们学习的"值等于给定值"的二分查找确实不怎么会被用到，二分查找更适合用在"近似"查找问题，在这类问题上，二分查找的优势就更加明显了。比如，今天学习的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。\n\n\n# d44(2020/11/12)\n\n今天要学习的是跳表的数据结构。\n\n上两节我们讲解了二分查找算法。当时我们讲到，因为二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，就真的没有办法来用二分查找算法了吗？\n\n实际上，我们只需要对链表稍加改造，就可以支持类似"二分"的查找算法。我们把改造之后的数据结构叫做跳表 (skip list)。\n\n跳表这种数据结构，是一种各方面性能都比较优秀的动态数据结构。可以支持快速地插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树。\n\nredis 中的有序集合(sorted set)就是用跳表来实现的。我们也知道红黑树也是可以实现快速地插入、删除和查找操作。\n\n那redis 为什么会选择用跳表来实现有序集合呢？为什么不用红黑树呢？\n\n\n# 如何理解跳表\n\n对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是o(n)。\n\n\n\n那么如何来提高查找效率呢？如果像图中那样，对链表建立一级"索引"，查找起来是不是就会更快一些呢？每两个结点提取一个结点到上一级，我们把抽出来的那一级叫做索引或索引层。如下图所示，down表示down指针，指向下一级结点。\n\n\n\n如果我们现在要查找某个结点，比如16。我们可以先在索引层遍历，当遍历到索引层中值为13的结点时，我们发现下一个结点是17，那要查找的结点16肯定就在这两个结点之间。然后我们通过索引层结点的down指针，下降到原始链表这一层，继续遍历。这个时候，我们只需要再遍历2个结点，就可以找到值等于16的这个结点了。这样，原来如果要查找16，需要遍历10个结点，现在只需要遍历7个结点。\n\n从这个例子来看，加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就说查找效率提高了。那如果我们再加一级索引呢？效率会不会提升更多呢？\n\n跟前面建立第一级索引的方式相似，我们在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。现在我们再来查找16，只需要遍历6个结点了，需要遍历的结点数量又减少了。\n\n\n\n下面的例子中，我画了一个包含64个结点的链表，按照前面讲的思路，建立了五级索引。\n\n\n\n从图中我们可以看出，原来没有索引的时候，查找62需要遍历62个结点，现在只需要遍历11个节点，速度提高了很多。所以，当链表的长度n比较大的时候，比如1000、10000的时候，在构建索引之后，查找效率的提升就会非常明显。\n\n前面讲的这种链表加多级索引的结构，就是跳表。我们通过例子展示了跳表是如何减少查询次数的，由此来提高查询效率。接下来，我们会定量地分析一下，用跳表查询到底有多快呢？\n\n\n# 用跳表查询到底有多快？\n\n前面我们讲解过，算法的执行效率可以通过时间复杂度来度量的。我们知道，在一个单链表中查询某个数据的时间复杂度是o(n)。那在一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢？\n\n这个时间复杂度的分析方法比较难以想到。我们这里把问题分解一下，先来看这样一个问题，如果链表里有n个结点，会有多少级索引呢？\n\n按照我们刚才讲的，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是n/8，依次类推，也就是说，第k级索引的结点个数是第k-1级索引的结点个数的1/2，那第k级索引结点的个数就是 n/(2^k)。\n\n假设索引有h级，最高级的索引有2个结点。通过上面的公式，我们可以得到n/(2^k) = 2, 从而求得h= $\\log_2 k$ -1 。如果包含原始链表这一层，那么整个跳表的高度就是 $log_2 n$。我们在跳表中查询某个数据的时候，如果每一层都要遍历m个结点，那么在跳表中查询一个数据的时候的时间复杂度就是o(m*$log_2 n$) .\n\n那么这个m的值到底是多少来着呢？按照前面这种索引结构，我们每一级索引都最多只需要遍历3个结点，也就是说m=3，为什么呢？\n\n假设我们要查找的数据是x，在第k级索引中，我们遍历到y结点之后，发现x大于y，小于后面的结点z，所以我们通过y的down指针，从第k级索引下降到第k-1级索引。在第k-1级索引中，y和z之间只有3个节点(包含y和z)，所以，我们在k-1级索引中最多只需要遍历3个结点，依次类推，每一级索引都最多只需要遍历3个结点。\n\n\n\n通过上面的分析，我们得到m=3，所以在跳表中查询任意数据的时间复杂度就是o($log n$). 这个查找的时间复杂度跟二分查找是一样的。换句话说，我们其实是基于单链表实现了二分查找。\n\n但是这种查询效率的提升，前提是建立了很多级的索引，也就是我们所说的用空间来换时间的设计思路。\n\n\n# 跳表是不是很浪费内存\n\n比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。那到底需要消耗多少额外的存储空间呢？ 我们来分析一下跳表的空间复杂度。\n\n跳表的空间复杂度分析并不难，在前面所说的，假设原始链表大小为n，那第一级索引大约有n/2个结点，第二级索引大约有n/4个结点，以此类推，每上升一级就减少一半，直到剩下2个结点。如果我们把每层索引的结点数都写出来，就是一个等比数列。\n\n\n\n这几级索引的结点总和就是n/2 + n/4 + n/8 ... +8 +4+2 =n-2。所以，跳表的空间复杂度是 o(n)。也就是说，如果将包含n个结点的单链表构造成跳表，我们需要额外再用接近n个结点的存储空间。那我们有没有办法降低索引上占用的内存空间呢？\n\n我们前面都是每两个结点抽一个结点到上级索引，如果我们每三个结点或五个结点，抽一个结点到上级索引，是不是就不用那么多索引结点了呢？下面是一个每三个结点抽一个的示意图，我们可以看下。\n\n\n\n从图中可以看出，第一级索引需要大约n/3个结点，第二级索引需要大约n/9个结点。每往上一级，索引结点个数都除以3。为了方便计算，我们假设最高一级的索引结点个数是1。我们把每级索引的结点个数都写下来，也是一个等比数列。\n\n\n\n通过等比数列求和公式，总的索引结点大约就是n/3 + n/9 + n/27 + ...+9+3+1 = n/2。尽管空间复杂度还是o(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结构存储空间。\n\n实际上，在软件开发中，我们不必太在意索引占用的额外空间。在讲数据结构和算法时，我们习惯性把要处理的数据看成是整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。\n\n\n# 高效的动态插入和删除\n\n上面描述了跳表的查找操作。实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是o(logn)。\n\n\n# 动态插入\n\n下面来看下，如何在跳表中插入一个数据，以及它是如何做到o(logn)的时间复杂度。\n\n我们知道，在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是o(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。\n\n对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的时间复杂度是o(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是o(logn)。\n\n\n# 动态删除\n\n如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。\n\n\n# 跳表索引动态更新\n\n当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某2个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。\n\n\n\n作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡，也就是说，如果链表中结点过多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。\n\n我们了解到红黑树、avl树这样平衡二叉树，它们是通过左右旋的方式保持左右子树的大小平衡，而跳表是通过随机函数来维护前面提到的"平衡性"。\n\n当我们往跳表中插入数据的嘶吼，我们可以选择同时将这个数据插入到部分索引层中。如何选择加入哪些索引层呢？\n\n我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值k，那我们将这个结点添加到第一级到第k级索引中。\n\n\n\n随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。\n\n\n# 解答开篇\n\n我们来讲解一下开篇的思考题：为什么redis要用跳表来实现有序集合，而不是红黑树。\n\nredis中的有序集合是通过跳表来实现的，严格来说，其实还用到了散列表。不如果我们去查看redis 的开发手册，就会发现，redis中的有序集合支持的核心操作主要有下面这几个：\n\n * 插入一个数据；\n * 删除一个数据；\n * 查找一个数据；\n * 按照区间查找数据(比如查找值在[100, 356]之间的数据)；\n * 迭代输出有序序列。\n\n其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表一样。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。\n\n对于按照区间查找数据这个操作，跳表可以做到o(logn)的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。\n\n当然，redis 之所以用跳表来实现有序集合，还有其他原因，比如，跳表更容易代码实现。虽然跳表的实现也不简单，但比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。\n\n不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的map类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果我们想要使用跳表，必须要自己实现。\n\n\n# 内容小结\n\n今天我们学习了跳表这种数据结构。跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的"二分查找"。跳表是一种动态数据结构，支持快速地插入、删除、查找操作，时间复杂度都是o(logn).\n\n跳表的空间复杂度是o(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向于跳表。\n\n\n# d45(2020/11/19)\n\n今天要学习的是散列表中的第一部分的内容。\n\n\n# 散列思想\n\n散列表的英文叫"hash table"，我们平时也叫它"哈希表"或者"hash表"。\n\n散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就会数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。\n\n这里用一个例子来解释一下。假如我们有89名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。这89名选手的编号依次是1到89。现在我们希望编程实现这样一个功能，通过编号快速找到对应的选手信息。\n\n我们可以把这89名选手的信息放在数组里。编号为1的选手，我们放到数组中下标为1的位置；编号为2的选手，我们放到数组中下标为2的位置。以此类推，编号为k的选手放到数组中下标为k的位置。\n\n因为参赛编号跟数组下标一一对应，当我们需要查询参赛编号为x的选手的时候，我们只需要将下标为x的数组元素取出来就可以了，时间复杂度就是o(1)。这样按照编号查找选手信息。\n\n实际上，这个例子已经用到了散列的思想。在这个例子里，参赛编号是自然数，并且与数组的下标形成一一映射，所以利用数组支持根据下标随机访问的时候，时间复杂度是o(1)这一特性，就可以实现快速查找编号对应的选手的信息。\n\n假设校长说，参赛编号不能设置得这么简单，要加上年级、班级这些更详细的信息，所以我们把编号的规则稍微修改了一下，用6位数字来表示。比如051167，其中，前两位05表示年级，中间两位11表示班级，最后两位还是原来的编号1到89。这个时候我们该如何存储选手信息，才能够支持通过编号来快速查找选手信息呢？\n\n思路还是跟前面类似。尽管我们不能直接把编号作为数组下标，但我们可以截取参数编号的后两位作为数组下标，来存取选手信息数据。当通过参数编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。\n\n这就是典型的散列思想。其中，参赛选手的编号我们叫做键(key)或者关键字。我们用它来标识一个选手。我们把参数编号转化为数组下标的映射方法就叫做散列函数(或hash函数 哈希函数)，而散列函数计算得到的值就叫做散列值。\n\n\n\n通过这个例子，我们可以总结出这样的规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是o(1)的特性。我们通过散列函数把元素的键值映射为下标，然后量数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。\n\n\n# 散列函数\n\n散列函数在散列表中起着非常关键的作用。\n\n散列函数，顾名思义，它是一个函数。我们可以把它定义为hash(key)，其中key表示元素的键值，hash(key)的值表示经过散列函数计算得到的散列值。\n\n那第一个例子中，编号就是数组下标，所以hash(key)就等于key。改造后的例子，写成散列函数稍微有点复杂。下面用伪代码将它写成函数如下：\n\nint hash(string key) {\n  // 获取后两位字符\n  string lasttwochars = key.substr(length-2, length);\n  // 将后两位字符转换为整数\n  int hashvalue = convert lasttwochas to int-type;\n  return hashvalue;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n刚刚举的学校运动会的例子，散列函数比较简单，也比较容易想到。但是，如果参数选手的编号是随机生成的6位数字，又或者用的是a到z之间的字符串，该如何构造散列函数呢？我总结了三点散列函数设计的基本要求：\n\n 1. 散列函数计算得到的散列值是一个非负整数；\n 2. 如果key1 = key2，那hash(key1) == hash(key2);\n 3. 如果key1 != key2，那hash(key1) != hash(key2)。\n\n其中，第一点理解起来应该没有任何问题。因为数组下标是从0开始的，所以散列函数生成的散列值也要是非负整数。第二点也很好理解。相同的key，经过散列函数得到的散列值也应该是相同的。\n\n第三点理解起来可能会有问题。这个要求看起来合情合理，但是在真实的情况下，要想找到一个不同的key对应的散列值都不一样的散列函数，几乎是不可能的。即便是业界著名的md5、sha、crc等哈希算法，也无法完全避免这种散列冲突。\n\n所以我们几乎无法找到一个完美的无冲突的散列函数，即便能够找到，付出的时间成本、计算成本也是很大的，所以针对散列冲突问题，我们需要通过其他途径 解决。\n\n\n# 散列冲突\n\n再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法和链表法。\n\n\n# 开放寻址法\n\n开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法，线性探测。\n\n当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，知道找到为止。\n\n如下面图示来看，这里面黄色的色块表示空闲位置，橙色的色块表示已经存储了数据。\n\n\n\n从图中可以看出，散列表的大小为10，在元素x插入散列表之前，已经6个元素插入到散列表中。x经过hash算法之后，被散列到位置下标为7的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置2，于是将其插入到这个位置。\n\n在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。\n\n\n\n散列表跟数组一样，不仅支持插入、查找操作，还支持删除操作。对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。\n\n在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。\n\n我们可以将删除的元素，特殊标记为deleted。当线性探测查找的时候，遇到标记为deleted的空间，并不是停下来，而是继续往下探测。\n\n\n\n我们可能已经发现了，线性探测法其实存在很大的问题。当散列表中插入的数据越来越多的时候，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下时间复杂度是o(n)。同理，在删除和查找的时候，也有可能会线性探测整张散列表，才能找到要查找或删除的数据。\n\n对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测和双重散列。\n\n所谓二次探测，跟线性探测很像，线性探测的每次探测的步长是1，那它探测的下标序列就是hash(key)+0，hash(key)+1，hash(key)+2 .... 而二次探测探测的步长就变成了原来的"二次方"，也就是说，它探测的下标序列就是hash(key)+0，hash(key)+1^2 , hash(key)+2^2 ....\n\n所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数hash1(key)，hash2(key)，hash3(key) ....我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，知道找到空闲的存储位置。\n\n不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子来表示空位的多少。\n\n装载因子的计算公式是：\n\n散列表的装载因子 = 填入表中的元素个数/散列表的长度\n\n装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。\n\n\n# 链表法\n\n链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。我们来看下面的图示，在散列表中，每个"桶(bucket)"或者"槽(slot)"会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。\n\n\n\n当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是o(1)。当查找、删除一个元素的时候，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或删除。\n\n实际上，这两个操作的时间复杂度跟链表的长度k成正比，也就是o(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中n表示散列中数据的个数，m表示散列表中"槽"的个数。\n\n\n# 解答开篇\n\nword文档中单词拼写检查功能是如何实现的？\n\n常用的英文单词有20万个左右，假设单词的平均长度是10个字母，平均一个单词占用10个字节的内存空间，那20w英文单词大约占2mb的存储空间，就算放大10倍也就是20mb。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。\n\n当用户输入某个英文单词的时候，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。\n\n\n# 内容小结\n\n散列表来源于数组，它借助散列山水对数组这种数据结构进行扩展，利用的是数组支持按照下标随机访问元素的特性。散列表两个核心问题是散列函数设计和散列冲突解决。散列冲突有两种常用的解决方法，开放寻址法和链表法。散列函数的设计好坏决定了散列冲突的概率，也就决定散列表的性能。\n\n\n# d46(2020/11/23)\n\n今天要学习的是散列表中的第二部分的内容。\n\n通过上一节的学习，我们知道，散列表的查询效率并不能笼统地说成是o(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。\n\n在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。如果我们使用的是基于链表的冲突解决方法，那这个时候，散列表就会退化为链表，查询的时间复杂度就从o(1)急速退化为o(n)。\n\n如果散列表中有10万个数据，退化后的散列表查询的效率就下降了10万倍。更直接点说，如果之前运行100次查询只需要0.1秒，那现在就需要1万秒。这样就有可能因为查询操作消耗大量cpu或线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击(dos)的目的。这也就是散列表碰撞攻击的基本原理。\n\n今天，我们就来学习一下，如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？\n\n\n# 如何设计散列函数？\n\n散列函数设计的好坏，决定了散列表冲突的概率大小，也直接决定了散列表的性能。那什么才是好的散列函数呢？\n\n首先，散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接影响到散列表的性能。\n\n其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。\n\n实际工作中，我们还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。散列函数各式各样，我举几个常用的、简单的散列函数的设计方法，让我们有个直观的感受。\n\n第一个例子就是我们上一节的学生运动会的例子，我们通过分析参赛编号的特征，把编号中的后两位作为散列值。我们还可以用类似的散列函数处理手机号码，因为手机号码前几位重复的可能性很大，但是后面几位就比较随机，我们可以取手机号的后四位作为散列值。这种散列函数的设计方法，我们一般叫做"数据分析法"。\n\n第二个例子就是上一节的开篇思考题，如何实现word拼写检查功能。这里面的散列函数，我们就可以这样设计：将单词中每个字母的ascii码值"进位"相加，然后再跟散列表的大小求余、取模，作为散列值。\n\nhash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978\n\n\n1\n\n\n实际上，散列函数的设计方法还有很多，比如直接寻址法、平方取中法、折叠法、随机数法等。\n\n\n# 装载因子过大了怎么办？\n\n我们上一节讲到散列表的装载因子的时候说过，转载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。\n\n对于没有频繁插入和删除的静态数据集合来说，我们很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数，因为毕竟之前数据都是已知的。\n\n对于动态散列表来说，数据集合是频繁变动的，我们事先无法预估将要加入的数据个数，所以我们也无法事先申请一个足够大的散列表。随着数据慢慢加入，装载因子就会慢慢变大。当装载因子大到一定程度之后，散列冲突就会变得不可接受。这个时候，我们该如何处理呢？\n\n我们可以参照之前学习的"动态扩容"的知识点，我们在数组、栈、队列的时候是如何实现动态扩容的。\n\n针对散列表，当装载因子过大的时候，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了0.4。\n\n针对数据的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。\n\n可以看到图里这个例子。在原来的散列表中，21这个元素原来存储在下标wie0的位置，搬移到新的散列表中，存储在下标为7的位置。\n\n\n\n对于支持动态扩容的散列表，插入操作的时间复杂度是多少呢？\n\n插入一个数据，最好情况下，不需要扩容，最好时间复杂度是o(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是o(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是o(1).\n\n实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在转载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。\n\n我们前面讲到，当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。\n\n装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于1。\n\n\n# 如何避免低效的扩容？\n\n我们刚刚分析得到，大部分情况下，动态扩容的散列表插入一个数据都很快，但是在特殊情况下，当装载因子已经达到阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢，甚至会无法接受。\n\n这里有个极端的例子，如果散列表当前大小为1gb，要想扩容为原来的两倍大小，那就需要对1gb的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表，听起来就很耗时。\n\n如果我们的业务代码直接服务于用户，尽管大部分情况下，插入一个数据的操作都很快，但是，极个别非常慢的插入操作，也会让用户崩溃。这个时候，"一次性"扩容的机制就不合适了。\n\n为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。\n\n当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表中。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。\n\n\n\n这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。\n\n通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是o(1)。\n\n\n# 如何选择冲突解决方法？\n\n上一节中我们讲解了两种主要的散列冲突的解决办法，开放寻址法和链表法。这两种冲突解决办法在实际的软件开发中都非常常用。比如，java中linkedhashmap就采用了链表法解决冲突，threadlocalmap是通过线性探测的开放寻址法来解决冲突。\n\n\n# 开放寻址法\n\n开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用cpu缓存来加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没有那么容易。序列化是很常用的场景。\n\n开放寻址法又有哪些缺点呢？\n\n用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。\n\n我总结一下，当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是java中的threadlocalmap使用开放寻址法解决散列冲突的原因。\n\n\n# 链表法\n\n首先，链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。\n\n链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于1的情况。接近1时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。\n\n我们在讲解链表的时候，提及。链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对cpu缓存是不友好的，这方面对于执行效率也有一定的影响。\n\n当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小(4个字节或8个字节)，那链表中指针的内存消耗在大对象面前就可以忽略了。\n\n实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是o(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。\n\n\n\n所以，我总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。\n\n\n# 工业级散列表举例分析\n\n现在，我们就以java中的hashmap为例，来介绍工业级的散列表。\n\n\n# 初始大小\n\nhashmap默认的初始大小是16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高hashmap的性能。\n\n\n# 装载因子和动态扩容\n\n最大装载因子默认是0.75，当hashmap中元素个数超过0.75*capacity(capacity表示散列报的容量)的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。\n\n\n# 散列冲突解决办法\n\nhashmap底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响hashmap的性能。\n\n于是，在jdk1.8版本中，为了对hashmap做进一步优化，我们引入了红黑树。而当链表长度太长(默认超过8)时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高hashmap的性能。当红黑树结点个数少于8个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。\n\n\n# 散列函数\n\n散列函数的设计并不复杂，追求的是简单高效、分布均匀。如下所示：\n\nint hash(object key) {\n    int h = key.hashcode()；\n    return (h ^ (h >>> 16)) & (capicity -1); //capicity表示散列表的大小\n}\n\n\n1\n2\n3\n4\n\n\n其中，hashcode()返回的是java对象的hash code。比如string类型的对象的hashcode()就是下面：\n\npublic int hashcode() {\n  int var1 = this.hash;\n  if(var1 == 0 && this.value.length > 0) {\n    char[] var2 = this.value;\n    for(int var3 = 0; var3 < this.value.length; ++var3) {\n      var1 = 31 * var1 + var2[var3];\n    }\n    this.hash = var1;\n  }\n  return var1;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 解答开篇\n\n现在，我们来分析一下开篇的问题：如何设计一个工业级的散列函数？如果这是一道面试题或摆在我们面前的实际开发问题，我们会从哪几个方面思考呢？\n\n首先，我们会思考，什么是一个工业级的散列表？工业级的散列表应该具有哪些特性？\n\n结合已经学习过的散列知识，我们觉得应该有这样几点要求：\n\n * 支持快速地查询、插入、删除操作；\n * 内存占用合理，不能浪费过多的内存空间；\n * 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。\n\n如何实现这样一个散列表呢？根据前面讲到的知识，我们会从这三个方面来考虑设计思路：\n\n * 设计一个合适的散列函数；\n * 定义装载因子阈值，并且设计动态扩容策略；\n * 选择合适的散列冲突解决方法。\n\n\n# 内容小结\n\n今天的内容偏向于实战。主要讲解了如何设计一个工业级的散列表，以及如何应对各种异常情况，防止在极端情况下，散列表的性能退化过于严重。我们分了三部分来讲解这些内容：分别是：如何设计散列函数，如何根据转载因子动态扩容，以及如何选择散列冲突解决方法。\n\n关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，这样会尽可能地减少散列冲突，即便冲突之后，分配到每个槽内的数据也比较均匀。除此之外，散列函数的设计也不能太复杂，太复杂就会太耗时间，也会影响散列表的性能。\n\n关于散列冲突解决方法的选择，我对比了开放寻址法和链表法两种方法的优劣和适应的场景。大部分情况下，链表法更加普适。而且，我们还可以通过将链表法中的链表改造成其他动态查找数据结构，比如红黑树，来避免散列表时间复杂度退化成o(n)，抵御散列碰撞攻击。但是，对于小规模数据、装载因子不高的散列表，比较适合用开放寻址法。\n\n对于动态散列表来说，不管我们如何设计散列函数，选择什么样的散列冲突解决方法。随着数据的不断增加，散列表总会出现装载因子过高的情况。这个时候，我们就需要启动动态扩容。\n\n\n# d47(2020/11/24)\n\n今天要学习的是散列表的第三篇，重点考虑散列表和链表的结合。\n\n在链表那一节，我们讲到如何用链表来实现lru缓存淘汰算法，但是链表实现的lru缓存淘汰算法的时间复杂度是o(n)，我们可以通过散列表将这个时间复杂度降低到o(1)。\n\n在跳表那一节，我们提到redis的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表。当时我们也提到，redis有序集合不仅使用了跳表，还用到了散列表。\n\n除此之外，我们会发现在java编程语言中，linkedhashmap这样一个常用的容器，也用到了散列表和链表两种数据结构。\n\n这里我们就来讨论，散列表和链表都是如何组合起来使用的，以及为什么散列表和链表会经常放到一块使用。\n\n\n# lru缓存淘汰算法\n\n在链表那一节中，我们提到，借助于散列表，我们可以把lru缓存淘汰算法的时间复杂度降低为o(1)。\n\nlru也就是least recently used，也就是最近最少使用，选择最近最久未使用的页面予以淘汰。\n\n下面是我们之前利用链表来实现lru缓存淘汰算法的。\n\n我们需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据，我们就直接将链表头部的结点删除。\n\n当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放入到链表的头部；如果找到了，我们就把它移动到链表的头部。因为查找数据需要遍历链表，所以单纯用链表实现的lru缓存淘汰算法的时间复杂度很高，是o(n)。\n\n实际上，我们总结一下，一个缓存(cache)系统主要包含下面这几个操作：\n\n * 往缓存中添加一个数据；\n * 从缓存中删除一个数据；\n * 在缓存中查找一个数据。\n\n这三个操作都涉及"查找"操作，如果单纯地采用链表的话，时间复杂度只能是o(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到o(1)。\n\n我们使用双向链表存储数据，链表中的每个结点处理存储数据(data)、前驱指针(prev)、后继指针(next)之外，还新增了一个特殊的字段hnext。这个hnext有什么用呢？\n\n因为我们的散列表示通过链表法来解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，hnext指针是为了将结点串在散列表的拉链中。\n\n了解了这个散列表和双向链表的组合存储结构之后，我们再来看，前面讲到的缓存的三个操作，是如何做到时间复杂度是o(1)的？\n\n 1. 每个页面的hash后的值确定的 数组下标位置，后面用链表结点来存。\n 2. 不同的页面hash后可以有相同的值，相同的数组下标后面用，各个链表结点连接起来。其中各个结点连接起来的指针就是这里的hnext。\n 3. 各个散列表槽位中的各个链表结点之间，还有一个prev和next指针，这个是用来维护的是页面的访问的时间的。\n 4. 这个双向链表中，第一个结点是访问最早的时间。最后一个结点是访问最后的时间。\n 5. 要访问新页面的时候，先计算该页面的hash值，到对应的槽位（数组下标后面一串链表结点中），去找一找是否有已有的页面。\n 6. 如果没有找到，那么就在刚才找的数组下标后面的一串链表结点末尾增加这个结点，并且让双向链表的末尾结点指向它。\n 7. 如果这个时候，缓存要满了，那就先删除双向链表的一个结点，再去增加双向链表的末尾结点。\n 8. 如果找到了，那么就调整双向链表中这个结点的顺序，这个结点塞到双向链表的最后。\n\n首先，我们来看如何查找一个数据。我们前面讲过，散列表中查找数据的时间复杂度接近o(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。\n\n其次，我们来看如何删除一个数据。我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在o(1)时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针o(1)时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要o(1)的时间复杂度。\n\n最后，我们来看如何添加一个数据。添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。\n\n这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在o(1)的时间复杂度内完成。所以，这三个操作的时间复杂度都是o(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持lru缓存淘汰算法的缓存系统原型。\n\n\n# redis有序集合\n\n在跳表那一节，讲到有序集合的操作时，我们稍稍做了些简化。实际上，在有序集合中，每个成员对象有两个重要的属性，key(键值)和score(分值)。我们不仅会通过score来查找数据，还会通过key来查找数据。\n\n举个例子，比如用户积分排行榜有这样一个功能：我们可以通过用户的id来查找积分信息，也可以通过积分区间来查找用户id或姓名信息。这里包含id、姓名和积分的用户信息，就是成员对象，用户id就是key，积分就是score。\n\n所以，如果我们细化一下redis有序集合的操作，那就是下面这样：\n\n * 添加一个成员对象；\n * 按照键值来删除一个成员对象；\n * 按照键值来查找一个成员对象；\n * 按照分值区间来查找数据，比如积分在[100,356]之间的成员对象；\n * 按照分值从小到大排序成员变量；\n\n如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢，解决方法与lru缓存淘汰算法的聚集方法类似。我们可以再按照键值构建一个散列表，这样按照key来删除、查找一个成员对象的时间复杂度就变成了o(1)。同时，借助跳表结构，其他操作也非常高效。\n\n实际上，redis有序集合的操作还有另外一类，也就是查找成员对象的排名(rank)或者根据排名区间查找成员对象。\n\n\n# java linkedhashmap\n\n前面我们学习了两个散列表和链表结合的例子，这里再看一个java中的linkedhashmap这种容器。\n\n在java中的hashmap底层就是通过散列表这种数据结构实现的。而linkedhashmap前面比hashmap多了一个 "linked"。\n\n实际上，linkedhashmap并没有这么简单，其中的"linked"也并不仅仅代表它是通过链表法解决散列冲突的。\n\nlinedhashmap也是通过散列表和链表组合在一起实现的。实际上，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。\n\n\n# 按照插入的顺序打印\n\n看下面的一段hashmap的代码，我们依次插入(3,11)，(1,12), (5,23), (2,22)。接下里打印出来，发现打印出来的顺序就是3,1,5,2。\n\nhashmap<integer, integer> m = new linkedhashmap<>();\nm.put(3, 11);\nm.put(1, 12);\nm.put(5, 23);\nm.put(2, 22);\n\nfor (map.entry e : m.entryset()) {\n  system.out.println(e.getkey());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n这个问题，实际上是比较奇怪的，按照我们对hashmap的字面意思，散列表中的数据不是经过散列函数打乱之后无规则存储的么，这里怎么就按照数据的插入顺序来遍历打印了呢？\n\n实际上，我们已经可以猜测出linkedhashmap也是通过散列表和链表组合在一起实现。\n\n实际上，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。\n\n\n# 按照访问顺序来遍历\n\n看如下的hashmap的代码\n\n// 10是初始大小，0.75是装载因子，true是表示按照访问时间排序\nhashmap<integer, integer> m = new linkedhashmap<>(10, 0.75f, true);\nm.put(3, 11);\nm.put(1, 12);\nm.put(5, 23);\nm.put(2, 22);\n\nm.put(3, 26);\nm.get(5);\n\nfor (map.entry e![hash_table_10](d:\\tupian\\hash_table_10.jpg) : m.entryset()) {\n  system.out.println(e.getkey());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n我们先是不断的put了3,1,5,2 四个 key/value对。然后再put 了 (3,26)，接着再get(5)。\n\n这段代码打印的结果是1，2，3，5。接下来我们具体分析一下，为什么这段代码会按照这样的顺序来打印。\n\n每次调用put()函数，往linkedhashmap中添加数据的时候，都会将数据添加到链表的尾部，所以，在前四个从操作完成之后，链表中的数据是下面这样：\n\n\n\n在第8行代码中，再次将键值为3的数据放入到linkedhashmap的时候，会先查找这个键值是否已经有了，然后，再将已经存在的(3,11)删除，并且将新的(3,26)放到链表的尾部。所以，这个时候链表中的数据就是下面这样：\n\n\n\n当第9行代码访问到key为5的数据的时候，我们将被访问的数据移动到链表的尾部。所以，第8行代码之后，链表中的数据是下面这样：\n\n\n\n所以，最后打印出来的数据是1,2,3,5。从上面的分析，我可以返现，按照访问时间排序的linkedhashmap本身就是一个支持lru缓存淘汰策略的缓存系统。\n\n现在总结一下，实际上，linkedhashmap是通过双向链表和散列表这两种数据结构组合实现的。linkedhashmap中的"linked"实际上是指的是双向链表，并非指用链表法解决散列冲突。\n\n\n# 解答开篇&内容小结\n\n为什么散列表和链表经常一块使用呢？\n\n散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就是说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。\n\n因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按照顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表(或者跳表)结合在一起使用。\n\n用链表或跳表的数据结构，来实现对散列表中的各个数据有序的排序。\n\n\n# d48(2020/11/25)\n\n今天我们要学习的哈希算法的第一篇。"如何防止数据库中的用户信息被脱库？"\n\n\n# 什么是哈希算法？\n\n哈希算法的定义和原理非常简单，将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。\n\n但是，要想设计一个优秀的哈希算法并不容易，可以参照如下几点要求：\n\n * 从哈希值不能反向推导出原始数据(所以哈希算法也叫单向哈希算法)；\n * 对输入数据非常敏感，哪怕原始数据只修改了一个bit，最后得到的哈希值也大不相同；\n * 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；\n * 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。\n\n这里拿md5这种哈希算法来具体说明一下。\n\n我们分别对"今天我来讲哈希算法"和"jiajia"这两个文本，计算md5哈希值，得到两串看起来毫无规律的字符串(md5的哈希值是128位的bit长度，为了方便表示，我们把它们转化成了16进制编码)。可以看出来，无论要哈希的文本有多长、多短，通过md5哈希之后，得到的哈希值的长度都是相同的，而且得到的哈希值看起来像一堆随机数，完全没有规律。\n\n我们再来看两个非常相似的文本，"我今天讲哈希算法!"和"我今天讲哈希算法"。这两个文本只有一个感叹号的区别。如果用md5的哈希算法分别计算它们的哈希值，我们会发现，尽管只有一字之差，得到的哈希值也是完全不同的。\n\n我们在前面也讲解过，通过哈希算法得到的哈希值，很难反向推导出原始数据。比如上面的例子中，我们很难通过哈希值反推出对应的文本。\n\n哈希算法要处理的文本可能是各种各样的。比如，对于非常长的文本，如果哈希算法的计算时间很长，那就只能停留在理论研究的层面，很难应用到实际的软件开发中。\n\n哈希算法的应用非常多，最常见的有七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。\n\n\n# 应用一：安全加密\n\n说到哈希算法的应用，最先想到的应该就是安全加密。最常用于加密的哈希算法是md5和sha。\n\n除了这两个之外，当然还有很多其他加密算法，比如des、aes。\n\n前面讲解的哈希算法四点要求，对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。\n\n第一点很好理解，加密的目的就是防止原始数据泄露，所以很难通过哈希值反向推导原始数据，这是一个最基本的要求。所以我们着重看下第二点。实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。\n\n这里就基于组合数学中一个非常基础的理论，鸽巢原理(也叫抽屉原理)。这个原理本身很简单，它是说，如果有10个鸽巢，有11个鸽子，那肯定有1个鸽巢中的鸽子数量多于1个，换句话说就是，肯定有2只鸽子在1个鸽巢内。\n\n有了鸽巢原理的铺垫之后，我们再来看，为什么哈希算法无法做到零冲突？\n\n我们知道，哈希算法产生的哈希值的长度是固定且有限的。比如上面举的md5的例子，哈希值是固定的128位二进制串，能表示的数据是有限的，最多能表示2^128个数据，而我们要哈希的数据是无穷的。基于鸽巢原理，如果我们对2^128+1个数据求哈希值，就必须会存在哈希值相同的情况。这里我们应该可以想到，一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。\n\n不过，即便哈希算法存在散列冲突的情况，但是因为哈希值的范围很大，冲突的概率极低，所以相对来说还是很难破解的。像md5，有2^128个不同的哈希值，这个数据已经是一个天文数字。所以，即便哈希算法存在冲突，但是在有限的时间和资源下，哈希算还是很难被破解的。\n\n除此之外，没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长。比如sha-256比sha-1要更复杂、更安全，相应的计算时间就会比较长。密码学界也一致致力于找到一种快速并且很难被破解的哈希算法。我们在实际的开发过程中，也需要权衡破解难度和计算时间，来决定究竟使用哪种加密算法。\n\n\n# 应用二：唯一标识\n\n我来先举一个例子。如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息(比如图片名称)来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那我们该如何搜索呢？\n\n我们知道，任何文件在计算中都可以表示成二进制码串，所以，比较笨的方法就是，拿要查找的图片的二进制码串与图库中的所有图片的二进制码串一一比对。如果相同，则说明图片在图库中存在。但是，每个图片小则几十kb、大则几mb，转化成二进制是一个非常长的串，比对起来非常耗时。有没有比较快的方法呢？\n\n我们可以给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取100个字节，从中间取100个字节，从最后再取100个字节，然后将这300个字节放到一块，通过哈希算法(比如md5)，得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。\n\n如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。\n\n如果不存在，那就说明这个图片不再图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。\n\n\n# 应用三：数据校验\n\n电驴这样的bt下载软件的原理是基于p2p协议的。我们从多个机器上并行下载一个2gb的电影，这个电影文件可能会被分割成很多文件块(比如可用分成100块，每块大约20mb)。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。\n\n我们知道，网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或是下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？\n\n具体的bt协议很复杂，校验方法也有很多，下面是其中的一种思路。\n\n我们通过哈希算法，对100个文件块分别去哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的额文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或被篡改了，需要再重新从其他宿主机上下载该文件块。\n\n\n# 应用四：散列函数\n\n散列函数也是哈希算法的一种应用。\n\n前面提及到，散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法来解决。\n\n不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关系。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。\n\n\n# 内容小结\n\n今天主要讲解了哈希算法的四个应用场景。\n\n第一个应用是唯一标识，哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。\n\n第二个应用是用于校验数据的完整性和正确性。\n\n第三个应用是安全加密，我们讲到任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。\n\n第四个应用是散列函数，在散列表中，对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率。\n\n\n# d49(2020/11/26)\n\n上一节，我们讲了哈希算法的四个应用，它们分别是：安全加密、数据校验、唯一标识、散列函数。今天，我们再来看剩余三种应用：负载均衡、数据分片、分布式存储。\n\n这三个应用都跟分布式系统有关。今天就是来研究学习下，哈希算法是如何解决这些分布式问题的。\n\n\n# 应用五：负载均衡\n\n我们知道，负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞(session sticky)的负载均衡算法呢？也就是说，我们需要在同一个客户端上，上一次会话中的所有请求都路由到同一个服务器上。\n\n最直接的方法就是，维护一张映射关系表，这张表的内容是客户端ip地址或会话id与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：\n\n * 如果客户端很多，映射表可能会很大，比较浪费内存空间；\n * 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大。\n\n如果借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端ip地址或会话id计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。这样，我们就可以把同一个ip过来的所有请求，都路由到同一个后端服务器上。\n\n\n# 应用六：数据分片\n\n哈希算法还可以用于数据的分片。这里有两个例子。\n\n\n# 如何统计"搜索关键词"出现的次数\n\n假如我们有1t的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？\n\n我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。\n\n针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用n台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号。\n\n这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。\n\n实际上，这里的处理过程也是mapreduce的基本设计思想。\n\n\n# 如何快速判断图片是否在图库中\n\n如何快速判断图片是否在图库中？上一节我们讲过这个例子，当时我们介绍了一种方法，就是给每个图片取唯一标识(或者信息摘要)，然后构建散列表。\n\n假设现在我们的图库中有1亿图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存是有限的，而1亿张图片构建散列表显然远远超过了单台机器的内存上限。\n\n我们同样可以对数据进行分片，然后采用多机处理。我们准备n台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数n 求余取模，得到的值就是要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。\n\n当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数n求余取模。假设得到的值是k，那就去编号k的机器构建的散列表中查找。\n\n现在，我们来估算一下，给这个1亿图片构建散列表大约需要多少台机器。\n\n散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设我们通过md5来计算哈希值，那长度就是128比特，也就是16字节。文件路径长度的上限是256字节，我们可以假设平均长度是128字节。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用8字节。所以，散列表中每个数据单元就占用 152字节。\n\n假设一台机器的内存大小为2gb，散列表的装载因子为0.75，那一台机器可以给大约1000万(2gb*0.75/152)张图片构建散列表。所以，如果要对1亿张图片构建索引，需要大约十几台机器。在工程中，这种估算还是很重要的，能让我们实现对需要投入的资源、资金有个大概的了解，能更好地平均解决方案的可行性。\n\n实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、cpu等资源的限制。\n\n\n# 应用七：分布式存储\n\n现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。\n\n该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。\n\n但是，如果数据增多，原来的10个机器已经无法承受了，我们就需要扩容了，比如扩到11个机器，这个时候麻烦就来了。因为，这里并不是简单地加个机器就可以了。\n\n原来的数据是通过与10来取模的。比如13这个数据，存储在编号为3这台机器上。但是新加了一台机器中，我们对数据按照11取模，原来13这个数据就被分配到2号这台机器上了。\n\n\n\n因此，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库。\n\n所以，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要可以用到了。\n\n假设我们有k个机器，数据的哈希值的范围是[0,max]。我们将整个范围划分成m个小区间(m远大于k)，每个机器负责m/k个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。\n\n一致性哈希算法的基本思想就是这样的。除此之外，它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来。\n\n\n# 解答开篇&内容小结\n\n今天我们讲解了三种哈希算法在分布式系统中的应用，它们分别是：复杂均衡、数据分片、分布式存储。\n\n在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。\n\n\n# d50(2020/11/28)(二叉树)\n\n今天学习二叉树基础的第一篇。\n\n前面我们讲的都是线性表结构，栈、队列等等。今天我们讲一种非线性表结构，树。树这种数据结构比线性表的数据结构要复杂得多，内容也比较多，会分四节来讲解。\n\n第一节，我们将讲解树和二叉树；第二节，我们将讲解二叉查找树；第三节，平衡二叉查找树、红黑树；第四节，递归树。\n\n今天提出的问题是：二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？\n\n\n# 树(tree)\n\n下面图示中的是一些树，观察这些树都有什么特征？\n\n\n\n树这种数据结构很像我们现实生活中的"树"，这里面每个元素我们叫做"节点"；用来连接相邻节点之间的关系，我们叫做"父子关系"。\n\n在下面的这幅画中，a节点就是b节点的父节点，b节点是a节点的子节点。b、c、d这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫做根节点，也就是图中的节点e。我们把没有子节点的节点叫做叶子节点或叶节点，比如图中的g、h、i、j、k、l都是叶子节点。\n\n\n\n除此之外，关于"树"，还有三个比较相似的概念：高度(height)、深度(depth)、层(level)。它们的定义是这样的：\n\n\n\n这三个概念的定义比较容易混淆，描述起来也比较空洞。从下图的图示中，来观察这些概念的定义。\n\n\n\n记住这几个概念，这里还有一个小窍门，就是类比"高度"、"深度"、"层"这几个名词在生活中的含义。\n\n在我们的生活中，"高度"这个概念，其实就是从下往上度量，比如我们要度量第10层楼的高度、第13层楼的高度，起点就是地面。所以，树这种数据结构的高度也是一样，从最底层开始计数，并且计数的起点是0。\n\n"深度"这个概念在生活中是从上往下度量的，比如水中鱼的深度，是从水平面开始度量的。所以，树这种数据结构的速度也是类似的，从根结点开始度量，并且计数起点也是0。\n\n"层数"跟深度的计算类似，不过，计数起点是1，也就是说根节点位于第1层。\n\n\n# 二叉树\n\n树结构多种多样，我们最常用的还是二叉树。\n\n二叉树，顾名思义，每个节点最多有两个"叉"，也就是两个子结节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。下面画的都是几个都是二叉树。\n\n\n\n这个图里面，有两个比较特殊的二叉树，分别是编号2和编号3这两个。\n\n其中，编号2的二叉树中，叶子节点全部在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做满二叉树。\n\n编号3的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做完全二叉树。\n\n满二叉树很好理解，也很好识别，但是完全二叉树，有的人就可能分不清了。下面图示中有几个完全二叉树和非完全二叉树的例子，可以对比看。\n\n\n\n你可能会说，满二叉树的特征非常明显，我们把它单独拎出来将，这个可以理解。但是完全二叉树的特征不怎么明显，单从长相上来看，完全二叉树并没有特别特殊的地方啊，更像是"芸芸众树"中的一种。\n\n那我们为什么还要特意把它拎出来讲呢？为什么偏偏把最后一层的叶子节点靠左排列的叫完全二叉树？如果靠右排序就不能叫完全二叉树了吗？这个定义的由来或说目的在哪里？\n\n要理解完全二叉树定义的由来，我们需要先了解，如何表示(或者存储)一棵二叉树？\n\n想要存储一棵二叉树，我们有两种方法，一种是基于指针或引用的二叉链式存储法，一种是基于数组的顺序存储法。\n\n我们先来看比较简单、直观的链式存储法。从图中你应该可以很清楚地看到，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。\n\n\n\n我们再来看，基于数组的顺序存储法。我们把根节点存储在下标i=1的位置，那左子节点存储在下标2*i =2的位置，右子节点存储在2*i +1=3的位置。以此类推，b节点的左子节点存储在2* i = 2 * 2 =4的位置，右子节点存储子啊2 * i + 1 = 2 * 2 +1 = 5的位置。\n\n\n\n我们来总结一下，如果节点x存储在数组中下标为i的位置，下标为2 * i的位置存储的就是左子节点，下标为2 * i +1 的位置存储的就是右子节点。反过来，下标为i/2的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置(一般情况下，为了方便计算子节点，根节点会存储在下标为1的位置)，这样就可以通过下标计算，把整棵树都串起来。\n\n上面的一棵完全二叉树的例子中，仅仅"浪费"了一个下标为0的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。可以看下面的例子。\n\n\n\n所以，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。\n\n当我们讲到堆和堆排序的时候，就会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。\n\n\n# 二叉树的遍历\n\n前面讲解了二叉树的基本定义和存储方法，现在来看二叉树中非常重要的操作，二叉树的遍历。\n\n如何将所有节点都遍历打印出来呢？经典的方法有三种，前序遍历、中序遍历和后序遍历。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。\n\n * 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。\n * 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。\n * 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。\n\n\n\n实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。\n\n写递归代码的关键，就是看能不能写出递推公式，而写递推公式的关键就是，如果要解决问题a，就假设子问题b、c已经解决，然后再来看如何利用b、c来解决a。所以，我们可以把前、中、后序遍历的递推公式都写出来。\n\n前序遍历的递推公式：\npreorder(r) = print r->preorder(r->left)->preorder(r->right)\n\n中序遍历的递推公式：\ninorder(r) = inorder(r->left)->print r->inorder(r->right)\n\n后序遍历的递推公式：\npostorder(r) = postorder(r->left)->postorder(r->right)->print r\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n有了递推公式，代码就可以写了。这三种遍历方式的代码，可以见如下的。\n\nvoid preorder(node* root) {\n  if (root == null) return;\n  print root // 此处为伪代码，表示打印root节点\n  preorder(root->left);\n  preorder(root->right);\n}\n\nvoid inorder(node* root) {\n  if (root == null) return;\n  inorder(root->left);\n  print root // 此处为伪代码，表示打印root节点\n  inorder(root->right);\n}\n\nvoid postorder(node* root) {\n  if (root == null) return;\n  postorder(root->left);\n  postorder(root->right);\n  print root // 此处为伪代码，表示打印root节点\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n二叉树的前、中、后序遍历的递归实现如上，二叉树遍历的时间复杂度是多少？\n\n从前面画的前、中、后序遍历的顺序图，可以看出来，每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数n成正比，也就是说二叉树遍历的时间复杂度是o(n)。\n\n\n# 解答开篇&内容小结\n\n今天，我们讲解了一种非线性表数据结构，树。关于苏，有几个比较常用的概念需要掌握，那就是：根节点、叶子节点、父节点、子节点、兄弟节点，还有节点的高度、深度、层数，以及树的高度。\n\n我们平时最常用的树就是二叉树。二叉树的每个节点最多有两个子结点，分别是左子节点和右子节点。二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。满二叉树又是完全二叉树的一种特殊情况。\n\n二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。除此之外，二叉树里非常重要的操作就是前、中、后序遍历操作，遍历的时间复杂度是o(n)，我们需要理解并能用递归代码来实现。\n\n\n# d51(2020/11/29) (二叉查找树)\n\n上一节我们学习了树、二叉树以及二叉树的遍历，今天我们再来学习一种特殊的二叉树，二叉查找树。二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。\n\n我们之前说过，散列表也是支持这些操作的，并且散列表的这些操作比二叉查找树更高效，时间复杂度是o(1)。既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？\n\n\n# 二叉查找树\n\n二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。\n\n这些都依赖于二叉查找树的特殊结构。二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。\n\n\n\n下面来看下二叉查找树的快速查找、插入、删除操作是如何实现的。\n\n\n# 二叉查找树的查找操作\n\n首先，我们看如何在二叉查找树中查找一个节点。我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。\n\n\n\n查看下如下的代码。\n\npublic class binarysearchtree {\n  private node tree;\n\n  public node find(int data) {\n    node p = tree;\n    while (p != null) {\n      if (data < p.data) p = p.left;\n      else if (data > p.data) p = p.right;\n      else return p;\n    }\n    return null;\n  }\n\n  public static class node {\n    private int data;\n    private node left;\n    private node right;\n\n    public node(int data) {\n      this.data = data;\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 二叉查找树的插入操作\n\n二叉查找树的插入过程有点类似查找操作。新插入的数据一般是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。\n\n如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。\n\n\n\n具体的代码如下：\n\npublic void insert(int data) {\n  if (tree == null) {\n    tree = new node(data);\n    return;\n  }\n\n  node p = tree;\n  while (p != null) {\n    if (data > p.data) {\n      if (p.right == null) {\n        p.right = new node(data);\n        return;\n      }\n      p = p.right;\n    } else { // data < p.data\n      if (p.left == null) {\n        p.left = new node(data);\n        return;\n      }\n      p = p.left;\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n# 二叉查找树的删除操作\n\n二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了。针对要删除的节点的子节点个数的不同，我们需要分三种情况来处理。\n\n第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除的节点的指针置为null。比如图中的删除节点55。\n\n第二种情况是，如果要删除的节点只有一个子节点(只有左子节点或右子节点)，我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点13。\n\n第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除这个最小节点，因为最小节点肯定没有左子节点(如果有左子节点，那就不是最小节点了)，所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点18。\n\n\n\n如下面的删除代码的示例：\n\npublic void delete(int data) {\n  node p = tree; // p指向要删除的节点，初始化指向根节点\n  node pp = null; // pp记录的是p的父节点\n  while (p != null && p.data != data) {\n    pp = p;\n    if (data > p.data) p = p.right;\n    else p = p.left;\n  }\n  if (p == null) return; // 没有找到\n\n  // 要删除的节点有两个子节点\n  if (p.left != null && p.right != null) { // 查找右子树中最小节点\n    node minp = p.right;\n    node minpp = p; // minpp表示minp的父节点\n    while (minp.left != null) {\n      minpp = minp;\n      minp = minp.left;\n    }\n    p.data = minp.data; // 将minp的数据替换到p中\n    p = minp; // 下面就变成了删除minp了\n    pp = minpp;\n  }\n\n  // 删除节点是叶子节点或者仅有一个子节点\n  node child; // p的子节点\n  if (p.left != null) child = p.left;\n  else if (p.right != null) child = p.right;\n  else child = null;\n\n  if (pp == null) tree = child; // 删除的是根节点\n  else if (pp.left == p) pp.left = child;\n  else pp.right = child;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n实际上，关于二叉查找树的删除操作，还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为"已删除"，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。而且，这种处理方法也并没有增加插入、查找操作代码实现的难度。\n\n\n# 二叉查找树的其他操作\n\n除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。\n\n二叉查找树除了支持上面几个操作之外，还有一个重要的特性，就是中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是o(n)，非常高效。因此，二叉查找树也叫做二叉排序树。\n\n\n# 支持重复数据的二叉查找树\n\n前面讲二叉查找树的时候，我们默认树中节点存储的都是数字。很多时候，在实际的软件开发中，我们在二叉查找树中存储的，是一个包含很多字段的对象。我们利用对象的某个字段作为键值(key)来构建二叉查找树。我们把对象中的其他字段叫做卫星数据。\n\n前面我们讲的二叉查找树的操作，针对的都是不存在键值相同的情况。那如果存储的两个对象键值相同，这种情况该怎么处理呢？这里有两种解决方法。\n\n第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因为我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。\n\n第二种方法比较不好理解，不过更加优雅。\n\n每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。\n\n\n\n当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。\n\n\n\n对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。\n\n\n\n\n# 二叉查找树的时间复杂度分析\n\n上面介绍了二叉查找树的常用操作的实现方式。现在，我们来分析一下，二叉查找树的插入、删除、查找操作的时间复杂度。\n\n实际上，二叉查找树的形态各式各样。比如下面图中，对于同一组数据，我们构造了三种二叉查找树。它们的查找、插入、删除操作的执行效率都不一样的。图中第一种二叉查找树，根节点的左右子树极度不平衡，已经退化成了链表，所以查找的时间复杂度就变成了o(n)。\n\n\n\n我们刚才其实分析了一种最糟糕的情况，我们现在来分析一个最理想的情况，二叉查找树是一棵完全二叉树(或满二叉树)。这个时候，插入、删除、查找的时间复杂度是多少呢？\n\n从前面的例子、图，还有代码来看，不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是o(height)。既然这样，现在问题就转变成另外一个了，也就是，如何求一棵包含n个节点的完全二叉树的高度？\n\n树的高度就等于最大层数减1，为了方便计算，我们转换成层来表示。从图中可以看出，包含n个节点的完全二叉树中，第一层包含1个节点，第二层中包含2个节点，第三层中包含4个节点，以此类推，下面一层节点个数是上一层的2倍，第k层包含的节点个数就是2^(k-1)。\n\n不过，对于完全二叉树来说，最后一层的节点个数有点不遵守上面的规律了。它包含的节点个数在1个到2^(l-1)个之间(我们假设最大层数是l)。如果我们把每一层的节点个数加起来就是总的节点个数n。也就是说，如果节点的个数是n，那么n满足这样一个关系：\n\nn >= 1+2+4+8+...+2^(l-2)+1\nn <= 1+2+4+8+...+2^(l-2)+2^(l-1)\n\n\n1\n2\n\n\n借助等比数列的求和公式，我们可以计算出，l的范围是[$log_2 (n+1)$, $log_2n +1$]。完全二叉树的层数小于等于$log_2 n +1$，也就是说，完全二叉树的高度小于等于$log_2 n$。\n\n显然，极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。我们需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树，这就是我们下一节可要详细讲的，一种特殊的二叉查找树，平衡二叉查找树。平衡二叉查找树的高度接近logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是o(logn)。\n\n\n# 解答开篇\n\n我们在散列表那节讲过，散列表的插入、删除、查找操作的时间复杂度可以做到常量级的o(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是o(logn)，相对散列表，好像并没有什么优势，那我们为什么还要使用二叉查找树呢？\n\n我认为有下面几个原因：\n\n第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在o(n)的时间复杂度内，输出有序的数据序列。\n\n第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在o(logn)。\n\n第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比logn小，所以实际的查找速度可能不一定比o(logn)快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。\n\n第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。\n\n最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。\n\n综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。\n\n\n# 内容小结\n\n今天学习了一种特殊的二叉树，二叉查找树。它支持快速地查找、插入、删除操作。\n\n二叉查找树中，每个节点的值都大于左子树节点的值，小于右子树节点的值。不过，这只是针对没有重复数据的情况。对于存在重复数据的二叉查找树，这里介绍了两种构建方法，一种是让每个节点存储多个值相同的数据；另一种是，每个节点中存储一个数据。针对这种情况，我们只需要稍加改造原来的插入、删除、查找操作即可。\n\n在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是o(n)和o(logn)，分别对应二叉树退化成链表的情况和完全二叉树。\n\n为了避免时间复杂度的退化，针对二叉查找树，我们又涉及了一种更加复杂的树，平衡二叉查找树，时间复杂度可以做到稳定的o(logn)。\n\n\n# d52(2020/12/01) 红黑树\n\n今天主要学习的是红黑树的第一篇。\n\n上面两节，我们依次讲了树、二叉树、二叉查找树。二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是o(logn)。\n\n不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于$log_2n$的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到o(n)。上一节所说，要解决这个复杂度退化的问题，我们需要设计一种平衡二叉查找树，也就是今天要讲的这种数据结构。\n\n很多书籍中，但凡讲到平衡二叉查找树，就会拿红黑树作为例子。我们在工程中，很多用到平衡二叉查找树的地方都会用红黑树。为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？\n\n\n# 什么是"平衡二叉查找树"\n\n平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于1。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。\n\n\n\n平衡二叉查找树不仅满足上面平衡二叉树的定义，还满足二叉查找树的特点。最先被发明的平衡二叉查找树是avl树，它严格符合我们刚讲的平衡二叉查找树的定义，即任何节点的左右子树高度相差不超过1，是一种高度平衡的二叉查找树。\n\n但是很多平衡二叉查找树其实并没有严格符合上面的定义(树中任意一个节点的左右子树的高度相差不能大于1)，比如我们下面要讲的红黑树，它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。\n\n我们学习数据结构和算法是为了应用到实际的开发中的，所以，我觉得没有必要去死抠定义。对于平衡二叉查找树这个概念，我觉得我们要从这个数据结构的由来，去理解"平衡"的意思。\n\n发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。\n\n所以，平衡二叉查找树中"平衡"的意思，其实就是让整棵树左右看起来比较"对称"、比较"平衡"，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。\n\n所以，如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比$log_2 n$大很多(比如树的高度仍然是对数量级的)，尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。\n\n\n# 如何定义一棵"红黑树"\n\n平衡二叉查找树其实有很多，比如，splay tree(伸展树)、treap(树堆)等，但是我们提到平衡二叉查找树，听到的基本都是红黑树。它的出镜率甚至要高于"平衡二叉查找树"这几个字，有时候，我们甚至默认平衡二叉查找树就是红黑树。\n\n红黑树，简称为r-b tree。它是一种不严格的平衡二叉查找树，前面也提及到了，它的定义是不严格符合平衡二叉查找树的定义的。\n\n顾名思义，红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：\n\n * 根节点是黑色的；\n * 每个叶子节点都是黑色的空节点(nil)，也就说，叶子节点不存储数据；\n * 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；\n * 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；\n\n这里的第二条要求"叶子节点都是黑色的空节点"，稍微有些奇怪，它主要是为了简化红黑树的代码实现而设置的，下一节我们讲红黑树的实现的时候会江街道。这节我们暂时不考虑这一点，所以，在画图和讲解的时候，我将黑色的、空的叶子节点都省略掉了。\n\n根据上面的定义，画出了两个红黑树的图例，可以参照对照来看。\n\n\n\n\n# 为什么说红黑树是"近似平衡"的？\n\n我们前面也提到，平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。所以，"平衡"的意思可以等价为性能不退化。"近似平衡"就等价为性能不会退化得太严重。\n\n上一节讲解过，二叉查找树很多操作的性能都跟树的高度成正比。一棵极其平衡的二叉树(满二叉树或完全二叉树)的高度大约是$log_2 n$ ，所以如果要证明红黑树是近似平衡的，我们只需要分析，红黑树的高度是否比较稳定地趋近$log_2 n$就好了。\n\n红黑树的高度不是很好分析。\n\n首先，我们来看，如果我们将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢？\n\n红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点(父节点的父节点)作为父节点。所以，之前的二叉树就变成了四叉树。\n\n\n\n前面红黑树的定义里有这么一条：从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。\n\n上一节我们提及，完全二叉树的高度近似$log_2 n$，这里的四叉"黑树"的高度要低于完全二叉树，所以去掉红色节点的"黑树"的高度也不会超过$log_2 n$。\n\n我们现在知道只包含黑色节点的"黑树"的高度，那我们现在把红色节点加回去，高度会变成多少呢？\n\n从上面我们画的红黑树的例子和定义看，在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过$log_2n$ ,所以加入红色节点之后，最长路径不会超过2$log_2 n $，也就是说，红黑树的高度近似2$log_2 n $。\n\n所以，红黑树的高度只比高度平衡的avl树的高度($log_2 n$)仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。\n\n\n# 解答开篇\n\n我们刚刚提到了很多平衡二叉查找树，现在我们来看下，为什么在工程中大家都喜欢用红黑树这种平衡二叉查找树。\n\n前面提到的treap、splay tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单词操作时间非常敏感的场景来说，它们并不适用。\n\navl树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利也有弊，avl树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用avl树的代价就有点高了。\n\n红黑树只是做了近似平衡，并不严格的平衡，所以在维护平衡的成本上，要比avl树要低。\n\n所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。\n\n\n# 内容小结\n\n红黑树很难，的确，它算是最难掌握的一种数据结构。其实红黑树最难的地方是它的实现，今天还没有涉及。\n\n不过呢，我们认为，其实我们不应该把学习的侧重点，放到它的实现上。红黑树，究竟要掌握哪些东西呢？\n\n我们学习数据结构和算法，要学习它的由来、特性、适用的场景以及它能解决的问题。对于红黑树，也不例外。如果我们能搞懂这几个问题，其实就已经足够了。\n\n红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似$log_2 n$，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是o($logn$)。\n\n因为红黑树是一种性能稳定的二叉查找树，所以，在工程上，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现其阿里比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向于用跳表来替代它。\n\n\n# 数据结构整理\n\n 1. 数组\n    \n    定义：连续的内存空间，支持按下标随机访问o(1)，插入和删除的时候，可能会涉及到数据的搬移，时间复杂度是o(n)。\n    \n    适用场景：数据规模较小，不经常变动的场景。\n    \n    缺点：对于内存连续性要求高，插入删除操作效率低。\n\n 2. 链表\n    \n    定义：查询效率不高o(n)，插入和删除效率o(1)，并且内存申请可以不连续。\n    \n    适用场景：插入和删除多于查询操作。顺序访问数据，数据维护比较频繁的场景。\n    \n    缺点：随机查找效率低，实际上删除之前先要查找，所以实际删除效率也不高。\n\n 3. 散列表\n    \n    定义：利用数组和链表两个基本数据结构设计了一个高效的动态数据结构。利用了数组的随机访问特性，用于满足根据某个属性来随机访问元素。基于key查找效率很高 o(1)。同时借助链表进行散列冲突解决的方法，删除和插入操作效率也可以接近o(1)。\n    \n    适用场景：海量数据随机访问、防止重复、缓存等。\n    \n    缺点：需要设计合理的散列函数，并且要考虑散列冲突和动态扩容。\n\n 4. 跳表\n    \n    定义：尽管散列表效率很高，但是散列表是无序的，跳表效率和散列表类似，并且支持区间序列的输出(因为基于链表)。\n    \n    适用场景：对有序元素的快速查找、插入和删除\n    \n    缺点：比较占用内存。\n\n 5. 红黑树\n    \n    定义：红黑树是平衡二叉查找树的一种近似实现。红黑树和跳表类似，但是实现方式有所差异。红黑树存在的价值是，它可以实现比较高效的查找，删除和插入。虽然相比高度平衡的avl树效率有所下降，但是红黑树不用耗费太多精力维护平衡。相比跳表，红黑树除了内存占用比较小，其他性能并不比跳表更优。但由于历史的原因，红黑树使用的更加广泛。\n    \n    缺点：实现比较复杂。\n\n\n# d53(2020/12/03) 递归树\n\n今天学习的主题的内容是，如何借助于树来求解递归算法的时间复杂度。\n\n我们都知道，递归代码的时间复杂度分析起来很麻烦。我们在第12节讲述过，如何利用递归公式，来求解归并排序、快速排序的时间复杂度，但是，有些情况，比如快排的平均时间复杂度的分析，用递推公式的话，会涉及非常复杂的数学推导。\n\n除了用递推公式这种比较复杂的分析方法，有没有更简单的方法呢？今天，我们就来学习另外一种方法，借助递归树来分析递归算法的时间复杂度。\n\n\n# 递归树与时间复杂度分析\n\n我们前面讲解过，递归的思想就是，将大问题分解为小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止。\n\n如果我们把这个一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫做递归树。我们这里画了一棵斐波那契数列的递归树。节点里的数字表示数据的规模，一个节点的求解可以分解为左右子节点两个问题的求解。\n\n> 斐波那契数列中，这个数列从第3项开始，每一项都等于前两项之和。$a_n$ = $a_{n-1}$ + $a_{n-2}$\n> \n> 0,1,1,2,3,5,8,13 ....\n\n\n\n通过这个例子，我们可以对递归树的样子应该有个感性的认识了，看其阿里并不复杂。现在，我们就来看，如何用递归树来求解时间复杂度。\n\n归并排序每次会将数据规模一分为二，我们把归并排序画成递归树。\n\n\n\n因为每次分解都是一分为二，所以代价很低，我们把时间上的消耗记作常量1。归并算法中比较耗时的是归并排序，也就是把两个子数组合并为大数组。从图中我们可以看出，每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记住n。\n\n现在，我们只需要知道这棵树的高度h，用高度h乘以每一层的时间消耗n，就可以得到总的时间复杂度o(n*h)。\n\n从归并排序的原理和递归树，可以看出来，归并排序递归树是一棵满二叉树。我们前两节中讲到，满二叉树的高度大约是$log_2 n$，所以归并排序递归实现的时间复杂度就是o(n$logn$)。这里的时间复杂度都是估算的，对树的高度的计算也没那么精确，但是这并不影响复杂度的计算结果。\n\n利用递归树的时间复杂度分析方法并不难理解，关键还是在实战，所以，接下来我会通过三个实际的递归算法，来实战理解一下递归的复杂度分析。学习完这节课之后，我们才能真正掌握递归代码的复杂度分析。\n\n\n# 实战一： 分析快速排序的时间复杂度\n\n在用递归树推导之前，我们先来回忆一下用递推公式的分析方法。我们可以回想一下，当时，我们为什么说用递推公式来求解平均时间复杂度非常复杂？\n\n快速排序在最好的情况下，每次分区都能一分为二，这个时候用递推公式t(n) = 2t(n/2) +n，很容易就推导出时间复杂度是o(nlogn)。但是，我们并不可能每次分区都这么幸运，正好一分为二。\n\n我们假设平均情况下，每次分区之后，两个分区的大小比例是1:k。当k=9时，如果用递推公式的方法来求解时间复杂度的话，递推公式就写成t(n) = t(n/10) + t(9n/10) + n。\n\n这个公式可以推导出时间复杂度，但是推导过程非常复杂。如果我们用递归树来分析快速排序的平均情况时间复杂度，是不是比较简单。\n\n我们还是取k等于9，也就是说，每次分区都很大平均，一个分区是另一个分区的9倍。如果我们把递归分解的过程画成递归树，就是下面的样子：\n\n\n\n快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是n。我们现在只要求出递归树的高度h，这个快排过程遍历的数据个数就是h*n，也就说，时间复杂度是o(h*n) 。\n\n因为每次分区并不是均匀地一分为二，所以递归树并不是满二叉树。这样一个递归树的高度是多少呢？\n\n我们知道，快速排序结束的条件就是待排序的小区间，大小为1，也就说叶子节点里的数据规模是1。从根节点n到叶子节点1，递归树中最短的一个路径每次都乘以1/10，最长的一个路径每次都乘以9/10。通过计算，我们可以得到，从根节点到叶子节点的最短路径是$log_{10} n$ ,最长的路径是$log{10/9} n$\n\n\n\n所以，遍历数据的个数总和就介于n$log_{10} n$ 和n $log{10/9} n$ 之间。根据复杂度的大o表示法，对数复杂度的底数不管是多少，我们统一写成logn，所以，当分区大小比例是1:9的时候，快速排序的时间复杂度仍然是o(nlogn)。\n\n刚刚我们假设k=9，那如果k=99，也就是说，每次分区及其不平均，两个区间大小是1:99，这个时候的时间复杂度是多少呢？\n\n我们可以类比上面k=9的分析过程。当k=99的时候，树的最短路径就是$log_{100} n$，最长路径是$log_{100/99} n$，所以总遍历数据个数介于n$log_{100} n$和nlog 100/99 n之间。尽管底数变了，但是时间复杂度也仍然是o(nlogn)。\n\n也就是说，对于k等于9，99，甚至是999，9999.....，只要k的值不随n变化，是一个事先确定的常量，那快排的时间复杂度就是o(nlogn)。所以，从概率论的角度来说，快排的平均时间复杂度就是o(nlogn)。\n\n\n# 实战二： 分析斐波那契数列的时间复杂度\n\n在递归那一节中，我们举了一个跨台阶的例子。那个例子实际上就是一个斐波那契数列。它的代码实现贴在如下：\n\nint f(int n) {\n  if (n == 1) return 1;\n  if (n == 2) return 2;\n  return f(n-1) + f(n-2);\n}\n\n\n1\n2\n3\n4\n5\n\n\n这样一段代码的时间复杂度是多少呢？如何利用递归树来分析。\n\n我们先把上面的递归代码画成递归树，就是下面这个样子：\n\n\n\n这棵递归树的高度是多少呢？\n\nf(n)分解为f(n-1)和f(n-2 )，每次数据规模都是-1或-2，叶子节点的数据规模是1或2。所以，从根节点走到叶子节点，每条路径是长短不一的。如果每次都是-1，那最长路径大约就是n；如果每次都是-2，那最短路径大约就是n/2。\n\n每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作1 ，第二层的总时间消耗是2，第三层额总时间消耗就是2^2。依次类推，第k层的时间消耗就是2^(k-1)，那整个算法的总的时间消耗就是每一层时间消耗之和。\n\n如果路径长度都为n，那这个总和就是2^n -1。\n\n\n\n如果路径长度都是n/2，那整个算法的总的时间消耗就是2^(n/2) -1.\n\n\n\n所以，这个算法的时间复杂度就介于o(2^n)和o(2^(n/2))之间。虽然这样得到的结果还不够精确，只是一个范围，但是我们也基本上知道了上面算法的时间复杂度是指数级的，非常高。\n\n\n# 实战三：分析全排列的时间复杂度\n\n前面两个复杂度分析\n\n\n# d54(2020/12/05) 堆排序\n\n我们今天讲另外一种特殊的树，"堆"。堆这种数据结构的应用场景非常多，最经典的莫过于堆排序。堆排序是一种原地的、时间复杂度为o(nlogn)的排序算法。\n\n前面我们学过快速排序，平均情况下，它的时间复杂度是o(nlogn)。尽管这两种排序算法的时间复杂度都是o(nlogn)，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？\n\n\n# 如何理解"堆"？\n\n前面我们提到，堆是一种特殊的树。现在来看，什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。\n\n * 堆是一个完全二叉树。\n * 堆中每一个节点的值都必须大于等于(或小于等于)其子树中每个节点的值。\n\n第一点，堆必须是一个完全二叉树。之前我们对完全二叉树的定义，完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。\n\n第二点，堆中的每个节点的值必须大于等于(或小于等于)其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于(或小于等于)其左右子节点的值。这两种表述是等价的。\n\n对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做"大顶堆"。对于每个节点的值多小于等于子树中每个节点值的堆，我们叫做"小顶堆"。\n\n下面我们来看看，下面的图示中，几个二叉树是不是堆？\n\n\n\n其中第1个和第2个是大顶堆，第3个是小顶堆，第4个不是堆。除此之外，从图中还可以看出来，对于同一组数据，我们可以构建多种不同形态的堆。\n\n\n# 如何实现一个堆？\n\n要实现一个堆，我们先要知道，堆都支持哪些操作以及如何存储一个堆。\n\n我们之前讲过，完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。\n\n下面是画了一个用数组存储堆的例子\n\n\n\n从图中我们可以看到，数组下标为i的节点的左子节点，就是下标为i*2的节点，右子节点就是下标为i*2+1 的节点，父节点就是下标为i/2的节点。\n\n知道了如何存储一个堆，那我们再来看看，堆上的操作有哪些呢？我罗列了几个非常核心的操作，分别是往堆中插入一个元素和删除堆顶元素。\n\n\n# 往堆中插入一个元素\n\n往堆中插入一个元素后，我们需要继续满足堆的两个特性。\n\n如果我们把新插入的元素放到堆的最后，可以看到下面的图，还是不符合堆的特性了。于是，我们就需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做堆化。\n\n堆化实际上有两种，从下往上和从上往下。这里我们先讲从下往上的堆化方法。\n\n\n\n堆化非常简单，就是顺着节点所在的路径，向上或向下，对比，然后交换。\n\n我这里画了一张堆化的过程分解图。我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。\n\n\n\n将上面讲的往堆中插入数据的过程，代码如下。\n\npublic class heap {\n  private int[] a; // 数组，从下标1开始存储数据\n  private int n;  // 堆可以存储的最大数据个数\n  private int count; // 堆中已经存储的数据个数\n\n  public heap(int capacity) {\n    a = new int[capacity + 1];\n    n = capacity;\n    count = 0;\n  }\n\n  public void insert(int data) {\n    if (count >= n) return; // 堆满了\n    ++count;\n    a[count] = data;\n    int i = count;\n    while (i/2 > 0 && a[i] > a[i/2]) { // 自下往上堆化\n      swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素\n      i = i/2;\n    }\n  }\n }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 删除堆顶元素\n\n从堆的定义的第二条中，任何节点的值都大于等于(或小于等于)子树节点的值，我们可以发现，堆顶元素存储的就是堆中数据的最大值或最小值。\n\n假设我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。\n\n这里我们也有一个分解图。不过这种方法有点问题，就是最后堆化出来的堆并不满足完全二叉树的特性。\n\n\n\n实际上，我们稍微改变一下思路，就可以解决这个问题。看下面画的这幅图，我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。\n\n因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的"空洞"，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。\n\n\n\n把上面的删除代码如下。\n\npublic void removemax() {\n  if (count == 0) return -1; // 堆中没有数据\n  a[1] = a[count];\n  --count;\n  heapify(a, count, 1);\n}\n\nprivate void heapify(int[] a, int n, int i) { // 自上往下堆化\n  while (true) {\n    int maxpos = i;\n    if (i*2 <= n && a[i] < a[i*2]) maxpos = i*2;\n    if (i*2+1 <= n && a[maxpos] < a[i*2+1]) maxpos = i*2+1;\n    if (maxpos == i) break;\n    swap(a, i, maxpos);\n    i = maxpos;\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n我们知道，一个包含n个节点的完全二叉树，树的高度不会超过$log_2 n$。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是o(logn)。插入数据和删除堆顶数据的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是o(logn)。\n\n\n# 如何基于堆实现排序？\n\n前面我们讲过好几种排序算法，我们再来回忆一下，有时间复杂度是o($n ^2$)的冒泡排序、插入排序、选择排序，有时间复杂度是o(nlogn)的归并排序、快速排序，还有线性排序。\n\n这里我们借助于堆这种数据结构实现的排序算法，就叫做堆排序。这种排序方法的时间复杂度非常稳定，是o(nlogn)，并且它还是原地排序算法。\n\n我们可以把堆排序的过程大致分解成两个大的步骤，建堆和排序。\n\n\n# 建堆\n\n我们首先将数组原地建成一个堆。所谓"原地"就是，不借助另一个数组，就在原数组上操作。建堆的过程，有两种思路。\n\n第一种是借助我们前面讲的，在堆中插入一个元素的思路。尽管数组中包含n个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为1的数据。然后，我们调用前面讲的插入操作，将下标从2到n的数据依次插入到堆中。这样我们就将包含n个数据的数组，组织成了堆。\n\n第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是从后往前处理数组，并且每个数据都是从上往下堆化。\n\n我举了一个例子，并且画了一个第二种实现思路的建堆分解步骤图，可以看下。因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从最后一个非叶子节点开始，依次堆化就行了。\n\n\n\n\n\n第二种实现思路翻译成了代码。\n\nprivate static void buildheap(int[] a, int n) {\n  for (int i = n/2; i >= 1; --i) {\n    heapify(a, n, i);\n  }\n}\n\nprivate static void heapify(int[] a, int n, int i) {\n  while (true) {\n    int maxpos = i;\n    if (i*2 <= n && a[i] < a[i*2]) maxpos = i*2;\n    if (i*2+1 <= n && a[maxpos] < a[i*2+1]) maxpos = i*2+1;\n    if (maxpos == i) break;\n    swap(a, i, maxpos);\n    i = maxpos;\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n我们可能已经发现了，在这段代码中，我们从下标从n/2开始到1的数据进行堆化，下标是n/2 + 1到n的节点是叶子节点，我们不需要堆化。实际上，对于完全二叉树来说，下标从n/2 + 1到n的节点都是叶子节点。\n\n现在我们来看一下，建堆操作的时间复杂度是多少？\n\n每个节点堆化的时间复杂度是o(logn)，那n/2 + 1个节点堆化的总时间复杂度是不是就是o(nlogn)呢？这个答案虽然也没有错，但是这个值不够精准。实际上，堆排序的建堆过程的时间复杂度是o(n)。\n\n因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度k成正比。\n\n我们把每一层的节点个数和对应的高度画出来，我们可以看下。我们只需要将每个节点的高度求和，得出的就是建堆的时间复杂度。\n\n\n\n我们将每个非叶子节点的高度求和，就是下面的这个公式：\n\n\n\n这个公式的求解稍微有点技巧，我们高中就应该学过：把公式左右都乘以2，就得到另一个公式s2。我们将s2错位对齐，并且用s2减去s1，就可以得到s了。\n\n\n\ns的中间部分是一个等比数列，所以最后可以用等比数列的求和公式来计算，最终的结果就是下面图中画的样子。\n\n\n\n因为h=$log_2 n$，代入公式s，就能得到s = o(n)，所以，建堆的时间复杂度就是o(n)。\n\n\n# 排序\n\n建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为n的位置。\n\n这个过程有点类似上面讲的"删除堆顶元素"的操作，当堆顶元素移除之后，我们把下标为n的元素放到堆顶，然后再通过堆化的方法，将剩下的n-1个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是n-1的位置，一直重复这个过程，知道最后堆中只剩下标为1的一个元素，排序工作就完成了。\n\n\n\n堆排序的过程的代码如下。\n\n// n表示数据的个数，数组a中的数据从下标1到n的位置。\npublic static void sort(int[] a, int n) {\n  buildheap(a, n);\n  int k = n;\n  while (k > 1) {\n    swap(a, 1, k);\n    --k;\n    heapify(a, k, 1);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n现在，我们再来分析一下堆排序的时间复杂度、空间复杂度以及稳定性。\n\n整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是o(n)，排序过程的时间复杂度是o(nlogn)，所以，堆排序整体的时间复杂度是o(nlogn)。\n\n堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。\n\n上面的讲解中，都是假设，堆中的数据是从数组下标为1的位置开始存储。如果从0开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了。\n\n如果节点的下标是i，那左子节点的下标就是2*i +1，右子节点的下标就是2*i + 2，父节点的下标就是 (i-1)/2 。\n\n\n# 解答开篇\n\n现在我们来看下开篇的问题，在实际开发中，为什么快速排序要比堆排序性能好？\n\n我觉得主要有两方面的原因。\n\n第一点，堆排序数据访问的方式没有快速排序友好。\n\n对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。比如，对排序中，最重要的一个操作就是数据的堆化。比如下面的这个例子，对堆顶节点进行堆化，会依次访问数组下标是1，2，4，8的元素，而不是像快速排序那样，局部顺序访问，所以，这样对cpu缓存是不友好的。\n\n\n\n第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。\n\n我们在讲排序的时候，提过两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换(或移动)。快速排序数据交换的次数不会比逆序度多。\n\n但是堆排序的第一步就是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。\n\n\n\n\n# 内容小结\n\n今天我们讲了堆这种数据结构，堆是一种完全二叉树。它最大的特性是：每个节点的值都大于等于(或小于等于)其子树节点的值。因此，堆被分成了两类，大顶堆和小顶堆。\n\n堆中比较重要的两个操作是插入一个数据和删除堆顶元素。这两个操作都要用到堆化。插入一个数据的时候，我们把新插入的数据放到数组的最后，然后从下往上堆化；删除堆顶数据的时候，我们把数组中的最后一个元素放到堆顶，然后从上往下堆化。这两个操作时间复杂度都是o(logn)。\n\n除此之外，我们还讲了堆的一个经典应用，堆排序。堆排序包含两个过程，建堆和排序。我们将下标n/2到1的节点，依次进行从上到下的堆化操作，然后就可以将数组中的数据组织成堆这种数据结构。接下来，我们迭代地将堆顶的元素放到堆的末尾，并将堆的大小减去一，然后再堆化，重复这个过程，直到堆中只剩下一个元素，整个数组中的数据就都有序排列了。\n\n\n# d55(2020/12/09) 堆应用\n\n假设现在我们有一个包含10亿个搜索关键词的日志文件，如何能快速获取到热门榜top 10的搜索关键词呢？\n\n这个问题就可以用堆来解决，这也是堆这种数据结构一个非常典型的应用。上一节我们讲了堆和堆排序的一些理论知识，今天我们就来讲一讲，堆这种数据结构几个非常重要的应用：优先级队列、求top k和求中位数。\n\n\n# 堆的应用一：优先级队列\n\n首先，我们来看第一个应用场景：优先级队列。\n\n优先级队列，顾名思义，它首先应该是一个队列。我们前面讲过，队列最大的特性就是先进先出。不过，在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。\n\n如果实现一个优先级队列呢？方法有很多，但是用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。\n\n不要小看这个优先级队列，它的应用场景非常多。我们后面要讲的很多数据结构和算法都要依赖它。比如，赫夫曼编码、图的最短路径、最小生成树算法等。不仅如此，很多语言中，都提供了优先级队列的实现。比如，java的priorityqueue，c++的priority_queue等。\n\n只讲这些应用场景比较空泛，现在，我举两个具体的例子，来感受一下优先级队列具体是怎么用的。\n\n\n# 合并有序小文件\n\n假设我们有100个小文件，每个文件的大小是100mb，每个文件中存储的都是有序的字符串。我们希望将这些100个小文件合并成一个有序的大文件。这里就会用到优先级队列。\n\n整体思路有点像归并排序中的合并函数。我们从这100个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除。\n\n假设，这个最小的字符串来自于13.txt这个小文件，我们就再从这个小文件取下一个字符串，放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，将它从数组中删除。\n\n这里我们用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。有没有更加高效方法呢？\n\n这里就可以用到优先级队列，也可以说是堆。我们将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将100个小文件中的数据依次放入到大文件中。\n\n我们知道，删除堆顶数据和往堆中插入数据的时间复杂度都是o(logn)，n表示堆中的数据个数，这里就是100。这样的话就比原来数组存储的方式要高效很多了。\n\n\n# 高性能定时器\n\n假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器没过一个很小的单位时间(比如1秒)，就扫描一遍任务，看是否任务到达设定的执行时间。如果到达了，就拿出来执行。\n\n\n\n但是，这样每过1秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。\n\n针对这些问题，我们就可以用优先级队列来解决。我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部(也就是小顶堆的堆顶)存储的是最先执行的任务。\n\n这样，定时器就不需要每隔1秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔t。\n\n这个时间间隔t就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在t秒之后，再来执行任务。从当前时间点到(t-1)秒这段时间里，定时器都不需要做任何事情。\n\n当t秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间与当前时间点的差值，把这个差值作为定时器执行下一个任务需要等待的时间。\n\n这样，定时器既不用间隔1秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。\n\n\n# 堆的应用二：利用堆求top k\n\n刚刚我们学习了优先级队列，我们现在来看，堆的另外一个非常重要的应用场景，那就是"求top k问题"。\n\n我们把这种求top k的问题抽象成两类。一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。\n\n针对静态数据，如何在一个包含n个数据的数组中，查找前k大数据呢？我们可以维护一个大小为k的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不作处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前k大数据了。\n\n遍历数组需要o(n)的时间复杂度，一次堆化操作需要o(logk)的时间复杂度，所以最坏情况下，n个元素都入堆一次，时间复杂度就是o(nlogk)。\n\n针对动态数据求得top k就是实时top k。例如，一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前k大数据。\n\n如果每次询问前k大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是o(nlogk)，n表示当前的数据的大小。实际上，我们可以一直都维护一个k大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前k大数据，我们都可以立刻返回给他。\n\n\n# 堆的应用三：利用堆求中位数\n\n如何求动态数据集合中的中位数。\n\n中位数，顾名思义，就是处于中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第n/2 +1个数据就是中位数；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第n/2个和第n/2 +1个数据，这个时候，我们可以随意取一个作为中位数，比如取两个数中靠前的那个，就是第n/2个数据。\n\n\n\n对于一组静态数据，中位数是固定的，我们可以先排序，第n/2个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了。所以，尽管排序的代价比较大，但是边际成本会很小。但是，如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。\n\n借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。\n\n我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。\n\n也就是说，如果有n个数据，n是偶数，我们从小到大排序，那前n/2个数据存储在大顶堆中，后n/2个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果n是奇数，情况是类似的，大顶堆就存储了n/2 + 1个数据，小顶堆中就存储n/2个数据。\n\n\n\n我们前面也提到，数据是动态变化的，当新添加一个数据的时候，我们如何调整两个堆，让大顶堆的堆顶元素继续是中位数呢？\n\n如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。\n\n这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果n是偶数，两个堆中的数据个数都是n/2；如果n是奇数，大顶堆有n/2 +1 个数据，小顶堆有n/2个数据。这个时候，我们可以从一个堆中不停地将堆顶元素移到到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。\n\n\n\n于是，我们就可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作。插入数据因为需要涉及堆化，所以时间复杂度变成了o(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是o(1)。\n\n实际上，利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。之前我们提到一个问题，"如何快速求接口的99%响应时间？" 我们现在来看下，利用两个堆如何来实现。\n\n在开始这个问题的讲解之前，我们先解释一下，什么是"99%响应时间"。\n\n中位数的概念就是将数据从小到大排列，处于中间位置，就叫中位数，这个数据会大于等于前面50%的数据。99百分位数的概念可以类比中位数，如果将一组数据从小到大排列，这个 99百分位数就是大于前面99%数据的那个数据。\n\n如果这个还是不好接，例如有100个数据，分别是1，2，3，。。。。。100，那99百分位数就是99，因为小于等于99的数占总个数的99%。\n\n\n\n我们再来看99%响应时间。如果有100个接口访问请求，每个接口请求的响应时间都不同，比如55毫秒、100毫秒、23毫秒等，我们把这100个接口的响应时间按照从小到大排列，排在第99的那个数据就是99%响应时间，也叫99百分位响应时间。\n\n我们来总结一下，如果有n个数据，将数据从小到大排列之后，99百分位数大约就是第n*99%个数据，同类，80百分位数大约就是第n*80%个数据。\n\n弄懂了这些，我们再来看如何求99%响应时间。\n\n我们维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是n，大顶堆中保存n*99%个数据，小顶堆中保存n*1%个数据。大顶堆堆顶的数据就是我们要找的99%响应时间。\n\n每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。\n\n但是，为了保持大顶堆中的数据占99%，小顶堆中的数据占1%，在每次新插入数据之后，我们都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合99:1这个比例。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法。\n\n通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是o(logn)。每次求99%响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是o(1)。\n\n\n# 解答开篇\n\n学懂了上面的一些应用场景的处理思路，我们来解答一下开篇的那个问题。假设现在我们有一个包含10亿个搜索关键词的日志文件，如何快速获取到top 10最热门的搜索关键词呢？\n\n处理这个问题，有很多高级的解决方法，比如使用mapreduce等。但是，如果我们将处理的场景限定为单机，可以使用的内存为1gb。那个这个问题该如何解决呢？\n\n因为用户搜索的关键词，有很多可能都是重复的，所以我们首先要统计每个搜索关键词出现的频率。我们可以通过散列表、平衡二叉查找树或者其他一些支持快速查找、插入的数据结构，来记录关键词及其出现的次数。\n\n假设我们选用散列表。我们就顺序扫描这10亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为1。以此类推，等遍历完这10亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。\n\n假设我们选用散列表。我们就顺序扫描这10亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为1。以此类推，等遍历完这10亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。\n\n然后，我们再根据前面讲的用堆求top k的方法，建立一个大小为10的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。\n\n以此类推，当遍历完整个散列表中的搜索关键词之后，堆中的搜索关键词就是出现次数最多的top 10搜索关键词了。\n\n上面的解决思路还是存在漏洞的。10亿的关键词还是很多的。我们假设10亿条搜索关键词中不重复的有1亿条，如果每个搜索关键词的平均长度是50个字节，那存储1亿个关键词起码需要5gb的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。而我们的机器只有1gb的可用内存空间，所以我们无法一次性将所有的搜索关键词加入到内存中。\n\n我们在哈希算法那一节讲过，相同数据经过哈希算法得到的哈希值是一样的。我们可以根据哈希算法的这个特定，将10亿条搜索关键词先通过哈希算法分片到10个文件中。\n\n具体可以这样做：我们创建10个空文件00，01，02，....，09。我们遍历这10亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同10取模，得到的结果就是这个搜索关键词应该被分到的文件编号。\n\n对这10亿个关键词分片之后，每个文件都只有1亿的关键词，去除重复的，可能就只有1000万个，每个关键词平均50个字节，所以总的大小就是500mb。1gb的内存完全可以放得下。\n\n我们针对每个包含1亿条搜索关键词的文件，利用散列表和堆，分别求出top 10，然后把这个10个top 10放在一块，然后取这100个关键词中，出现次数最多的10个关键词，这就是这10亿条数据中的top10最频繁的搜索关键词了。\n\n\n# 内容小结\n\n我们今天主要讲了堆的几个重要的应用，它们分别是：优先级队列、求top k问题和求中位数的问题。\n\n优先级队列是一种特殊的队列，优先级高的数据出队，而不再像普通的队列那样，先进先出。实际上，堆就可以看作优先级队列，知识称谓不一样罢了。求top k问题又可以分为针对静态数据和针对动态数据，只需要利用一个堆，就可以做到非常高效率地查询top k的数据。求中位数实际上还有很多变形，比如求99百分位数据、90百分位数据等，处理的思路都是一样的，即利用两个堆，一个大顶堆，一个小顶堆，随着数据的动态增加，动态调整两个堆中的数据，最后大顶堆的堆顶元素就是要求的数据。\n\n\n# d56(2020/12/12) 图的存储\n\n今天我们要学习的就是图这种数据结构。实际上，涉及图的算有很多，也非常复杂，比如图的搜索、最短路径、最小生成树、二分图等。我们今天聚焦在图存储这一方面，后面会分好几节来依次讲解图相关的算法。\n\n\n# 如何理解"图"\n\n我们前面讲过了树这种非线性表数据结构，今天我们要讲另一种非线性表数据结构，图。和数比起来，这是一种更加复杂的非线性表结构。\n\n我们知道，树中的元素我们称为节点，图中的元素我们就叫做顶点。从画的图中可以看出来，图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫做边。\n\n\n\n我们生活中就有很多符合图这种结构的例子。比如，开篇问题中讲到的社交网络，就是一个非常典型的图结构。\n\n我们就拿微信来举例子把。我们可以把每个用户看作一个顶点。如果两个用户之间互加好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。其中，每个用户有多少个好友，对应到图中，就叫做顶点的度，就是跟顶点相连接的边的条数。\n\n实际上，微博的社交关系跟微信还有点不一样，或者说更加复杂一点。微博允许单向关注，也就是说，用户a关注了用户b，但用户b可以不关注用户a。那我们如何用图来表示这种单向的社交关系呢？\n\n我们可以把刚刚讲的图结构稍微改造一下，引入边的"方向"的概念。\n\n如果用户a关注了用户b，我们就在图中画一条从a到b的带箭头的边，来表示边的方向。如果用户a和用户b互相关注了，那我们就画一条从a指向b的边，再画一条从b指向a的边。我们把这种边有方向的图叫做"有向图"。以此类推，我们把边没有方向的图就叫做"无向图"。\n\n\n\n我们刚刚讲过，无向图中有"度"这个概念，表示一个顶点有多少条边。在有向图中，我们把度分为入度和出度。\n\n顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。对应到微博的例子，入度就表示有多少粉丝，出度就表示关注了多少人。\n\n前面讲到了微信、微博、无向图、有向图，现在我们再来看另一种社交软件：qq。\n\nqq中的社交关系要更复杂一点。不知道有没有留意过qq亲密度这样一个功能。qq不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经常往来，亲密度就比较低。如果在图中记录这种好友关系的亲密度呢？\n\n这里就要用到另一种图，带权图。在带权图中，每条边都有一个权重，我们可以通过这个权重来表示qq好友间的亲密度。\n\n\n\n\n# 邻接矩阵存储方法\n\n图最直观的一种存储方法就是，邻接矩阵。\n\n邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点i与顶点j之间有边，我们就将a[i][j]和a[j][i]标记为1；对于有向图来说，如果顶点i到顶点j之间，有一条箭头从顶点i指向顶点j的边，那我们就将a[i][j]标记为1。同理，如果有一条箭头从顶点j指向顶点i的边，我们就将a[j][i]标记为1。对于带权图，数组中就存储相应的权重。\n\n用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。为什么呢？\n\n对于无向图来说，如果a[i][j]等于1，那么a[j][i]也肯定等于1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或下面这样一半的空间就足够了，另外一半白白浪费掉了。\n\n还有，如果我们存储的是稀疏图，也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了。比如微信有好多亿的用户，对应到图上就好几亿的顶点。但是每个用户的好友并不会很多，一般也就三五百个而已。如果我们用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。\n\n但这也并不是说，邻接矩阵的存储方法就完全没有优点。首先，邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。其次，用邻接矩阵存储图的另外一个好处是方便计算。这是因为，用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算。比如求解最短路径问题时会提到一个floyd算法，就是利用矩阵循环相乘若干次得到结果。\n\n\n# 邻接表存储方法\n\n针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，邻接表。\n\n下面画了一张邻接表的图，乍一看，邻接表有点像散列表。每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。另外我需要说明一下，图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点。\n\n\n\n还记得我们之前讲过的时间、空间复杂度互换的设计思想吗？邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储其阿里比较节省空间，但是使用起来就比较耗费时间。\n\n就像图中的例子，如果我们要确定，是否存在一条从顶点2到顶点4的边，那我们就要遍历顶点2对应的那条链表，看链表中是否存在顶点4。而且，我们前面也讲过，链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。\n\n在散列表的那几节中，我们讲到，在基于链表法解决冲突的散列表中，如果链表过长，为了提高查找效率，我们可以将链表换成其他更加高效的数据结构，比如平衡二叉查找树等。刚刚也提及，邻接表长得很像散列。所以，我们也可以将邻接表同散列表一样进行"改进升级"。\n\n我们可以将邻接表中的链表改成平衡二叉查找树。实际开发中，我们可以选择用红黑树。这样，我们就可以更加快速地查找两个顶点之间是否存在边了。当然，这里的二叉查找树可以换成其他动态数据结构，比如跳表、散列表等。除此之外，我们还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间是否存在边。\n\n\n# 解答开篇\n\n有了前面讲的理论知识，现在我们再来看下开篇的问题，如何存储微博、微信等社交网络中的好友关系？\n\n前面我们分析了，微博、微信是两种"图"，前者是有向图，后者是无向图。在这个问题上，两者的解决思路差不多，所以这里只拿微博来讲解。\n\n数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。针对微博用户关系，假设我们需要支持下面这样几个操作：\n\n * 判断用户a是否关注了用户b；\n * 判断用户a是否是用户b的粉丝；\n * 用户a关注用户b；\n * 用户a取消关注用户b；\n * 根据用户名称的首字母排序，分页获取用户的粉丝列表；\n * 根据用户名词的首字母排序，分页获取用户的关注列表。\n\n关于如何存储一个图，前面我们讲到两种主要的存储方法，邻接矩阵和邻接表。因为社交网络是一张稀疏图，使用邻接矩阵存储比较浪费存储空间。所以，这里我们采用邻接表来存储。\n\n不过，用一个邻接表来存储这种有向图是不够的。我们去查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就是用户的粉丝列表，是非常困难的。\n\n基于此，我们需要一个逆邻接表。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。对应到图上，邻接表中，每个顶点的链表中，存储的就是这个顶点指向的顶点，逆邻接表中，每个顶点的链表中，存储的是指向这个顶点的顶点。如果要查找某个用户关注了哪些用户，我们可以在邻接表中查找；如果要查找某个用户被哪些用户关注了，我们从逆邻接表中查找。\n\n\n\n基础的邻接表不适合快速判断两个用户之间是否是关注与被关注的关系，所以我们选择改进版本，将邻接表中的链表改为支持快速查找的动态数据结构。选择哪种动态数据结构呢？红黑树、跳表、有序动态数组还是散列表呢？\n\n因为我们需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或关注列表，用跳表这种结构再合适不过了。这是因为，跳表插入、删除、查找都非常高效，时间复杂度是o(logn)，空间复杂度上稍高，是o(n)。最重要的一点，跳表中存储的数据本来就是有序的了，分页获取粉丝列表或关注列表，就非常高效。\n\n如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，我们可以将整个社交关系存储在内存中，上面的解决思路是没有问题的。但是如果像微博那样有上亿用户，数据规模太大，我们就无法全部存储在内存中了。\n\n我们可以通过哈希算法等数据分片方式，将邻接表存储在不同的机器上。可以查看下面的图示，我们在机器1上存储顶点1，2，3的邻接表，在机器2上，存储顶点4，5的邻接表。逆邻接表的处理方式也一样。当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定义顶点所在的机器，然后再在相应的机器上查找。\n\n\n\n除此之外，我们还有另外一种解决思路，就是利用外部存储(比如硬盘)，因为外部存储的存储空间要比内存会宽裕很多。数据库是我们经常用来持久化存储关系数据的，下面就介绍一种数据库的存储方式。\n\n下面这张表来存储这样一个图。为了高效地支持前面定义的操作，我们可以在表上建立多个索引，比如第一列、第二列，给这两列都建立索引。\n\n\n\n\n# 内容小结\n\n今天我们学习了图这种非线性表数据结构，关于图，我们需要理解这样几个概念：无向图、有向图、带权图、顶点、边、度、入度、出度。除此之外，我们还学习了图的两个主要存储方式：邻接矩阵和邻接表。\n\n邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。\n\n\n# d57(2020/12/13) 深度和广度优先搜索\n\n上一节我们讲了图的表示方法，讲到如何用有向图、无向图来表示一个社交网络。在社交网络中，有一个六度分隔理论，具体是说，你与世界上的另一个间隔的关系不会超过六度，也就是说平均只需要六步就可以联系到任何两个互不相识的人。\n\n一个用户的一度连接用户很好理解，就是他的好友，二度连接用户就是他好友的好友，三度连接用户就是他好友的好友。在社交网络中，我们往往通过用户之间的连接关系，来实现推荐"可能认识的人"这么一个功能。今天的开篇问题就是，给你一个用户，如何找出这个用户的所有三度(其中包含一度、二度和三度)好友关系？\n\n这就要用到今天要讲的深度优先和广度优先搜索算法。\n\n\n# 什么是"搜索"算法？\n\n我们知道，算法是作用于具体数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于"图"这种数据结构的。这是因为，图这种数据结构的表达能力很强，大部分涉及受伤的场景都可以抽象成"图"。\n\n图上的搜索算法，最直接的理解就是，在图中找出从一个顶点触发，到另一顶点的路径。具体方法有很多，比如今天我们要讲的两种最简单、最"暴力"的深度优先、广度优先搜索，还有a*、ida*等启发式搜索算法。\n\n我们上一节讲过，图有两种主要存储方法，邻接表和邻接矩阵。今天我们会用邻接表来存储图。\n\n这里先给出了图的代码实现。需要说明一下，深度优先搜索算法和广度优先搜索算法，既可以用在无向图，也可以用在有向图上。在今天的讲解中，针对的是无向图的讲解。\n\npublic class graph { // 无向图\n  private int v; // 顶点的个数\n  private linkedlist<integer> adj[]; // 邻接表\n\n  public graph(int v) {\n    this.v = v;\n    adj = new linkedlist[v];\n    for (int i=0; i<v; ++i) {\n      adj[i] = new linkedlist<>();\n    }\n  }\n\n  public void addedge(int s, int t) { // 无向图一条边存两次\n    adj[s].add(t);\n    adj[t].add(s);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 广度优先搜索(bfs)\n\n广度优先搜索(breadth-first-search)，我们平常都简称为bfs。直观地讲，它其实就是一种"地毯式"层层推进的搜索策略，即先查找离起点顶点最近的，然后是次近的，依次往外搜索。理解起来并不难，下面有一张示意图。\n\n\n\n尽管广度优先搜索的原理挺简单，但代码实现还是稍微有点复杂度。所以，我们重点讲一下它的代码实现。\n\n这里面，bfs()函数就是基于之前定义的，图的广度优先搜索的代码实现。其中s表示起点顶点，t表示终止顶点。我们搜索一条从s到t的路径。实际上，这样求得的路径就是从s到t的最短路径。\n\npublic void bfs(int s, int t) {\n  if (s == t) return;\n  boolean[] visited = new boolean[v];\n  visited[s]=true;\n  queue<integer> queue = new linkedlist<>();\n  queue.add(s);\n  int[] prev = new int[v];\n  for (int i = 0; i < v; ++i) {\n    prev[i] = -1;\n  }\n  while (queue.size() != 0) {\n    int w = queue.poll();\n   for (int i = 0; i < adj[w].size(); ++i) {\n      int q = adj[w].get(i);\n      if (!visited[q]) {\n        prev[q] = w;\n        if (q == t) {\n          print(prev, s, t);\n          return;\n        }\n        visited[q] = true;\n        queue.add(q);\n      }\n    }\n  }\n}\n\nprivate void print(int[] prev, int s, int t) { // 递归打印s->t的路径\n  if (prev[t] != -1 && t != s) {\n    print(prev, s, prev[t]);\n  }\n  system.out.print(t + " ");\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n这段代码不是很好理解，里面有三个重要的辅助变量visited、queue、prev。只要理解三个变量，读懂这段代码估计就没什么问题了。\n\nvisited是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点q被访问，那相应的visited[q]会被设置为true。\n\nqueue是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第k层的顶点都访问完成之后，才能访问第k+1层的顶点。所以，我们用这个队列来实现记录的功能。\n\n> 入队，是指该顶点以及被访问了，但是该顶点的相连的顶点还没被访问。出队，是指该顶点以及被访问了，并且该顶点相连的顶点也被访问了。\n\nprev用来记录搜索路径。当我们从顶点s开始，广度优先搜索到顶点t后，prev数组中存储的就是搜索的路径。不过，这个路径是反向存储的。prev[w]存储的是，顶点w是从哪个前驱顶点遍历过来的。比如，我们通过顶点2的邻接表访问到顶点3，那prev[3]就等于2。为了正向打印出路径，我们需要递归地来打印，可以看下print()函数的实现方式。\n\n为了方便理解，这里画了一个广度优先搜索的分解图，可以结合着代码来看。\n\n\n\n掌握了广度优先搜索算法的原理，我们来看下，广度优先搜索的时间、空间复杂度是多少呢？\n\n最坏情况下，终止顶点t离起始顶点s很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边都会被访问一次，所以，广度优先搜索的时间复杂度是o(v+e)，其中，v表示顶点的个数，e表示边的个数。当然，对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，e肯定要大于等于v-1，所以，广度优先搜索的时间复杂度也可以简写为o(e)。\n\n广度优先搜索的空间消耗主要在几个辅助变量visited数组、queue队列、prev数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是o(v)。\n\n\n# 深度优先搜索(dfs)\n\n深度优先搜索(dfs)，最直观的例子就是"走迷宫"。\n\n假设我们站在迷宫的某个岔路口，然后想找到出口。我们随意选择一个岔路口来走，走着走着发现走不通的时候，我们就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。\n\n走迷宫的例子很容易能看懂，我们现在再来看下，如何在图中应用深度优先搜索，来找某个顶点到另一个顶点的路径。\n\n你可以看下面画的这幅图。搜索的起始顶点是s，终止顶点是t，我们希望在图中寻找一条从顶点s到顶点t的路径。如果映射到迷宫这个例子，s就是我们起始所在的位置，t就是出口。\n\n下面用深度递归算法，把整个搜索的路径标记出来了。张雷明实线箭头表示遍历，虚线箭头表示回退。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点s到顶点t的最短路径。\n\n\n\n实际上，深度优先搜索用的是一种比较著名的算分思想，回溯思想。这种思想解决问题的过程，非常适合用递归来实现。\n\n我们将上面的过程用递归来翻译出来，就是下面这个样子。我们发现，深度优先搜索代码实现也用到了prev、visited变量以及print()函数，它们跟广度优先搜索代码实现里的作用是一样的。不过，深度优先搜索代码实现里，有个比较特殊的变量found，它的作用是，当我们已经找到终止顶点t之后，我们就不再递归地继续查找了。\n\nboolean found = false; // 全局变量或者类成员变量\n\npublic void dfs(int s, int t) {\n  found = false;\n  boolean[] visited = new boolean[v];\n  int[] prev = new int[v];\n  for (int i = 0; i < v; ++i) {\n    prev[i] = -1;\n  }\n  recurdfs(s, t, visited, prev);\n  print(prev, s, t);\n}\n\nprivate void recurdfs(int w, int t, boolean[] visited, int[] prev) {\n  if (found == true) return;\n  visited[w] = true;\n  if (w == t) {\n    found = true;\n    return;\n  }\n  for (int i = 0; i < adj[w].size(); ++i) {\n    int q = adj[w].get(i);\n    if (!visited[q]) {\n      prev[q] = w;\n      recurdfs(q, t, visited, prev);\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\n理解了深度优先搜索算法之后，我们来看，深度优先搜索的时间、空间复杂度是多少呢？\n\n从前面画的图可以看出，每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是o(e)，e表示边的个数。\n\n深度优先搜索算法的消耗内存主要是visited、prev数组和递归调用栈。visited、prev数组的大小跟顶点的个数v成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是o(v)。\n\n\n# 解答开篇\n\n了解了深度优先搜索和广度优先搜索的原理之后，开篇的问题就变得很简单了。我们来看下，如何找出社交网络中某个用户的三度好友关系？\n\n社交网络可以用图来表示。这个问题就非常适合用图的广度优先搜索算法来解决，因为广度优先搜索是层层往外推进的。首先，遍历与起始顶点最近的一层顶点，也就是用户的一度好友，然后再遍历与用户举例的边数为2的顶点，也就是二度好友关系，以及与用户距离的边数为3的顶点，也就是三度好友关系。\n\n\n# 内容小结\n\n广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，比如a*、ida*等，要简单粗暴，没有什么优化，所以，也被叫做暴力搜索算法。所以，这两种搜索算法仅适用于状态空间不大，也就是说图不大的搜索。\n\n广度优先搜索，通俗来说，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先搜索的时间复杂度都是o(e)，空间复杂度是o(v)。\n\n\n# d58(2020/12/14)字符串匹配\n\n从今天开始，我们来学习字符串匹配算法。字符串匹配这样一个功能，大家都不会陌生。我们用的最多的就是编程语言提供的字符串查找函数，比如java中的indexof()，python中的find()函数等，它们底层就是依赖接下来要讲的字符串匹配算法。\n\n字符串匹配算法很多，这里会分为四节来讲解。今天会讲解两种比较简单那、好理解的，它们分别是：bf算法和rk算法。下一节，我们会学习比较难以理解、但更加高效的，它们是：bm算法和kmp算法。\n\n这两节讲的都是单模式串匹配的算分，也就是一个串跟一个串进行匹配。第三节、第四节，我们会讲两种多模式串匹配算法，也就是在一个串中同时查找多个串，它们分别是trie树和ac自动机。\n\n今天讲的两个算法中，rk算法是bf算法的改进，它巧妙借助了我们前面讲过的哈希算法，让匹配的效率有了很大的提升。那rk算法是如何借助哈希算法来实现高效字符串匹配的呢？\n\n\n# bf算法\n\nbf算法中的bf是brute force的缩写，中文叫做暴力匹配算法，也叫朴素匹配算法。从名字可以看出，这种算法的字符串匹配方式很"暴力"，当然研究会比较简单、好懂，但相应的性能也不高。\n\n在开始讲解这个算法之前，我们先定义两个概念，方便后面讲解。它们分别是主串和模式串。\n\n比方说，我们在字符串a中查找字符串b，那字符串a就是主串，字符串b就是模式串。我们把主串的长度记住n，模式串的长度记住m。因为我们是在主串中查找模式串，所以n>m。\n\n作为最简单、最暴力的字符串匹配算法，bf算法的思想可以用一句话来概括，那就是，我们在主串中，检查起始位置分别是0、1、2...n-m 且长度为m的n-m+1个子串，看有么有跟模式串匹配的。\n\n\n\n从上面的算法思想和例子，我们可以看出，在极端情况下，比如主串是"aaa...aaaa"(省略号表示有很多重复的字符a)，模式串是"aaaaab"。我们每次都比对m个字符，要比对n-m+1次，所以，这种算法的最坏情况时间复杂度是o(n*m)。\n\n尽管理论上，bf算法的时间复杂度很高，是o(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。\n\n第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把m个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是o(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。\n\n第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有bug也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常常所说的kiss(keep it simple and stupid)设计原则。\n\n所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。\n\n\n# rk算法\n\nrk算法理解起来也不是很难。个人觉得，它其实就是刚刚讲的bf算法的升级版。\n\n我们在讲解bf算法的时候讲过，如果模式串长度是m，主串的长度为n，那在主串中，就会有n-m+1个长度为m的子串，我们只需要暴力地对比这n-m+1个子串与模式串，就可以找出主串与模式串匹配的子串。\n\n但是，每次检查主串与子串是否匹配，需要依次比对每个字符，所以bf算法的时间复杂度就比较高，是o(n*m)。我们对朴素的字符串匹配算法稍加改造，引入哈希算法，时间复杂度立刻就会降低。\n\nrk算法的思路是这样的：我们通过哈希算法对主串中的n-m+1个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了(这里先不考虑哈希冲突的问题，后面我们会讲到)。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串与子串比较的效率就提高了。\n\n\n\n不过，通过哈希算法计算子串的哈希值的时候，我们需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。有没有方法可以提高哈希算法计算子串哈希值的效率呢？\n\n这就需要哈希算法设计的非常有巧妙了。我们假设要匹配的字符串的字符集中只包含k个字符，我们可以用一个k进制数来表示一个子串，这个k进制数转化成十进制数，作为子串的哈希值。表述起来有点抽象，举个如下的例子。\n\n比如要处理的字符串只包含a~z这26个小写字母，那我们就用二十六进制来表示一个字符串。我们把a~z这26个字符映射到0~25这26个数字，a就表示0，b就表示1，以此类推，z表示25。\n\n在十进制的表示法中，一个数字的值是通过下面的方式计算出来的。对应到二十六进制，一个包含a到z这26个字符的字符串，计算哈希的时候，我们只需要把进位从10改成26就可以。\n\n\n\n这个哈希算法应该可以看懂，现在，为了方便解释，在下面的讲解中，我假设字符串中只包含a~z这26个小写字符，我们用二十六进制来表示一个字符串，对应的哈希值就是二十六进制转化成十进制的结果。\n\n这种哈希算法有一个特点，在主串中，相邻两个子串的哈希值的计算公式有一定关系。这里有个例子，可以先看下规律。\n\n\n\n从这里例子中，我们很容易就能得出这样的规律：相邻两个子串s[i-1]和s[i] (i表示子串在主串中的起始位置，子串的长度都为m)，对应的哈希值计算公式有交集，也就是说，我们可以使用s[i-1]的哈希值很快的计算出s[i]的哈希值。如果用公式表示的话，就是下面这个样子：\n\n\n\n不过，这里有一个小细节需要注意，那就是26^(m-1)这部分的计算，我们可以通过查表的方式来提高效率。我们事先计算好26^0、26^1、26^2 .... 26^(m-1)，并且存储在一个长度为m的数组中，公式中的"次方"就对应数组的下标。当我们需要计算26的x次方的时候，就可以从数组的下标为x的位置取值，直接使用，省去了计算的时间。\n\n\n\n我们开头的时候提过，rk的算分的效率要比bf算分高，现在，我们来分析一下，rk算法的时间复杂度到底是多少呢？\n\n整个rk算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。第一部分，我们前面也分析了，可以通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是o(n)。\n\n模式串哈希值与每个子串哈希值之间的比较的时间复杂度是o(1)，总共需要比较n-m+1个子串的哈希值，所以，这部分的时间复杂度也是o(n)。所以，rk算法整体的时间复杂度就是o(n)。\n\n这里还有一个问题就是，模式串很长，相应的主串中的子串也会很长，铜鼓上面的哈希算法计算得到的哈希值就可能很大，如果超过了计算机中整型数据可以表示的访问，那该如何解决。\n\n刚刚我们设计的哈希算法是没有散列冲突的，也就说，一个字符串与一个二十六进制数一一对应，不同的字符串的哈希值肯定不一样。因为我们是基于进制来表示一个字符串的，我们可以类比成十进制、十六进制来思考一下。实际上，我们为了能将哈希值落在整型数据范围内，可以牺牲一下，允许哈希冲突。这个时候的哈希算法该如何设计呢？\n\n哈希算法的设计方法有很多，这里举一个例子说明一下。假设字符串中只包含a~z这26个英文字母，那我们每个字母对应一个数字，比如a对应1，b对应2，以此类推，z对应26。我们可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值。这种哈希算法产生的哈希值的数据范围就相对要小很多了。\n\n不过，我们也应该发现，这种哈希算法的哈希冲突概率也是挺高的。当然，这里也只是举了一个最简单的设计方法，还有很多更加优化的方法，比如将每一个字母从小到达对应一个素数，而不是1，2，3.。。这样的自然数，这样冲突的概率就会降低一些。\n\n那现在新的问题来了。之前我们只需要比较一下模式串和子串的哈希值，如果两个值相等，那这个子串就一定可以匹配模式串。但是，当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身不匹配。\n\n实际上，解决方法很简单。当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。当然，如果子串的哈希值与模式串的哈希值不相等，那对应的子串和模式串肯定也是不匹配的，就不需要对比子串和模式串本身了。\n\n所以，哈希算法的冲突概率要相对控制得低一些，如果存在大量冲突，就会导致rk算法的时间复杂度退化，效率下降。极端情况下，如果存在大量的冲突，每次都要再对比子串和模式串本身，那时间复杂度就会退化到o(n*m)。但是也不要悲观，一般情况下，冲突不会很多，rk算法的效率还是比bf算法高的。\n\n\n# 解答开篇&内容小结\n\n今天我们讲了两种字符串匹配算法，bf算法和rk算法。\n\nbf算法是最简单、粗暴的字符串匹配算法，它的实现思路是，拿模式串与主串中是所有子串匹配，看是否有能匹配的子串。所以，时间复杂度也比较高，是o(n*m)，n、m表示主串和模式串的长度。不过，在实际的软件开发中，因为这种算法实现简单，对于处理小规模的字符串匹配很好用。\n\nrk算法是借助哈希算法对bf算法进行改造，即对每个子串分别求哈希值，然后拿子串的哈希值与模式串的哈希值比较，减少了比较的时间。所以，理想的情况下，rk算法的时间复杂度是o(n)，跟bf算法相比，效率提高了很多。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为o(n*m)。\n\n\n# d59(2020/12/16) 字符串匹配\n\n文本编辑器中的查找替换功能，比如，我们在word中把一个单词统一替换成另一个，用的就是这个功能。那么这个是怎么实现的呢？\n\n当然，我们可以用上一节的bf算法和rk算法，也可以实现这个功能，但是在某些极端情况下，bf算法性能会退化的比较严重，而rk算法需要用到哈希算法，而设计一个可以应对各种类型字符的哈希算法并不简单。\n\n对于工业级的软件开发来说，我们希望算法尽可能的高效，并且在极端情况下，性能也不要退化的太严重。那么，对于查找功能是重要功能的软件来说，比如一些文本编辑器，它们的查找功能都是用哪些算法来实现的呢？有没有比bf算法和rk算法更加高效的字符串匹配算法呢？\n\n今天，我们就来学习bm(boyer-moore)算法。它是一种非常高效的字符串匹配算法，有实验统计，它的性能是著名的kmp算法的3到4倍。bm算法的原理很复杂，比较难懂，学起来会比较烧脑。\n\n\n# bm算法的核心思想\n\n我们把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，bf算法和rk算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。\n\n在这个例子里，主串中的c，在模式串中是不存在的，所以，模式串向后滑动的时候，只要c与模式串有重合，肯定无法匹配。所以，我们可以一次性把模式串往后多滑动几位，把模式串移动到c的后面。\n\n由现象找规律，我们可以思考一下，当遇到不匹配的字符时，有什么固定的规律，可以加个模式串往后多滑动几位呢？这样一次性往后滑动好几位，那匹配的效率岂不是就提高了？\n\n我们今天要学习的bm算法，本质上其实就是在寻找这种规律。借助这种规律，在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后躲滑动几位。\n\n\n# bm算法原理分析\n\nbm算法包含两部分，分别是坏字符规则和好后缀规则。下面来依次来看，这两个规则分别都是怎么工作的。\n\n\n# 坏字符规则\n\n前面两节讲的算法，在匹配的过程中，我们都是按模式串的下标从小到大的顺序，依次与主串中的字符进行匹配的。这种匹配顺序比较符合我们的思维习惯，而bm算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的。如下面的图示来看。\n\n\n\n调整后的匹配顺序是如下：\n\n\n\n我们从模式串的末尾往前倒着匹配，当我们发现某个字符没法匹配的时候。我们把这个没有匹配的字符叫做坏字符(主串中的字符)。\n\n\n\n我们拿坏字符c在模式串中查找，发现模式串中并不存在这个字符，也就说，字符c与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位，将模式串滑动到c后面的位置，再从模式串的末尾字符开始比较。\n\n滑动如下了：\n\n\n\n这个时候，我们发现，模式串中最后一个字符d，还是无法跟主串中的a匹配，这个时候，还能将模式串往后滑动三位吗？答案是不行的。因为这个时候，怀字符a在模式串中是存在的，模式串中下标是0的位置也是字符a。这种情况下，我们可以将模式串往后滑动两位，让两个a上下对齐，然后再从模式串的末尾字符开始，重新匹配。\n\n\n\n第一次不匹配的时候，我们滑动了三位，第二次不匹配的时候，我们将模式串后移两位，那具体滑动多少位，到底有没有规律呢？\n\n当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作si。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作xi。如果不存在，我们把xi记作-1。那模式串往后移动的位数就等于si - xi。(注意，我们这里说的下标，都是字符在模式串的下标)。\n\n\n\n这里我要特别说明一点，如果坏字符在模式串里多次出现，那我们在计算xi的时候，选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略多。\n\n利用坏字符规则，bm算法在最好情况下的时间复杂度非常低，是o(n/m)。比如，主串是aaabaaabaaabaaab，模式串是aaaa。每次对比，模式串都可以直接后移四位，所以，匹配具有类似特点的模式串和主串的时候，bm算法非常高效。\n\n不过，单纯使用坏字符规则还是不够的。因为根据si-xi计算出来的移动位数，有可能是负数，比如主串是aaaaaaaaaa，模式串是baaa。不但不会向后滑动模式串，还有可能倒退。所以，bm算法还需要用到"好后缀规则"。\n\n\n# 好后缀规则\n\n好后缀规则实际上跟坏字符规则的思路很类似。看下面的图示。当模式串滑动到图中的位置的时候，模式串和主串有2个字符是匹配的，倒数第3个字符发生了不匹配的情况。\n\n\n\n这个时候该如何滑动模式串呢？当然，我们还可以利用坏字符规则来计算模式串的滑动位数，不过，我们也可以使用好后缀处理规则。两种规则到底如何选择，这边会稍后讲解。抛开这个问题，现在我们来看，好后缀规则是怎么工作的？\n\n我们把已经匹配的bc叫作好后缀，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。\n\n\n\n如果在模式串中找不到另一个等于{u}的子串，我们就直接将模式串，滑动到主串中{u}的后面，因为之前的任何一次往后滑动，都没有匹配主串中{u}的情况。\n\n\n\n不过，当模式串中不存在等于{u}的子串的时候，我们直接将模式串滑动到主串{u}的后面。这样做是否太过了？我们可以来看下面的这个例子。这里面bc是好后缀，尽管在模式串中没有另外一个相匹配的子串{u*}，但是如果我们将模式串移动到好后缀的后面，如图所示，那就会错过模式串和主串可以匹配的情况。\n\n\n\n如果好后缀在模式串中不存在可匹配的子串，那在我们一步一步往后滑动模式串的过程中，只要主串中的{u}与模式串有重合，那肯定就无法完全匹配。但是当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况。\n\n\n\n所以，针对这种情况，我们不仅要看好后缀在模式串中，是否有另一个匹配的子串，我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。\n\n所谓某个字符串s的后缀子串，就是最后一个字符跟s对齐的子串，比如abc的后缀子串就包括c,bc。所谓前缀子串，就是起始字符跟s对其的子串，比如abc的前缀子串有a,ab。我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的，假设是{v}，然后将模式串滑动到如图所示的位置。\n\n\n\n坏字符和好后缀的基本原理都讲完了，现在回答一下前面那个问题。当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？\n\n我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。\n\n\n# bm算法代码实现\n\n\n# bm算法的性能分析及优化\n\n我们先来分析bm算法的内存消耗。整个算法用到了额外的3个数组，其中bc数组的大小跟字符集大小有关，suffix数组和prefix数组的大小跟模式串长度m有关。\n\n如果我们处理字符集很大的字符串匹配问题，bc数组对内存的消耗就会比较多。因为好后缀和坏字符规则是独立的，如果我们运行的环境对内存要去苛刻，可以只使用好后缀规则，不使用坏字符规则，这样就可以避免bc数组过多的内存消耗。不过，单纯使用好后缀规则的bm算法效率就会下降一些了。\n\n实际上，我前面讲的bm算法是个初步版本。为了能更容易理解，有些复杂的优化这里没有将。基于目前讲的这个版本，在极端情况下，预处理计算suffix数组、 prefix 数组的性能会比较差。\n\n比如模式串是aaaaaaa这种包含很多重复的字符的模式串，预处理的时间复杂度就是o(m^2)。当然，大部分情况下，时间复杂度不会这么差。\n\n\n# 解答开篇&内容小结\n\n今天，我们讲了一种比较复杂的字符串匹配算法，bm算法。尽管复杂、难懂，但匹配的效率却很高，在实际的软件开发中，特别是一些文本编辑器中，应用比较多。\n\nbm算法的核心思想是，利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。bm算法构建的规则有两类，坏字符规则和好后缀规则。好后缀规则可以独立于坏字符规则使用。因为坏字符规则的实现比较耗内存，为了节省内存，我们可以只用好后缀规则来实现bm算法。\n\n\n# d60(2020/12/18) 字符串匹配\n\n上一节我们讲了bm算法，尽管它很复杂，也不好立即，但确实工程中非常常用的一种高效字符串匹配算法。有统计说，它是最高效、最常用的字符串匹配算法。不过，在所有的字符串匹配算法里，要说最知名的一种的话，那就非kmp算法莫属。很多时候，提到字符串匹配，我们首先想要的就是kmp算法。\n\n尽管在实际的开发中，我们几乎不大可能自己亲手实现一个kmp算法。但是，学习这个算法的思想，可以开拓眼界、锻炼下逻辑思维，也是极好的。\n\n实际上，kmp算法跟bm算法的本质是一样的。上一节，我们讲了好后缀和坏字符规则，今天，我们就来看下，如何借助于上一节的bm算分的讲解思路，更好地理解kmp算法？\n\n\n# kmp算法基本原理\n\nkmp算法的核心思想，跟上一节讲的bm算法非常接近。我们假设主串是a，模式串是b。在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候，我们希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况。\n\n上一节中我们讲解到了好后缀和坏字符。这里我们可以类比一下，在模式串和主串匹配的过程中，把不能匹配的那个字符仍然叫做坏字符，把已经匹配的那段字符串叫做好前缀。\n\n\n\n当遇到坏字符的时候，我们就要把模式串往后滑动，在滑动的过程中，只要模式串和好前缀有上下重合，前面几个字符的比较，就相当于拿好前缀的后缀子串，跟模式串的前缀子串在比较。可不可以不用一个字符一个字符地比较？\n\n\n\nkmp算法就是在试图寻找一种规律：在模式串和主串匹配的过程中，当遇到坏字符后，对于已经比对过的好前缀，能否找到一种规律，将模式串一次性滑动很多位？\n\n\n# d61(2020/12/18) trie树\n\n搜索引擎的搜索关键词提示功能，为了方便快速输入，当我们在搜索引擎的搜索框中，输入要搜索的文字的某一部分的时候，搜索引擎就会自动弹出下拉框，里面是各种关键词提示。我们可以直接从下拉框中选择我们要搜索的东西，而不用把所有内容都输入进去，一定程度上节省了我们的搜索时间。\n\n像谷歌、百度这样的搜索引擎，它们的关键词提示功能非常全面和精准，肯定做了很多优化，但是万变不离其宗，底层最基本的原理就是今天要讲的这种数据结构：trie树。\n\n\n# 什么是"trie"树\n\ntrie树，也叫"字典树"。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。\n\n当然，这样一个问题可以有多种解决办法，比如散列表、红黑树，或者我们前面几节讲到的一些字符串匹配算法，但是，trie树在这个问题的解决上，有它特有的优点。不仅如此，trie树能解决的问题也不限于此。\n\n下面来看个简单的例子来说明一下。我们有6个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这6个字符串依次进行字符串匹配，那效率就比较低，有没有更高效的方法呢？\n\n这个时候，我们就可以先对这6个字符串做一下预处理，组织成trie树的结构，之后每次查找，都是在trie树中进行匹配查找。trie树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。最后构造出来的就是下面这个图中的样子。\n\n其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串(注意：红色节点并不都是叶子节点)。\n\n为了让我们更容易理解trie树是怎么构造出来的，这里画了一个trie树构造的分解过程。构造过程的每一步，都相当于往trie树中插入一个字符串。当所有字符串都插入完成之后，trie树就构造好了。\n\n\n\n当我们在trie树中查找一个字符串的时候，比如查找字符串"her"，那我们将要查找的字符串分隔成单个的字符h,e,r，然后从trie树的根节点开始匹配。如图所示，绿色的路径就是在trie树中匹配的路径。\n\n\n\n如果我们要查找的是字符串"he"呢？我们还用上面同样的方法，从根节点开始，沿着某条路径来匹配，如图所示，绿色的路径，是字符串"he"匹配的路径。但是，路径的最后一个节点"e"并不是红色的。也就是说，"he"是某个字符串的前缀子串，但并不能完全匹配任何字符串。\n\n\n\n\n# 如何实现一棵trie树？\n\n现在来看下，如何用代码来实现一个trie树。\n\n从刚刚trie树的介绍来看，trie树主要有两个操作，一个是将字符串集合构造成trie树。这个过程分解开来的话，就是一个将字符串插入到trie树的过程。另一个是在trie树中查询一个字符串。\n\n了解了trie树的两个主要操作之后，我们再来看下，如何存储一个trie树？\n\n从前面的图中，我们可以看出，trie树是一个多叉树。我们知道，二叉树中，一个节点的左右子节点是通过两个指针来存储的，如下所示java代码。那对于多叉树来说，我们怎么存储一个节点的所有子节点的指针呢？\n\nclass binarytreenode {\n  char data;\n  binarytreenode left;\n  binarytreenode right;  \n}\n\n\n1\n2\n3\n4\n5\n\n\n我们先介绍其中一个存储方式，也是经典的存储方式，大部分数据结构和算法书籍中都是这么讲解的。还记得我们前面讲到的散列表吗？借助散列表的思想，我们通过一个下标与字符一一映射的数组，来存储子节点的指针。这句话稍微有点抽象，不怎么好理解。\n\n假设我们的字符串只有a到z这26个小写字母，我们在数组中下标为0的位置，存储指向子节点a的指针，下标为1的位置存储指向子节点b的指针，以此类推，下标为25的位置，存储的是指向的子节点z的指针。如果某个字符的子节点不存在，我们就在对应的下标的位置存储null。\n\nclass trienode {\n  char data;\n  trienode children[26];\n}\n\n\n1\n2\n3\n4\n\n\n当我们在trie树中查找字符串的时候，我们就可以通过字符的ascii码减去"a"的ascii码，迅速找到匹配的子节点的指针。比如，d的ascii码减去a的ascii码就是3，那子节点d的指针就存储在数组中下标为3的位置中。\n\n把上面的描述翻译成了代码如下。\n\npublic class trie {\n  private trienode root = new trienode(\'/\'); // 存储无意义字符\n\n  // 往trie树中插入一个字符串\n  public void insert(char[] text) {\n    trienode p = root;\n    for (int i = 0; i < text.length; ++i) {\n      int index = text[i] - \'a\';\n      if (p.children[index] == null) {\n        trienode newnode = new trienode(text[i]);\n        p.children[index] = newnode;\n      }\n      p = p.children[index];\n    }\n    p.isendingchar = true;\n  }\n\n  // 在trie树中查找一个字符串\n  public boolean find(char[] pattern) {\n    trienode p = root;\n    for (int i = 0; i < pattern.length; ++i) {\n      int index = pattern[i] - \'a\';\n      if (p.children[index] == null) {\n        return false; // 不存在pattern\n      }\n      p = p.children[index];\n    }\n    if (p.isendingchar == false) return false; // 不能完全匹配，只是前缀\n    else return true; // 找到pattern\n  }\n\n  public class trienode {\n    public char data;\n    public trienode[] children = new trienode[26];\n    public boolean isendingchar = false;\n    public trienode(char data) {\n      this.data = data;\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n在trie树中，查找某个字符串的时间复杂度是多少？\n\n如果要在一组字符串中，频繁地查询某些字符串，用trie树会非常高效。构建trie树的过程，需要扫描所有的字符串，时间复杂度是o(n). n表示所有字符串的长度和。但是一旦构建成功之后，后续的查询操作会非常高效。\n\n每次查询的时候，如果要查询的字符串长度是k，那我们只需要比对大约k个节点，就能完成查询操作。跟原本那组字符串的长度和个数没有任何关系。所以说，构建好trie树后，在其中查找字符串的时间复杂度是o(k)，k表示要查找的字符串的长度。\n\n\n# trie树真的很耗内存吗？\n\n前面也讲了trie树的实现，也分析了时间复杂度。现在可以知道，trie树是一种非常独特的、高效的字符串匹配方法。但是，关于trie树，有一种说法是："trie树是非常耗内存的，用的是一种空间换时间的思路"。这是什么原因呢？\n\n刚刚我们在讲trie树的实现的时候，讲到用数组来存储一个节点的子节点的指针。如果字符串中包含从a到z这26个字符，那每个节点都要存储一个长度为26的数组，并且每个数组元素要存储一个8字节指针(或者是4字节，这个大小跟cpu、操作系统、编译器等有关)。而且，即便一个节点只有很少的子节点，远小于26个，比如3，4个，我们也要维护一个长度为26的数组。\n\n前面讲过，trie树的本质是避免重复存储一组字符串的相同前缀子串，但是现在每个字符(对应一个节点)的存储远远大于1个字节。按照我们上面举的例子，数组长度为26，每个元素是8字节，那么每个节点就会额外需要26*8=208个字节。而且这还是只包含26个字符的情况。\n\n如果字符串中不仅包含小写字母，还包含大写字母、数字、甚至是中文，那需要的存储空间就更多了。所以，也就是说，在某些情况下，trie树不一定会节省存储空间。在重复的前缀并不多的情况下，trie树不但不能节省内存，还有可能会浪费更多的内存。\n\n当然，我们不可否认，trie树尽管有可能很浪费内存，但是确实非常高效。那为了解决这个内存问题，我们是否有其他方法呢？\n\n我们可以稍微牺牲一点查询的效率，将每个节点中的数组换成其他数据结构，来存储一个节点的子节点指针。用哪种数据结构呢？我们的选择其实有很多，比如有序数组、跳表、散列表、红黑树等。\n\n假设我们用有序数组，数组中的指针按照所指向的子节点中的字符的大小顺序排列。查询的时候，我们可以通过二分查找的方法，快速查找到某个字符应该匹配的子节点的指针。但是，在往trie树中插入一个字符串的时候，我们为了维护数组中数据的有序性，就会稍微慢了点。\n\n替换成其他数据结构的思路是类似的。\n\n实际上，trie树的变体有很多，都可以在一定程度上解决内存消耗的问题。比如，缩点优化，就是对只有一个子节点的节点，而且此节点不是一个串的结束节点，可以将此节点与子节点合并。这样可以节省空间，但却增加了编码难度。这里我就不展开了。\n\n\n\n\n# trie树与散列表、红黑树的比较\n\n实际上，字符串的匹配问题，笼统上讲，其实就是数据的查找问题。对于支持动态数据高效操作的数据结构，例如散列表、红黑树、跳表等等。实际上，这些数据结构也可以实现在一组字符串中查找字符串的功能。我们选了你数据结构，散列表和红黑树，跟trie树比较一下，看看它们各自的优缺点和应用场景。\n\n在刚刚讲的这个场景，在一组字符串中查找字符串，trie树实际上表现得并不好。它对要处理的字符串有及其严格的要求。\n\n第一，字符串中包含的字符集不能太大。我们前面讲到，如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。\n\n第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。\n\n第三，如果要用trie树解决问题，那我们就要自己从零开始实现一个trie树，还要保证没有bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。\n\n第四，我们知道，通过指针串起来的数据库是不连续的，而trie树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。\n\n综合这几点，针对在一组字符串中查找字符串的问题，我们在工程中，更倾向于用散列表或者红黑树。因为这两种数据结构，我们都不需要自己去实现，直接利用编译语言中提供的现成类库就行了。\n\n那trie树是不是就没用了呢？实际上，trie树是不适合精确匹配查找，这种问题更适合用散列表或红黑树来解决。trie树比较适合的是查找前缀匹配的字符串。\n\n\n# 解答开篇\n\n如何利用trie树，实现搜索关键词的提示功能？\n\n我们假设关键词库由用户的热门搜索关键词组成。我们将这个词库构建成一个trie树。当用户输入其中某个单词的时候，把这个词作为一个前缀子串在trie树中匹配。为了将解方便，我们假设词库只有hello、her、hi、how、so、see这6个关键词。当用户输入了字母h的时候，我们就把以h为前缀的hello、her、hi、how展示在搜索提示框内。当用户继续键入字母e的时候我们就把以he为前缀的hello、her展示在搜索提示框内。这就是搜索关键词提示的最基本的算法原理。\n\n\n\n不过，我将的都只是最基本的实现原理，实际上，搜索引擎的搜索关键词提示功能远非我们讲的这么简单。如果再稍微深入一点，我们就会想到，上面的解决问题遇到下面几个问题：\n\n * 我刚讲的思路是针对英文的搜索关键词提示，对于更加复杂的中文来说，词库中的数据又该如何构建成trie树呢？\n * 如果词库中有很多关键词，在搜索提示的时候，用户输入关键词，作为前缀在trie树中可以匹配的关键词也有很多，如何选择展示哪些内容呢？\n * 像google这样的搜索引擎，用户单词拼写错误的情况下，google还是可以使用正确的拼写来做关键词提示，这个又是怎么做到的呢？\n\n实际上，trie树的这个应用可以扩展到更加广泛的一个应用上，就是自动输入补全，比如输入法自动补全功能、ide代码编辑器自动补全功能、浏览器网址输入的自动补全功能等等。\n\n\n# 内容小结\n\n今天我们讲了一种特殊的树，trie树。trie树是一种解决字符串快速匹配问题的数据结构。如果用来构建trie树的这一组字符串中，前缀重复的情况不是很多，那trie树这种数据结构总体上来讲是比较耗费内存的，是一种空间换时间的解决问题思路。\n\n尽管比较耗费内存，但是对内存不敏感或者内存消耗在接受范围内的情况下，在trie树中做字符串匹配还是比较高效的，时间复杂度是o(k)，k表示要匹配的字符串的长度。\n\n但是，trie树的优势并不在于，用它来做动态集合数据的查找，因为，这个工作完全可以用更加合适的散列表或红黑树来替代。trie树最有优势的是查找前缀匹配的字符串，比如搜索引擎中的关键词提示功能这个场景，就比较适合用它来解决，也是trie树比较经典的应用场景。\n\n\n# d62(2020/12/21) ac自动机\n\n今天要学习的内容是，如何用多模式串匹配实现敏感词过滤功能。\n\n很多支持用户发表的文本内容的网站，比如bbs，大都会有敏感词过滤功能，用来过滤掉用户输入的一些淫秽、反动、谩骂等内容。\n\n实际上，这些功能最基本的原理就是字符串匹配算法，也就是通过维护一个敏感词的字典，当用户输入一段文字内容之后，通过字符串匹配算法，来查找用户输入的这段文字，是否包含敏感词。如果有，就用星号替换掉。\n\n我们前面讲过的好几种字符串匹配算，它们都可以处理这个问题。但是，对于访问量巨大的网站来说，比如淘宝，用户每天的评论数有几亿、甚至几十亿。这个时候，我们对敏感词过滤系统的性能要求就要很高。毕竟，我们也不想，用户输入内容之后，要等几秒才能发送出去？我们也不想，为了这个功能耗费过多的机器。那如何才能实现一个高性能的敏感词过滤系统呢？这就是今天所说的多模式串匹配算法。\n\n\n# 基于单模式串和trie树实现的敏感词过滤\n\n我们前面几节讲了好几种字符串匹配算法，有bf算法、rk算法、bm算法、kmp算法，还有trie树。前面四种算法都是单模式匹配算法，只有trie树是多模式串匹配算法。\n\n我们讲解过，单模式串匹配算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串。多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。\n\n尽管，单模式串匹配算法也能完成多模式串的匹配工作。例如开篇的思考题，我们可以针对每个敏感词，通过单模式串匹配算法(比如kmp算法)与用户输入的文字内容进行匹配。但是，这样做的话，每个匹配过程都需要扫描一遍用户输入的内容。整个过程下来就要扫描很多遍用户输入的内容。如果敏感词很多，比如几千个，并且用户输入的内容很长，加入有上千个字符，那我们就需要扫描几千遍这样的输入内容。很显然，这种处理思路比较低效。\n\n与单模式匹配算法相比，多模式匹配算法在这个问题的处理上就很高效了。它只需要扫描一遍主串，就能在主串中一次性查找多个模式串是否存在，从而大大提高匹配效率。我们知道，trie树就是一种多模式匹配算法。那如何用trie树来实现敏感词过滤功能呢？\n\n我们可以对敏感词字典进行预处理，构建成trie树结构。这个预处理的操作只需要做一次，如果敏感字典动态更新了，比如删除、添加了一个敏感词，那我们只需要动态更新一下trie树就可以了。\n\n当用户输入一个文本内容后，我们把用户输入的内容作为主串，从第一个字符(假设是字符c)开始，在trie树中匹配。当匹配到trie树的叶子节点，或者中途遇到不匹配字符的时候，我们将主串的开始匹配位置后移一位，也就是从字符c的下一个字符开始，重新在trie树中匹配。\n\n基于trie树的这种处理方法，有点类似单模式串匹配的bf算分。我们知道，单模式串匹配算法中，kmp算法对bf算法进行改进，引入了next数组，让匹配失败时，尽可能将模式串往后多滑动几位。借鉴单模式串的优化改进方法，能否对多模式串trie树进行改进，进一步提高trie树的效率呢？这就要用到ac自动机算法了。\n\n\n# 经典的多模式串匹配算法：ac自动机\n\n其实，trie树跟ac自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟kmp算法之间的关系一样，只不过前者针对的是多模式串而已。所以，ac自动机实际上就是在trie树之上，加了类似kmp的next数组，只不过此处的next数组是构建在树上罢了。\n\n\n# 解答开篇\n\n\n# 内容小结\n\n\n# d63(2020/12/22) 贪心算法\n\n接下来的几节，我们会讲解几种更加基本的算法。它们分别是贪心算法、分治算法、回溯算法、动态规划。更加确切地说，它们应该是算法思想，并不是具体的算法，常用来指导我们设计具体的算法和编码等。\n\n贪心、分治、回溯、动态规划这4个算法思想，原理解释起来都很简单，但是要真正掌握且灵活应用，并不是件容易的事情。所以，接下来的这4个算法思想的讲解，我依旧不会长篇大论地去讲理论，而是结合具体的问题，让我们自己感觉这些算法是怎么工作的，是如何解决问题的，让我们在问题中体会这些算法的本质。我觉得，这比单纯记忆原理和定义要更有价值。\n\n今天，我们先来学习一下贪心算法。贪心算法有很多经典的应用，比如霍夫曼编码、prim和最小生成树算法、还有单源最短路径算法。今天讲解的是在霍夫曼编码中，是如何利用贪心算法来实现对数据压缩编码，有效节省数据存储空间的。\n\n\n# 如何理解"贪心算法"\n\n假设我们有一个可以容纳100kg物品的背包，可以装各种物品。我们有以下5种豆子，每种豆子的总量和总价值都各不相同。为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？\n\n\n\n实际上，这个问题很简单，没错，我只要先算一算每个物品的单价，按照单价由高到低依次来装就好了。单价从高到低排列，依次是：黑豆、绿豆、红豆、青豆、黄豆，所以，我们可以往背包里装20kg黑豆、30kg绿豆、50kg红豆。\n\n第一步，当我们看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。\n\n类比到刚刚的例子，限制值就是重量不能超过100kg，期望值就是物品的总价值。这组数据就是5种豆子。我们从中选出一部分，满足重量不超过100kg，并且总价值最大。\n\n第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。\n\n类比到刚刚的例子，我们每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。\n\n第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来看，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。\n\n实际上，用贪心算法解决问题的思路，并不总能给出最优解。\n\n这里来举一个例子。在一个有权图中，我们从顶点s开始，找一条到顶点t的最短路径(路径中边的权值和最小)。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点t。按照这种思路，我们求出的最短路径是s->a->e->t，路径长度是1+4+4=9\n\n\n\n但是，这种贪心的选择方式，最终求的路径并不是最短路径，因为路径s->b->d->t 才是最短路径，因为这条路径的长度是2+2+2 =6。为什么贪心算法在这个问题上不工作了呢？\n\n在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果我们第一步从顶点s走到顶点a，那接下来面对的顶点和边，跟第一步从顶点s走到顶点b，是完全不同的。所以，即便我们第一步选择最优的走法(边最短)，但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。\n\n\n# 贪心算法实战分析\n\n对于贪心算法，如果死抠理论的话，确实很难理解透彻。掌握贪心算法的关键是多练习。下面来分析几个具体的例子，帮助来深入理解贪心算法。\n\n\n# 分糖果\n\n我们有m个糖果和n个汉字。我们现在要把糖果分给这些孩子吃但是糖果少，孩子多(m<n)，所以糖果只能分配给一部分孩子。\n\n每个糖果的大小不等，这m个糖果的打下分别是s1,s2,s3, ....., sm。除此之外，每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。假设这n个孩子对糖果大小的需求分别是g1,g2,g3, .... , gn。\n\n我的问题是，如何分配糖果，能尽可能满足最多数量的孩子？\n\n我们可以把这个问题抽象成，从n个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数(期望值)是最大的。这个问题的限制值就是糖果个数m。\n\n现在来看如何用贪心算法来解决。对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。\n\n我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。\n\n\n# 钱币找零\n\n这个问题在我们的日常生活中更加普遍。假设我们有1元，2元，5元，10元，20元，50元，100元这些面额的纸币，它们的张数分别是c1、c2、c5、c20、c50、c100。我们现在要用这些钱来支付k元，最少用多少张纸币呢？\n\n在生活中，我们肯定要先用面值最大的来支付，如果不够，就继续用更小一点的面值的，以此类推，最后剩下的用1元来补齐。\n\n在贡献相同期望值(纸币数目)的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。直觉告诉我们，这种处理方法就是最好的。实际上，要严谨地证明这种贪心算法的正确性，需要比较复杂的、有技巧的数学推导。\n\n\n# 区间覆盖\n\n假设我们有n个区间，区间的起始端点和结束端点分别是[l1,r1]，[l2,r2]，[l3,r3]，...., [ln,rn]。我们从这n个区间中选出一部分区间，这部分区间满足两两不想交(端点相交的情况不算相交)，最多能选出多少个区间呢？\n\n\n\n这个问题的处理思路稍微不是那么好懂，不过，我建议你最好能弄懂，因为这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。\n\n这个问题的解决思路是这样的：我们假设这n个区间中最左端点是lmin，最右端点是rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将[lmin, rmax]覆盖上。我们按照起始端点从小到大的顺序对这n个区间排序。\n\n我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。\n\n\n\n\n# 解答开篇\n\n现在我们来看下开篇的问题，如何用贪心算法实现霍夫曼编码？\n\n假设我有一个包含1000个字符的文件，每个字符占1个byte (1byte=8bits)，存储这1000个字符就一共需要8000bits，那有没有更加节省空间的存储方式呢？\n\n假设我们通过统计分析发现，这1000个字符中只包含6种不同字符，假设它们分别是a、b、c、d、e、f。而3个二进制位(bit)就可以表示8个不同的字符，所以，为了尽量减少存储空间，每个字符我们用3个二进制位来表示。那存储这1000个字符只需要3000bits就可以了，比原来的存储方式节省了很多空间。不过，还有没有更加节省空间的存储方式呢？\n\na(000)、b(001)、c(010)、d(011)、e(100)、f(101)\n\n\n1\n\n\n霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在20% ~ 90%之间。\n\n霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码视图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？ 根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长的一些编码。\n\n对于等长的编码来说，我们解压缩起来很简单。比如刚才那个例子中，我们用3个bit表示一个字符。在解压缩的时候，我们每次从文本中读取3位二进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取1位还是2位、3位等等来解压缩呢？这个问题就导致霍夫曼编码解压缩起来比较复杂。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。\n\n\n\n假设这6个字符出现的频率从高到低依次是a、b、c、d、e、f。我们把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀，在解压缩的时候，我们每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这1000个字符只需要2100bits就可以了。\n\n\n\n尽管霍夫曼编码的思想并不难理解，但是如何根据字符出现的频率的不同，给不同的字符进行不同长度的编码呢？这里的处理稍微有些技巧。\n\n我们把每个字符看作一个节点，并且附带着把频率放到优先级队列中。我们从队列中取出频率最小的两个节点a、b，然后新建一个节点c，把频率设置为两个节点的频率之和，并把这个新节点c作为a、b的父节点。最后再把c节点放入到优先级队列中。重复这个过程，直到队列中没有数据。\n\n\n\n现在，我们给每一条边加上画一个权值，指向左子节点的边我们统统标记为0，指向右子节点的边，我们统统标记为1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。\n\n\n\n\n# 内容小结\n\n今天我们学习了贪心算法。\n\n实际上，贪心算法适用的场景比较有限。这种算法思想更多的是指导设计基础算法。比如最小生成树算法、单源最短路径算法，这些算法都用到了贪心算法。从我个人的学习经验来讲，不要刻意去记忆贪心算法的原理，多练习才是最有效的学习方法。\n\n贪心算法的最难的一块是如何将要解决的问题抽象成贪心算法模型，只要这一步搞定之后，贪心算法的编码一般都很简单。贪心算法解决问题的正确性虽然很多时候都看起来是显而易见的，但是要严谨地证明算法能够得到最优解，并不是件容易的事。所以，很多时候，我们只需要多举几个例子，看一下贪心算法的解决方案是否真的能得到最优解就可以了。\n\n\n# d64(2020/12/23) 分治算法\n\nmapreduce是谷歌大数据处理的三驾马车之一，另外两个是gfs和bigtable。它在倒排索引、pagerank计算、网页分析等搜索引擎相关的技术中都有大量的应用。\n\n尽管开发一个mapreduce看起来很高深，感觉跟我们遥不可及。实际上，万变不离其宗，它的本质就是我们今天要学的这种算法思想，分治算法。\n\n\n# 如何理解分治算法？\n\n为什么说mapredue的本质就是分治算法呢？我们先来看，什么是分治算法？\n\n分治算法的核心思想其实就是四个字，分而治之，也就是将原问题划分成n个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。\n\n这个定义看起来有点难类似递归的定义。关于分治和递归的区别，我们在排序(下)的时候讲过，分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现呢。分治算法的递归实现中，每一层递归都会涉及这样三个操作：\n\n * 分解：将原问题分解成一系列子问题；\n * 解决：递归地求解各个子问题，若子问题足够小，则直接求解；\n * 合并：将子问题的结果合并成原问题。\n\n分治算法能解决的问题，一般需要满足下面这几个条件：\n\n * 原问题与分解成的小问题具有相同的模式；\n * 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；\n * 具有分解终止条件，也就是说，当问题足够小时，可以直接求解；\n * 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。\n\n\n# 分治算法应用举例分析\n\n理解分治算法的原理并不难，但是要想灵活应用并不容易。所以，接下来，我们会用分治算法解决我们在讲排序的时候涉及的一个问题，加深我们对分治算法的理解。\n\n还记得我们在排序算法里讲到的数据的有序度、逆序度的概念吗？当时我们讲到，我们用有序度来表示一组数据的有序程度，用逆序度表示一组数据的无序程度。\n\n假设我们有n个数据，我们期望数据从小到大排序，那完全有序的数据的有序度就是n(n-1)/2，逆序度等于0；相反，倒序排列的数据的有序度就是0，逆序度是n(n-1)/2。除了这两种极端情况外，我们通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。\n\n\n\n现在的问题是，如何编程求出一组数据的有序对个数或逆序对个数呢？因为有序对个数和逆序对个数的求解方式是类似的，所以我们只需要思考逆序对个数的求解方法。\n\n最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的k值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是o(n^2)。那有没有更加高效的处理方法呢？\n\n我们用分治算法来试试。我们套用分治的思想来求数组a的逆序对个数。我们可以将数组分成前后两半a1和a2，分别计算a1和a2的逆序对个数k1和k2，然后再计算a1与a2之间的逆序对个数k3。那数组a的逆序对个数就等于k1+k2+k3。\n\n我们前面讲过，使用分治算法其中一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了。那回到这个问题，如何快速计算出两个子问题a1与a2之间的逆序对个数呢？\n\n这里就要借助归并排序算法了。\n\n归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。实际上，在这个合并的过程中，我们就可以计算这两个小数组的逆序对个数了。每次合并操作，我们都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。\n\n\n\n具体的代码如下：\n\nprivate int num = 0; // 全局变量或者成员变量\n\npublic int count(int[] a, int n) {\n  num = 0;\n  mergesortcounting(a, 0, n-1);\n  return num;\n}\n\nprivate void mergesortcounting(int[] a, int p, int r) {\n  if (p >= r) return;\n  int q = (p+r)/2;\n  mergesortcounting(a, p, q);\n  mergesortcounting(a, q+1, r);\n  merge(a, p, q, r);\n}\n\nprivate void merge(int[] a, int p, int q, int r) {\n  int i = p, j = q+1, k = 0;\n  int[] tmp = new int[r-p+1];\n  while (i<=q && j<=r) {\n    if (a[i] <= a[j]) {\n      tmp[k++] = a[i++];\n    } else {\n      num += (q-i+1); // 统计p-q之间，比a[j]大的元素个数\n      tmp[k++] = a[j++];\n    }\n  }\n  while (i <= q) { // 处理剩下的\n    tmp[k++] = a[i++];\n  }\n  while (j <= r) { // 处理剩下的\n    tmp[k++] = a[j++];\n  }\n  for (i = 0; i <= r-p; ++i) { // 从tmp拷贝回a\n    a[p+i] = tmp[i];\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n但是，如果我告诉你可以借助归并排序算法来解决，那我们就应该要想到如何改造归并排序，来求解这个问题了。\n\n关于分治算法，这里还有两道比较经典的问题，我们可以自己练习一下。\n\n * 二维平面上有n个点，如何快速计算出两个距离最近的点对？\n * 有两个n*n的矩阵a，b，如何快速求解两个矩阵的乘积c=a*b ？\n\n\n# 分治思想在海量数据处理中的应用\n\n分治算法思想的应用是非常广泛的，并不仅局限于指导编程和算法设计。它还经常用在海量数据处理的场景中。我们前面讲的数据结构和算法，大部分都是基于内存存储和单机处理。但是，如果要处理的数据量非常大，没法一次性放到内存中，这个时候，这些数据结构和算法就无法工作了。\n\n比如，给10gb的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有10gb，而我们的机器的内存可能只有2、3gb这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。\n\n要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想。我们可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据结合合并成大数据集合。实际上，利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或多机处理，加快处理的速度。\n\n比如刚刚举的那个例子，给10gb的订单排序，我们就可以先扫描一遍订单，根据订单的金额，将10gb的文件划分为几个金额区间。比如订单净额为1到100元的放到一个小文件，101到200之间的放到另一个文件，以此类推。这样每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终的10gb订单数据了。\n\n如果订单数据存储在类似gfs这样的分布式系统上，当10gb的订单被划分成多个小文件的时候，每个文件可以并行加载到多台机器上处理，最后再将结果合并在一起，这样并行处理的速度也加快了很多。不过，这里有一个点要注意，就是数据的存储与计算所在的机器是同一个或者在网络中靠的很近(比如一个局域网内，数据存取速度很快)，否则就会因为数据访问的速度，导致整个处理过程不但不会变快，反而有可能变慢。\n\n\n# 解答开篇\n\n这里我们来看下，为什么说mapreduce的本质就是分治思想呢？\n\n我们刚刚举的订单的例子，数据有10gb大小，可能给你的感受还不强烈。那如果我们要处理的数据是1t、10t、100t这样子的，那一台机器处理的效率肯定是非常低的。而对于谷歌搜索引擎来说，网页爬取、清洗、分析、分词、计算权重、倒排索引等等各个环节中，都会面对如此海量的数据(比如网页)。所以，利用集群并行处理显然是大势所趋。\n\n一台机器过于低效，那我们就把任务拆分到多台机器上来处理。如果拆分之后的小任务之间互不干扰，独立计算，最后再将结果合并。这不就是分治思想吗？\n\n实际上，mapreduce框架只是一个任务调度器，底层依赖gfs来存储数据，依赖borg管理机器。它从gfs中拿数据，交给borg中的机器执行，并且时刻监控机器执行的进度，一旦出现机器宕机、进度卡壳等，就重新从borg中调度一台机器执行。\n\n尽管mapreduce的模型非常简单，但是在谷歌内部引用非常广泛。它除了可以用来处理这种数据与数据之间存在关系的任务，比如mapreduce的经典例子，就是统计文件中单词出现的频率。除此之外，它还可以用来处理数据与数据之间没有关系的任务，比如对网页分析、分词等，每个网页可以独立的分析、分词，而这两个网页之间并没有关系。网页几十亿、上百亿，如果单机处理，效率低下，我们就可以利用mapreduce提供的高可靠、高性能、高容错的并行计算框架，并行地处理这几十亿、上百亿的网页。\n\n\n# 内容小结\n\n今天我们讲解了一种应用非常广泛的算法思想，分治算法。\n\n分治算法用四个字概括就是"分而治之"，将原问题划分成n个规模较小而结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。这个思想非常简单，好理解。\n\n今天我们讲了两种分治算法的典型的应用场景，一个是用来指导编码，降低问题求解的时间复杂度，另一个是解决海量数据处理问题。比如mapreduce本质上就是利用了分治思想。\n\n\n# d65(2020/12/25) 回溯算法\n\n在之前我们学习图的过程中，提到了深度优先搜索算法利用的是回溯算法思想。这个算法思想非常简单，但是应用内非常广泛，它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。\n\n除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1背包、图的着色、旅行商问题、全排列等等。既然应用如此广泛，我们今天就来学习一下这个算法思想，看看它是如何知道我们解决问题的？\n\n\n# 如何理解"回溯算法"？\n\n在我们的一生中，会遇到很多重要的岔路口。在岔路口，每个选择都会影响我们今后的人生。有的人在每个岔路口都能做出最正确的选择，最后生活、事业都达到了一个很高的高度；而有的人一路选错，最后碌碌无为。如果人生可以量化，那如何才能在岔路口做出最正确的选择，让自己的人生"最优"呢？\n\n我们可以借助前面学过的贪心算法，在每次面对岔路口的时候，都做出看起来最优的选择，期望这一组选择可以使得我们的人生达到"最优"。但是，我们前面也讲过，贪心算法并不一定能得到最优解。那有没有什么办法能得到最优解呢？\n\n笼统地讲，回溯算法很多时候都应用在"搜索"这类问题上。不过这里说的搜索，并不是狭义的指我们前面讲过的图的搜索算法，而是在一组可能的解中，搜索满足期望的解。\n\n回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候(不符合期望的解)，就回退到上一个岔路口，另选一种走法继续走。\n\n理论的东西还是过于抽象，老规矩，这里还是举例说明一下。这里举一个经典的回溯的例子，八皇后的问题。\n\n我们有一个8*8的棋盘，希望往里放8个棋子(皇后)，每个棋子所在的行、列、对角线都不能有另一个棋子。可以看下面的图示，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。\n\n\n\n我们把这个问题划分成8个阶段，依次将8个棋子放到第一行、第二行、第三行....第八行。在放置的过程中，我们不停地检查当前放法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种放法，继续尝试。\n\n回溯算法非常适合用递归代码实现，所以，我把八皇后的算分翻译成了代码。我在代码里添加了详细的注释，可以对比看看。\n\nint[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列\npublic void cal8queens(int row) { // 调用方式：cal8queens(0);\n  if (row == 8) { // 8个棋子都放置好了，打印结果\n    printqueens(result);\n    return; // 8行棋子都放好了，已经没法再往下递归了，所以就return\n  }\n  for (int column = 0; column < 8; ++column) { // 每一行都有8中放法\n    if (isok(row, column)) { // 有些放法不满足要求\n      result[row] = column; // 第row行的棋子放到了column列\n      cal8queens(row+1); // 考察下一行\n    }\n  }\n}\n\nprivate boolean isok(int row, int column) {//判断row行column列放置是否合适\n  int leftup = column - 1, rightup = column + 1;\n  for (int i = row-1; i >= 0; --i) { // 逐行往上考察每一行\n    if (result[i] == column) return false; // 第i行的column列有棋子吗？\n    if (leftup >= 0) { // 考察左上对角线：第i行leftup列有棋子吗？\n      if (result[i] == leftup) return false;\n    }\n    if (rightup < 8) { // 考察右上对角线：第i行rightup列有棋子吗？\n      if (result[i] == rightup) return false;\n    }\n    --leftup; ++rightup;\n  }\n  return true;\n}\n\nprivate void printqueens(int[] result) { // 打印出一个二维矩阵\n  for (int row = 0; row < 8; ++row) {\n    for (int column = 0; column < 8; ++column) {\n      if (result[row] == column) system.out.print("q ");\n      else system.out.print("* ");\n    }\n    system.out.println();\n  }\n  system.out.println();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 两个回溯算法的经典应用\n\n回溯算法的理论知识很容易弄懂。不过，对于新手来说，比较难的是用递归来实现。所以，我们再通过两个例子，来练习一下回溯算法的应用和实现。\n\n\n# 0-1背包\n\n0-1背包是非常经典的算法问题，很多场景都可以抽象成这个问题模型。这个问题的经典解法是动态规划，不过还有一种简单但没有那么高效的解法，那就是今天讲的回溯算法。\n\n0-1背包问题有很多变体，这里介绍一种比较基础的。我们有一个背包，背包总的承载重量是wkg。现在我们有n个物品，每个物品的重量不等，并且不可分割。我们现在期望选择几件物品，装载到背包中。在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大？\n\n实际上，背包问题我们在贪心算法那一节，已经讲过一个了，不过那里讲的物品是可以分割的，我可以装某个物品的一部分到背包里面。今天讲的这个背包问题，物品是不可分割的，要么装要么不装，所以叫0-1背包问题。显然，这个问题已经无法通过贪心算法来解决了。我们现在来看看，用回溯算法如何来解决。\n\n对于每个物品来说，都有两种选择，装进背包或不装进背包。对于n个物品来说，总的装法就有2^n种，去掉总重量超过wkg的，从剩下的装法中选择总重量最接近wkg的。不过，我们如何才能不重复地穷举出这2^n种装法呢？\n\n这里就可以用回溯的方法。我们可以把物品依次排列，整个问题就分解为了n个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或不装进去，然后再递归地处理剩下的物品。\n\n这里还稍微用到了一点搜索减枝的技巧，就是当发现已经选择的物品的总量超过1wkg之后，我们就停止继续探测剩下的物品。\n\npublic int maxw = integer.min_value; //存储背包中物品总重量的最大值\n// cw表示当前已经装进去的物品的重量和；i表示考察到哪个物品了；\n// w背包重量；items表示每个物品的重量；n表示物品个数\n// 假设背包可承受重量100，物品个数10，物品重量存储在数组a中，那可以这样调用函数：\n// f(0, 0, a, 10, 100)\npublic void f(int i, int cw, int[] items, int n, int w) {\n  if (cw == w || i == n) { // cw==w表示装满了;i==n表示已经考察完所有的物品\n    if (cw > maxw) maxw = cw;\n    return;\n  }\n  f(i+1, cw, items, n, w);\n  if (cw + items[i] <= w) {// 已经超过可以背包承受的重量的时候，就不要再装了\n    f(i+1,cw + items[i], items, n, w);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 正则表达式\n\n看懂了0-1背包问题，我们再来看另外一个例子，正则表达式匹配。\n\n对于一个开发工程师来说，正则表达式应该不会陌生。在平时的开发中，或多或少都应该用过。实际上，正则表达式里最重要的一种算法思想就是回溯。\n\n正则表达式中，最重要的就是通配符，通配符结合在一起，可以表达非常丰富的语义。为了方便讲解，我们假设正则表达式中只包含"*"和"?"这两种通配符，并且对这两个通配符的语义稍微做些改变，其中，"*"匹配任意多个(大于等于0个)任意字符，"?"匹配零个或者一个任意字符。基于以上背景假设，我们看下，如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？\n\n我们依次考察正则表达式的每个字符，当是非通配符的时候，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。\n\n如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口，比如"*"有多种匹配方案，可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。\n\n有了前面的基础，这个问题就好懂多了。可以详细看下下面的代码。\n\npublic class pattern {\n  private boolean matched = false;\n  private char[] pattern; // 正则表达式\n  private int plen; // 正则表达式长度\n\n  public pattern(char[] pattern, int plen) {\n    this.pattern = pattern;\n    this.plen = plen;\n  }\n\n  public boolean match(char[] text, int tlen) { // 文本串及长度\n    matched = false;\n    rmatch(0, 0, text, tlen);\n    return matched;\n  }\n\n  private void rmatch(int ti, int pj, char[] text, int tlen) {\n    if (matched) return; // 如果已经匹配了，就不要继续递归了\n    if (pj == plen) { // 正则表达式到结尾了\n      if (ti == tlen) matched = true; // 文本串也到结尾了\n      return;\n    }\n    if (pattern[pj] == \'*\') { // *匹配任意个字符\n      for (int k = 0; k <= tlen-ti; ++k) {\n        rmatch(ti+k, pj+1, text, tlen);\n      }\n    } else if (pattern[pj] == \'?\') { // ?匹配0个或者1个字符\n      rmatch(ti, pj+1, text, tlen);\n      rmatch(ti+1, pj+1, text, tlen);\n    } else if (ti < tlen && pattern[pj] == text[ti]) { // 纯字符匹配才行\n      rmatch(ti+1, pj+1, text, tlen);\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n\n# 内容小结\n\n回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。\n\n\n# d66(2020/12/26) 动态规划\n\n淘宝的"双十一"购物节有各种促销活动，比如"满200减50元"。假设购物车中有n个(n>100)想买的商品，她希望从里面选几个，在凑满满减条件的前提下，让选出来的商品价格和最大程度低接近满减条件(200元)，这样就可以极大限度地"薅羊毛"。\n\n要想高效地解决这个问题，就要用到我们今天讲的动态规划。\n\n\n# 动态规划学习路线\n\n动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。不过，它也是出了名的难学。它的主要学习难点跟递归类似，那就是，求解问题的过程不太符合人类常规的思维方式。对于新手来说，要想入门确实不容易。不过，等我们掌握了之后，就会发现，实际上并没有想象中那么难。\n\n为了让我们更容易理解动态规划，这里分了三节来讲解。这三节分别是，初识动态规划、动态规划理论、动态规划实战。\n\n第一节，我们会通过两个非常经典的动态规划问题模型，来展示我们为什么需要动态规划，以及动态规划解题方法是如何演化过来的。实际上，我们只要掌握了这两个例子的解决思路，对于其他很多动态规划问题，我们都可以套用类似的思路来解决。\n\n第二节，我们会总结动态规划适合解决的问题的特征，以及动态符合解题思路。除此之外，我们还会将贪心、分治、回溯、动态规划这四种算法思想放在一起，对比分析它们各自的特点以及适用的场景。\n\n第三节，我们会讲解如何应用第二节中讲的动态规划理论知识，实战解决三个非常经典的动态规划问题，加深对理论的理解。弄懂了这三节中的例子，对于动态规划这个知识点，我们就算是入门了。\n\n\n# 0-1背包问题\n\n我们在讲贪心算法、回溯算法的时候，多次讲到背包问题。现在，我们依旧拿这个问题来举例。\n\n关于这个问题，我们上一节讲了回溯的解决方法，也就是穷举搜索所有的可能的装法，然后找出满足条件的最大值。不过，回溯算法的复杂度比较高，是指数级别的。那有没有什么规律，可以有效降低时间复杂度呢？\n\n// 回溯算法实现。注意：我把输入的变量都定义成了成员变量。\nprivate int maxw = integer.min_value; // 结果放到maxw中\nprivate int[] weight = {2，2，4，6，3};  // 物品重量\nprivate int n = 5; // 物品个数\nprivate int w = 9; // 背包承受的最大重量\npublic void f(int i, int cw) { // 调用f(0, 0)\n  if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了\n    if (cw > maxw) maxw = cw;\n    return;\n  }\n  f(i+1, cw); // 选择不装第i个物品\n  if (cw + weight[i] <= w) {\n    f(i+1,cw + weight[i]); // 选择装第i个物品\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n规律是不是不好找？那我们就举了例子、画个图看看。我们假设背包的最大承载重量是9。我们有5个不同的物品，每个物品的重量分别是2，2，4，6，3。如果我们把这个例子的回溯求解过程，用递归树画出来，就是下面这个样子。\n\n\n\n递归树中的每个节点表示一种状态，我们用(i,cw)来表示。其中，i表示将要决策第几个物品是否装入背包，cw表示当前背包中物品的总重量。\n\n\n# 内容小结\n\n从今天讲解的例子中来看，应该能发现，大部分动态规则能解决的问题，都可以通过回溯算法来执行，只不过回溯算法解决起来效率比较地，时间复杂度是指数级的。动态规划算法，在执行效率方面，要高很多。尽管执行效率提高了，但是动态规划的空间复杂度也提高了，所以，很多时候，我们会说，动态规划是一种空间换时间的算法思想。\n\n\n# d67(2020/12/26) b+树索引\n\n数据库索引是如何实现的呢？底层使用的是什么数据结构和算法呢？\n\n\n# 算法解析\n\n\n# 解决问题的前提是定义清楚问题\n\n如何定义清楚问题呢？除了对问题进行详细的调研，还有一个办法，那就是，通过对一些模糊的需求进行假设，来限定要解决的问题的范围。\n\n如果我们对数据库的操作非常了解，针对我们现在这个问题，我们就能把索引的需求定义得非常清楚。但是，对于大部分软件工程师来说，我们可能只了解一小部分常用的sql语句，所以，这里我们假设要解决的问题，只包含这样两个常用的需求：\n\n * 根据某个值查找数据，比如 select * from user where id =1234;\n * 根据区间值来查找某些数据，比如select * from user where id > 1234 and id < 2345;\n\n在执行效率方面，我们希望通过索引，查询数据的效率尽可能地高；在存储空间方面，我们希望索引不要消耗太多的内存空间。\n\n\n# 尝试用学过的数据结构解决这个问题\n\n问题的需求大致定义清楚了，我们现在回想一下，能否利用已经学习过的数据结构解决这个问题呢？支持快速查询、插入等操作的动态数据结构，我们已经学习过散列表、平衡二叉树、跳表。\n\n我们先来看散列表。散列表的查询性能很好，时间复杂度是o(1)。但是，散列表不能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。\n\n我们再来看平衡二叉查找树。尽管平衡二叉查找树查询的性能也很高，时间复杂度是o(logn)。而且，对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但是这个仍然不足以支持按照区间快速查找数据。\n\n我们再来看跳表。跳表是在链表之上加上多层索引构成的。它支持快速地插入、查找、删除数据，对应的时间复杂度是o(logn)。并且，跳表也支持按照区间快速地查找数据。我们只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止，这期间遍历得到的数据就是满足区间值的数据。\n\n\n\n这样看来，跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫做b+树。不过，它是通过二叉查找树演化过来的，而非跳表。为了还原发明b+树的整个思考过程，所以，接下来，我们还要从二叉查找树讲起，看它是如何一步一步被改造成b+树的。\n\n\n# 改造二叉查找树来解决这个问题\n\n为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造：树中的节点并不存储数据本身，而是只是作为索引。除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的。经过改造之后的二叉树，就像图中这样，看起来很像跳表。\n\n\n\n改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后，我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。所有遍历到的数据，就是符合区间值的所有数据。\n\n\n\n但是，我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。\n\n比如，我们给一亿个数据构建二叉查找数索引，那索引中会包含大约1亿个节点，每个节点假设占用16个字节，那就需要大约1gb的内存空间。给一张表建立索引，我们需要1gb的内存空间。如果我们要给10张表建立索引，那就内存的需求是无法满足的。如何解决这个索引占用太多内存的问题呢？\n\n我们可以借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。\n\n这种将索引存储在硬盘中的方案，尽管减少了内存消耗，但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。\n\n二叉查找树，经过改造之后，支持区间查找的功能就实现了。不过，为了节省内存，如果把树存储在硬盘中，那么每个节点的读取(或者访问)，都对应一次磁盘io操作。树的高度就等于每次查找数据时磁盘io操作的次数。\n\n我们前面讲到，比起内存读写操作，磁盘io操作非常耗时，所以我们优化的重点就是尽量减少磁盘io操作，也就是，尽量降低树的高度。那如何降低树的高度呢？\n\n我们来看下，如果我们把索引构建成m叉树，高度是不是比二叉树要小呢？如图所示，给16个数据构建二叉树索引，树的高度是4，查找一个数据，就需要4个磁盘io操作(如果根节点存储在内存中，其他节点存储在磁盘中)，如果对16个数据构建五叉树索引，那高度只有2，查找一个数据，对应只需要2次此案操作。如果m叉树中的m是100，那对一亿个数据构建索引，树的高度也只有3，最多只要3次磁盘io就能获取到数据。磁盘io变少了，查找数据的效率也就提高了。\n\n\n\n\n\n如果我们将m叉树实现b+树索引，用代码实现出来，就是下面这个样子(假设我们给int类型的数据库字段添加索引，所以代码中的keywords是int类型的):\n\n/**\n * 这是b+树非叶子节点的定义。\n *\n * 假设keywords=[3, 5, 8, 10]\n * 4个键值将数据分为5个区间：(-inf,3), [3,5), [5,8), [8,10), [10,inf)\n * 5个区间分别对应：children[0]...children[4]\n *\n * m值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：\n * page_size = (m-1)*4[keywordss大小]+m*8[children大小]\n */\npublic class bplustreenode {\n  public static int m = 5; // 5叉树\n  public int[] keywords = new int[m-1]; // 键值，用来划分数据区间\n  public bplustreenode[] children = new bplustreenode[m];//保存子节点指针\n}\n\n/**\n * 这是b+树中叶子节点的定义。\n *\n * b+树中的叶子节点跟内部节点是不一样的,\n * 叶子节点存储的是值，而非区间。\n * 这个定义里，每个叶子节点存储3个数据行的键值及地址信息。\n *\n * k值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：\n * page_size = k*4[keyw..大小]+k*8[dataad..大小]+8[prev大小]+8[next大小]\n */\npublic class bplustreeleafnode {\n  public static int k = 3;\n  public int[] keywords = new int[k]; // 数据的键值\n  public long[] dataaddress = new long[k]; // 数据地址\n\n  public bplustreeleafnode prev; // 这个结点在链表中的前驱结点\n  public bplustreeleafnode next; // 这个结点在链表中的后继结点\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n\n\n对于相同个数的数据构建m叉树索引，m叉树中的m越大，那树的高度就越小，那m叉树中的m是不是越大越好呢？到底多大才最合适呢？\n\n不管是内存中的数据，还是磁盘中的数据，操作系统都是按页(一页大小通常是4kb，这个值可以通过getconfig page_size命令查看)来读取的，一次会读一页的数据。如果要读取的数据量超过一页的大小，就会触发多次io操作。所以，我们在选择m大小的时候，要尽量让每个节点的大小等于一个页的大小。读取一个节点，只需要一次磁盘io操作。\n\n\n\n尽管索引可以提高数据库的查询效率，但是，作为一名开发工程师，我们应该也知道，索引有利也有弊，它也会让写入数据的效率下降。这是为什么呢？\n\n数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。\n\n对于一个b+树来说，m值是根据页的大小事先计算好的，也就是说，每个节点最多只能有m个子节点。在往数据库中写入数据的过程中，这样就有可能使索引中某些节点的子节点个数超过m，这个节点的大小超过了一个页的大小，读取这样一个节点，就会导致多次磁盘io操作。我们该如何解决这个问题呢？\n\n实际上，处理思路并不复杂。我们只需要将这个节点分裂成两个节点。但是，节点分裂之后，其上层父节点的子节点个数就有可能超过m个。不过这也没关系，我们可以用同样的方法，将父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点。这个分裂过程，我们可以结合着下面这个图一起看，会更容易理解(图中的b+树是一个三叉树。我们限定叶子节点中，数据的个数超过2个就分裂节点；非叶子节点中，子节点的个数超过3个就分裂节点)。\n\n\n\n正是因为要时刻保证b+树索引是一个m叉树，所以，索引的存在会导致数据库写入的速度降低。实际上，不光写入数据会变慢，删除数据也会变慢。这是为什么呢？\n\n我们在删除某个数据的时候，也要对应地更新索引节点。这个处理思路有点类似跳表中删除数据的处理思路。频繁的数据删除，就会导致某些节点中，子节点的个数变得非常少，长此以往，如果每个节点的子节点都比较少，势必会影响索引的效率。\n\n我们可以设置一个阀值。在b+树中，这个阀值等于m/2。如果某个节点的子节点个数小于m/2，我们就将它跟相邻的兄弟节点合并。不过，合并之后节点的子节点个数有可能会超过m。针对这种情况，我们可以借助插入数据时候的处理方法，再分裂节点。\n\n文字描述不是很直观，这里举了一个删除操作的例子，我们可以对比看着下(图中的b+树是一个五叉树。我们限定叶子节点中，数据的个数少于2个就合并节点；非叶子节点中，子节点的个数少于3个就合并节点。)\n\n\n\n数据库索引以及b+树的由来，到此已经讲完了。我们可以发现，b+树的结构和操作，跟跳表非常类似。理论上讲，对跳表稍加改造，也可以替代b+树，作为数据库的索引实现的。\n\n\n# 总结引申\n\n今天，我们讲解了数据库索引实现，依赖的底层数据结构，b+树。它通过存储在磁盘的多叉树结构，做到了时间、空间的平衡，既保证了执行效率，又节省了内存。\n\n前面的讲解中，为了一步一步详细地给你介绍b+树的由来，内容看起来比较零散。为了方便掌握和记忆，这里再总结一下b+树的特点：\n\n * 每个节点中子节点的个数不能超过m，也不能小于m/2;\n * 根节点的子节点个数可以不超过m/2，这是一个例外；\n * m叉树只存储索引，并不真正存储数据，这个有点类似跳表；\n * 通过链表将叶子节点串联在一起，这样可以方便按区间查找；\n * 一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。\n\n除了b+树，可能还听说过b树、b-树。实际上，b-树就是b树，英文翻译为b-tree，这里的"-"并不是相对b+树中的"+"，而只是一个连接符。\n\n而b树实际上是低级版的b+树，或者说b+树是b树的改进版。b树跟b+树的不同点主要集中在这几个地方：\n\n * b+树中的节点不存储数据，只是索引，而b树中的节点存储数据；\n * b树中的叶子节点并不需要链表来串联。\n\n也就是说，b树只是一个每个节点的子节点个数不能小于m/2的m叉树。\n\n\n# d68(2020/12/27) 索引：如何在海量数据中快速查找某个数据\n\n在之前的学习中，我们讲解了mysql数据库索引的实现原理。mysql底层依赖的是b+树这种数据结构。那类似redis这样的key-value数据库中索引，又是怎么实现的呢？底层依赖的又是什么数据结构呢？\n\n今天，我们就来讲一下索引这种常用的技术解决思路，底层往往会依赖哪些数据结构。同时，通过索引这个应用场景，也来回顾一下，之前我们学过的几种支持动态集合的数据结构。\n\n\n# 为什么需要索引？\n\n在实际的软件开发中，业务纷繁复杂，功能千变万化，但是，万变不离其宗。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为"对数据的存储和计算"。对应到数据结构和算法中，那"存储"需要的就是数据结构，"计算"需要的就是算法。\n\n对于存储的需求，功能上无外乎增删改查。这其实并不复杂。但是，一旦存储的数据很多，那性能就成了这些系统要关注的重点，特别是在一些跟存储相关的基础系统(比如mysql数据库、分布式文件系统等)、中间件(比如消息中间件rocketmq等)中。\n\n"如何节省存储空间、如何提高数据增删改查的执行效率"，这样的问题就成了设计的重点。而这些系统的实现，都离不开一个东西，那就是索引。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。\n\n索引这个概念，非常好理解。我们可以类比书籍的目录来理解。如果没有目录，我们想要查找某个知识点的时候，就要一页一页翻。通过目录，我们就可以快速定位相关知识点的页数，查找的速度也会有质的提高。\n\n\n# 索引的需求定义\n\n接下来，我们就分析一下，在设计索引的过程中，需要考虑到的一些因素，换句话说就是，我们该如何定义清楚需求呢？\n\n对于系统设计需求，我们一般可以从功能性需求和非功能性需求两方面来分析，这个问题也不例外。\n\n\n# 功能性需求\n\n对于功能性需求需要考虑的点，我把我们大致概括成下面的这几点。\n\n数据是格式化数据还是非格式化数据？要构建索引的原始数据，类型有很多。我把它分为两类，一类是结构化数据，比如，mysql中的数据；另一类是非结构化数据，比如搜索引擎中网页。对于非结构化数据，我们一般需要做预处理，提取出查询关键词，对关键词构建索引。\n\n数据是静态数据还是动态数据？如果原始数据是一组静态数据，也就是说，不会有数据的增加、删除、更新操作，所以，我们在构建索引的时候，只需要考虑查询效率就可以了。这样，索引的构建就相对简单些。不过，不部分情况下，我们都是对动态数据构建索引，也就是说，我们不仅要考虑到索引的查询效率，在原始数据更新的同时，我们还需要动态地更新索引。支持动态数据集合的索引，设计起来相对也要更加复杂些。\n\n索引存储在内存还是硬盘？如果索引存储在内存中，那查询的速度肯定要比存储在磁盘中的高。但是，如果原始数据量很大的情况下，对应的索引可能也会很大。这个时候，因为内存有限，我们可能就不得不将索引存储在磁盘中了。实际上，还有第三种情况，那就是一部分存储在内存，一部分存储在磁盘，这样就可以兼顾内存消耗和查询效率。\n\n单值查找还是区间查找？所谓单值查找，也就是根据关键词等于某个值的数据。这种查询需求最常见。所谓区间查找，就是查找关键词处于某个区间值的所有数据。我们可以类比mysql数据库的查询需求。实际上，不同的应用场景，查询的需求会多种多样。\n\n单关键词查找还是多关键词组合查找？比如，搜索引擎中构建的索引，既要支持一个关键词的查找，比如\'数据结构\'，也要支持组合关键词查找，比如"数据结构and算法"。对于单关键词的查找，索引构建起来相对简单些。对于多关键词查询来说，要分多种情况。像mysql这种结构化数据的查询需求，我们可以实现正对多个关键词的组合，建立索引；对于像搜索引擎这样的非结构化的查询需求，我们可以针对单个关键词构建索引，然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。\n\n实际上，不同的场景，不同的原始数据，对于索引的需求也会千差万别。我们这里只是列举了一些比较有共性的需求。\n\n\n# 非功能性需求\n\n讲完功能性需求，我们再来看，索引设计的非功能性需求。\n\n不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻。毕竟内存空间非常有限，一个中间件启动后就占用几个gb的内存，开发者显然是无法接受的。如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些。但是，我们也不能掉以轻心。因为，有时候，索引对存储空间的消耗会查过原始数据。\n\n在考虑索引查询效率的同时，我们还要考虑索引的维护成本。索引的目的是提高查询效率，但是，基于动态数据集合构建的索引，我们还要考虑到，索引的维护成本。因为在原始数据动态增删改查的同时，我们也需要动态地更新索引。而索引的更新势必会影响到增删改查操作的性能。\n\n\n# 构建索引常用的数据结构有哪些？\n\n我们刚刚从宏观的角度，总结了在索引设计的过程中，需要考虑的一些共性因素。现在，我们就来看，对于不同需求的索引结构，底层一般使用哪种数据结构。\n\n实际上，常用来构建索引的数据结构，就是我们之前讲过的几种支持动态数据集合的数据结构。比如，散列表、红黑树、跳表、b+树。除此之外，位图、布隆过滤器可以作为辅助索引，有序数组可以用来对静态数据构建索引。\n\n我们知道，散列表增删改查操作的性能非常好，时间复杂度是o(1)。一些键值数据库，比如redis、memcache，就是使用散列表来构建索引的。这类索引，一般都构建在内存中。\n\n红黑树作为一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是o(logn)，也非常适用来构建内存索引。ext文件系统中，对磁盘块的索引，用的就是红黑树。\n\nb+树比起红黑树来说，更加适合构建存储在磁盘中的索引。b+树是一个多叉树，所以，对相同个数的数据构建索引，b+树的高度要低于红黑树。当借助索引查询数据的时候，读取b+树索引，需要的磁盘io次数会更少。所以，大部分关系型数据库的所以，比如mysql、oracle，都是用b+树来实现的。\n\n跳表也支持快速添加、删除、查找数据。而且，我们通过公灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率。redis的有序集合，就是用跳表来构建的。\n\n除了散列表、红黑树、b+树、跳表之外，位图和布隆过滤器这两个数据结构，也可以用索引中，辅助存储在磁盘中的索引，加速数据查找的效率。\n\n我们知道，布隆过滤器有一定的判错率。但是，我们可以规避它的短处，发挥它的长处。尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在。而且，布隆过滤器还有一个更大的特点，那就是内存占用非常少。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。\n\n实际上，有序数组也可以被作为索引。如果数据是静态的，也就是不会有插入、删除、更新操作，那我们可以把数据的关键词(查询用的)抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。\n\n\n# 总结引申\n\n今天这节算是一节总结课。我们从索引这个非常常用的技术方案，展示了散列表、红黑树、跳表、位图、布隆过滤器、有序数组这些数据结构的应用场景。从这一节内容中，我们应该可以看出，架构设计离不开数据结构和算法。要想成长为一个优秀的业务架构师、基础架构师，数据结构和算法的根基一定要打稳。因为，那些看似很惊艳的架构设计思路，实际上，都是来自最常用的数据结构和算法。\n\n\n# d69(2020/12/28) 并行算法\n\n时间复杂度是衡量算法执行效率的一种标准。但是，时间复杂度并不能跟性能划等号。在真实的软件开发中，即便在不降低时间复杂度的情况下，也可以通过一些优化手段，提升代码的执行效率。毕竟，对于实际的软件开发来说，即便是像10%、20%这样微小的性能提升，也是非常可观的。\n\n算法的目的就是为了提高代码执行的效率。当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢？我们今天就来讲解一种非常简单但又非常好用的优化方法，那就是并行计算。今天，我们就通过几个例子，来展示一下，如何借助并行计算的处理思想对算法进行改造？\n\n\n# 并行排序\n\n假设我们要给大小为8gb的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据。对于排序来说，最常用的就是时间复杂度为o(nlogn)的三种排序算法，归并排序、快速排序、堆排序。从理论上讲，这个排序问题已经很难再从算法层面优化了。而利用并行的处理思想，我们可以很轻松地将这个给8gb数据排序问题的执行效率提高很多倍。具体的实现思路有下面几种。\n\n\n# 归并排序中并行\n\n第一种是对归并排序并行化处理。\n\n我们可以将这8gb的数据划分成16个小的数据集合，每个集合包含500mb的数据。我们用16个线程，并行地对这16个500mb的数据集合进行排序。这16个小集合分别排序完成之后，我们再将这16个有序集合合并。\n\n\n# 快速排序中并行\n\n第二种是对快速排序并行化处理。\n\n我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成16个小区间。我们将8gb的数据划分到对应的区间中。针对这16个小区间的数据，我们启动了16个线程，并行地进行排序。等到16个线程都执行结束之后，得到的数据就是有序数据了。\n\n对比这两种处理思路，它们利用的都是分治的思想，对数据进行分片，然后并行处理。它们的区别在于，第一种处理思路是，先随意地对数据分片，排序之后再合并。第二种处理思路是，先对数据按照大小划分区间，然后在排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。\n\n\n# 并行查找\n\n我们知道，散列表是一种非常适合快速查找的数据结构。\n\n如果我们是给动态数据构建索引，在数据不断加入的时候，散列表的装载因子就越来越大。为了保证散列表性能不下降，我们就需要对散列表进行动态扩容。对如此大的散列表进行动态扩容，一方面比较耗时，另一方面比较消耗内存。比如，我们给一个2gb大小的散列表进行扩容，扩展到原来的1.5倍，也就是3gb大小。这个时候，实际存储在散列表中的数据只有不到2gb，所以内存的利用率只有60%，有1gb的内是空闲的。\n\n实际上，我们可以将数据随机分割成k份(比如16份)，每份中的数据只有原来的1/k，然后我们针对这k个小数据集合分别构建散列表。这样，散列表的维护成本就变低了。当某个小散列表的装载因子过大的时候，我们可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。\n\n还是刚才那个例子，假设现在有2gb的数据，我们放到16个散列表中，每个散列表中的数据大约是150mb。当某个散列表需要扩容的时候，我们只需要额外增加150*0.5=75mb的内存(假设还是扩容到原来的1.5倍)。无论从扩容的执行效率还是内存的利用率上，这种多个小散列表的处理方法，都要比大散列表高效。\n\n当我们要查找某个数据的时候，我们只需要通过16个线程，并行地在这16个散列表中查找数据。这样的查找性能，比起一个大散列表的做法，也并不会下降，反倒有可能提高。\n\n当往散列表中添加数据的时候，我们可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。\n\n\n# 并行字符串匹配\n\n我们前面学过，在文本中查找某个关键词这样一个功能，可以通过字符串匹配算法来实现。我们之前学过的字符串匹配算法有kmp、bm、rk、bf等。当在一个不是很长的文本中查找关键词的时候，这些字符串匹配算法中的任何一个，都可以表现得非常高效。但是，如果我们处理的是超级大的文本，那处理的瞬间可能就会变得很长，那有没有办法加快匹配速度呢？\n\n我们可以把大的文本，分割成k个小文本。假设k是16，我们就启动16个线程，并行地在这16个小文本中查找关键词，这样整个查找的性能就提高了16倍。16倍效率的提升，从理论的角度来说并不多。但是，对于真实的软件开发来说，这显然是一个非常可观的优化。\n\n不过，这里还有一个细节要处理，那就是原本包含在大文本中的关键词，被一分为二，分隔到两个小文本中，这就会导致尽管大文本中包含这个关键词，但在这16个小文本中查找不到它。实际上，这个问题也不难解决，我们只需要针对这种特殊情况，做一些特殊处理就可以了。\n\n我们假设关键词的长度是m。我们在每个小文本的结尾和开始各取m个字符串。前一个小文本的末尾m个字符和后一个小文本的开头m个字符，组成一个长度是2m的字符串。我们再拿关键词，在这个长度为2m的字符串中再重新查找一遍，就可以补上刚才的漏洞了。\n\n\n# 并行搜索\n\n前面我们学习过好几种搜索算法，它们分别是广度优先搜索、深度优先搜索、dijkstra最短路径算法、a*启发式搜索算。对于广度优先搜索算法，我们也可以将其改造成并行算法。\n\n广度优先搜索是一种逐层搜索的搜索策略。基于当前这一层顶点，我们可以启动多个线程，并行地搜索下一层的顶点。在代码实现方面，原来广度优先搜索的代码实现，是通过一个队列来记录已经遍历到但还没有扩展的顶点。现在，经过改造之后的并行广度优先搜索算法，我们需要利用两个队列来完成扩展顶点的工作。\n\n假设这两个队列分别是队列a和队列b。多线程并行处理队列a中的顶点，并将扩展得到的顶点存储在队列b 中。等队列a中的顶点都扩展完成之后，队列a被清空，我们再并行地扩展队列b中的顶点，并将扩展出来的顶点存储在队列a。这样两个队列循环使用，就可以实现并行广度优先搜索算法。\n\n\n# 总结引申\n\n上一节，我们通过实际软件开发中的"索引"这一技术点，回顾了之前学过的一些支持动态数据集合的数据结构。今天，我们又通过"并行算法"这个话题，回顾了之前学过的一些算法。\n\n通过一些例子，比如并行排序、查找、搜索、字符串匹配，展示了并行处理的实现思路，也就是对数据进行分片，对没有依赖关系的任务，并行地执行。\n\n并行计算是一个工程上的实现思路，尽管跟算法关系不大，但是，在实际的软件开发中，它确实可以非常巧妙地提高程序的运行效率，是一种非常好用的性能优化手段。\n\n特别是，当要处理的数据规模达到一定程度之后，我们无法通过继续优化算法，来提高执行效率的时候，我们就需要在实现的思路上做文章，利用更多的硬件资源，来加快执行的效率。所以，在很多超大规模数据处理中，并行处理的思想，应用非常广泛，比如mapreduce实际上就是一种并行计算框架。\n\n\n# d70(2020/12/29) 算法实战：redis\n\n今天我们就来学习一下，经典数据库redis中的常用数据类型，底层都是用哪种数据结构实现的？\n\n\n# redis数据库介绍\n\nredis是一种键值(key-value)数据库。相对于关系型数据库mysql，redis也被叫做非关系型数据库。\n\n像mysql这样的关系型数据库，表的结构比较复杂，会包含很多字段，可以通过sql语句，来实现非常复杂的查询需求。而redis中只包含"键"和"值"两部分，只能通过"键"来查询"值"。正是因为这样简单的存储结构，也让redis的读写效率非常高。\n\n除此之外，redis主要是作为内存数据库来使用，也就是说，数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中。\n\nredis中，键的数据类型是字符串，但是为了丰富数据存储的方式，方便开发者使用，值的数据类型有很多，常用的数据类型有这样几种，它们分别是字符串、列表、字典、集合、有序集合。\n\n"字符串string"这种数据类型非常简单，对应到数据结构里，就是字符串。\n\n下面我们重点看下，其他四种比较复杂的数据类型，看看它们底层都依赖哪些数据结构。\n\n\n# 列表(list)\n\n我们先来看看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是压缩列表ziplist，另一种是双向循环链表。\n\n当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面的两个条件：\n\n * 列表中保存的单个数据(有可能是字符串类型的)小于64字节；\n * 列表中数据少于512个。\n\n关于压缩列表，这里稍微解释一下。它并不是基础数据结构，而是redis字节设计的一种数据结构。它有点类似数组，通过一片连续的内存空间，来存储数据。不过，它跟数组不同的一点是，它允许存储的数据大小不同。具体的存储结构也非常简单，可以看下面的图示。\n\n\n\n现在，我们来看看，压缩列表中的"压缩"两个字该如何理解？\n\n听到"压缩"两个字，直观的反应就是节省内存。之所以说这种存储结构节省内存，是相较于数组的存储思路而言的。我们知道，数组要求每个元素的大小相同，如果我们要存储不同长度的字符串，那我们就需要用最大长度的字符串大小作为元素的大小(假设是20个字节)。那当我们存储小于20个字节长度的字符串的时候，便会浪费部分存储空间。\n\n\n\n压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。而且，因为数据存储在一片连续的内存空间，通过键来获取值为列表类型的数据，读取的效率也非常高。\n\n当列表中存储的数据量比较大的时候，也就是不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。\n\n在链表里，我们已经讲过双向循环链表这种数据结构了，这里我们着重看一下redis中双向链表的编码实现方式。\n\nredis的这种双向链表的实现方式，非常值得借鉴。它额外定义了一个list结构体，来组织链表的首、尾指针，还有长度等信息。\n\n// 以下是c语言代码，因为redis是用c语言实现的。\ntypedef struct listnode {\n  struct listnode *prev;\n  struct listnode *next;\n  void *value;\n} listnode;\n\n\ntypedef struct list {\n  listnode *head;\n  listnode *tail;\n  unsigned long len;\n  // ....省略其他定义\n} list;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n# 字典(hash)\n\n字典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的压缩列表，另一种是散列表。\n\n同样，只有当存储的数据量比较小的情况下，redis才使用压缩列表来实现字典类型。具体需要满足两个条件：\n\n * 字典中保存的键和值的大小都要小于64字节。\n * 字典中键值对的个数要小于512个。\n\n当不能同时满足上面两个条件的时候，redis就使用散列表来实现字典类型。redis使用murmurhash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，redis使用链表法来解决。除此之外，redis还支持散列表的动态扩容、缩容。\n\n当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于1的时候，redis会触发扩容，将散列表扩大为原来大小的2倍左右。\n\n当数据动态减少之后，为了节省内存，当装载因子小于0.1的时候，redis就会触发缩容，缩小为字典中数据个数的大约2倍大小。\n\n我们前面讲过，扩缩容要做大量的数据搬移和哈希值的重新计算，所以比较耗时。针对这个问题，redis使用我们在散列表中讲的渐进式扩容缩容策略，将数据的搬移分批进行，避免了大量数据一次性搬移导致的服务停顿。\n\n\n# 集合(set)\n\n集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。\n\n当要存储的数据，同时满足下面这样两个条件的时候，redis就采用有序数组，来实现集合这种数据类型。\n\n * 存储的数据都是整数；\n * 存储的数据元素个数不超过512个\n\n当不能同时满足这两个条件的时候，redis就使用散列表来存储集合中的数据。\n\n\n# 有序集合(sortedset)\n\n有序集合这种数据类型，我们在跳表中已经讲解过了。它用来存储一组数组，并且每个数据会附带一个得分。通过得分的大小，我们将数据组织成跳表这样的数据结构，以支持快速地按照得分值、得分区间获取数据。\n\n实际上，跟redis的其他数据类型一样，有序集合也并不仅仅只有跳表这一种实现方式。当数据量比较小的时候，redis会用压缩列表来实现有序集合。具体点说就是，使用压缩列表来实现有序集合的前提，有这样两个：\n\n * 所有数据的大小都要小于64字节。\n * 元素个数要小于128个。\n\n\n# 数据结构持久化\n\n尽管redis经常会被用作内存数据库，但是，它也支持数据落盘，也就是将内存中的数据存储到硬盘中。这样，当机器断电的时候，存储在redis中的数据也不会丢失。在机器重新启动之后，redis只需要再将存储在硬盘中的数据，重新读取到内存，就可以继续工作了。\n\n刚刚我们讲到，redis的数据格式由"键"和"值"两部分组成。而"值"又支持很多数据类型，比如字符串、列表、字典、集合、有序集合。像字典、集合等类型，底层用到了散列表，散列表中有指针的概念，而指针指向的是内存中的存储地址。那redis 是如何将这一个跟具体内存地址有关的数据结构存储到磁盘中的呢？\n\n实际上，redis遇到的这个问题并不特殊，很多场景都会遇到。我们把它叫做数据结构的持久化问题，或者对象的持久化问题。这里的"持久化"，我们可以笼统地理解为"存储到磁盘"。\n\n如何将数据结构持久化到磁盘？我们主要有两种解决思路。\n\n第一种是清除原有的存储结构，只将数据存储到磁盘中。当我们需要从磁盘还原数据到内存的时候，再重新将数据组织成原来的数据结构。实际上，redis采用的就是这种持久化思路。\n\n不过，这种方式也有一定的弊端。那就是数据从磁盘还原到内存的过程，会耗用比较多的时间。比如，我们现在要将散列表中的数据存储到磁盘。当我们从磁盘中，取出数据重新构建散列表的时候，需要重新计算每个数据的哈希值。如果磁盘中存储的是几gb的数据，那重构数据结构的耗时就不可忽视了。\n\n第二种是保留原来的存储格式，将数据按照原有的格式存储在磁盘中。我们拿散列表这样的数据结构来举例。我们可以将散列表的大小、每个数据被散列到的槽的编号的信息，都保存在磁盘中。有了这些信息，我们从磁盘中将数据还原到内存中的时候，就可以避免重新计算哈希值。\n\n\n# 总结引申\n\n今天，我们学习了redis中常用数据类型底层依赖的数据结构，总结婴喜爱大概有这五种：压缩列表(可以看作一种特殊的数组)、有序数组、链表、散列表、跳表。实际上，redis就是这些常用数据结构的封装。\n\n\n# d71(2020/12/31) 高性能队列disruptor\n\ndisruptor是一种内存消息队列。从功能上讲，它其实有点类似kafka。不过，和kafka不同的是，disruptor是线程之间用于消息传递的队列。它在apache storm、camel、log4j2等很多知名项目中都有广泛应用。\n\n之所以如此受青睐，主要还是因为它的性能表现非常优秀。它比java中另外一个非常常用的内存消息队列arrayblockingqueue(abs)的性能，要高一个数量级，可以算得上是最快的内存消息队列了。\n\n如此高性能的内存消息队列，在设计和实现上，必然后它独到的地方。\n\n\n# 基于循环队列的"生产者——消费者模型"\n\n内存消息队列，来源于"生产者——消费者模型"。在这个模型中，"生产者"生产数据，并且将数据放到一个中心存储容器中。之后，"消费者"从中心存储容器中，取出数据消费。\n\n这个模型还是非常好理解的，这里面存储数据的中心存储容器，是用什么样的数据结构来实现的呢？\n\n实际上，实现中心存储容器最常用的一种数据结构，就是我们所讲的队列。队列支持数据的先进先出。正是这个特性，使得数据被霞飞的顺序性可以得到保证，也就是说，早被生产的数据就会早被消费。\n\n我们在第9节中讲过，队列有两种实现思路。一种是基于链表实现的链式队列，另一种是基于数组实现的顺序队列。不同的需求背景下，我们会选择不同的实现方式。\n\n如果我们要实现一个无界队列，也就是说，队列的大小事先不确定，理论上可以支持无限大。这种情况下，我们适合选用链表来实现队列。因为链表支持快速地动态扩容。如果我们要实现一个有界队列，也就是说，队列的大小实现确定，当队列中数据满了之后，生产者就需要等待。直到消费者消费了数据，队列有空闲位置的时候，生产者才能将数据放入。\n\n实际上，相较于无界队列，有界队列的应用场景更加广泛。毕竟，我们的机器内存是有限的。而无界队列占用的内存数量是不可控的。对于实际的软件开发来说，这种不可控的因素，就会有潜在的风险。在某些极端情况下，无界队列就有可能因为内存持续增长，而导致oom错误。\n\n在第9节中，我们还讲过了一种特殊的顺序队列，循环队列。我们讲过，非循环的顺序队列在添加、删除数据的工程中，会涉及数据的搬移操作，导致性能变差。而循环队列正好可以解决这个数据搬移的问题。所以，性能更加好。所以，大部分用到顺序队列的场景中，我们都选择用顺序队列中的循环队列。\n\n实际上，循环队列这种数据结构，就是我们今天要讲的内存消息队列的雏形。我们借助循环队列，实现了一个最简单的"生产者——消费者模型"。\n\n为了方便理解，对于生产者和消费者之间操作的同步，我并没有用到线程相关的操作。而是采用了"当队列满了之后，生产者就轮询等待；当队列空了之后，消费者就轮训等待"这样的措施。\n\npublic class queue {\n  private long[] data;\n  private int size = 0, head = 0, tail = 0;\n  public queue(int size) {\n    this.data = new long[size];\n    this.size = size;\n  }\n\n  public boolean add(long element) {\n    if ((tail + 1) % size == head) return false;\n    data[tail] = element;\n    tail = (tail + 1) % size;\n    return true;\n  }\n\n  public long poll() {\n    if (head == tail) return null;\n    long ret = data[head];\n    head = (head + 1) % size;\n    return ret;\n  }\n}\n\npublic class producer {\n  private queue queue;\n  public producer(queue queue) {\n    this.queue = queue;\n  }\n\n  public void produce(long data) throws interruptedexception {\n    while (!queue.add(data)) {\n      thread.sleep(100);\n    }\n  }\n}\n\npublic class consumer {\n  private queue queue;\n  public consumer(queue queue) {\n    this.queue = queue;\n  }\n\n  public void comsume() throws interruptedexception {\n    while (true) {\n      long data = queue.poll();\n      if (data == null) {\n        thread.sleep(100);\n      } else {\n        // todo:...消费数据的业务逻辑...\n      }\n    }\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n\n\n\n# 基于加锁的并发"生产者-消费者模型"\n\n实际上，刚刚的"生产者——消费者模型"实现代码，是不完善的。\n\n如果我们只有一个生产者往队列中写数据，一个消费者从队列中读取数据，那上面的代码是没有问题的。但是，如果有多个生产者在并发地往队列中写入数据，或者多个消费者并发地从队列中消费数据，那上面的代码就不能正确工作了。\n\n在多个生产者或多个消费者并发操作队列的情况下，刚刚的代码主要会有下面的两个问题：\n\n * 多个生产者写入的数据可能会互相覆盖；\n * 多个消费者可能会读取重复的数据。\n\n因为第一个问题和第二个问题产生的原理是类似的。所以，我着重讲解第一个问题是如何产生的以及该如何解决。对于第二个问题，也可以类比对第一个问题的解决思路来想一想。\n\n两个线程同时往队列中添加数据，也就相当于两个线程同时执行类queue中的add()函数。我们假设队列的大小size是10，当前的tail指向下标7，head指向下标3，也就是说，队列中还有空闲空间。这个时候，线程1调用add()函数，往队列中添加一个值为12的数据；线程2调用add()函数，往队列中添加一个值为15的数据。在极端的情况下，本来是往队列中添加了两个数据(12和15)，最终可能只有一个数据添加成功，另一个数据会被覆盖。这是为什么呢？\n\n\n\n为了方便查看队列queue中的add()函数，我们把它从上面的代码中摘录了出来。\n\npublic boolean add(long element) {\n  if ((tail + 1) % size == head) return false;\n  data[tail] = element;\n  tail = (tail + 1) % size;\n  return true;\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n从这段代码中，我们可以看出，第3行给data[tail]赋值，然后第4行才给tail的值加一。赋值和tail加一两个操作，并非原子操作。这就会导致这样的情况发生：当线程1和线程2同时执行add()函数的时候，线程1先执行完了 3行语句，将data[7] (tail等于7)的值设置为12。在线程1还未执行到第行语句之前，也就是还未将tail 加一之前，线程2执行了第3行语句，又将data[7]的值设置为15，也就是说，那线程2插入的数据覆盖了线程1插入的数据。原本应该插入两个数据(12和15)的，现在只插入了一个数据(15)。\n\n\n\n那如何解决这种线程并发往队列中添加数据时，导致的数据覆盖、运行不正确问题呢？\n\n最简单的处理方法就是给这段代码加锁，同一时间只允许一个线程执行add()函数。这就相当于将这段代码的执行，由并行改成了串行，也就不存在我们刚刚说的问题了。\n\n不过，加锁将并行改为串行，必然导致多个生产者同时生成数据的时候，执行效率的下降。当然，我们可以继续优化代码，用cas操作等减少加锁的粒度。我这里直接看下disruptor的处理方法。\n\n\n# 基于无锁的并发"生产者 —— 消费者模型"\n\n尽管disruptor的源码读起来很复杂，但是基本其实非常简单。实际上，它是换了一种队列和"生产者- 消费者模型"的实现思路。\n\n之前的实现思路中，队列只支持两个操作，添加数据和读取并移除数据，分别对应代码中的add()函数和poll()函数，而disruptor采用了另一种实现思路。\n\n对于生产者来说，它往队列中添加数据之前，先申请可用空闲存储单元，并且是批量地申请连续的n个(n>=1)存储单元。当申请到这组连续的存储单元之后，后续往队列中添加元素，就可以不用加锁了，因为这组存储单元是这个线程独享的。不过，从刚刚的描述中，我们可以看出，申请存储单元的过程是需要加锁的。\n\n对于消费者来说，处理的过程跟生产者是类似的。它先去申请一批连续可读的存储单元(这个申请的过程也是需要加锁的)，当申请到这批存储单元之后，后续的读取操作就可以不用加锁了。\n\n不过，还有一个需要特别注意的地方，那就是，如果生产者a申请到了一组连续的存储单元，假设是下标为3到6的存储单元，生产者b紧跟着申请到了下标是7到9的存储单元，那在3到6没有完全写入数据之前，7到9的数据是无法读取的。这个也是disruptor实现思路的一个弊端。\n\n如下图所示：\n\n\n\n实际上，disruptor采用的是ringbuffer和availablebuffer这两个结构，来实现上面的功能。不过，因为我们主要聚焦在数据结构和算法上，所以对这两种结构做了简化，但是基本思想是一致的。\n\n\n# 总结引申\n\n今天，我们讲解了如何实现一个高性能的并发队列。这里的"并发"两个字，实际上就是多线程安全的意思。\n\n常见的内存队列往往采用循环队列来实现。这种实现方法，对于只有一个生产者和一个消费者的场景，已经足够了。但是，当存在多个生产者或多个消费者的时候，单纯的循环队列的实现方式，就无法正确工作了。\n\n这主要是因为，多个生产者在同时往队列中写入数据的时候，在某些情况下，会存在数据覆盖的问题。而多个消费者同时消费数据，在某些情况下，会存在消费重复数据的问题。\n\n针对这个问题，最简单、暴力的解决方法就是，对写入和读取过程加锁。这种处理方法，相当于将原来可以并行执行的操作，强制串行执行，相应地就会导致操作性能的下降。\n\n为了在保证逻辑正确的前提下，尽可能地提高队列在并发情况下的性能，disruptor采用了"两阶段写入"的方法。在写入数据之前，先加锁申请批量的空闲存储单元，之后往队列中写入数据的操作就不需要加锁了，写入的性能因此就提高了。disruptor对消费过程的改造，跟对生产过程的改造是类似的。它先加锁申请批量的可读取的存储单元，之后从队列中读取数据的操作也就不需要加锁了，读取的性能因此也就提高了。\n\n这个优化思路非常简单。实际上，不管架构设计还是产品设计，往往越简单的设计思路，越能更好地解决问题。',charsets:{cjk:!0},lastUpdated:"2022/01/21, 21:25:26",lastUpdatedTimestamp:1642771526e3}],themeConfig:{nav:[{text:"首页",link:"/"},{text:"数据库",link:"/db/",items:[{text:"ELK相关文档",items:[{text:"《ES相关知识点整理》",link:"/pages/843f56/"},{text:"《ES架构规划》",link:"/pages/127f0c/"},{text:"《ES_API集合》",link:"/pages/8d99ee/"},{text:"《ES_常见问题列表》",link:"/pages/2f7a8b/"},{text:"《ECE知识点总结》",link:"/pages/b3f7e8/"},{text:"《ECE考试总结》",link:"/pages/42d30d/"}]}]},{text:"中间件",link:"/midware/",items:[{text:"K8S基础知识点整理",link:"/pages/cea09f/"},{text:"Harbor镜像管理",link:"/pages/0765a4/"},{text:"华为CKA认证",link:"/pages/5cdc7e/"}]},{text:"编程",link:"/code/",items:[{text:"Python基础知识点",link:"/pages/107c9b/"},{text:"《数据结构与算法之美》读书笔记",link:"/pages/98f6c7/"}]},{text:"生活",link:"/life/",items:[{text:"《原子习惯》读书笔记",link:"/pages/8a0e8c/"},{text:"《暗时间》读书笔记",link:"/pages/e5d8ed/"},{text:"《大脑强人》读书笔记",link:"/pages/3a1429/"},{text:"《直线学习法》读书笔记",link:"/pages/485aeb/"},{text:"友情链接",link:"/friends/"}]},{text:"关于",link:"/about/"},{text:"收藏",link:"/pages/beb6c0bd8a66cea6/"},{text:"索引",link:"/archives/",items:[{text:"分类",link:"/categories/"},{text:"标签",link:"/tags/"},{text:"归档",link:"/archives/"}]}],sidebarDepth:2,logo:"/img/logo.png",repo:"xugaoyi/vuepress-theme-vdoing",searchMaxSuggestions:10,lastUpdated:"上次更新",docsDir:"docs",editLinks:!0,editLinkText:"编辑",sidebar:{"/00.目录页/":[["01.数据库.md","数据库","/db"],["02.中间件.md","页面","/midware"],["03.编程.md","技术","/code"],["04.生活.md","更多","/life"]],catalogue:{"数据库":"/db","中间件":"/midware","编程":"/code","生活":"/life"},"/01.数据库/":[{title:"ELK",collapsable:!0,children:[["05.ELK/01.ES相关知识点整理.md","ES相关知识点整理","/pages/843f56/"],["05.ELK/02.ES架构规划.md","ES架构规划","/pages/127f0c/"],["05.ELK/03.ES_API集合.md","ES_API集合","/pages/8d99ee/"],["05.ELK/04.ES_常见问题列表.md","ES_常见问题列表","/pages/2f7a8b/"],["05.ELK/85.ECE考试总结.md","ECE认证考试经历总结","/pages/42d30d/"]]}],"/02.中间件/":[{title:"K8S",collapsable:!0,children:[["01.K8S/01.K8S基础知识点整理.md","K8S基础知识点整理","/pages/cea09f/"],["01.K8S/02.Registry镜像管理.md","Registry","/pages/97264c/"],["01.K8S/03.Harbor镜像管理.md","Harbor","/pages/0765a4/"],["01.K8S/80.华为CKA认证.md","华为CKA认证","/pages/5cdc7e/"]]}],"/03.编程/":[{title:"Python",collapsable:!0,children:[["10.Python/01.Python基础知识点.md","Python基础知识点","/pages/107c9b/"],["10.Python/03.《从Python开始学编程》读书笔记.md","《从Python开始学编程》读书笔记","/pages/63a425/"]]},{title:"数据结构和算法",collapsable:!0,children:[["80.数据结构和算法/01.《数据结构与算法之美》读书笔记.md","《数据结构与算法之美》读书笔记","/pages/98f6c7/"],["80.数据结构和算法/70.883数据结构和算法.md","883数据结构和算法","/pages/349c56/"]]}],"/04.生活/":[{title:"学习方法",collapsable:!0,children:[["01.学习方法/01.《原子习惯》读书笔记.md","《原子习惯》读书笔记","/pages/8a0e8c/"],["01.学习方法/02.《暗时间》读书笔记.md","《暗时间》读书笔记","/pages/e5d8ed/"],["01.学习方法/03.《大脑强人》读书笔记.md","《大脑强人》读书笔记","/pages/3a1429/"],["01.学习方法/04.《直线学习法》读书笔记.md","《直线学习法》读书笔记","/pages/485aeb/"]]},{title:"杂记",collapsable:!0,children:[["80.杂记/01.学习杂记.md","学习杂记","/pages/e80362/"]]},["99.友情链接.md","友情链接","/friends"]],"/05.关于/":[["01.关于.md","关于","/about"]],"/06.收藏夹/":[["01.网站.md","网站","/pages/beb6c0bd8a66cea6"],["02.常用的前端轮子.md","常用的前端轮子","/pages/47cf96/"]]},author:{name:"chuck",link:"https://github.com/chuck6"},blogger:{avatar:"/img/paizhao.jpg",name:"梵一的世界",slogan:"等风来"},social:{icons:[{iconClass:"icon-youjian",title:"发邮件",link:"mailto:492018329@qq.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/chuck6"},{iconClass:"icon-erji",title:"听音乐",link:"https://music.163.com/#/playlist?id=755597173"}]},footer:{createYear:2020,copyrightInfo:'Evan Xu | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a>'},htmlModules:{homeSidebarB:'<div style="padding: 0.95rem">\n    <p style="\n      color: var(--textColor);\n      opacity: 0.9;\n      font-size: 20px;\n      font-weight: bold;\n      margin: 0 0 8px 0;\n    ">公众号</p>\n    <img src="https://cdn.jsdelivr.net/gh/chuck6/image@master/img1/扫码_搜索联合传播样式-标准色版.png"  style="width:100%;" />\n    <p>\n    持续读书，学习，分享！让自己的历史不断向前！\n    </p>\n    </div>'}},locales:{"/":{lang:"zh-CN",title:"梵一的博客",description:"人生没有白走的路，每一步都算数！",path:"/"}}},ss=(t(154),t(211),t(143),t(221)),cs=t(222),ds=(t(373),t(235),t(43));var us={computed:{$filterPosts:function(){return this.$site.pages.filter((function(n){var e=n.frontmatter,t=e.pageComponent,i=e.article,r=e.home;return!(t||!1===i||!0===r)}))},$sortPosts:function(){return(n=this.$filterPosts).sort((function(n,e){var t=n.frontmatter.sticky,i=e.frontmatter.sticky;return t&&i?t==i?Object(ds.a)(n,e):t-i:t&&!i?-1:!t&&i?1:Object(ds.a)(n,e)})),n;var n},$sortPostsByDate:function(){return(n=this.$filterPosts).sort((function(n,e){return Object(ds.a)(n,e)})),n;var n},$groupPosts:function(){return function(n){for(var e={},t={},i=function(i,r){var a=n[i].frontmatter,o=a.categories,l=a.tags;"array"===Object(ds.n)(o)&&o.forEach((function(t){t&&(e[t]||(e[t]=[]),e[t].push(n[i]))})),"array"===Object(ds.n)(l)&&l.forEach((function(e){e&&(t[e]||(t[e]=[]),t[e].push(n[i]))}))},r=0,a=n.length;r<a;r++)i(r);return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags:function(){return function(n){var e=[],t=[];for(var i in n.categories)e.push({key:i,length:n.categories[i].length});for(var r in n.tags)t.push({key:r,length:n.tags[r].length});return{categories:e,tags:t}}(this.$groupPosts)}}};Ar.component(ss.default),Ar.component(cs.default);function ps(n){return n.toString().padStart(2,"0")}t(377);Ar.component("CodeBlock",(function(){return Promise.resolve().then(t.bind(null,221))})),Ar.component("CodeGroup",(function(){return Promise.resolve().then(t.bind(null,222))})),Ar.component("Badge",(function(){return Promise.all([t.e(0),t.e(8)]).then(t.bind(null,794))}));t(378);var ms,hs,fs=t(50),gs=(t(381),t(137),t(220)),vs=t.n(gs),ys=t(102);"valine"===(hs="gitalk")?t.e(39).then(t.t.bind(null,757,7)).then((function(n){return n.default})):"gitalk"===hs&&Promise.all([t.e(0),t.e(38)]).then(t.t.bind(null,758,7)).then((function(){return t.e(37).then(t.t.bind(null,759,7))})).then((function(n){return ms=n.default}));function bs(n,e){var t={};return Reflect.ownKeys(n).forEach((function(i){if("string"==typeof n[i])try{t[i]=vs.a.render(n[i],e)}catch(e){console.warn('Comment config option error at key named "'.concat(i,'"')),console.warn("More info: ".concat(e.message)),t[i]=n[i]}else t[i]=n[i]})),t}console.log('How to use "'.concat("gitalk",'" in ').concat(ys.name,"@v").concat(ys.version,":"),ys.homepage);var xs={render:function(n,e){var t=document.createElement("div");t.id=e,document.querySelector("main.page").appendChild(t),new ms(bs({clientID:"a6e1355287947096b88b",clientSecret:"f0e77d070fabfcd5af95bebb82b2d574d7248d71",repo:"blog-gitalk-comment",owner:"chuck",admin:["chuck"],pagerDirection:"last",id:"<%- (frontmatter.permalink || frontmatter.to.path).slice(-16) %>",title:"「评论」<%- frontmatter.title %>",labels:["Gitalk","Comment"],body:"页面：<%- window.location.origin + (frontmatter.to.path || window.location.pathname) %>"},{frontmatter:n})).render(e)},clear:function(n){var e=document.querySelector("#".concat(n));return e&&e.remove(),!0}},_s=null;function ks(n){return xs.clear("vuepress-plugin-comment")}function ws(n){return!1!==n.comment&&!1!==n.comments}function Ts(n){if(clearTimeout(_s),document.querySelector("main.page"))return xs.render(n,"vuepress-plugin-comment");_s=setTimeout((function(){return Ts(n)}),200)}var Is={mounted:function(){var n=this;_s=setTimeout((function(){var e=Object(fs.a)({to:{},from:{}},n.$frontmatter);ks()&&ws(e)&&Ts(e)}),1e3),this.$router.afterEach((function(e,t){if(!e||!t||e.path!==t.path){var i=Object(fs.a)({to:e,from:t},n.$frontmatter);ks()&&ws(i)&&Ts(i)}}))}},zs=Object(rs.a)(Is,(function(){var n=this.$createElement;return(this._self._c||n)("div")}),[],!1,null,null,null).exports,Ss=[function(n){n.Vue,n.options,n.router,n.siteData},function(n){var e=n.Vue,t=(n.options,n.router,n.siteData);t.pages.map((function(n){var e=n.frontmatter,i=e.date,r=e.author;"string"==typeof i&&"Z"===i.charAt(i.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return"".concat(n.getUTCFullYear(),"-").concat(ps(n.getUTCMonth()+1),"-").concat(ps(n.getUTCDate())," ").concat(ps(n.getUTCHours()),":").concat(ps(n.getUTCMinutes()),":").concat(ps(n.getUTCSeconds()))}(i)),r?n.author=r:t.themeConfig.author&&(n.author=t.themeConfig.author)})),e.mixin(us)},{},function(n){n.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},{},function(n){n.router;"undefined"!=typeof window&&function(){var n=document.createElement("script"),e=window.location.protocol.split(":")[0];n.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(n,t)}()},function(n){var e=n.router;"undefined"!=typeof window&&(window._hmt=window._hmt||[],function(){var n=document.createElement("script");n.src="https://hm.baidu.com/hm.js?503f098e7e5b3a5b5d8c5fc2938af002";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(n,e)}(),e.afterEach((function(n){_hmt.push(["_trackPageview",n.fullPath])})))},function(n){n.Vue.component("Comment",zs)}],js=["Comment"];t(214);function Ps(n,e){return(Ps=Object.setPrototypeOf||function(n,e){return n.__proto__=e,n})(n,e)}t(215);function Es(n){return(Es=Object.setPrototypeOf?Object.getPrototypeOf:function(n){return n.__proto__||Object.getPrototypeOf(n)})(n)}function qs(n,e){if(e&&("object"===zo(e)||"function"==typeof e))return e;if(void 0!==e)throw new TypeError("Derived constructors may only return object or undefined");return function(n){if(void 0===n)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return n}(n)}function Os(n){var e=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(n){return!1}}();return function(){var t,i=Es(n);if(e){var r=Es(this).constructor;t=Reflect.construct(i,arguments,r)}else t=i.apply(this,arguments);return qs(this,t)}}var As=function(n){!function(n,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");n.prototype=Object.create(e&&e.prototype,{constructor:{value:n,writable:!0,configurable:!0}}),Object.defineProperty(n,"prototype",{writable:!1}),e&&Ps(n,e)}(t,n);var e=Os(t);function t(){return cl(this,t),e.apply(this,arguments)}return ul(t)}(function(){function n(){cl(this,n),this.store=new Ar({data:{state:{}}})}return ul(n,[{key:"$get",value:function(n){return this.store.state[n]}},{key:"$set",value:function(n,e){Ar.set(this.store.state,n,e)}},{key:"$emit",value:function(){var n;(n=this.store).$emit.apply(n,arguments)}},{key:"$on",value:function(){var n;(n=this.store).$on.apply(n,arguments)}}]),n}());Object.assign(As.prototype,{getPageAsyncComponent:Uo,getLayoutAsyncComponent:Fo,getAsyncComponent:Ho,getVueComponent:Go});var $s={install:function(n){var e=new As;n.$vuepress=e,n.prototype.$vuepress=e}};function Cs(n){n.beforeEach((function(e,t,i){if(Ls(n,e.path))i();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){var r=e.path.replace(/\/$/,"")+".html";Ls(n,r)?i(r):i()}else i();else{var a=e.path+"/",o=e.path+".html";Ls(n,o)?i(o):Ls(n,a)?i(a):i()}}))}function Ls(n,e){var t=e.toLowerCase();return n.options.routes.some((function(n){return n.path.toLowerCase()===t}))}var Ms={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(n){var e=this.pageKey||this.$parent.$page.key;return Wo("pageKey",e),Ar.component(e)||Ar.component(e,Uo(e)),Ar.component(e)?n(e):n("")}},Ds={functional:!0,props:{slotKey:String,required:!0},render:function(n,e){var t=e.props,i=e.slots;return n("div",{class:["content__".concat(t.slotKey)]},i()[t.slotKey])}},Rs={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Bs=(t(386),t(387),Object(rs.a)(Rs,(function(){var n=this.$createElement,e=this._self._c||n;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function Ns(){return(Ns=Object(i.a)(regeneratorRuntime.mark((function n(e){var t,i,r,a;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:ls.routerBase||ls.base,Cs(i=new To({base:t,mode:"history",fallback:!1,routes:os,scrollBehavior:function(n,e,t){return t||(n.hash?!Ar.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})}})),r={},n.prev=4,n.next=7,Promise.all(Ss.filter((function(n){return"function"==typeof n})).map((function(n){return n({Vue:Ar,options:r,router:i,siteData:ls,isServer:e})})));case 7:n.next=12;break;case 9:n.prev=9,n.t0=n.catch(4),console.error(n.t0);case 12:return a=new Ar(Object.assign(r,{router:i,render:function(n){return n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},js.map((function(e){return n(e)})))])}})),n.abrupt("return",{app:a,router:i});case 14:case"end":return n.stop()}}),n,null,[[4,9]])})))).apply(this,arguments)}Ar.config.productionTip=!1,Ar.use(To),Ar.use($s),Ar.mixin(function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:Ar;Io(e),t.$vuepress.$set("siteData",e);var i=n(t.$vuepress.$get("siteData")),r=new i,a=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(r)),o={};return Object.keys(a).reduce((function(n,e){return e.startsWith("$")&&(n[e]=a[e].get),n}),o),{computed:o}}((function(n){return function(){function e(){cl(this,e)}return ul(e,[{key:"setPage",value:function(n){this.__page=n}},{key:"$site",get:function(){return n}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var n,e,t=this.$site.locales,i=void 0===t?{}:t;for(var r in i)"/"===r?e=i[r]:0===this.$page.path.indexOf(r)&&(n=i[r]);return n||e||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var n=this.$page.frontmatter.canonicalUrl;return"string"==typeof n&&n}},{key:"$title",get:function(){var n=this.$page,e=this.$page.frontmatter.metaTitle;if("string"==typeof e)return e;var t=this.$siteTitle,i=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?i?i+" | "+t:t:i||"VuePress"}},{key:"$description",get:function(){var n=function(n){if(n){var e=n.filter((function(n){return"description"===n.name}))[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(n,e){for(var t=0;t<n.length;t++){var i=n[t];if(i.path.toLowerCase()===e.toLowerCase())return i}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),e}()}),ls)),Ar.component("Content",Ms),Ar.component("ContentSlotsDistributor",Ds),Ar.component("OutboundLink",Bs),Ar.component("ClientOnly",{functional:!0,render:function(n,e){var t=e.parent,i=e.children;if(t._isMounted)return i;t.$once("hook:mounted",(function(){t.$forceUpdate()}))}}),Ar.component("Layout",Fo("Layout")),Ar.component("NotFound",Fo("NotFound")),Ar.prototype.$withBase=function(n){var e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.5",hash:"32f91fa"},function(n){return Ns.apply(this,arguments)}(!1).then((function(n){var e=n.app;n.router.onReady((function(){e.$mount("#app")}))}))}]);